[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2502.13145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13145v1",
                "updated": "2025-02-18T18:59:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    59,
                    57,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T18:59:57Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    59,
                    57,
                    1,
                    49,
                    0
                ],
                "title": "Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation"
                },
                "summary": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba"
                },
                "authors": [
                    {
                        "name": "Bencheng Liao"
                    },
                    {
                        "name": "Hongyuan Tao"
                    },
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Tianheng Cheng"
                    },
                    {
                        "name": "Yingyue Li"
                    },
                    {
                        "name": "Haoran Yin"
                    },
                    {
                        "name": "Wenyu Liu"
                    },
                    {
                        "name": "Xinggang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinggang Wang"
                },
                "author": "Xinggang Wang",
                "arxiv_comment": "Code and model are available at https://github.com/hustvl/mmMamba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13063v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13063v1",
                "updated": "2025-02-18T17:08:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    8,
                    45,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T17:08:45Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    8,
                    45,
                    1,
                    49,
                    0
                ],
                "title": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity"
                },
                "summary": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches allow to\nreduce the amount of compute in existing language models. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches allow to\nreduce the amount of compute in existing language models. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign."
                },
                "authors": [
                    {
                        "name": "Yuri Kuratov"
                    },
                    {
                        "name": "Mikhail Arkhipov"
                    },
                    {
                        "name": "Aydar Bulatov"
                    },
                    {
                        "name": "Mikhail Burtsev"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Burtsev"
                },
                "author": "Mikhail Burtsev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13063v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12875v1",
                "updated": "2025-02-18T14:05:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    5,
                    12,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T14:05:12Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    5,
                    12,
                    1,
                    49,
                    0
                ],
                "title": "A Survey on DRL based UAV Communications and Networking: DRL\n  Fundamentals, Applications and Implementations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on DRL based UAV Communications and Networking: DRL\n  Fundamentals, Applications and Implementations"
                },
                "summary": "Unmanned aerial vehicles (UAVs) are playing an increasingly pivotal role in\nmodern communication networks,offering flexibility and enhanced coverage for a\nvariety of applica-tions. However, UAV networks pose significant challenges due\nto their dynamic and distributed nature, particularly when dealing with tasks\nsuch as power allocation, channel assignment, caching,and task offloading.\nTraditional optimization techniques often struggle to handle the complexity and\nunpredictability of these environments, leading to suboptimal performance. This\nsurvey provides a comprehensive examination of how deep reinforcement learning\n(DRL) can be applied to solve these mathematical optimization problems in UAV\ncommunications and networking.Rather than simply introducing DRL methods, the\nfocus is on demonstrating how these methods can be utilized to solve complex\nmathematical models of the underlying problems. We begin by reviewing the\nfundamental concepts of DRL, including value-based, policy-based, and\nactor-critic approaches. Then,we illustrate how DRL algorithms are applied to\nspecific UAV network tasks by discussing from problem formulations to DRL\nimplementation. By framing UAV communication challenges as optimization\nproblems, this survey emphasizes the practical value of DRL in dynamic and\nuncertain environments. We also explore the strengths of DRL in handling\nlarge-scale network scenarios and the ability to continuously adapt to changes\nin the environment. In addition, future research directions are outlined,\nhighlighting the potential for DRL to further enhance UAV communications and\nexpand its applicability to more complex,multi-agent settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmanned aerial vehicles (UAVs) are playing an increasingly pivotal role in\nmodern communication networks,offering flexibility and enhanced coverage for a\nvariety of applica-tions. However, UAV networks pose significant challenges due\nto their dynamic and distributed nature, particularly when dealing with tasks\nsuch as power allocation, channel assignment, caching,and task offloading.\nTraditional optimization techniques often struggle to handle the complexity and\nunpredictability of these environments, leading to suboptimal performance. This\nsurvey provides a comprehensive examination of how deep reinforcement learning\n(DRL) can be applied to solve these mathematical optimization problems in UAV\ncommunications and networking.Rather than simply introducing DRL methods, the\nfocus is on demonstrating how these methods can be utilized to solve complex\nmathematical models of the underlying problems. We begin by reviewing the\nfundamental concepts of DRL, including value-based, policy-based, and\nactor-critic approaches. Then,we illustrate how DRL algorithms are applied to\nspecific UAV network tasks by discussing from problem formulations to DRL\nimplementation. By framing UAV communication challenges as optimization\nproblems, this survey emphasizes the practical value of DRL in dynamic and\nuncertain environments. We also explore the strengths of DRL in handling\nlarge-scale network scenarios and the ability to continuously adapt to changes\nin the environment. In addition, future research directions are outlined,\nhighlighting the potential for DRL to further enhance UAV communications and\nexpand its applicability to more complex,multi-agent settings."
                },
                "authors": [
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Shaoxin Cui"
                    },
                    {
                        "name": "Wen Qiu"
                    },
                    {
                        "name": "Zhiqiang He"
                    },
                    {
                        "name": "Zhi Liu"
                    },
                    {
                        "name": "Xiao Zheng"
                    },
                    {
                        "name": "Bomin Mao"
                    },
                    {
                        "name": "Nei Kato"
                    }
                ],
                "author_detail": {
                    "name": "Nei Kato"
                },
                "author": "Nei Kato",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12665v1",
                "updated": "2025-02-18T09:11:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    9,
                    11,
                    51,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T09:11:51Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    9,
                    11,
                    51,
                    1,
                    49,
                    0
                ],
                "title": "A$^2$ATS: Retrieval-Based KV Cache Reduction via Windowed Rotary\n  Position Embedding and Query-Aware Vector Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A$^2$ATS: Retrieval-Based KV Cache Reduction via Windowed Rotary\n  Position Embedding and Query-Aware Vector Quantization"
                },
                "summary": "Long context large language models (LLMs) pose significant challenges for\nefficient serving due to the large memory footprint and high access overhead of\nKV cache. Retrieval-based KV cache reduction methods can mitigate these\nchallenges, typically by offloading the complete KV cache to CPU and retrieving\nnecessary tokens on demand during inference. However, these methods still\nsuffer from unsatisfactory accuracy degradation and extra retrieval overhead.\nTo address these limitations, this paper proposes A$^2$ATS, a novel\nretrieval-based KV cache reduction method. A$^2$ATS aims to obtain an accurate\napproximation of attention scores by applying the vector quantization technique\nto key states, thereby enabling efficient and precise retrieval of the top-K\ntokens. First, we propose Windowed Rotary Position Embedding, which decouples\nthe positional dependency from query and key states after position embedding.\nThen, we propose query-aware vector quantization that optimizes the objective\nof attention score approximation directly. Finally, we design the heterogeneous\ninference architecture for KV cache offloading, enabling long context serving\nwith larger batch sizes. Experimental results demonstrate that A$^2$ATS can\nachieve a lower performance degradation with similar or lower overhead compared\nto existing methods, thereby increasing long context serving throughput by up\nto $2.7 \\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context large language models (LLMs) pose significant challenges for\nefficient serving due to the large memory footprint and high access overhead of\nKV cache. Retrieval-based KV cache reduction methods can mitigate these\nchallenges, typically by offloading the complete KV cache to CPU and retrieving\nnecessary tokens on demand during inference. However, these methods still\nsuffer from unsatisfactory accuracy degradation and extra retrieval overhead.\nTo address these limitations, this paper proposes A$^2$ATS, a novel\nretrieval-based KV cache reduction method. A$^2$ATS aims to obtain an accurate\napproximation of attention scores by applying the vector quantization technique\nto key states, thereby enabling efficient and precise retrieval of the top-K\ntokens. First, we propose Windowed Rotary Position Embedding, which decouples\nthe positional dependency from query and key states after position embedding.\nThen, we propose query-aware vector quantization that optimizes the objective\nof attention score approximation directly. Finally, we design the heterogeneous\ninference architecture for KV cache offloading, enabling long context serving\nwith larger batch sizes. Experimental results demonstrate that A$^2$ATS can\nachieve a lower performance degradation with similar or lower overhead compared\nto existing methods, thereby increasing long context serving throughput by up\nto $2.7 \\times$."
                },
                "authors": [
                    {
                        "name": "Junhui He"
                    },
                    {
                        "name": "Junna Xing"
                    },
                    {
                        "name": "Nan Wang"
                    },
                    {
                        "name": "Rui Xu"
                    },
                    {
                        "name": "Shangyu Wu"
                    },
                    {
                        "name": "Peng Zhou"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Chun Jason Xue"
                    },
                    {
                        "name": "Qingan Li"
                    }
                ],
                "author_detail": {
                    "name": "Qingan Li"
                },
                "author": "Qingan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05996v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05996v2",
                "updated": "2025-02-18T07:58:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    7,
                    58,
                    29,
                    1,
                    49,
                    0
                ],
                "published": "2024-08-12T08:46:30Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "title": "Value-based Proactive Caching for Sensing Data in Vehicular Networks: An\n  Operator's Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value-based Proactive Caching for Sensing Data in Vehicular Networks: An\n  Operator's Perspective"
                },
                "summary": "Access to sensing data (SD) is crucial for vehicular networks to ensure safe\nand efficient transportation services. Given the vast volume of data involved,\nproactive caching required SD is a pivotal strategy for alleviating network\ncongestion and improving data accessibility. Despite merits, existing studies\npredominantly address SD caching within a single slot. Therefore, these\napproaches lack scalability for scenarios involving multi-slots and are not\nwell-suited for network operators who manage resources within a long-term cost\nbudget. Moreover, the oversight of service capacity at caching nodes may result\nin substantial queuing delays for SD reception. To tackle these limitations, we\njointly consider the problem of anchoring SD caching and allocating from an\noperator's perspective. A value model incorporating both temporal and spacial\ncharacteristics is given to estimate the significance of various caching\ndecisions. Subsequently, a stochastic programming model is proposed to optimize\nthe long-term system performance, which is converted into a series of online\noptimization problem by leveraging the Lyapunov method and linearized via\nintroducing auxiliary variables. To expedite the solution, we provide a binary\nquantum particle swarm optimization based algorithm with quadratic time\ncomplexity. Numerical investigations demonstrate the superiority of proposed\nalgorithms compared with other schemes in terms of energy consumption, response\nlatency, and cache-hit ratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Access to sensing data (SD) is crucial for vehicular networks to ensure safe\nand efficient transportation services. Given the vast volume of data involved,\nproactive caching required SD is a pivotal strategy for alleviating network\ncongestion and improving data accessibility. Despite merits, existing studies\npredominantly address SD caching within a single slot. Therefore, these\napproaches lack scalability for scenarios involving multi-slots and are not\nwell-suited for network operators who manage resources within a long-term cost\nbudget. Moreover, the oversight of service capacity at caching nodes may result\nin substantial queuing delays for SD reception. To tackle these limitations, we\njointly consider the problem of anchoring SD caching and allocating from an\noperator's perspective. A value model incorporating both temporal and spacial\ncharacteristics is given to estimate the significance of various caching\ndecisions. Subsequently, a stochastic programming model is proposed to optimize\nthe long-term system performance, which is converted into a series of online\noptimization problem by leveraging the Lyapunov method and linearized via\nintroducing auxiliary variables. To expedite the solution, we provide a binary\nquantum particle swarm optimization based algorithm with quadratic time\ncomplexity. Numerical investigations demonstrate the superiority of proposed\nalgorithms compared with other schemes in terms of energy consumption, response\nlatency, and cache-hit ratio."
                },
                "authors": [
                    {
                        "name": "Yantong Wang"
                    },
                    {
                        "name": "Ke Liu"
                    },
                    {
                        "name": "Hui Ji"
                    },
                    {
                        "name": "Jiande Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jiande Sun"
                },
                "author": "Jiande Sun",
                "arxiv_comment": "14 pages,10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05996v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05996v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12574v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12574v1",
                "updated": "2025-02-18T06:26:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    6,
                    26,
                    5,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T06:26:05Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    6,
                    26,
                    5,
                    1,
                    49,
                    0
                ],
                "title": "HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading"
                },
                "summary": "Transformer-based large language models (LLMs) demonstrate impressive\nperformance in long context generation. Extending the context length has\ndisproportionately shifted the memory footprint of LLMs during inference to the\nkey-value cache (KV cache). In this paper, we propose HEADINFER, which offloads\nthe KV cache to CPU RAM while avoiding the need to fully store the KV cache for\nany transformer layer on the GPU. HEADINFER employs a fine-grained, head-wise\noffloading strategy, maintaining only selective attention heads KV cache on the\nGPU while computing attention output dynamically. Through roofline analysis, we\ndemonstrate that HEADINFER maintains computational efficiency while\nsignificantly reducing memory footprint. We evaluate HEADINFER on the\nLlama-3-8B model with a 1-million-token sequence, reducing the GPU memory\nfootprint of the KV cache from 128 GB to 1 GB and the total GPU memory usage\nfrom 207 GB to 17 GB, achieving a 92% reduction compared to BF16 baseline\ninference. Notably, HEADINFER enables 4-million-token inference with an 8B\nmodel on a single consumer GPU with 24GB memory (e.g., NVIDIA RTX 4090) without\napproximation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) demonstrate impressive\nperformance in long context generation. Extending the context length has\ndisproportionately shifted the memory footprint of LLMs during inference to the\nkey-value cache (KV cache). In this paper, we propose HEADINFER, which offloads\nthe KV cache to CPU RAM while avoiding the need to fully store the KV cache for\nany transformer layer on the GPU. HEADINFER employs a fine-grained, head-wise\noffloading strategy, maintaining only selective attention heads KV cache on the\nGPU while computing attention output dynamically. Through roofline analysis, we\ndemonstrate that HEADINFER maintains computational efficiency while\nsignificantly reducing memory footprint. We evaluate HEADINFER on the\nLlama-3-8B model with a 1-million-token sequence, reducing the GPU memory\nfootprint of the KV cache from 128 GB to 1 GB and the total GPU memory usage\nfrom 207 GB to 17 GB, achieving a 92% reduction compared to BF16 baseline\ninference. Notably, HEADINFER enables 4-million-token inference with an 8B\nmodel on a single consumer GPU with 24GB memory (e.g., NVIDIA RTX 4090) without\napproximation methods."
                },
                "authors": [
                    {
                        "name": "Cheng Luo"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Jinqi Xiao"
                    },
                    {
                        "name": "Bo Yuan"
                    },
                    {
                        "name": "Wen Xiao"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Jiawei Zhao"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Anima Anandkumar"
                    }
                ],
                "author_detail": {
                    "name": "Anima Anandkumar"
                },
                "author": "Anima Anandkumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12574v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12574v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12224v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12224v1",
                "updated": "2025-02-17T14:54:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    17,
                    14,
                    54,
                    14,
                    0,
                    48,
                    0
                ],
                "published": "2025-02-17T14:54:14Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    14,
                    54,
                    14,
                    0,
                    48,
                    0
                ],
                "title": "Accurate Expert Predictions in MoE Inference via Cross-Layer Gate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate Expert Predictions in MoE Inference via Cross-Layer Gate"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\nvarious tasks, and their application in edge scenarios has attracted\nsignificant attention. However, sparse-activated Mixture-of-Experts (MoE)\nmodels, which are well suited for edge scenarios, have received relatively\nlittle attention due to their high memory demands. Offload-based methods have\nbeen proposed to address this challenge, but they face difficulties with expert\nprediction. Inaccurate expert predictions can result in prolonged inference\ndelays. To promote the application of MoE models in edge scenarios, we propose\nFate, an offloading system designed for MoE models to enable efficient\ninference in resource-constrained environments. The key insight behind Fate is\nthat gate inputs from adjacent layers can be effectively used for expert\nprefetching, achieving high prediction accuracy without additional GPU\noverhead. Furthermore, Fate employs a shallow-favoring expert caching strategy\nthat increases the expert hit rate to 99\\%. Additionally, Fate integrates\ntailored quantization strategies for cache optimization and IO efficiency.\nExperimental results show that, compared to Load on Demand and Expert\nActivation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in\nprefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively,\nwhile maintaining inference quality. Moreover, Fate's performance improvements\nare scalable across different memory budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive performance across\nvarious tasks, and their application in edge scenarios has attracted\nsignificant attention. However, sparse-activated Mixture-of-Experts (MoE)\nmodels, which are well suited for edge scenarios, have received relatively\nlittle attention due to their high memory demands. Offload-based methods have\nbeen proposed to address this challenge, but they face difficulties with expert\nprediction. Inaccurate expert predictions can result in prolonged inference\ndelays. To promote the application of MoE models in edge scenarios, we propose\nFate, an offloading system designed for MoE models to enable efficient\ninference in resource-constrained environments. The key insight behind Fate is\nthat gate inputs from adjacent layers can be effectively used for expert\nprefetching, achieving high prediction accuracy without additional GPU\noverhead. Furthermore, Fate employs a shallow-favoring expert caching strategy\nthat increases the expert hit rate to 99\\%. Additionally, Fate integrates\ntailored quantization strategies for cache optimization and IO efficiency.\nExperimental results show that, compared to Load on Demand and Expert\nActivation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in\nprefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively,\nwhile maintaining inference quality. Moreover, Fate's performance improvements\nare scalable across different memory budgets."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Fang"
                    },
                    {
                        "name": "Zicong Hong"
                    },
                    {
                        "name": "Yuegui Huang"
                    },
                    {
                        "name": "Yufeng Lyu"
                    },
                    {
                        "name": "Wuhui Chen"
                    },
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Fan Yu"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12224v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12224v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14838v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14838v2",
                "updated": "2025-02-17T14:34:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    17,
                    14,
                    34,
                    58,
                    0,
                    48,
                    0
                ],
                "published": "2024-12-19T13:28:42Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "title": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs"
                },
                "summary": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased."
                },
                "authors": [
                    {
                        "name": "Xiabin Zhou"
                    },
                    {
                        "name": "Wenbin Wang"
                    },
                    {
                        "name": "Minyan Zeng"
                    },
                    {
                        "name": "Jiaxian Guo"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Liang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Liang Ding"
                },
                "author": "Liang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14838v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14838v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12216v1",
                "updated": "2025-02-17T08:39:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    17,
                    8,
                    39,
                    43,
                    0,
                    48,
                    0
                ],
                "published": "2025-02-17T08:39:43Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    8,
                    39,
                    43,
                    0,
                    48,
                    0
                ],
                "title": "Tactic: Adaptive Sparse Attention with Clustering and Distribution\n  Fitting for Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tactic: Adaptive Sparse Attention with Clustering and Distribution\n  Fitting for Long-Context LLMs"
                },
                "summary": "Long-context models are essential for many applications but face\ninefficiencies in loading large KV caches during decoding. Prior methods\nenforce fixed token budgets for sparse attention, assuming a set number of\ntokens can approximate full attention. However, these methods overlook\nvariations in the importance of attention across heads, layers, and contexts.\nTo address these limitations, we propose Tactic, a sparsity-adaptive and\ncalibration-free sparse attention mechanism that dynamically selects tokens\nbased on their cumulative attention scores rather than a fixed token budget. By\nsetting a target fraction of total attention scores, Tactic ensures that token\nselection naturally adapts to variations in attention sparsity. To efficiently\napproximate this selection, Tactic leverages clustering-based sorting and\ndistribution fitting, allowing it to accurately estimate token importance with\nminimal computational overhead. We show that Tactic outperforms existing sparse\nattention algorithms, achieving superior accuracy and up to 7.29x decode\nattention speedup. This improvement translates to an overall 1.58x end-to-end\ninference speedup, making Tactic a practical and effective solution for\nlong-context LLM inference in accuracy-sensitive applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context models are essential for many applications but face\ninefficiencies in loading large KV caches during decoding. Prior methods\nenforce fixed token budgets for sparse attention, assuming a set number of\ntokens can approximate full attention. However, these methods overlook\nvariations in the importance of attention across heads, layers, and contexts.\nTo address these limitations, we propose Tactic, a sparsity-adaptive and\ncalibration-free sparse attention mechanism that dynamically selects tokens\nbased on their cumulative attention scores rather than a fixed token budget. By\nsetting a target fraction of total attention scores, Tactic ensures that token\nselection naturally adapts to variations in attention sparsity. To efficiently\napproximate this selection, Tactic leverages clustering-based sorting and\ndistribution fitting, allowing it to accurately estimate token importance with\nminimal computational overhead. We show that Tactic outperforms existing sparse\nattention algorithms, achieving superior accuracy and up to 7.29x decode\nattention speedup. This improvement translates to an overall 1.58x end-to-end\ninference speedup, making Tactic a practical and effective solution for\nlong-context LLM inference in accuracy-sensitive applications."
                },
                "authors": [
                    {
                        "name": "Kan Zhu"
                    },
                    {
                        "name": "Tian Tang"
                    },
                    {
                        "name": "Qinyu Xu"
                    },
                    {
                        "name": "Yile Gu"
                    },
                    {
                        "name": "Zhichen Zeng"
                    },
                    {
                        "name": "Rohan Kadekodi"
                    },
                    {
                        "name": "Liangyu Zhao"
                    },
                    {
                        "name": "Ang Li"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Baris Kasikci"
                    }
                ],
                "author_detail": {
                    "name": "Baris Kasikci"
                },
                "author": "Baris Kasikci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11501v1",
                "updated": "2025-02-17T07:05:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    17,
                    7,
                    5,
                    36,
                    0,
                    48,
                    0
                ],
                "published": "2025-02-17T07:05:36Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    7,
                    5,
                    36,
                    0,
                    48,
                    0
                ],
                "title": "Token Pruning in Multimodal Large Language Models: Are We Solving the\n  Right Problem?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Pruning in Multimodal Large Language Models: Are We Solving the\n  Right Problem?"
                },
                "summary": "Multimodal large language models (MLLMs) have shown remarkable performance\nfor cross-modal understanding and generation, yet still suffer from severe\ninference costs. Recently, abundant works have been proposed to solve this\nproblem with token pruning, which identifies the redundant tokens in MLLMs and\nthen prunes them to reduce the computation and KV storage costs, leading to\nsignificant acceleration without training. While these methods claim efficiency\ngains, critical questions about their fundamental design and evaluation remain\nunanswered: Why do many existing approaches underperform even compared to naive\nrandom token selection? Are attention-based scoring sufficient for reliably\nidentifying redundant tokens? Is language information really helpful during\ntoken pruning? What makes a good trade-off between token importance and\nduplication? Are current evaluation protocols comprehensive and unbiased? The\nignorance of previous research on these problems hinders the long-term\ndevelopment of token pruning. In this paper, we answer these questions one by\none, providing insights into the design of future token pruning methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have shown remarkable performance\nfor cross-modal understanding and generation, yet still suffer from severe\ninference costs. Recently, abundant works have been proposed to solve this\nproblem with token pruning, which identifies the redundant tokens in MLLMs and\nthen prunes them to reduce the computation and KV storage costs, leading to\nsignificant acceleration without training. While these methods claim efficiency\ngains, critical questions about their fundamental design and evaluation remain\nunanswered: Why do many existing approaches underperform even compared to naive\nrandom token selection? Are attention-based scoring sufficient for reliably\nidentifying redundant tokens? Is language information really helpful during\ntoken pruning? What makes a good trade-off between token importance and\nduplication? Are current evaluation protocols comprehensive and unbiased? The\nignorance of previous research on these problems hinders the long-term\ndevelopment of token pruning. In this paper, we answer these questions one by\none, providing insights into the design of future token pruning methods."
                },
                "authors": [
                    {
                        "name": "Zichen Wen"
                    },
                    {
                        "name": "Yifeng Gao"
                    },
                    {
                        "name": "Weijia Li"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11444v1",
                "updated": "2025-02-17T05:02:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    17,
                    5,
                    2,
                    25,
                    0,
                    48,
                    0
                ],
                "published": "2025-02-17T05:02:25Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    5,
                    2,
                    25,
                    0,
                    48,
                    0
                ],
                "title": "Does RAG Really Perform Bad For Long-Context Processing?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does RAG Really Perform Bad For Long-Context Processing?"
                },
                "summary": "The efficient processing of long context poses a serious challenge for large\nlanguage models (LLMs). Recently, retrieval-augmented generation (RAG) has\nemerged as a promising strategy for this problem, as it enables LLMs to make\nselective use of the long context for efficient computation. However, existing\nRAG approaches lag behind other long-context processing methods due to inherent\nlimitations on inaccurate retrieval and fragmented contexts. To address these\nchallenges, we introduce RetroLM, a novel RAG framework for long-context\nprocessing. Unlike traditional methods, RetroLM employs KV-level retrieval\naugmentation, where it partitions the LLM's KV cache into contiguous pages and\nretrieves the most crucial ones for efficient computation. This approach\nenhances robustness to retrieval inaccuracy, facilitates effective utilization\nof fragmented contexts, and saves the cost from repeated computation. Building\non this framework, we further develop a specialized retriever for precise\nretrieval of critical pages and conduct unsupervised post-training to optimize\nthe model's ability to leverage retrieved information. We conduct comprehensive\nevaluations with a variety of benchmarks, including LongBench, InfiniteBench,\nand RULER, where RetroLM significantly outperforms existing long-context LLMs\nand efficient long-context processing methods, particularly in tasks requiring\nintensive reasoning or extremely long-context comprehension.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficient processing of long context poses a serious challenge for large\nlanguage models (LLMs). Recently, retrieval-augmented generation (RAG) has\nemerged as a promising strategy for this problem, as it enables LLMs to make\nselective use of the long context for efficient computation. However, existing\nRAG approaches lag behind other long-context processing methods due to inherent\nlimitations on inaccurate retrieval and fragmented contexts. To address these\nchallenges, we introduce RetroLM, a novel RAG framework for long-context\nprocessing. Unlike traditional methods, RetroLM employs KV-level retrieval\naugmentation, where it partitions the LLM's KV cache into contiguous pages and\nretrieves the most crucial ones for efficient computation. This approach\nenhances robustness to retrieval inaccuracy, facilitates effective utilization\nof fragmented contexts, and saves the cost from repeated computation. Building\non this framework, we further develop a specialized retriever for precise\nretrieval of critical pages and conduct unsupervised post-training to optimize\nthe model's ability to leverage retrieved information. We conduct comprehensive\nevaluations with a variety of benchmarks, including LongBench, InfiniteBench,\nand RULER, where RetroLM significantly outperforms existing long-context LLMs\nand efficient long-context processing methods, particularly in tasks requiring\nintensive reasoning or extremely long-context comprehension."
                },
                "authors": [
                    {
                        "name": "Kun Luo"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09383v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09383v2",
                "updated": "2025-02-16T18:31:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    18,
                    31,
                    10,
                    6,
                    47,
                    0
                ],
                "published": "2025-02-13T14:59:03Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    14,
                    59,
                    3,
                    3,
                    44,
                    0
                ],
                "title": "Capitalizing on a Crisis: A Computational Analysis of all Five Million\n  British Firms During the Covid-19 Pandemic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capitalizing on a Crisis: A Computational Analysis of all Five Million\n  British Firms During the Covid-19 Pandemic"
                },
                "summary": "The Covid-19 pandemic brought unprecedented changes to business ownership in\nthe UK which affects a generation of entrepreneurs and their employees.\nNonetheless, the impact remains poorly understood. This is because research on\ncapital accumulation has typically lacked high-quality, individualized,\npopulation-level data. We overcome these barriers to examine who benefits from\neconomic crises through a computationally orientated lens of firm creation.\nLeveraging a comprehensive cache of administrative data on every UK firm and\nall nine million people running them, combined with probabilistic algorithms,\nwe conduct individual-level analyses to understand who became Covid\nentrepreneurs. Using these techniques, we explore characteristics of\nentrepreneurs--such as age, gender, region, business experience, and\nindustry--which potentially predict Covid entrepreneurship. By employing an\nautomated time series model selection procedure to generate counterfactuals, we\nshow that Covid entrepreneurs were typically aged 35-49 (40.4%), men (73.1%),\nand had previously held roles in existing firms (59.4%). For most industries,\ngrowth was disproportionately concentrated around London. It was therefore\nexisting corporate elites who were most able to capitalize on the Covid crisis\nand not, as some hypothesized, young entrepreneurs who were setting up their\nfirst businesses. In this respect, the pandemic will likely impact future\nwealth inequalities. Our work offers methodological guidance for future\npolicymakers during economic crises and highlights the long-term consequences\nfor capital and wealth inequality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Covid-19 pandemic brought unprecedented changes to business ownership in\nthe UK which affects a generation of entrepreneurs and their employees.\nNonetheless, the impact remains poorly understood. This is because research on\ncapital accumulation has typically lacked high-quality, individualized,\npopulation-level data. We overcome these barriers to examine who benefits from\neconomic crises through a computationally orientated lens of firm creation.\nLeveraging a comprehensive cache of administrative data on every UK firm and\nall nine million people running them, combined with probabilistic algorithms,\nwe conduct individual-level analyses to understand who became Covid\nentrepreneurs. Using these techniques, we explore characteristics of\nentrepreneurs--such as age, gender, region, business experience, and\nindustry--which potentially predict Covid entrepreneurship. By employing an\nautomated time series model selection procedure to generate counterfactuals, we\nshow that Covid entrepreneurs were typically aged 35-49 (40.4%), men (73.1%),\nand had previously held roles in existing firms (59.4%). For most industries,\ngrowth was disproportionately concentrated around London. It was therefore\nexisting corporate elites who were most able to capitalize on the Covid crisis\nand not, as some hypothesized, young entrepreneurs who were setting up their\nfirst businesses. In this respect, the pandemic will likely impact future\nwealth inequalities. Our work offers methodological guidance for future\npolicymakers during economic crises and highlights the long-term consequences\nfor capital and wealth inequality."
                },
                "authors": [
                    {
                        "name": "Naomi Muggleton"
                    },
                    {
                        "name": "Charles Rahal"
                    },
                    {
                        "name": "Aaron Reeves"
                    }
                ],
                "author_detail": {
                    "name": "Aaron Reeves"
                },
                "author": "Aaron Reeves",
                "arxiv_doi": "10.1007/s42001-025-00360-4",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s42001-025-00360-4",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.09383v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09383v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Journal of Computational Social Science, 8(2), 1-29 (2025)",
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07627v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07627v2",
                "updated": "2025-02-16T16:41:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    16,
                    41,
                    43,
                    6,
                    47,
                    0
                ],
                "published": "2024-11-12T08:17:15Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    17,
                    15,
                    1,
                    317,
                    0
                ],
                "title": "Leveraging Previous Steps: A Training-free Fast Solver for Flow\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Previous Steps: A Training-free Fast Solver for Flow\n  Diffusion"
                },
                "summary": "Flow diffusion models (FDMs) have recently shown potential in generation\ntasks due to the high generation quality. However, the current ordinary\ndifferential equation (ODE) solver for FDMs, e.g., the Euler solver, still\nsuffers from slow generation since ODE solvers need many number function\nevaluations (NFE) to keep high-quality generation. In this paper, we propose a\nnovel training-free flow-solver to reduce NFE while maintaining high-quality\ngeneration. The key insight for the flow-solver is to leverage the previous\nsteps to reduce the NFE, where a cache is created to reuse these results from\nthe previous steps. Specifically, the Taylor expansion is first used to\napproximate the ODE. To calculate the high-order derivatives of Taylor\nexpansion, the flow-solver proposes to use the previous steps and a polynomial\ninterpolation to approximate it, where the number of orders we could\napproximate equals the number of previous steps we cached. We also prove that\nthe flow-solver has a more minor approximation error and faster generation\nspeed. Experimental results on the CIFAR-10, CelebA-HQ, LSUN-Bedroom,\nLSUN-Church, ImageNet, and real text-to-image generation prove the efficiency\nof the flow-solver. Specifically, the flow-solver improves the FID-30K from\n13.79 to 6.75, from 46.64 to 19.49 with $\\text{NFE}=10$ on CIFAR-10 and\nLSUN-Church, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow diffusion models (FDMs) have recently shown potential in generation\ntasks due to the high generation quality. However, the current ordinary\ndifferential equation (ODE) solver for FDMs, e.g., the Euler solver, still\nsuffers from slow generation since ODE solvers need many number function\nevaluations (NFE) to keep high-quality generation. In this paper, we propose a\nnovel training-free flow-solver to reduce NFE while maintaining high-quality\ngeneration. The key insight for the flow-solver is to leverage the previous\nsteps to reduce the NFE, where a cache is created to reuse these results from\nthe previous steps. Specifically, the Taylor expansion is first used to\napproximate the ODE. To calculate the high-order derivatives of Taylor\nexpansion, the flow-solver proposes to use the previous steps and a polynomial\ninterpolation to approximate it, where the number of orders we could\napproximate equals the number of previous steps we cached. We also prove that\nthe flow-solver has a more minor approximation error and faster generation\nspeed. Experimental results on the CIFAR-10, CelebA-HQ, LSUN-Bedroom,\nLSUN-Church, ImageNet, and real text-to-image generation prove the efficiency\nof the flow-solver. Specifically, the flow-solver improves the FID-30K from\n13.79 to 6.75, from 46.64 to 19.49 with $\\text{NFE}=10$ on CIFAR-10 and\nLSUN-Church, respectively."
                },
                "authors": [
                    {
                        "name": "Kaiyu Song"
                    },
                    {
                        "name": "Hanjiang Lai"
                    }
                ],
                "author_detail": {
                    "name": "Hanjiang Lai"
                },
                "author": "Hanjiang Lai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07627v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07627v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15332v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15332v2",
                "updated": "2025-02-16T14:50:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    14,
                    50,
                    0,
                    6,
                    47,
                    0
                ],
                "published": "2024-10-20T08:42:29Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    8,
                    42,
                    29,
                    6,
                    294,
                    0
                ],
                "title": "EPIC: Efficient Position-Independent Context Caching for Serving Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPIC: Efficient Position-Independent Context Caching for Serving Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) are critical for a wide range of applications,\nbut serving them efficiently becomes increasingly challenging as inputs become\nmore complex. Context caching improves serving performance by exploiting\ninter-request dependency and reusing key-value (KV) cache across requests, thus\nimproving time-to-first-token (TTFT). However, existing prefix-based context\ncaching requires exact token prefix matches, limiting cache reuse in few-shot\nlearning, multi-document QA, or retrieval-augmented generation, where prefixes\nmay vary. In this paper, we present EPIC, an LLM serving system that introduces\nposition-independent context caching (PIC), enabling modular KV cache reuse\nregardless of token chunk position (or prefix). EPIC features two key designs:\nAttnLink, which leverages static attention sparsity to minimize recomputation\nfor accuracy recovery, and KVSplit, a customizable chunking method that\npreserves semantic coherence. Our experiments demonstrate that Epic delivers up\nto 8x improvements in TTFT and 7x throughput over existing systems, with\nnegligible or no accuracy loss. By addressing the limitations of traditional\ncaching approaches, Epic enables more scalable and efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are critical for a wide range of applications,\nbut serving them efficiently becomes increasingly challenging as inputs become\nmore complex. Context caching improves serving performance by exploiting\ninter-request dependency and reusing key-value (KV) cache across requests, thus\nimproving time-to-first-token (TTFT). However, existing prefix-based context\ncaching requires exact token prefix matches, limiting cache reuse in few-shot\nlearning, multi-document QA, or retrieval-augmented generation, where prefixes\nmay vary. In this paper, we present EPIC, an LLM serving system that introduces\nposition-independent context caching (PIC), enabling modular KV cache reuse\nregardless of token chunk position (or prefix). EPIC features two key designs:\nAttnLink, which leverages static attention sparsity to minimize recomputation\nfor accuracy recovery, and KVSplit, a customizable chunking method that\npreserves semantic coherence. Our experiments demonstrate that Epic delivers up\nto 8x improvements in TTFT and 7x throughput over existing systems, with\nnegligible or no accuracy loss. By addressing the limitations of traditional\ncaching approaches, Epic enables more scalable and efficient LLM inference."
                },
                "authors": [
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Wenrui Huang"
                    },
                    {
                        "name": "Haoyi Wang"
                    },
                    {
                        "name": "Weidong Wang"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Qin Zhang"
                    },
                    {
                        "name": "Hao Feng"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Tao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tao Xie"
                },
                "author": "Tao Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15332v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15332v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11147v1",
                "updated": "2025-02-16T14:28:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    14,
                    28,
                    52,
                    6,
                    47,
                    0
                ],
                "published": "2025-02-16T14:28:52Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    14,
                    28,
                    52,
                    6,
                    47,
                    0
                ],
                "title": "Efficient Long-Decoding Inference with Reasoning-Aware Attention\n  Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Long-Decoding Inference with Reasoning-Aware Attention\n  Sparsity"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong capabilities across\nvarious domains, with recent advancements in challenging reasoning tasks such\nas mathematics and programming. However, solving reasoning tasks often requires\nlong decoding chains (of thoughts), which incur $O(N)$ time and memory\nconsumption, where $N$ is the chain length. To mitigate $O(N)$ time and memory\nconsumption, existing sparsity-based algorithms propose retaining only the most\ncritical token's intermediate data (i.e., key-value cache) and discarding the\nrest. However, these existing algorithms struggle with the ``impossible\ntrinity'' of accuracy, time, and memory. For example, the state-of-the-art\nalgorithm, Quest, achieves high accuracy with $O(L)$ time but $O(N)$ memory\n($L$ is the cache budget, $L \\ll N$). To address this issue, in this paper, we\nidentify a new attention pattern during the decode stage of reasoning tasks,\nwhere milestone tokens (analogous to lemmas in mathematical proofs) emerge, are\nutilized, and then become unimportant afterward. Based on this pattern, we\npropose a new algorithm named RaaS that identifies and retains milestone tokens\nonly until they are no longer needed, achieving high accuracy with $O(L)$ time\nand $O(L)$ memory complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong capabilities across\nvarious domains, with recent advancements in challenging reasoning tasks such\nas mathematics and programming. However, solving reasoning tasks often requires\nlong decoding chains (of thoughts), which incur $O(N)$ time and memory\nconsumption, where $N$ is the chain length. To mitigate $O(N)$ time and memory\nconsumption, existing sparsity-based algorithms propose retaining only the most\ncritical token's intermediate data (i.e., key-value cache) and discarding the\nrest. However, these existing algorithms struggle with the ``impossible\ntrinity'' of accuracy, time, and memory. For example, the state-of-the-art\nalgorithm, Quest, achieves high accuracy with $O(L)$ time but $O(N)$ memory\n($L$ is the cache budget, $L \\ll N$). To address this issue, in this paper, we\nidentify a new attention pattern during the decode stage of reasoning tasks,\nwhere milestone tokens (analogous to lemmas in mathematical proofs) emerge, are\nutilized, and then become unimportant afterward. Based on this pattern, we\npropose a new algorithm named RaaS that identifies and retains milestone tokens\nonly until they are no longer needed, achieving high accuracy with $O(L)$ time\nand $O(L)$ memory complexity."
                },
                "authors": [
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Wenrui Huang"
                    },
                    {
                        "name": "Weidong Wang"
                    },
                    {
                        "name": "Zhenwen Li"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Zhixia Liu"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Tao Xie"
                    },
                    {
                        "name": "Yizhou Shan"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Shan"
                },
                "author": "Yizhou Shan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11101v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11101v1",
                "updated": "2025-02-16T12:33:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    12,
                    33,
                    16,
                    6,
                    47,
                    0
                ],
                "published": "2025-02-16T12:33:16Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    12,
                    33,
                    16,
                    6,
                    47,
                    0
                ],
                "title": "CacheFocus: Dynamic Cache Re-Positioning for Efficient\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheFocus: Dynamic Cache Re-Positioning for Efficient\n  Retrieval-Augmented Generation"
                },
                "summary": "Large Language Models (LLMs) excel across a variety of language tasks yet are\nconstrained by limited input lengths and high computational costs. Existing\napproaches\\textemdash such as relative positional encodings (e.g., RoPE, ALiBi)\nand sliding window mechanisms\\textemdash partially alleviate these issues but\noften require additional training or suffer from performance degradation with\nlonger inputs. In this paper, we introduce \\textbf{\\textit{CacheFocus}}, a\nmethod that enhances length normalization and reduces inference latency without\nany further training. Our approach leverages query-independent, offline caching\nto efficiently reuse a Context KV Cache Store. We address the amplification of\nabnormal token distributions problem by re-positioning cached keys and\nintroducing Layer-Adaptive Cache Pruning to discard low-relevance caches during\npre-filling. Additionally, our Adaptive Positional Allocation Strategy\ndynamically reassigns cache positions to maximize the use of the available\npositional encoding range. Experiments on the Natural Questions and TriviaQA\ndatasets demonstrate that CacheFocus outperforms alternative methods even when\ninputs exceed the $4$K limit of the \\texttt{LLaMA-2} model, emphasizing its\npractical effectiveness for long-context LLMs. Moreover, even with large\nmaximum input length of \\texttt{Qwen2}, the performance of CacheFocus shows\nthat it maintains consistent performance even as the number of documents\nincreases, effectively managing long-text generation without degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel across a variety of language tasks yet are\nconstrained by limited input lengths and high computational costs. Existing\napproaches\\textemdash such as relative positional encodings (e.g., RoPE, ALiBi)\nand sliding window mechanisms\\textemdash partially alleviate these issues but\noften require additional training or suffer from performance degradation with\nlonger inputs. In this paper, we introduce \\textbf{\\textit{CacheFocus}}, a\nmethod that enhances length normalization and reduces inference latency without\nany further training. Our approach leverages query-independent, offline caching\nto efficiently reuse a Context KV Cache Store. We address the amplification of\nabnormal token distributions problem by re-positioning cached keys and\nintroducing Layer-Adaptive Cache Pruning to discard low-relevance caches during\npre-filling. Additionally, our Adaptive Positional Allocation Strategy\ndynamically reassigns cache positions to maximize the use of the available\npositional encoding range. Experiments on the Natural Questions and TriviaQA\ndatasets demonstrate that CacheFocus outperforms alternative methods even when\ninputs exceed the $4$K limit of the \\texttt{LLaMA-2} model, emphasizing its\npractical effectiveness for long-context LLMs. Moreover, even with large\nmaximum input length of \\texttt{Qwen2}, the performance of CacheFocus shows\nthat it maintains consistent performance even as the number of documents\nincreases, effectively managing long-text generation without degradation."
                },
                "authors": [
                    {
                        "name": "Kun-Hui Lee"
                    },
                    {
                        "name": "Eunhwan Park"
                    },
                    {
                        "name": "Donghoon Han"
                    },
                    {
                        "name": "Seung-Hoon Na"
                    }
                ],
                "author_detail": {
                    "name": "Seung-Hoon Na"
                },
                "author": "Seung-Hoon Na",
                "arxiv_comment": "11 pages (Work in progress)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11101v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11101v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11083v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11083v1",
                "updated": "2025-02-16T11:37:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    11,
                    37,
                    14,
                    6,
                    47,
                    0
                ],
                "published": "2025-02-16T11:37:14Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    11,
                    37,
                    14,
                    6,
                    47,
                    0
                ],
                "title": "Streamlining the Collaborative Chain of Models into A Single Forward\n  Pass in Generation-Based Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streamlining the Collaborative Chain of Models into A Single Forward\n  Pass in Generation-Based Tasks"
                },
                "summary": "In Retrieval-Augmented Generation (RAG) and agent-based frameworks, the\n\"Chain of Models\" approach is widely used, where multiple specialized models\nwork sequentially on distinct sub-tasks. This approach is effective but\nincreases resource demands as each model must be deployed separately. Recent\nadvancements attempt to address this by applying prompt tuning, which allows a\nshared base model to adapt to multiple tasks with minimal parameter changes.\nHowever, a key challenge remains: intermediate outputs, passed between models\nas plain text, require recomputation of hidden states (i.e., Key and Value (KV)\nstates in Transformers) during inference. In this paper, we introduce FTHSS, a\nnovel prompt-tuning method that enables models to share KV hidden states,\neliminating redundant forward passes and reducing KV cache storage. By\nmodifying input and attention masks during training, FTHSS allows models to\neffectively utilize KV hidden states from prior models in both single- and\nmulti-round scenarios. Empirical results on four tasks show that FTHSS matches\nthe performance of traditional model chains while improving inference\nefficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Retrieval-Augmented Generation (RAG) and agent-based frameworks, the\n\"Chain of Models\" approach is widely used, where multiple specialized models\nwork sequentially on distinct sub-tasks. This approach is effective but\nincreases resource demands as each model must be deployed separately. Recent\nadvancements attempt to address this by applying prompt tuning, which allows a\nshared base model to adapt to multiple tasks with minimal parameter changes.\nHowever, a key challenge remains: intermediate outputs, passed between models\nas plain text, require recomputation of hidden states (i.e., Key and Value (KV)\nstates in Transformers) during inference. In this paper, we introduce FTHSS, a\nnovel prompt-tuning method that enables models to share KV hidden states,\neliminating redundant forward passes and reducing KV cache storage. By\nmodifying input and attention masks during training, FTHSS allows models to\neffectively utilize KV hidden states from prior models in both single- and\nmulti-round scenarios. Empirical results on four tasks show that FTHSS matches\nthe performance of traditional model chains while improving inference\nefficiency."
                },
                "authors": [
                    {
                        "name": "Yuanjie Lyu"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Yuhao Chen"
                    },
                    {
                        "name": "Yong Chen"
                    },
                    {
                        "name": "Tong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Tong Xu"
                },
                "author": "Tong Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11083v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11083v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11046v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11046v1",
                "updated": "2025-02-16T09:08:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    9,
                    8,
                    36,
                    6,
                    47,
                    0
                ],
                "published": "2025-02-16T09:08:36Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    9,
                    8,
                    36,
                    6,
                    47,
                    0
                ],
                "title": "Enabling Efficient Transaction Processing on CXL-Based Memory Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Efficient Transaction Processing on CXL-Based Memory Sharing"
                },
                "summary": "Transaction processing systems are the crux for modern data-center\napplications, yet current multi-node systems are slow due to network overheads.\nThis paper advocates for Compute Express Link (CXL) as a network alternative,\nwhich enables low-latency and cache-coherent shared memory accesses. However,\ndirectly adopting standard CXL primitives leads to performance degradation due\nto the high cost of maintaining cross-node cache coherence. To address the CXL\nchallenges, this paper introduces CtXnL, a software-hardware co-designed system\nthat implements a novel hybrid coherence primitive tailored to the loosely\ncoherent nature of transactional data. The core innovation of CtXnL is\nempowering transaction system developers with the ability to selectively\nachieve data coherence. Our evaluations on OLTP workloads demonstrate that\nCtXnL enhances performance, outperforming current network-based systems and\nachieves with up to 2.08x greater throughput than vanilla CXL memory sharing\narchitectures across universal transaction processing policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transaction processing systems are the crux for modern data-center\napplications, yet current multi-node systems are slow due to network overheads.\nThis paper advocates for Compute Express Link (CXL) as a network alternative,\nwhich enables low-latency and cache-coherent shared memory accesses. However,\ndirectly adopting standard CXL primitives leads to performance degradation due\nto the high cost of maintaining cross-node cache coherence. To address the CXL\nchallenges, this paper introduces CtXnL, a software-hardware co-designed system\nthat implements a novel hybrid coherence primitive tailored to the loosely\ncoherent nature of transactional data. The core innovation of CtXnL is\nempowering transaction system developers with the ability to selectively\nachieve data coherence. Our evaluations on OLTP workloads demonstrate that\nCtXnL enhances performance, outperforming current network-based systems and\nachieves with up to 2.08x greater throughput than vanilla CXL memory sharing\narchitectures across universal transaction processing policies."
                },
                "authors": [
                    {
                        "name": "Zhao Wang"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Cong Li"
                    },
                    {
                        "name": "Dimin Niu"
                    },
                    {
                        "name": "Tianchan Guan"
                    },
                    {
                        "name": "Zhaoyang Du"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11046v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11046v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05231v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05231v2",
                "updated": "2025-02-15T23:54:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    15,
                    23,
                    54,
                    38,
                    5,
                    46,
                    0
                ],
                "published": "2024-05-08T17:27:11Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    17,
                    27,
                    11,
                    2,
                    129,
                    0
                ],
                "title": "DiskGNN: Bridging I/O Efficiency and Model Accuracy for Out-of-Core GNN\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiskGNN: Bridging I/O Efficiency and Model Accuracy for Out-of-Core GNN\n  Training"
                },
                "summary": "Graph neural networks (GNNs) are machine learning models specialized for\ngraph data and widely used in many applications. To train GNNs on large graphs\nthat exceed CPU memory, several systems store data on disk and conduct\nout-of-core processing. However, these systems suffer from either read\namplification when reading node features that are usually smaller than a disk\npage or degraded model accuracy by treating the graph as disconnected\npartitions. To close this gap, we build a system called DiskGNN, which achieves\nhigh I/O efficiency and thus fast training without hurting model accuracy. The\nkey technique used by DiskGNN is offline sampling, which helps decouple graph\nsampling from model computation. In particular, by conducting graph sampling\nbeforehand, DiskGNN acquires the node features that will be accessed by model\ncomputation, and such information is utilized to pack the target node features\ncontiguously on disk to avoid read amplification. Besides, \\name{} also adopts\ndesigns including four-level feature store to fully utilize the memory\nhierarchy to cache node features and reduce disk access, batched packing to\naccelerate the feature packing process, and pipelined training to overlap disk\naccess with other operations. We compare DiskGNN with Ginex and MariusGNN,\nwhich are state-of-the-art systems for out-of-core GNN training. The results\nshow that DiskGNN can speed up the baselines by over 8x while matching their\nbest model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural networks (GNNs) are machine learning models specialized for\ngraph data and widely used in many applications. To train GNNs on large graphs\nthat exceed CPU memory, several systems store data on disk and conduct\nout-of-core processing. However, these systems suffer from either read\namplification when reading node features that are usually smaller than a disk\npage or degraded model accuracy by treating the graph as disconnected\npartitions. To close this gap, we build a system called DiskGNN, which achieves\nhigh I/O efficiency and thus fast training without hurting model accuracy. The\nkey technique used by DiskGNN is offline sampling, which helps decouple graph\nsampling from model computation. In particular, by conducting graph sampling\nbeforehand, DiskGNN acquires the node features that will be accessed by model\ncomputation, and such information is utilized to pack the target node features\ncontiguously on disk to avoid read amplification. Besides, \\name{} also adopts\ndesigns including four-level feature store to fully utilize the memory\nhierarchy to cache node features and reduce disk access, batched packing to\naccelerate the feature packing process, and pipelined training to overlap disk\naccess with other operations. We compare DiskGNN with Ginex and MariusGNN,\nwhich are state-of-the-art systems for out-of-core GNN training. The results\nshow that DiskGNN can speed up the baselines by over 8x while matching their\nbest model accuracy."
                },
                "authors": [
                    {
                        "name": "Renjie Liu"
                    },
                    {
                        "name": "Yichuan Wang"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Haitian Jiang"
                    },
                    {
                        "name": "Zhenkun Cai"
                    },
                    {
                        "name": "Minjie Wang"
                    },
                    {
                        "name": "Bo Tang"
                    },
                    {
                        "name": "Jinyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinyang Li"
                },
                "author": "Jinyang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.05231v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05231v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01939v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01939v2",
                "updated": "2025-02-15T18:09:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    15,
                    18,
                    9,
                    50,
                    5,
                    46,
                    0
                ],
                "published": "2024-06-04T03:48:08Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    3,
                    48,
                    8,
                    1,
                    156,
                    0
                ],
                "title": "Speeding up Policy Simulation in Supply Chain RL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speeding up Policy Simulation in Supply Chain RL"
                },
                "summary": "Simulating a single trajectory of a dynamical system under some\nstate-dependent policy is a core bottleneck in policy optimization (PO)\nalgorithms. The many inherently serial policy evaluations that must be\nperformed in a single simulation constitute the bulk of this bottleneck. In\napplying PO to supply chain optimization (SCO) problems, simulating a single\nsample path corresponding to one month of a supply chain can take several\nhours. We present an iterative algorithm to accelerate policy simulation,\ndubbed Picard Iteration. This scheme carefully assigns policy evaluation tasks\nto independent processes. Within an iteration, any given process evaluates the\npolicy only on its assigned tasks while assuming a certain \"cached\" evaluation\nfor other tasks; the cache is updated at the end of the iteration. Implemented\non GPUs, this scheme admits batched evaluation of the policy across a single\ntrajectory. We prove that the structure afforded by many SCO problems allows\nconvergence in a small number of iterations independent of the horizon. We\ndemonstrate practical speedups of 400x on large-scale SCO problems even with a\nsingle GPU, and also demonstrate practical efficacy in other RL environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating a single trajectory of a dynamical system under some\nstate-dependent policy is a core bottleneck in policy optimization (PO)\nalgorithms. The many inherently serial policy evaluations that must be\nperformed in a single simulation constitute the bulk of this bottleneck. In\napplying PO to supply chain optimization (SCO) problems, simulating a single\nsample path corresponding to one month of a supply chain can take several\nhours. We present an iterative algorithm to accelerate policy simulation,\ndubbed Picard Iteration. This scheme carefully assigns policy evaluation tasks\nto independent processes. Within an iteration, any given process evaluates the\npolicy only on its assigned tasks while assuming a certain \"cached\" evaluation\nfor other tasks; the cache is updated at the end of the iteration. Implemented\non GPUs, this scheme admits batched evaluation of the policy across a single\ntrajectory. We prove that the structure afforded by many SCO problems allows\nconvergence in a small number of iterations independent of the horizon. We\ndemonstrate practical speedups of 400x on large-scale SCO problems even with a\nsingle GPU, and also demonstrate practical efficacy in other RL environments."
                },
                "authors": [
                    {
                        "name": "Vivek Farias"
                    },
                    {
                        "name": "Joren Gijsbrechts"
                    },
                    {
                        "name": "Aryan Khojandi"
                    },
                    {
                        "name": "Tianyi Peng"
                    },
                    {
                        "name": "Andrew Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Zheng"
                },
                "author": "Andrew Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01939v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01939v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10659v1",
                "updated": "2025-02-15T03:56:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    15,
                    3,
                    56,
                    22,
                    5,
                    46,
                    0
                ],
                "published": "2025-02-15T03:56:22Z",
                "published_parsed": [
                    2025,
                    2,
                    15,
                    3,
                    56,
                    22,
                    5,
                    46,
                    0
                ],
                "title": "Pushing up to the Limit of Memory Bandwidth and Capacity Utilization for\n  Efficient LLM Decoding on Embedded FPGA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pushing up to the Limit of Memory Bandwidth and Capacity Utilization for\n  Efficient LLM Decoding on Embedded FPGA"
                },
                "summary": "The extremely high computational and storage demands of large language models\nhave excluded most edge devices, which were widely used for efficient machine\nlearning, from being viable options. A typical edge device usually only has 4GB\nof memory capacity and a bandwidth of less than 20GB/s, while a large language\nmodel quantized to 4-bit precision with 7B parameters already requires 3.5GB of\ncapacity, and its decoding process is purely bandwidth-bound. In this paper, we\naim to explore these limits by proposing a hardware accelerator for large\nlanguage model (LLM) inference on the Zynq-based KV260 platform, equipped with\n4GB of 64-bit 2400Mbps DDR4 memory. We successfully deploy a LLaMA2-7B model,\nachieving a decoding speed of around 5 token/s, utilizing 93.3% of the memory\ncapacity and reaching 85% decoding speed of the theoretical memory bandwidth\nlimit. To fully reserve the memory capacity for model weights and key-value\ncache, we develop the system in a bare-metal environment without an operating\nsystem. To fully reserve the bandwidth for model weight transfers, we implement\na customized dataflow with an operator fusion pipeline and propose a data\narrangement format that can maximize the data transaction efficiency. This\nresearch marks the first attempt to deploy a 7B level LLM on a standalone\nembedded field programmable gate array (FPGA) device. It provides key insights\ninto efficient LLM inference on embedded FPGA devices and provides guidelines\nfor future architecture design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The extremely high computational and storage demands of large language models\nhave excluded most edge devices, which were widely used for efficient machine\nlearning, from being viable options. A typical edge device usually only has 4GB\nof memory capacity and a bandwidth of less than 20GB/s, while a large language\nmodel quantized to 4-bit precision with 7B parameters already requires 3.5GB of\ncapacity, and its decoding process is purely bandwidth-bound. In this paper, we\naim to explore these limits by proposing a hardware accelerator for large\nlanguage model (LLM) inference on the Zynq-based KV260 platform, equipped with\n4GB of 64-bit 2400Mbps DDR4 memory. We successfully deploy a LLaMA2-7B model,\nachieving a decoding speed of around 5 token/s, utilizing 93.3% of the memory\ncapacity and reaching 85% decoding speed of the theoretical memory bandwidth\nlimit. To fully reserve the memory capacity for model weights and key-value\ncache, we develop the system in a bare-metal environment without an operating\nsystem. To fully reserve the bandwidth for model weight transfers, we implement\na customized dataflow with an operator fusion pipeline and propose a data\narrangement format that can maximize the data transaction efficiency. This\nresearch marks the first attempt to deploy a 7B level LLM on a standalone\nembedded field programmable gate array (FPGA) device. It provides key insights\ninto efficient LLM inference on embedded FPGA devices and provides guidelines\nfor future architecture design."
                },
                "authors": [
                    {
                        "name": "Jindong Li"
                    },
                    {
                        "name": "Tenglong Li"
                    },
                    {
                        "name": "Guobin Shen"
                    },
                    {
                        "name": "Dongcheng Zhao"
                    },
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Yi Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zeng"
                },
                "author": "Yi Zeng",
                "arxiv_comment": "Accepted by DATE2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10389v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10389v1",
                "updated": "2025-02-14T18:59:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    59,
                    36,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T18:59:36Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    59,
                    36,
                    4,
                    45,
                    0
                ],
                "title": "Region-Adaptive Sampling for Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Region-Adaptive Sampling for Diffusion Transformers"
                },
                "summary": "Diffusion models (DMs) have become the leading choice for generative tasks\nacross diverse domains. However, their reliance on multiple sequential forward\npasses significantly limits real-time performance. Previous acceleration\nmethods have primarily focused on reducing the number of sampling steps or\nreusing intermediate results, failing to leverage variations across spatial\nregions within the image due to the constraints of convolutional U-Net\nstructures. By harnessing the flexibility of Diffusion Transformers (DiTs) in\nhandling variable number of tokens, we introduce RAS, a novel, training-free\nsampling strategy that dynamically assigns different sampling ratios to regions\nwithin an image based on the focus of the DiT model. Our key observation is\nthat during each sampling step, the model concentrates on semantically\nmeaningful regions, and these areas of focus exhibit strong continuity across\nconsecutive steps. Leveraging this insight, RAS updates only the regions\ncurrently in focus, while other regions are updated using cached noise from the\nprevious step. The model's focus is determined based on the output from the\npreceding step, capitalizing on the temporal consistency we observed. We\nevaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving speedups up\nto 2.36x and 2.51x, respectively, with minimal degradation in generation\nquality. Additionally, a user study reveals that RAS delivers comparable\nqualities under human evaluation while achieving a 1.6x speedup. Our approach\nmakes a significant step towards more efficient diffusion transformers,\nenhancing their potential for real-time applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models (DMs) have become the leading choice for generative tasks\nacross diverse domains. However, their reliance on multiple sequential forward\npasses significantly limits real-time performance. Previous acceleration\nmethods have primarily focused on reducing the number of sampling steps or\nreusing intermediate results, failing to leverage variations across spatial\nregions within the image due to the constraints of convolutional U-Net\nstructures. By harnessing the flexibility of Diffusion Transformers (DiTs) in\nhandling variable number of tokens, we introduce RAS, a novel, training-free\nsampling strategy that dynamically assigns different sampling ratios to regions\nwithin an image based on the focus of the DiT model. Our key observation is\nthat during each sampling step, the model concentrates on semantically\nmeaningful regions, and these areas of focus exhibit strong continuity across\nconsecutive steps. Leveraging this insight, RAS updates only the regions\ncurrently in focus, while other regions are updated using cached noise from the\nprevious step. The model's focus is determined based on the output from the\npreceding step, capitalizing on the temporal consistency we observed. We\nevaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving speedups up\nto 2.36x and 2.51x, respectively, with minimal degradation in generation\nquality. Additionally, a user study reveals that RAS delivers comparable\nqualities under human evaluation while achieving a 1.6x speedup. Our approach\nmakes a significant step towards more efficient diffusion transformers,\nenhancing their potential for real-time applications."
                },
                "authors": [
                    {
                        "name": "Ziming Liu"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Yiqi Zhang"
                    },
                    {
                        "name": "Lili Qiu"
                    },
                    {
                        "name": "Yang You"
                    },
                    {
                        "name": "Yuqing Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yuqing Yang"
                },
                "author": "Yuqing Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10389v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10389v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09057v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09057v2",
                "updated": "2025-02-14T17:17:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    17,
                    17,
                    20,
                    4,
                    45,
                    0
                ],
                "published": "2024-12-12T08:33:39Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    33,
                    39,
                    3,
                    347,
                    0
                ],
                "title": "PhishIntel: Toward Practical Deployment of Reference-Based Phishing\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhishIntel: Toward Practical Deployment of Reference-Based Phishing\n  Detection"
                },
                "summary": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) have achieved notable advancements\nin detection accuracy, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) have achieved notable advancements\nin detection accuracy, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility."
                },
                "authors": [
                    {
                        "name": "Yuexin Li"
                    },
                    {
                        "name": "Hiok Kuek Tan"
                    },
                    {
                        "name": "Qiaoran Meng"
                    },
                    {
                        "name": "Mei Lin Lock"
                    },
                    {
                        "name": "Tri Cao"
                    },
                    {
                        "name": "Shumin Deng"
                    },
                    {
                        "name": "Nay Oo"
                    },
                    {
                        "name": "Hoon Wei Lim"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "arxiv_doi": "10.1145/3701716.3715192",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3701716.3715192",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.09057v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09057v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by WWW 2025 (Demo Track)",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10220v1",
                "updated": "2025-02-14T15:14:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    14,
                    53,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T15:14:53Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    14,
                    53,
                    4,
                    45,
                    0
                ],
                "title": "Optimal and Coordinated Voltage Control: Case Study on a 132 kV\n  Norwegian Grid Subsystem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal and Coordinated Voltage Control: Case Study on a 132 kV\n  Norwegian Grid Subsystem"
                },
                "summary": "This work presents a framework for dynamic performance assessment of the\nhigher layers in the hierarchical voltage regulation scheme, with case studies\napplied to specific areas of the Norwegian grid. Unlike the primary (PVR)\nlevel, the secondary (SVR) and tertiary (TVR) levels are not tuned to a single\ndevice at a time, handling instead several reactive power resources available\nwithin a control zone including generator units, static VAr compensators and\nothers. Proper SVR-TVR coordination for realistic transmission systems is a\nchallenging topic at the core of many ongoing discussions in voltage control\nliterature. Special focus is placed on practical considerations from the system\noperator perspective, since this research is also aimed at simplifying daily\ncontrol centre routines. Dynamic simulation results concern a 21-bus equivalent\nof a 132 kV network model that accurately represents a Norwegian grid\nsubsystem. Case studies address daily grid operation with real-life load demand\nand wind power generation profiles, showing that the proposed strategy is\neffective not only to minimize total active power losses as much as possible\nwithin system-wide limitations, but also to maintain adequate voltage profiles\nand reactive power flows. Findings pertaining to this work showcase the\nbenefits of applying hierarchical voltage regulation layers as an asset to\nday-to-day control center management of a realistic transmission network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a framework for dynamic performance assessment of the\nhigher layers in the hierarchical voltage regulation scheme, with case studies\napplied to specific areas of the Norwegian grid. Unlike the primary (PVR)\nlevel, the secondary (SVR) and tertiary (TVR) levels are not tuned to a single\ndevice at a time, handling instead several reactive power resources available\nwithin a control zone including generator units, static VAr compensators and\nothers. Proper SVR-TVR coordination for realistic transmission systems is a\nchallenging topic at the core of many ongoing discussions in voltage control\nliterature. Special focus is placed on practical considerations from the system\noperator perspective, since this research is also aimed at simplifying daily\ncontrol centre routines. Dynamic simulation results concern a 21-bus equivalent\nof a 132 kV network model that accurately represents a Norwegian grid\nsubsystem. Case studies address daily grid operation with real-life load demand\nand wind power generation profiles, showing that the proposed strategy is\neffective not only to minimize total active power losses as much as possible\nwithin system-wide limitations, but also to maintain adequate voltage profiles\nand reactive power flows. Findings pertaining to this work showcase the\nbenefits of applying hierarchical voltage regulation layers as an asset to\nday-to-day control center management of a realistic transmission network."
                },
                "authors": [
                    {
                        "name": "Hugo Rodrigues de Brito"
                    },
                    {
                        "name": "Daniel Simon Baltensperger"
                    },
                    {
                        "name": "Kjetil Obstfelder Uhlen"
                    }
                ],
                "author_detail": {
                    "name": "Kjetil Obstfelder Uhlen"
                },
                "author": "Kjetil Obstfelder Uhlen",
                "arxiv_comment": "11 pages, 8 figures, CIGRE Symposium 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10167v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10167v1",
                "updated": "2025-02-14T13:55:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    55,
                    1,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T13:55:01Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    55,
                    1,
                    4,
                    45,
                    0
                ],
                "title": "Modeling and Simulating Emerging Memory Technologies: A Tutorial",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling and Simulating Emerging Memory Technologies: A Tutorial"
                },
                "summary": "Non-volatile Memory (NVM) technologies present a promising alternative to\ntraditional volatile memories such as SRAM and DRAM. Due to the limited\navailability of real NVM devices, simulators play a crucial role in\narchitectural exploration and hardware-software co-design. This tutorial\npresents a simulation toolchain through four detailed case studies, showcasing\nits applicability to various domains of system design, including hybrid\nmain-memory and cache, compute-in-memory, and wear-leveling design. These case\nstudies provide the reader with practical insights on customizing the toolchain\nfor their specific research needs. The source code is open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-volatile Memory (NVM) technologies present a promising alternative to\ntraditional volatile memories such as SRAM and DRAM. Due to the limited\navailability of real NVM devices, simulators play a crucial role in\narchitectural exploration and hardware-software co-design. This tutorial\npresents a simulation toolchain through four detailed case studies, showcasing\nits applicability to various domains of system design, including hybrid\nmain-memory and cache, compute-in-memory, and wear-leveling design. These case\nstudies provide the reader with practical insights on customizing the toolchain\nfor their specific research needs. The source code is open-sourced."
                },
                "authors": [
                    {
                        "name": "Yun-Chih Chen"
                    },
                    {
                        "name": "Tristan Seidl"
                    },
                    {
                        "name": "Nils Hlscher"
                    },
                    {
                        "name": "Christian Hakert"
                    },
                    {
                        "name": "Minh Duy Truong"
                    },
                    {
                        "name": "Jian-Jia Chen"
                    },
                    {
                        "name": "Joo Paulo C. de Lima"
                    },
                    {
                        "name": "Asif Ali Khan"
                    },
                    {
                        "name": "Jeronimo Castrillon"
                    },
                    {
                        "name": "Ali Nezhadi"
                    },
                    {
                        "name": "Lokesh Siddhu"
                    },
                    {
                        "name": "Hassan Nassar"
                    },
                    {
                        "name": "Mahta Mayahinia"
                    },
                    {
                        "name": "Mehdi Baradaran Tahoori"
                    },
                    {
                        "name": "Jrg Henkel"
                    },
                    {
                        "name": "Nils Wilbert"
                    },
                    {
                        "name": "Stefan Wildermann"
                    },
                    {
                        "name": "Jrgen Teich"
                    }
                ],
                "author_detail": {
                    "name": "Jrgen Teich"
                },
                "author": "Jrgen Teich",
                "arxiv_comment": "DFG Priority Program 2377 - Disruptive Memory Technologies",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10167v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10167v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09921v1",
                "updated": "2025-02-14T05:19:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    5,
                    19,
                    46,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T05:19:46Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    5,
                    19,
                    46,
                    4,
                    45,
                    0
                ],
                "title": "INF^2: High-Throughput Generative Inference of Large Language Models\n  using Near-Storage Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "INF^2: High-Throughput Generative Inference of Large Language Models\n  using Near-Storage Processing"
                },
                "summary": "The growing memory and computational demands of large language models (LLMs)\nfor generative inference present significant challenges for practical\ndeployment. One promising solution to address these challenges is\noffloading-based batched inference, which leverages host memory and disk as an\nextended memory hierarchy for GPUs. While the approach cost-effectively enables\nLLM inference, its performance is limited by substantial I/O overhead,\nprimarily due to the large key-value (KV) cache sizes, which increase with\nbatch size and LLM context window length.\n  In this paper, we introduce INFerence-INFinity (INF^2), a framework that\nboosts generative inference throughput using computational storage devices\n(CSDs). The core of INF^2 is attention-near storage, which offloads\nmemory-intensive self-attention operations to near-storage accelerators,\nsignificantly reducing traffic through the system interconnect. We also propose\ndelayed KV cache writeback to hide storage write latency by delaying newly\ngenerated KV cache writes until the cache reaches sufficient size in system\nmemory. Additionally, we introduce cooperative X-cache, a technique designed to\nfurther trade off the remaining memory capacity for storage bandwidth. Our\nmethods effectively minimize idle time for computation, improving the overall\nthroughput.\n  To demonstrate the effectiveness of our approach, \\thiswork has been\nimplemented on PyTorch and evaluated on a real system. Our experiments show\nthat INF^2 achieves up to 3.46$\\times$ throughput improvement compared to\nstate-of-the-art baselines. We will open-source INF^2 to facilitate broader\nadoption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing memory and computational demands of large language models (LLMs)\nfor generative inference present significant challenges for practical\ndeployment. One promising solution to address these challenges is\noffloading-based batched inference, which leverages host memory and disk as an\nextended memory hierarchy for GPUs. While the approach cost-effectively enables\nLLM inference, its performance is limited by substantial I/O overhead,\nprimarily due to the large key-value (KV) cache sizes, which increase with\nbatch size and LLM context window length.\n  In this paper, we introduce INFerence-INFinity (INF^2), a framework that\nboosts generative inference throughput using computational storage devices\n(CSDs). The core of INF^2 is attention-near storage, which offloads\nmemory-intensive self-attention operations to near-storage accelerators,\nsignificantly reducing traffic through the system interconnect. We also propose\ndelayed KV cache writeback to hide storage write latency by delaying newly\ngenerated KV cache writes until the cache reaches sufficient size in system\nmemory. Additionally, we introduce cooperative X-cache, a technique designed to\nfurther trade off the remaining memory capacity for storage bandwidth. Our\nmethods effectively minimize idle time for computation, improving the overall\nthroughput.\n  To demonstrate the effectiveness of our approach, \\thiswork has been\nimplemented on PyTorch and evaluated on a real system. Our experiments show\nthat INF^2 achieves up to 3.46$\\times$ throughput improvement compared to\nstate-of-the-art baselines. We will open-source INF^2 to facilitate broader\nadoption."
                },
                "authors": [
                    {
                        "name": "Hongsun Jang"
                    },
                    {
                        "name": "Siung Noh"
                    },
                    {
                        "name": "Changmin Shin"
                    },
                    {
                        "name": "Jaewon Jung"
                    },
                    {
                        "name": "Jaeyong Song"
                    },
                    {
                        "name": "Jinho Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jinho Lee"
                },
                "author": "Jinho Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09888v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09888v1",
                "updated": "2025-02-14T03:25:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    3,
                    25,
                    9,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T03:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    3,
                    25,
                    9,
                    4,
                    45,
                    0
                ],
                "title": "An Efficient Large Recommendation Model: Towards a Resource-Optimal\n  Scaling Law",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Large Recommendation Model: Towards a Resource-Optimal\n  Scaling Law"
                },
                "summary": "The pursuit of scaling up recommendation models confronts intrinsic tensions\nbetween expanding model capacity and preserving computational tractability.\nWhile prior studies have explored scaling laws for recommendation systems,\ntheir resource-intensive paradigms -- often requiring tens of thousands of A100\nGPU hours -- remain impractical for most industrial applications. This work\naddresses a critical gap: achieving sustainable model scaling under strict\ncomputational budgets. We propose Climber, a resource-efficient recommendation\nframework comprising two synergistic components: the ASTRO model architecture\nfor algorithmic innovation and the TURBO acceleration framework for engineering\noptimization. ASTRO (Adaptive Scalable Transformer for RecOmmendation) adopts\ntwo core innovations: (1) multi-scale sequence partitioning that reduces\nattention complexity from O(n^2d) to O(n^2d/Nb) via hierarchical blocks,\nenabling more efficient scaling with sequence length; (2) dynamic temperature\nmodulation that adaptively adjusts attention scores for multimodal\ndistributions arising from inherent multi-scenario and multi-behavior\ninteractions. Complemented by TURBO (Two-stage Unified Ranking with Batched\nOutput), a co-designed acceleration framework integrating gradient-aware\nfeature compression and memory-efficient Key-Value caching, Climber achieves\n5.15x throughput gains without performance degradation. Comprehensive offline\nexperiments on multiple datasets validate that Climber exhibits a more ideal\nscaling curve. To our knowledge, this is the first publicly documented\nframework where controlled model scaling drives continuous online metric growth\n(12.19% overall lift) without prohibitive resource costs. Climber has been\nsuccessfully deployed on Netease Cloud Music, one of China's largest music\nstreaming platforms, serving tens of millions of users daily.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pursuit of scaling up recommendation models confronts intrinsic tensions\nbetween expanding model capacity and preserving computational tractability.\nWhile prior studies have explored scaling laws for recommendation systems,\ntheir resource-intensive paradigms -- often requiring tens of thousands of A100\nGPU hours -- remain impractical for most industrial applications. This work\naddresses a critical gap: achieving sustainable model scaling under strict\ncomputational budgets. We propose Climber, a resource-efficient recommendation\nframework comprising two synergistic components: the ASTRO model architecture\nfor algorithmic innovation and the TURBO acceleration framework for engineering\noptimization. ASTRO (Adaptive Scalable Transformer for RecOmmendation) adopts\ntwo core innovations: (1) multi-scale sequence partitioning that reduces\nattention complexity from O(n^2d) to O(n^2d/Nb) via hierarchical blocks,\nenabling more efficient scaling with sequence length; (2) dynamic temperature\nmodulation that adaptively adjusts attention scores for multimodal\ndistributions arising from inherent multi-scenario and multi-behavior\ninteractions. Complemented by TURBO (Two-stage Unified Ranking with Batched\nOutput), a co-designed acceleration framework integrating gradient-aware\nfeature compression and memory-efficient Key-Value caching, Climber achieves\n5.15x throughput gains without performance degradation. Comprehensive offline\nexperiments on multiple datasets validate that Climber exhibits a more ideal\nscaling curve. To our knowledge, this is the first publicly documented\nframework where controlled model scaling drives continuous online metric growth\n(12.19% overall lift) without prohibitive resource costs. Climber has been\nsuccessfully deployed on Netease Cloud Music, one of China's largest music\nstreaming platforms, serving tens of millions of users daily."
                },
                "authors": [
                    {
                        "name": "Songpei Xu"
                    },
                    {
                        "name": "Shijia Wang"
                    },
                    {
                        "name": "Da Guo"
                    },
                    {
                        "name": "Xianwen Guo"
                    },
                    {
                        "name": "Qiang Xiao"
                    },
                    {
                        "name": "Fangjian Li"
                    },
                    {
                        "name": "Chuanjiang Luo"
                    }
                ],
                "author_detail": {
                    "name": "Chuanjiang Luo"
                },
                "author": "Chuanjiang Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09888v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09888v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09726v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09726v1",
                "updated": "2025-02-13T19:16:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    16,
                    39,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T19:16:39Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    16,
                    39,
                    3,
                    44,
                    0
                ],
                "title": "Analysis of Robust and Secure DNS Protocols for IoT Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis of Robust and Secure DNS Protocols for IoT Devices"
                },
                "summary": "The DNS (Domain Name System) protocol has been in use since the early days of\nthe Internet. Although DNS as a de facto networking protocol had no security\nconsiderations in its early years, there have been many security enhancements,\nsuch as DNSSec (Domain Name System Security Extensions), DoT (DNS over\nTransport Layer Security), DoH (DNS over HTTPS) and DoQ (DNS over QUIC). With\nall these security improvements, it is not yet clear what resource-constrained\nInternet-of-Things (IoT) devices should be used for robustness. In this paper,\nwe investigate different DNS security approaches using an edge DNS resolver\nimplemented as a Virtual Network Function (VNF) to replicate the impact of the\nprotocol from an IoT perspective and compare their performances under different\nconditions. We present our results for cache-based and non-cached responses and\nevaluate the corresponding security benefits. Our results and framework can\ngreatly help consumers, manufacturers, and the research community decide and\nimplement their DNS protocols depending on the given dynamic network conditions\nand enable robust Internet access via DNS for different devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The DNS (Domain Name System) protocol has been in use since the early days of\nthe Internet. Although DNS as a de facto networking protocol had no security\nconsiderations in its early years, there have been many security enhancements,\nsuch as DNSSec (Domain Name System Security Extensions), DoT (DNS over\nTransport Layer Security), DoH (DNS over HTTPS) and DoQ (DNS over QUIC). With\nall these security improvements, it is not yet clear what resource-constrained\nInternet-of-Things (IoT) devices should be used for robustness. In this paper,\nwe investigate different DNS security approaches using an edge DNS resolver\nimplemented as a Virtual Network Function (VNF) to replicate the impact of the\nprotocol from an IoT perspective and compare their performances under different\nconditions. We present our results for cache-based and non-cached responses and\nevaluate the corresponding security benefits. Our results and framework can\ngreatly help consumers, manufacturers, and the research community decide and\nimplement their DNS protocols depending on the given dynamic network conditions\nand enable robust Internet access via DNS for different devices."
                },
                "authors": [
                    {
                        "name": "Abdullah Aydeger"
                    },
                    {
                        "name": "Sanzida Hoque"
                    },
                    {
                        "name": "Engin Zeydan"
                    },
                    {
                        "name": "Kapal Dev"
                    }
                ],
                "author_detail": {
                    "name": "Kapal Dev"
                },
                "author": "Kapal Dev",
                "arxiv_comment": "6 pages, 2 tables, 2 figures. This paper has been accepted in the\n  2025 IEEE International Conference on Communications (ICC): SAC Cloud\n  Computing, Networking, and Storage Track. The final version will be published\n  in the IEEE Xplore",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09726v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09720v1",
                "updated": "2025-02-13T19:11:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    11,
                    40,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T19:11:40Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    11,
                    40,
                    3,
                    44,
                    0
                ],
                "title": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs"
                },
                "summary": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent work have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Meta's SpinQuant (perplexity 7.3). Comparisons on various\nLLM evaluation benchmarks also show a reduction in performance degradation\ninduced by quantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent work have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Meta's SpinQuant (perplexity 7.3). Comparisons on various\nLLM evaluation benchmarks also show a reduction in performance degradation\ninduced by quantization."
                },
                "authors": [
                    {
                        "name": "Semyon Savkin"
                    },
                    {
                        "name": "Eitan Porat"
                    },
                    {
                        "name": "Or Ordentlich"
                    },
                    {
                        "name": "Yury Polyanskiy"
                    }
                ],
                "author_detail": {
                    "name": "Yury Polyanskiy"
                },
                "author": "Yury Polyanskiy",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07864v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07864v2",
                "updated": "2025-02-13T18:07:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    7,
                    4,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-11T18:20:18Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    20,
                    18,
                    1,
                    42,
                    0
                ],
                "title": "TransMLA: Multi-Head Latent Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TransMLA: Multi-Head Latent Attention Is All You Need"
                },
                "summary": "Modern large language models (LLMs) often encounter communication bottlenecks\non current hardware, rather than purely computational constraints. Multi-head\nLatent Attention (MLA) tackles this challenge by using low-rank matrices in the\nkey-value (KV) layers, thereby allowing compressed latent KV states to be\ncached. This approach significantly reduces the KV cache size relative to\ntraditional multi-head attention, leading to faster inference. Moreover, MLA\nemploys an up-projection matrix to increase expressiveness, trading additional\ncomputation for reduced communication overhead. Although MLA has demonstrated\nefficiency and effectiveness in Deepseek V2/V3/R1, many major model providers\nstill rely on Group Query Attention (GQA) and have not announced any plans to\nadopt MLA. In this paper, we show that GQA can always be represented by MLA\nwhile maintaining the same KV cache overhead, but the converse does not hold.\nTo encourage broader use of MLA, we introduce TransMLA, a post-training method\nthat converts widely used GQA-based pre-trained models (e.g., LLaMA, Qwen,\nMixtral) into MLA-based models. After conversion, the model can undergo\nadditional training to boost expressiveness without increasing the KV cache\nsize. Furthermore, we plan to develop MLA-specific inference acceleration\ntechniques to preserve low latency in transformed models, thus enabling more\nefficient distillation of Deepseek R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language models (LLMs) often encounter communication bottlenecks\non current hardware, rather than purely computational constraints. Multi-head\nLatent Attention (MLA) tackles this challenge by using low-rank matrices in the\nkey-value (KV) layers, thereby allowing compressed latent KV states to be\ncached. This approach significantly reduces the KV cache size relative to\ntraditional multi-head attention, leading to faster inference. Moreover, MLA\nemploys an up-projection matrix to increase expressiveness, trading additional\ncomputation for reduced communication overhead. Although MLA has demonstrated\nefficiency and effectiveness in Deepseek V2/V3/R1, many major model providers\nstill rely on Group Query Attention (GQA) and have not announced any plans to\nadopt MLA. In this paper, we show that GQA can always be represented by MLA\nwhile maintaining the same KV cache overhead, but the converse does not hold.\nTo encourage broader use of MLA, we introduce TransMLA, a post-training method\nthat converts widely used GQA-based pre-trained models (e.g., LLaMA, Qwen,\nMixtral) into MLA-based models. After conversion, the model can undergo\nadditional training to boost expressiveness without increasing the KV cache\nsize. Furthermore, we plan to develop MLA-specific inference acceleration\ntechniques to preserve low latency in transformed models, thus enabling more\nefficient distillation of Deepseek R1."
                },
                "authors": [
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Zengwei Yao"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "arxiv_comment": "https://github.com/fxmeng/TransMLA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07864v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07864v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09541v1",
                "updated": "2025-02-13T17:57:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    17,
                    57,
                    5,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T17:57:05Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    17,
                    57,
                    5,
                    3,
                    44,
                    0
                ],
                "title": "Vortex: Overcoming Memory Capacity Limitations in GPU-Accelerated\n  Large-Scale Data Analytics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vortex: Overcoming Memory Capacity Limitations in GPU-Accelerated\n  Large-Scale Data Analytics"
                },
                "summary": "Despite the high computational throughput of GPUs, limited memory capacity\nand bandwidth-limited CPU-GPU communication via PCIe links remain significant\nbottlenecks for accelerating large-scale data analytics workloads. This paper\nintroduces Vortex, a GPU-accelerated framework designed for data analytics\nworkloads that exceed GPU memory capacity. A key aspect of our framework is an\noptimized IO primitive that leverages all available PCIe links in multi-GPU\nsystems for the IO demand of a single target GPU. It routes data through other\nGPUs to such target GPU that handles IO-intensive analytics tasks. This\napproach is advantageous when other GPUs are occupied with compute-bound\nworkloads, such as popular AI applications that typically underutilize IO\nresources. We also introduce a novel programming model that separates GPU\nkernel development from IO scheduling, reducing programmer burden and enabling\nGPU code reuse. Additionally, we present the design of certain important query\noperators and discuss a late materialization technique based on GPU's zero-copy\nmemory access. Without caching any data in GPU memory, Vortex improves the\nperformance of the state-of-the-art GPU baseline, Proteus, by 5.7$\\times$ on\naverage and enhances price performance by 2.5$\\times$ compared to a CPU-based\nDuckDB baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the high computational throughput of GPUs, limited memory capacity\nand bandwidth-limited CPU-GPU communication via PCIe links remain significant\nbottlenecks for accelerating large-scale data analytics workloads. This paper\nintroduces Vortex, a GPU-accelerated framework designed for data analytics\nworkloads that exceed GPU memory capacity. A key aspect of our framework is an\noptimized IO primitive that leverages all available PCIe links in multi-GPU\nsystems for the IO demand of a single target GPU. It routes data through other\nGPUs to such target GPU that handles IO-intensive analytics tasks. This\napproach is advantageous when other GPUs are occupied with compute-bound\nworkloads, such as popular AI applications that typically underutilize IO\nresources. We also introduce a novel programming model that separates GPU\nkernel development from IO scheduling, reducing programmer burden and enabling\nGPU code reuse. Additionally, we present the design of certain important query\noperators and discuss a late materialization technique based on GPU's zero-copy\nmemory access. Without caching any data in GPU memory, Vortex improves the\nperformance of the state-of-the-art GPU baseline, Proteus, by 5.7$\\times$ on\naverage and enhances price performance by 2.5$\\times$ compared to a CPU-based\nDuckDB baseline."
                },
                "authors": [
                    {
                        "name": "Yichao Yuan"
                    },
                    {
                        "name": "Advait Iyer"
                    },
                    {
                        "name": "Lin Ma"
                    },
                    {
                        "name": "Nishil Talati"
                    }
                ],
                "author_detail": {
                    "name": "Nishil Talati"
                },
                "author": "Nishil Talati",
                "arxiv_comment": "VLDB 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07115v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07115v2",
                "updated": "2025-02-13T12:54:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    12,
                    54,
                    36,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-10T23:11:44Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    23,
                    11,
                    44,
                    0,
                    41,
                    0
                ],
                "title": "Online Scheduling for LLM Inference with KV Cache Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Scheduling for LLM Inference with KV Cache Constraints"
                },
                "summary": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment."
                },
                "authors": [
                    {
                        "name": "Patrick Jaillet"
                    },
                    {
                        "name": "Jiashuo Jiang"
                    },
                    {
                        "name": "Chara Podimata"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07115v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07115v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09003v1",
                "updated": "2025-02-13T06:44:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    6,
                    44,
                    33,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T06:44:33Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    6,
                    44,
                    33,
                    3,
                    44,
                    0
                ],
                "title": "RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach\n  for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach\n  for Large Language Models"
                },
                "summary": "Supervised fine-tuning is a standard method for adapting pre-trained large\nlanguage models (LLMs) to downstream tasks. Quantization has been recently\nstudied as a post-training technique for efficient LLM deployment. To obtain\nquantized fine-tuned LLMs, conventional pipelines would first fine-tune the\npre-trained models, followed by post-training quantization. This often yields\nsuboptimal performance as it fails to leverage the synergy between fine-tuning\nand quantization. To effectively realize low-bit quantization of weights,\nactivations, and KV caches in LLMs, we propose an algorithm named Rotated\nStraight-Through-Estimator (RoSTE), which combines quantization-aware\nsupervised fine-tuning (QA-SFT) with an adaptive rotation strategy that\nidentifies an effective rotation configuration to reduce activation outliers.\nWe provide theoretical insights on RoSTE by analyzing its prediction error when\napplied to an overparameterized least square quantized training problem. Our\nfindings reveal that the prediction error is directly proportional to the\nquantization error of the converged weights, which can be effectively managed\nthrough an optimized rotation configuration. Experiments on Pythia and Llama\nmodels of different sizes demonstrate the effectiveness of RoSTE. Compared to\nexisting post-SFT quantization baselines, our method consistently achieves\nsuperior performances across various tasks and different LLM architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised fine-tuning is a standard method for adapting pre-trained large\nlanguage models (LLMs) to downstream tasks. Quantization has been recently\nstudied as a post-training technique for efficient LLM deployment. To obtain\nquantized fine-tuned LLMs, conventional pipelines would first fine-tune the\npre-trained models, followed by post-training quantization. This often yields\nsuboptimal performance as it fails to leverage the synergy between fine-tuning\nand quantization. To effectively realize low-bit quantization of weights,\nactivations, and KV caches in LLMs, we propose an algorithm named Rotated\nStraight-Through-Estimator (RoSTE), which combines quantization-aware\nsupervised fine-tuning (QA-SFT) with an adaptive rotation strategy that\nidentifies an effective rotation configuration to reduce activation outliers.\nWe provide theoretical insights on RoSTE by analyzing its prediction error when\napplied to an overparameterized least square quantized training problem. Our\nfindings reveal that the prediction error is directly proportional to the\nquantization error of the converged weights, which can be effectively managed\nthrough an optimized rotation configuration. Experiments on Pythia and Llama\nmodels of different sizes demonstrate the effectiveness of RoSTE. Compared to\nexisting post-SFT quantization baselines, our method consistently achieves\nsuperior performances across various tasks and different LLM architectures."
                },
                "authors": [
                    {
                        "name": "Quan Wei"
                    },
                    {
                        "name": "Chung-Yiu Yau"
                    },
                    {
                        "name": "Hoi-To Wai"
                    },
                    {
                        "name": "Yang"
                    },
                    {
                        "name": "Zhao"
                    },
                    {
                        "name": "Dongyeop Kang"
                    },
                    {
                        "name": "Youngsuk Park"
                    },
                    {
                        "name": "Mingyi Hong"
                    }
                ],
                "author_detail": {
                    "name": "Mingyi Hong"
                },
                "arxiv_affiliation": "Katie",
                "author": "Mingyi Hong",
                "arxiv_comment": "18 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08982v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08982v1",
                "updated": "2025-02-13T05:40:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    5,
                    40,
                    28,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T05:40:28Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    5,
                    40,
                    28,
                    3,
                    44,
                    0
                ],
                "title": "Outback: Fast and Communication-efficient Index for Key-Value Store on\n  Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Outback: Fast and Communication-efficient Index for Key-Value Store on\n  Disaggregated Memory"
                },
                "summary": "Disaggregated memory systems achieve resource utilization efficiency and\nsystem scalability by distributing computation and memory resources into\ndistinct pools of nodes. RDMA is an attractive solution to support\nhigh-throughput communication between different disaggregated resource pools.\nHowever, existing RDMA solutions face a dilemma: one-sided RDMA completely\nbypasses computation at memory nodes, but its communication takes multiple\nround trips; two-sided RDMA achieves one-round-trip communication but requires\nnon-trivial computation for index lookups at memory nodes, which violates the\nprinciple of disaggregated memory. This work presents Outback, a novel indexing\nsolution for key-value stores with a one-round-trip RDMA-based network that\ndoes not incur computation-heavy tasks at memory nodes. Outback is the first to\nutilize dynamic minimal perfect hashing and separates its index into two\ncomponents: one memory-efficient and compute-heavy component at compute nodes\nand the other memory-heavy and compute-efficient component at memory nodes. We\nimplement a prototype of Outback and evaluate its performance in a public\ncloud. The experimental results show that Outback achieves higher throughput\nthan both the state-of-the-art one-sided RDMA and two-sided RDMA-based\nin-memory KVS by 1.06-5.03x, due to the unique strength of applying a separated\nperfect hashing index.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated memory systems achieve resource utilization efficiency and\nsystem scalability by distributing computation and memory resources into\ndistinct pools of nodes. RDMA is an attractive solution to support\nhigh-throughput communication between different disaggregated resource pools.\nHowever, existing RDMA solutions face a dilemma: one-sided RDMA completely\nbypasses computation at memory nodes, but its communication takes multiple\nround trips; two-sided RDMA achieves one-round-trip communication but requires\nnon-trivial computation for index lookups at memory nodes, which violates the\nprinciple of disaggregated memory. This work presents Outback, a novel indexing\nsolution for key-value stores with a one-round-trip RDMA-based network that\ndoes not incur computation-heavy tasks at memory nodes. Outback is the first to\nutilize dynamic minimal perfect hashing and separates its index into two\ncomponents: one memory-efficient and compute-heavy component at compute nodes\nand the other memory-heavy and compute-efficient component at memory nodes. We\nimplement a prototype of Outback and evaluate its performance in a public\ncloud. The experimental results show that Outback achieves higher throughput\nthan both the state-of-the-art one-sided RDMA and two-sided RDMA-based\nin-memory KVS by 1.06-5.03x, due to the unique strength of applying a separated\nperfect hashing index."
                },
                "authors": [
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Minghao Xie"
                    },
                    {
                        "name": "Shouqian Shi"
                    },
                    {
                        "name": "Yuanchao Xu"
                    },
                    {
                        "name": "Heiner Litz"
                    },
                    {
                        "name": "Chen Qian"
                    }
                ],
                "author_detail": {
                    "name": "Chen Qian"
                },
                "author": "Chen Qian",
                "arxiv_doi": "10.14778/3705829.3705849",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.14778/3705829.3705849",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.08982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08982v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "PVLDB, 18(2): 335-348, 2024",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08910v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08910v1",
                "updated": "2025-02-13T02:52:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    2,
                    52,
                    1,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T02:52:01Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    2,
                    52,
                    1,
                    3,
                    44,
                    0
                ],
                "title": "InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on\n  a Single GPU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on\n  a Single GPU"
                },
                "summary": "In modern large language models (LLMs), handling very long context lengths\npresents significant challenges as it causes slower inference speeds and\nincreased memory costs. Additionally, most existing pre-trained LLMs fail to\ngeneralize beyond their original training sequence lengths. To enable efficient\nand practical long-context utilization, we introduce InfiniteHiP, a novel, and\npractical LLM inference framework that accelerates processing by dynamically\neliminating irrelevant context tokens through a modular hierarchical token\npruning algorithm. Our method also allows generalization to longer sequences by\nselectively applying various RoPE adjustment methods according to the internal\nattention patterns within LLMs. Furthermore, we offload the key-value cache to\nhost memory during inference, significantly reducing GPU memory pressure. As a\nresult, InfiniteHiP enables the processing of up to 3 million tokens on a\nsingle L40s 48GB GPU -- 3x larger -- without any permanent loss of context\ninformation. Our framework achieves an 18.95x speedup in attention decoding for\na 1 million token context without requiring additional training. We implement\nour method in the SGLang framework and demonstrate its effectiveness and\npracticality through extensive evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern large language models (LLMs), handling very long context lengths\npresents significant challenges as it causes slower inference speeds and\nincreased memory costs. Additionally, most existing pre-trained LLMs fail to\ngeneralize beyond their original training sequence lengths. To enable efficient\nand practical long-context utilization, we introduce InfiniteHiP, a novel, and\npractical LLM inference framework that accelerates processing by dynamically\neliminating irrelevant context tokens through a modular hierarchical token\npruning algorithm. Our method also allows generalization to longer sequences by\nselectively applying various RoPE adjustment methods according to the internal\nattention patterns within LLMs. Furthermore, we offload the key-value cache to\nhost memory during inference, significantly reducing GPU memory pressure. As a\nresult, InfiniteHiP enables the processing of up to 3 million tokens on a\nsingle L40s 48GB GPU -- 3x larger -- without any permanent loss of context\ninformation. Our framework achieves an 18.95x speedup in attention decoding for\na 1 million token context without requiring additional training. We implement\nour method in the SGLang framework and demonstrate its effectiveness and\npracticality through extensive evaluations."
                },
                "authors": [
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Geon Park"
                    },
                    {
                        "name": "Jaduk Suh"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08910v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08910v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02690v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02690v2",
                "updated": "2025-02-12T14:32:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    14,
                    32,
                    46,
                    2,
                    43,
                    0
                ],
                "published": "2024-04-03T12:37:34Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    12,
                    37,
                    34,
                    2,
                    94,
                    0
                ],
                "title": "How Sparse Attention Approximates Exact Attention? Your Attention is\n  Naturally $n^C$-Sparse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Sparse Attention Approximates Exact Attention? Your Attention is\n  Naturally $n^C$-Sparse"
                },
                "summary": "Sparse Attention is a technique that approximates standard attention\ncomputation with sub-quadratic complexity. This is achieved by selectively\nignoring smaller entries in the attention matrix during the softmax function\ncomputation. Variations of this technique, such as pruning KV cache,\nsparsity-based fast attention, and Sparse Transformer, have been extensively\nutilized for efficient Large Language Models (LLMs) deployment. Despite its\nwidespread use, a theoretical understanding of the conditions under which\nsparse attention performs on par with traditional attention remains elusive.\nThis work aims to $\\textbf{bridge this gap by examining the inherent sparsity\nof standard attention processes}$. Our theoretical framework reveals several\nbrand-new key insights:\n  $\\bullet$ Attention is $n^{C}$-sparse, implying that considering only the\nlargest $\\Omega(n^{C})$ entries out of all $n$ entries is sufficient for sparse\nattention to approximate the exact attention matrix with decreasing loss. Here,\n$n$ represents the input length and $C \\in (0, 1)$ is a constant.\n  $\\bullet$ Stable $o(\\log(n))$-sparse attention, which approximates attention\ncomputation with $\\log(n)$ or fewer entries, may not be feasible since the\nerror will persist at a minimum of $O(1)$.\n  $\\bullet$ An adaptive strategy ($\\alpha \\cdot n^C, \\alpha \\in \\mathbb{R}$)\nfor the window size of efficient attention methods rather than a fixed one is\nguaranteed to perform more accurately and efficiently in a task for inference\non flexible context lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Attention is a technique that approximates standard attention\ncomputation with sub-quadratic complexity. This is achieved by selectively\nignoring smaller entries in the attention matrix during the softmax function\ncomputation. Variations of this technique, such as pruning KV cache,\nsparsity-based fast attention, and Sparse Transformer, have been extensively\nutilized for efficient Large Language Models (LLMs) deployment. Despite its\nwidespread use, a theoretical understanding of the conditions under which\nsparse attention performs on par with traditional attention remains elusive.\nThis work aims to $\\textbf{bridge this gap by examining the inherent sparsity\nof standard attention processes}$. Our theoretical framework reveals several\nbrand-new key insights:\n  $\\bullet$ Attention is $n^{C}$-sparse, implying that considering only the\nlargest $\\Omega(n^{C})$ entries out of all $n$ entries is sufficient for sparse\nattention to approximate the exact attention matrix with decreasing loss. Here,\n$n$ represents the input length and $C \\in (0, 1)$ is a constant.\n  $\\bullet$ Stable $o(\\log(n))$-sparse attention, which approximates attention\ncomputation with $\\log(n)$ or fewer entries, may not be feasible since the\nerror will persist at a minimum of $O(1)$.\n  $\\bullet$ An adaptive strategy ($\\alpha \\cdot n^C, \\alpha \\in \\mathbb{R}$)\nfor the window size of efficient attention methods rather than a fixed one is\nguaranteed to perform more accurately and efficiently in a task for inference\non flexible context lengths."
                },
                "authors": [
                    {
                        "name": "Yichuan Deng"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Chiwun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Chiwun Yang"
                },
                "author": "Chiwun Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02690v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02690v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05431v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05431v2",
                "updated": "2025-02-12T13:54:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    13,
                    54,
                    1,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-08T03:41:16Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    41,
                    16,
                    5,
                    39,
                    0
                ],
                "title": "APE: Faster and Longer Context-Augmented Generation via Adaptive\n  Parallel Encoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APE: Faster and Longer Context-Augmented Generation via Adaptive\n  Parallel Encoding"
                },
                "summary": "Context-augmented generation (CAG) techniques, including RAG and ICL, require\nthe efficient combination of multiple contexts to generate responses to user\nqueries. Directly inputting these contexts as a sequence introduces a\nconsiderable computational burden by re-encoding the combined selection of\ncontexts for every request. To address this, we explore the promising potential\nof parallel encoding to independently pre-compute and cache each context's KV\nstates. This approach enables the direct loading of cached states during\ninference while accommodating more contexts through position reuse across\ncontexts. However, due to misalignments in attention distribution, directly\napplying parallel encoding results in a significant performance drop. To enable\neffective and efficient CAG, we propose Adaptive Parallel Encoding\n($\\textbf{APE}$), which brings shared prefix, attention temperature, and\nscaling factor to align the distribution of parallel encoding with sequential\nencoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98%\nand 93% sequential encoding performance using the same inputs while\noutperforming parallel encoding by 3.6% and 7.9%, respectively. It also scales\nto many-shot CAG, effectively encoding hundreds of contexts in parallel.\nEfficiency evaluation shows that APE can achieve an end-to-end 4.5$\\times$\nspeedup by reducing 28$\\times$ prefilling time for a 128K-length context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-augmented generation (CAG) techniques, including RAG and ICL, require\nthe efficient combination of multiple contexts to generate responses to user\nqueries. Directly inputting these contexts as a sequence introduces a\nconsiderable computational burden by re-encoding the combined selection of\ncontexts for every request. To address this, we explore the promising potential\nof parallel encoding to independently pre-compute and cache each context's KV\nstates. This approach enables the direct loading of cached states during\ninference while accommodating more contexts through position reuse across\ncontexts. However, due to misalignments in attention distribution, directly\napplying parallel encoding results in a significant performance drop. To enable\neffective and efficient CAG, we propose Adaptive Parallel Encoding\n($\\textbf{APE}$), which brings shared prefix, attention temperature, and\nscaling factor to align the distribution of parallel encoding with sequential\nencoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98%\nand 93% sequential encoding performance using the same inputs while\noutperforming parallel encoding by 3.6% and 7.9%, respectively. It also scales\nto many-shot CAG, effectively encoding hundreds of contexts in parallel.\nEfficiency evaluation shows that APE can achieve an end-to-end 4.5$\\times$\nspeedup by reducing 28$\\times$ prefilling time for a 128K-length context."
                },
                "authors": [
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05431v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05431v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08363v1",
                "updated": "2025-02-12T12:50:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    12,
                    50,
                    15,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T12:50:15Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    12,
                    50,
                    15,
                    2,
                    43,
                    0
                ],
                "title": "Top-Theta Attention: Sparsifying Transformers by Compensated\n  Thresholding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Top-Theta Attention: Sparsifying Transformers by Compensated\n  Thresholding"
                },
                "summary": "The attention mechanism is essential for the impressive capabilities of\ntransformer-based Large Language Models (LLMs). However, calculating attention\nis computationally intensive due to its quadratic dependency on the sequence\nlength. We introduce a novel approach called Top-Theta Attention, or simply\nTop-$\\theta$, which selectively prunes less essential attention elements by\ncomparing them against carefully calibrated thresholds. This method greatly\nimproves the efficiency of self-attention matrix multiplication while\npreserving model accuracy, reducing the number of required V cache rows by 3x\nduring generative decoding and the number of attention elements by 10x during\nthe prefill phase. Our method does not require model retraining; instead, it\nrequires only a brief calibration phase to be resilient to distribution shifts,\nthus not requiring the thresholds for different datasets to be recalibrated.\nUnlike top-k attention, Top-$\\theta$ eliminates full-vector dependency, making\nit suitable for tiling and scale-out and avoiding costly top-k search. A key\ninnovation of our approach is the development of efficient numerical\ncompensation techniques, which help preserve model accuracy even under\naggressive pruning of attention scores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The attention mechanism is essential for the impressive capabilities of\ntransformer-based Large Language Models (LLMs). However, calculating attention\nis computationally intensive due to its quadratic dependency on the sequence\nlength. We introduce a novel approach called Top-Theta Attention, or simply\nTop-$\\theta$, which selectively prunes less essential attention elements by\ncomparing them against carefully calibrated thresholds. This method greatly\nimproves the efficiency of self-attention matrix multiplication while\npreserving model accuracy, reducing the number of required V cache rows by 3x\nduring generative decoding and the number of attention elements by 10x during\nthe prefill phase. Our method does not require model retraining; instead, it\nrequires only a brief calibration phase to be resilient to distribution shifts,\nthus not requiring the thresholds for different datasets to be recalibrated.\nUnlike top-k attention, Top-$\\theta$ eliminates full-vector dependency, making\nit suitable for tiling and scale-out and avoiding costly top-k search. A key\ninnovation of our approach is the development of efficient numerical\ncompensation techniques, which help preserve model accuracy even under\naggressive pruning of attention scores."
                },
                "authors": [
                    {
                        "name": "Konstantin Berestizshevsky"
                    },
                    {
                        "name": "Renzo Andri"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Cavigelli"
                },
                "author": "Lukas Cavigelli",
                "arxiv_comment": "8 pages, 11 figures, work under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T01",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16909v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16909v2",
                "updated": "2025-02-12T11:05:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    11,
                    5,
                    5,
                    2,
                    43,
                    0
                ],
                "published": "2025-01-28T12:57:53Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    12,
                    57,
                    53,
                    1,
                    28,
                    0
                ],
                "title": "Measuring GPU utilization one level deeper",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring GPU utilization one level deeper"
                },
                "summary": "GPU hardware is vastly underutilized. Even resource-intensive AI applications\nhave diverse resource profiles that often leave parts of GPUs idle. While\ncolocating applications can improve utilization, current spatial sharing\nsystems lack performance guarantees. Providing predictable performance\nguarantees requires a deep understanding of how applications contend for shared\nGPU resources such as block schedulers, compute units, L1/L2 caches, and memory\nbandwidth. We propose a methodology to profile resource interference of GPU\nkernels across these dimensions and discuss how to build GPU schedulers that\nprovide strict performance guarantees while colocating applications to minimize\ncost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU hardware is vastly underutilized. Even resource-intensive AI applications\nhave diverse resource profiles that often leave parts of GPUs idle. While\ncolocating applications can improve utilization, current spatial sharing\nsystems lack performance guarantees. Providing predictable performance\nguarantees requires a deep understanding of how applications contend for shared\nGPU resources such as block schedulers, compute units, L1/L2 caches, and memory\nbandwidth. We propose a methodology to profile resource interference of GPU\nkernels across these dimensions and discuss how to build GPU schedulers that\nprovide strict performance guarantees while colocating applications to minimize\ncost."
                },
                "authors": [
                    {
                        "name": "Paul Elvinger"
                    },
                    {
                        "name": "Foteini Strati"
                    },
                    {
                        "name": "Natalie Enright Jerger"
                    },
                    {
                        "name": "Ana Klimovic"
                    }
                ],
                "author_detail": {
                    "name": "Ana Klimovic"
                },
                "author": "Ana Klimovic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16909v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16909v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v3",
                "updated": "2025-02-12T07:02:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    7,
                    2,
                    6,
                    2,
                    43,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_comment": "This work was submitted for review on Sept. 5, 2024, and the initial\n  version was uploaded to Arxiv on Sept. 30, 2024. The latest version reflects\n  the up-to-date experimental results",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07903v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07903v1",
                "updated": "2025-02-11T19:17:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    19,
                    17,
                    35,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T19:17:35Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    19,
                    17,
                    35,
                    1,
                    42,
                    0
                ],
                "title": "HexGen-2: Disaggregated Generative Inference of LLMs in Heterogeneous\n  Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HexGen-2: Disaggregated Generative Inference of LLMs in Heterogeneous\n  Environment"
                },
                "summary": "Disaggregating the prefill and decoding phases represents an effective new\nparadigm for generative inference of large language models (LLM), which\neliminates prefill-decoding interference and optimizes resource allocation.\nHowever, it is still an open problem about how to deploy the disaggregated\ninference paradigm across a group of heterogeneous GPUs, which can be an\neconomical alternative to deployment over homogeneous high-performance GPUs.\nTowards this end, we introduce HexGen-2, a distributed system for efficient and\neconomical LLM serving on heterogeneous GPUs following the disaggregated\nparadigm. Built on top of HexGen, the core component of HexGen-2 is a\nscheduling algorithm that formalizes the allocation of disaggregated LLM\ninference computations and communications over heterogeneous GPUs and network\nconnections as a constraint optimization problem. We leverage the graph\npartitioning and max-flow algorithms to co-optimize resource allocation,\nparallel strategies for distinct inference phases, and the efficiency of\ninter-phase key-value (KV) cache communications. We conduct extensive\nexperiments to evaluate HexGen-2, i.e., on OPT (30B) and Llama-2 (70B) models\nin various real-world settings, the results reveal that HexGen-2 delivers up to\na 2.0 times and on average a 1.3 times improvement in serving throughput,\nreduces the average inference latency by 1.5 times compared with\nstate-of-the-art systems given the same price budget, and achieves comparable\ninference performance with a 30% lower price budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregating the prefill and decoding phases represents an effective new\nparadigm for generative inference of large language models (LLM), which\neliminates prefill-decoding interference and optimizes resource allocation.\nHowever, it is still an open problem about how to deploy the disaggregated\ninference paradigm across a group of heterogeneous GPUs, which can be an\neconomical alternative to deployment over homogeneous high-performance GPUs.\nTowards this end, we introduce HexGen-2, a distributed system for efficient and\neconomical LLM serving on heterogeneous GPUs following the disaggregated\nparadigm. Built on top of HexGen, the core component of HexGen-2 is a\nscheduling algorithm that formalizes the allocation of disaggregated LLM\ninference computations and communications over heterogeneous GPUs and network\nconnections as a constraint optimization problem. We leverage the graph\npartitioning and max-flow algorithms to co-optimize resource allocation,\nparallel strategies for distinct inference phases, and the efficiency of\ninter-phase key-value (KV) cache communications. We conduct extensive\nexperiments to evaluate HexGen-2, i.e., on OPT (30B) and Llama-2 (70B) models\nin various real-world settings, the results reveal that HexGen-2 delivers up to\na 2.0 times and on average a 1.3 times improvement in serving throughput,\nreduces the average inference latency by 1.5 times compared with\nstate-of-the-art systems given the same price budget, and achieves comparable\ninference performance with a 30% lower price budget."
                },
                "authors": [
                    {
                        "name": "Youhe Jiang"
                    },
                    {
                        "name": "Ran Yan"
                    },
                    {
                        "name": "Binhang Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Binhang Yuan"
                },
                "author": "Binhang Yuan",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07903v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07903v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07776v1",
                "updated": "2025-02-11T18:58:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    58,
                    4,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T18:58:04Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    58,
                    4,
                    1,
                    42,
                    0
                ],
                "title": "Auditing Prompt Caching in Language Model APIs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auditing Prompt Caching in Language Model APIs"
                },
                "summary": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known."
                },
                "authors": [
                    {
                        "name": "Chenchen Gu"
                    },
                    {
                        "name": "Xiang Lisa Li"
                    },
                    {
                        "name": "Rohith Kuditipudi"
                    },
                    {
                        "name": "Percy Liang"
                    },
                    {
                        "name": "Tatsunori Hashimoto"
                    }
                ],
                "author_detail": {
                    "name": "Tatsunori Hashimoto"
                },
                "author": "Tatsunori Hashimoto",
                "arxiv_comment": "20 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19392v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19392v2",
                "updated": "2025-02-11T18:45:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    45,
                    12,
                    1,
                    42,
                    0
                ],
                "published": "2025-01-31T18:47:42Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    47,
                    42,
                    4,
                    31,
                    0
                ],
                "title": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models"
                },
                "summary": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models."
                },
                "authors": [
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "Vladimir Malinovskii"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Denis Mazur"
                    },
                    {
                        "name": "Nikita Surkov"
                    },
                    {
                        "name": "Ivan Ermakov"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19392v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19392v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03743v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03743v2",
                "updated": "2025-02-11T17:48:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    48,
                    15,
                    1,
                    42,
                    0
                ],
                "published": "2024-09-05T17:56:19Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    56,
                    19,
                    3,
                    249,
                    0
                ],
                "title": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)"
                },
                "summary": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%."
                },
                "authors": [
                    {
                        "name": "Hans Winderix"
                    },
                    {
                        "name": "Marton Bognar"
                    },
                    {
                        "name": "Lesly-Ann Daniel"
                    },
                    {
                        "name": "Frank Piessens"
                    }
                ],
                "author_detail": {
                    "name": "Frank Piessens"
                },
                "author": "Frank Piessens",
                "arxiv_doi": "10.1145/3658644.3690319",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3658644.3690319",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.03743v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03743v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11779v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11779v2",
                "updated": "2025-02-11T17:36:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    36,
                    32,
                    1,
                    42,
                    0
                ],
                "published": "2025-01-20T23:10:13Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    23,
                    10,
                    13,
                    0,
                    20,
                    0
                ],
                "title": "Glinthawk: A Two-Tiered Architecture for Offline LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Glinthawk: A Two-Tiered Architecture for Offline LLM Inference"
                },
                "summary": "We introduce Glinthawk, an architecture for offline Large Language Model\n(LLM) inference. By leveraging a two-tiered structure, Glinthawk optimizes the\nutilization of the high-end accelerators (\"Tier 1\") by offloading the attention\nmechanism to lower-end compute tier (\"Tier 2\"). This separation allows the\nmemory demand of the attention, known as the key-value cache, to scale\nindependently from the model weights, enabling larger batch sizes and more\nefficient accelerator usage. Prototyped with NVIDIA T4 GPUs and standard CPU\nVMs, Glinthawk improves throughput by $5.9\\times$ and reduces cost of\ngeneration by $2.8\\times$, compared to paged attention baselines. For long\nsequence lengths, it achieves $16.3\\times$ throughput improvement at\n$2.4\\times$ less cost. Our evaluation shows that this architecture can tolerate\nmoderate network latency with minimal performance degradation, making it highly\neffective for latency-tolerant, throughput-focused applications such as batch\nprocessing. The prototype is publicly available at\nhttps://github.com/microsoft/glinthawk.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Glinthawk, an architecture for offline Large Language Model\n(LLM) inference. By leveraging a two-tiered structure, Glinthawk optimizes the\nutilization of the high-end accelerators (\"Tier 1\") by offloading the attention\nmechanism to lower-end compute tier (\"Tier 2\"). This separation allows the\nmemory demand of the attention, known as the key-value cache, to scale\nindependently from the model weights, enabling larger batch sizes and more\nefficient accelerator usage. Prototyped with NVIDIA T4 GPUs and standard CPU\nVMs, Glinthawk improves throughput by $5.9\\times$ and reduces cost of\ngeneration by $2.8\\times$, compared to paged attention baselines. For long\nsequence lengths, it achieves $16.3\\times$ throughput improvement at\n$2.4\\times$ less cost. Our evaluation shows that this architecture can tolerate\nmoderate network latency with minimal performance degradation, making it highly\neffective for latency-tolerant, throughput-focused applications such as batch\nprocessing. The prototype is publicly available at\nhttps://github.com/microsoft/glinthawk."
                },
                "authors": [
                    {
                        "name": "Pouya Hamadanian"
                    },
                    {
                        "name": "Sadjad Fouladi"
                    }
                ],
                "author_detail": {
                    "name": "Sadjad Fouladi"
                },
                "author": "Sadjad Fouladi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11779v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11779v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07861v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07861v1",
                "updated": "2025-02-11T17:18:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    18,
                    17,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T17:18:17Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    18,
                    17,
                    1,
                    42,
                    0
                ],
                "title": "BalanceKV: KV Cache Compression through Discrepancy Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BalanceKV: KV Cache Compression through Discrepancy Theory"
                },
                "summary": "Large language models (LLMs) have achieved impressive success, but their high\nmemory requirements present challenges for long-context token generation. The\nmemory complexity of long-context LLMs is primarily due to the need to store\nKey-Value (KV) embeddings in their KV cache. We present BalanceKV, a KV cache\ncompression method based on geometric sampling process stemming from\nBanaszczyk's vector balancing theory, which introduces dependencies informed by\nthe geometry of keys and value tokens, and improves precision. BalanceKV offers\nboth theoretically proven and empirically validated performance improvements\nover existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved impressive success, but their high\nmemory requirements present challenges for long-context token generation. The\nmemory complexity of long-context LLMs is primarily due to the need to store\nKey-Value (KV) embeddings in their KV cache. We present BalanceKV, a KV cache\ncompression method based on geometric sampling process stemming from\nBanaszczyk's vector balancing theory, which introduces dependencies informed by\nthe geometry of keys and value tokens, and improves precision. BalanceKV offers\nboth theoretically proven and empirically validated performance improvements\nover existing methods."
                },
                "authors": [
                    {
                        "name": "Insu Han"
                    },
                    {
                        "name": "Michael Kapralov"
                    },
                    {
                        "name": "Ekaterina Kochetkova"
                    },
                    {
                        "name": "Kshiteej Sheth"
                    },
                    {
                        "name": "Amir Zandieh"
                    }
                ],
                "author_detail": {
                    "name": "Amir Zandieh"
                },
                "author": "Amir Zandieh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07861v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07861v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03736v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03736v3",
                "updated": "2025-02-11T15:42:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    15,
                    42,
                    19,
                    1,
                    42,
                    0
                ],
                "published": "2024-06-06T04:22:11Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    4,
                    22,
                    11,
                    3,
                    158,
                    0
                ],
                "title": "Your Absorbing Discrete Diffusion Secretly Models the Conditional\n  Distributions of Clean Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Your Absorbing Discrete Diffusion Secretly Models the Conditional\n  Distributions of Clean Data"
                },
                "summary": "Discrete diffusion models with absorbing processes have shown promise in\nlanguage modeling. The key quantities to be estimated are the ratios between\nthe marginal probabilities of two transitive states at all timesteps, called\nthe concrete score. In this paper, we reveal that the concrete score in\nabsorbing diffusion can be expressed as conditional probabilities of clean\ndata, multiplied by a time-dependent scalar in an analytic form. Motivated by\nthis finding, we propose reparameterized absorbing discrete diffusion (RADD), a\ndedicated diffusion model without time-condition that characterizes the\ntime-independent conditional probabilities. Besides its simplicity, RADD can\nreduce the number of function evaluations (NFEs) by caching the output of the\ntime-independent network when the noisy sample remains unchanged in a sampling\ninterval, which enables sampling acceleration. Built upon the new perspective\nof conditional distributions, we further unify absorbing discrete diffusion and\nany-order autoregressive models (AO-ARMs), showing that the upper bound on the\nnegative log-likelihood for the diffusion model can be interpreted as an\nexpected negative log-likelihood for AO-ARMs. Further, our RADD models achieve\nSOTA performance among diffusion models on 5 zero-shot language modeling\nbenchmarks (measured by perplexity) at the GPT-2 scale. Our code is available\nat https://github.com/ML-GSAI/RADD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discrete diffusion models with absorbing processes have shown promise in\nlanguage modeling. The key quantities to be estimated are the ratios between\nthe marginal probabilities of two transitive states at all timesteps, called\nthe concrete score. In this paper, we reveal that the concrete score in\nabsorbing diffusion can be expressed as conditional probabilities of clean\ndata, multiplied by a time-dependent scalar in an analytic form. Motivated by\nthis finding, we propose reparameterized absorbing discrete diffusion (RADD), a\ndedicated diffusion model without time-condition that characterizes the\ntime-independent conditional probabilities. Besides its simplicity, RADD can\nreduce the number of function evaluations (NFEs) by caching the output of the\ntime-independent network when the noisy sample remains unchanged in a sampling\ninterval, which enables sampling acceleration. Built upon the new perspective\nof conditional distributions, we further unify absorbing discrete diffusion and\nany-order autoregressive models (AO-ARMs), showing that the upper bound on the\nnegative log-likelihood for the diffusion model can be interpreted as an\nexpected negative log-likelihood for AO-ARMs. Further, our RADD models achieve\nSOTA performance among diffusion models on 5 zero-shot language modeling\nbenchmarks (measured by perplexity) at the GPT-2 scale. Our code is available\nat https://github.com/ML-GSAI/RADD."
                },
                "authors": [
                    {
                        "name": "Jingyang Ou"
                    },
                    {
                        "name": "Shen Nie"
                    },
                    {
                        "name": "Kaiwen Xue"
                    },
                    {
                        "name": "Fengqi Zhu"
                    },
                    {
                        "name": "Jiacheng Sun"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Chongxuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Chongxuan Li"
                },
                "author": "Chongxuan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03736v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03736v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07578v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07578v1",
                "updated": "2025-02-11T14:25:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    25,
                    20,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T14:25:20Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    25,
                    20,
                    1,
                    42,
                    0
                ],
                "title": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference"
                },
                "summary": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users to pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.3$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users to pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.3$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs."
                },
                "authors": [
                    {
                        "name": "Yufeng Gu"
                    },
                    {
                        "name": "Alireza Khadem"
                    },
                    {
                        "name": "Sumanth Umesh"
                    },
                    {
                        "name": "Ning Liang"
                    },
                    {
                        "name": "Xavier Servot"
                    },
                    {
                        "name": "Onur Mutlu"
                    },
                    {
                        "name": "Ravi Iyer"
                    },
                    {
                        "name": "Reetuparna Das"
                    }
                ],
                "author_detail": {
                    "name": "Reetuparna Das"
                },
                "author": "Reetuparna Das",
                "arxiv_doi": "10.1145/3676641.3716267",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3716267",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.07578v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07578v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, Volume\n  2 (ASPLOS'25)",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00279v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00279v2",
                "updated": "2025-02-10T18:34:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    34,
                    53,
                    0,
                    41,
                    0
                ],
                "published": "2024-12-31T05:24:30Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "title": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement"
                },
                "summary": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Li"
                },
                "author": "Junjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00279v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00279v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13629v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13629v2",
                "updated": "2025-02-10T17:19:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    17,
                    19,
                    21,
                    0,
                    41,
                    0
                ],
                "published": "2025-01-23T12:58:14Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    58,
                    14,
                    3,
                    23,
                    0
                ],
                "title": "Sigma: Differential Rescaling of Query, Key and Value for Efficient\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sigma: Differential Rescaling of Query, Key and Value for Efficient\n  Language Models"
                },
                "summary": "We introduce Sigma, an efficient large language model specialized for the\nsystem domain, empowered by a novel architecture including DiffQKV attention,\nand pre-trained on our meticulously collected system domain data. DiffQKV\nattention significantly enhances the inference efficiency of Sigma by\noptimizing the Query (Q), Key (K), and Value (V) components in the attention\nmechanism differentially, based on their varying impacts on the model\nperformance and efficiency indicators. Specifically, we (1) conduct extensive\nexperiments that demonstrate the model's varying sensitivity to the compression\nof K and V components, leading to the development of differentially compressed\nKV, and (2) propose augmented Q to expand the Q head dimension, which enhances\nthe model's representation capacity with minimal impacts on the inference\nspeed. Rigorous theoretical and empirical analyses reveal that DiffQKV\nattention significantly enhances efficiency, achieving up to a 33.36%\nimprovement in inference speed over the conventional grouped-query attention\n(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various\nsources, including 19.5B system domain data that we carefully collect and 1T\ntokens of synthesized and rewritten data. In general domains, Sigma achieves\ncomparable performance to other state-of-arts models. In the system domain, we\nintroduce the first comprehensive benchmark AIMicius, where Sigma demonstrates\nremarkable performance across all tasks, significantly outperforming GPT-4 with\nan absolute improvement up to 52.5%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Sigma, an efficient large language model specialized for the\nsystem domain, empowered by a novel architecture including DiffQKV attention,\nand pre-trained on our meticulously collected system domain data. DiffQKV\nattention significantly enhances the inference efficiency of Sigma by\noptimizing the Query (Q), Key (K), and Value (V) components in the attention\nmechanism differentially, based on their varying impacts on the model\nperformance and efficiency indicators. Specifically, we (1) conduct extensive\nexperiments that demonstrate the model's varying sensitivity to the compression\nof K and V components, leading to the development of differentially compressed\nKV, and (2) propose augmented Q to expand the Q head dimension, which enhances\nthe model's representation capacity with minimal impacts on the inference\nspeed. Rigorous theoretical and empirical analyses reveal that DiffQKV\nattention significantly enhances efficiency, achieving up to a 33.36%\nimprovement in inference speed over the conventional grouped-query attention\n(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various\nsources, including 19.5B system domain data that we carefully collect and 1T\ntokens of synthesized and rewritten data. In general domains, Sigma achieves\ncomparable performance to other state-of-arts models. In the system domain, we\nintroduce the first comprehensive benchmark AIMicius, where Sigma demonstrates\nremarkable performance across all tasks, significantly outperforming GPT-4 with\nan absolute improvement up to 52.5%."
                },
                "authors": [
                    {
                        "name": "Zhenghao Lin"
                    },
                    {
                        "name": "Zihao Tang"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Yeyun Gong"
                    },
                    {
                        "name": "Yi Cheng"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Ying Xin"
                    },
                    {
                        "name": "Ziyue Yang"
                    },
                    {
                        "name": "Kailai Yang"
                    },
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Shuai Lu"
                    },
                    {
                        "name": "Yiming Huang"
                    },
                    {
                        "name": "Zheheng Luo"
                    },
                    {
                        "name": "Lei Qu"
                    },
                    {
                        "name": "Xuan Feng"
                    },
                    {
                        "name": "Yaoxiang Wang"
                    },
                    {
                        "name": "Yuqing Xia"
                    },
                    {
                        "name": "Feiyang Chen"
                    },
                    {
                        "name": "Yuting Jiang"
                    },
                    {
                        "name": "Yasen Hu"
                    },
                    {
                        "name": "Hao Ni"
                    },
                    {
                        "name": "Binyang Li"
                    },
                    {
                        "name": "Guoshuai Zhao"
                    },
                    {
                        "name": "Jui-Hao Chiang"
                    },
                    {
                        "name": "Zhongxin Guo"
                    },
                    {
                        "name": "Chen Lin"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Yelong Shen"
                    },
                    {
                        "name": "Jian Jiao"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13629v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13629v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09425v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09425v2",
                "updated": "2025-02-10T15:17:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    17,
                    49,
                    0,
                    41,
                    0
                ],
                "published": "2024-11-14T13:22:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity"
                },
                "summary": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully."
                },
                "authors": [
                    {
                        "name": "Xiao Lv"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Shijie Guan"
                    },
                    {
                        "name": "Xiaoyou Zhou"
                    },
                    {
                        "name": "Zhiguang Qi"
                    },
                    {
                        "name": "Yaqiang Zang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09425v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09425v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06327v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06327v1",
                "updated": "2025-02-10T10:28:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    10,
                    28,
                    11,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T10:28:11Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    10,
                    28,
                    11,
                    0,
                    41,
                    0
                ],
                "title": "Prompt-Driven Continual Graph Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt-Driven Continual Graph Learning"
                },
                "summary": "Continual Graph Learning (CGL), which aims to accommodate new tasks over\nevolving graph data without forgetting prior knowledge, is garnering\nsignificant research interest. Mainstream solutions adopt the memory\nreplay-based idea, ie, caching representative data from earlier tasks for\nretraining the graph model. However, this strategy struggles with scalability\nissues for constantly evolving graphs and raises concerns regarding data\nprivacy. Inspired by recent advancements in the prompt-based learning paradigm,\nthis paper introduces a novel prompt-driven continual graph learning\n(PROMPTCGL) framework, which learns a separate prompt for each incoming task\nand maintains the underlying graph neural network model fixed. In this way,\nPROMPTCGL naturally avoids catastrophic forgetting of knowledge from previous\ntasks. More specifically, we propose hierarchical prompting to instruct the\nmodel from both feature- and topology-level to fully address the variability of\ntask graphs in dynamic continual learning. Additionally, we develop a\npersonalized prompt generator to generate tailored prompts for each graph node\nwhile minimizing the number of prompts needed, leading to constant memory\nconsumption regardless of the graph scale. Extensive experiments on four\nbenchmarks show that PROMPTCGL achieves superior performance against existing\nCGL approaches while significantly reducing memory consumption. Our code is\navailable at https://github.com/QiWang98/PromptCGL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Graph Learning (CGL), which aims to accommodate new tasks over\nevolving graph data without forgetting prior knowledge, is garnering\nsignificant research interest. Mainstream solutions adopt the memory\nreplay-based idea, ie, caching representative data from earlier tasks for\nretraining the graph model. However, this strategy struggles with scalability\nissues for constantly evolving graphs and raises concerns regarding data\nprivacy. Inspired by recent advancements in the prompt-based learning paradigm,\nthis paper introduces a novel prompt-driven continual graph learning\n(PROMPTCGL) framework, which learns a separate prompt for each incoming task\nand maintains the underlying graph neural network model fixed. In this way,\nPROMPTCGL naturally avoids catastrophic forgetting of knowledge from previous\ntasks. More specifically, we propose hierarchical prompting to instruct the\nmodel from both feature- and topology-level to fully address the variability of\ntask graphs in dynamic continual learning. Additionally, we develop a\npersonalized prompt generator to generate tailored prompts for each graph node\nwhile minimizing the number of prompts needed, leading to constant memory\nconsumption regardless of the graph scale. Extensive experiments on four\nbenchmarks show that PROMPTCGL achieves superior performance against existing\nCGL approaches while significantly reducing memory consumption. Our code is\navailable at https://github.com/QiWang98/PromptCGL."
                },
                "authors": [
                    {
                        "name": "Qi Wang"
                    },
                    {
                        "name": "Tianfei Zhou"
                    },
                    {
                        "name": "Ye Yuan"
                    },
                    {
                        "name": "Rui Mao"
                    }
                ],
                "author_detail": {
                    "name": "Rui Mao"
                },
                "author": "Rui Mao",
                "arxiv_comment": "12 pages, 7figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06327v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06327v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06166v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06166v1",
                "updated": "2025-02-10T05:33:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    5,
                    33,
                    25,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T05:33:25Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    5,
                    33,
                    25,
                    0,
                    41,
                    0
                ],
                "title": "Portable, High-Frequency, and High-Voltage Control Circuits for\n  Untethered Miniature Robots Driven by Dielectric Elastomer Actuators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Portable, High-Frequency, and High-Voltage Control Circuits for\n  Untethered Miniature Robots Driven by Dielectric Elastomer Actuators"
                },
                "summary": "In this work, we propose a high-voltage, high-frequency control circuit for\nthe untethered applications of dielectric elastomer actuators (DEAs). The\ncircuit board leverages low-voltage resistive components connected in series to\ncontrol voltages of up to 1.8 kV within a compact size, suitable for\nfrequencies ranging from 0 to 1 kHz. A single-channel control board weighs only\n2.5 g. We tested the performance of the control circuit under different load\nconditions and power supplies. Based on this control circuit, along with a\ncommercial miniature high-voltage power converter, we construct an untethered\ncrawling robot driven by a cylindrical DEA. The 42-g untethered robots\nsuccessfully obtained crawling locomotion on a bench and within a pipeline at a\ndriving frequency of 15 Hz, while simultaneously transmitting real-time video\ndata via an onboard camera and antenna. Our work provides a practical way to\nuse low-voltage control electronics to achieve the untethered driving of DEAs,\nand therefore portable and wearable devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we propose a high-voltage, high-frequency control circuit for\nthe untethered applications of dielectric elastomer actuators (DEAs). The\ncircuit board leverages low-voltage resistive components connected in series to\ncontrol voltages of up to 1.8 kV within a compact size, suitable for\nfrequencies ranging from 0 to 1 kHz. A single-channel control board weighs only\n2.5 g. We tested the performance of the control circuit under different load\nconditions and power supplies. Based on this control circuit, along with a\ncommercial miniature high-voltage power converter, we construct an untethered\ncrawling robot driven by a cylindrical DEA. The 42-g untethered robots\nsuccessfully obtained crawling locomotion on a bench and within a pipeline at a\ndriving frequency of 15 Hz, while simultaneously transmitting real-time video\ndata via an onboard camera and antenna. Our work provides a practical way to\nuse low-voltage control electronics to achieve the untethered driving of DEAs,\nand therefore portable and wearable devices."
                },
                "authors": [
                    {
                        "name": "Qi Shao"
                    },
                    {
                        "name": "Xin-Jun Liu"
                    },
                    {
                        "name": "Huichan Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Huichan Zhao"
                },
                "author": "Huichan Zhao",
                "arxiv_comment": "7 pages, 10 figures, accepted by ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06166v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06166v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04603v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04603v2",
                "updated": "2025-02-09T20:52:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    9,
                    20,
                    52,
                    26,
                    6,
                    40,
                    0
                ],
                "published": "2024-10-06T19:36:34Z",
                "published_parsed": [
                    2024,
                    10,
                    6,
                    19,
                    36,
                    34,
                    6,
                    280,
                    0
                ],
                "title": "Self-compensating Light Calorimetry with Liquid Argon Time Projection\n  Chamber for GeV Neutrino Physics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-compensating Light Calorimetry with Liquid Argon Time Projection\n  Chamber for GeV Neutrino Physics"
                },
                "summary": "The Liquid Argon Time Projection Chamber (LArTPC) is a powerful dual\ncalorimeter capable of estimating particle energy from both ionization charge\nand scintillation light. Our study shows that, due to the recombination\nluminescence, the LArTPC functions as a self-compensating light calorimeter:\nthe missing energy in the hadronic component is compensated for by the\nincreased luminescence relative to the electromagnetic component. Using 0.5--5\nGeV electron neutrino charged current interactions as a case study, we show\nthat good compensation of the electron-to-hadron response ratio (e/h) from\n1--1.05 can be achieved across a broad range of drift electric fields (0.2--1.8\nkV/cm), with better performance for neutrino energies above 2 GeV. This study\nhighlights the potential of light calorimetry in LArTPCs for GeV neutrino\nenergy reconstruction, complementing traditional charge calorimetry. Under\nideal conditions of uniform light collection, we show that LArTPC light\ncalorimetry can achieve an energy resolution comparable to the charge imaging\ncalorimetry. Challenges arising from nonuniform light collection in large\nLArTPCs can be mitigated with a position-dependent light yield correction\nderived from 3D charge signal imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Liquid Argon Time Projection Chamber (LArTPC) is a powerful dual\ncalorimeter capable of estimating particle energy from both ionization charge\nand scintillation light. Our study shows that, due to the recombination\nluminescence, the LArTPC functions as a self-compensating light calorimeter:\nthe missing energy in the hadronic component is compensated for by the\nincreased luminescence relative to the electromagnetic component. Using 0.5--5\nGeV electron neutrino charged current interactions as a case study, we show\nthat good compensation of the electron-to-hadron response ratio (e/h) from\n1--1.05 can be achieved across a broad range of drift electric fields (0.2--1.8\nkV/cm), with better performance for neutrino energies above 2 GeV. This study\nhighlights the potential of light calorimetry in LArTPCs for GeV neutrino\nenergy reconstruction, complementing traditional charge calorimetry. Under\nideal conditions of uniform light collection, we show that LArTPC light\ncalorimetry can achieve an energy resolution comparable to the charge imaging\ncalorimetry. Challenges arising from nonuniform light collection in large\nLArTPCs can be mitigated with a position-dependent light yield correction\nderived from 3D charge signal imaging."
                },
                "authors": [
                    {
                        "name": "Xuyang Ning"
                    },
                    {
                        "name": "Wei Shi"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Ciro Riccio"
                    },
                    {
                        "name": "Jay Hyun Jo"
                    }
                ],
                "author_detail": {
                    "name": "Jay Hyun Jo"
                },
                "author": "Jay Hyun Jo",
                "arxiv_doi": "10.1103/PhysRevD.111.032007",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevD.111.032007",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.04603v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04603v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "18 pages, 14 figures",
                "arxiv_journal_ref": "Phys. Rev. D 111, 032007 (2025)",
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06901v1",
                "updated": "2025-02-09T20:02:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    9,
                    20,
                    2,
                    5,
                    6,
                    40,
                    0
                ],
                "published": "2025-02-09T20:02:05Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    20,
                    2,
                    5,
                    6,
                    40,
                    0
                ],
                "title": "Enabling Autoregressive Models to Fill In Masked Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Autoregressive Models to Fill In Masked Tokens"
                },
                "summary": "Historically, LLMs have been trained using either autoregressive (AR) or\nmasked language modeling (MLM) objectives, with AR models gaining dominance in\nrecent years. However, AR models are inherently incapable of masked infilling,\nwhich is the ability to predict masked tokens between past and future context.\nIn contrast, MLM models suffer from intrinsic computational inefficiencies\nduring both training and inference that hinder their scalability. This work\nintroduces MARIA (Masked and Autoregressive Infilling Architecture), a novel\napproach that leverages the strengths of both paradigms to achieve\nstate-of-the-art masked infilling performance. MARIA combines a pre-trained MLM\nand AR model by training a linear decoder that takes their concatenated hidden\nstates as input. This minimal modification enables the AR model to perform\ninfilling while retaining its inherent advantages in terms of faster inference\nwith KV caching. Our results demonstrate that MARIA significantly outperforms\nexisting methods, namely discrete diffusion models, on masked infilling tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Historically, LLMs have been trained using either autoregressive (AR) or\nmasked language modeling (MLM) objectives, with AR models gaining dominance in\nrecent years. However, AR models are inherently incapable of masked infilling,\nwhich is the ability to predict masked tokens between past and future context.\nIn contrast, MLM models suffer from intrinsic computational inefficiencies\nduring both training and inference that hinder their scalability. This work\nintroduces MARIA (Masked and Autoregressive Infilling Architecture), a novel\napproach that leverages the strengths of both paradigms to achieve\nstate-of-the-art masked infilling performance. MARIA combines a pre-trained MLM\nand AR model by training a linear decoder that takes their concatenated hidden\nstates as input. This minimal modification enables the AR model to perform\ninfilling while retaining its inherent advantages in terms of faster inference\nwith KV caching. Our results demonstrate that MARIA significantly outperforms\nexisting methods, namely discrete diffusion models, on masked infilling tasks."
                },
                "authors": [
                    {
                        "name": "Daniel Israel"
                    },
                    {
                        "name": "Aditya Grover"
                    },
                    {
                        "name": "Guy Van den Broeck"
                    }
                ],
                "author_detail": {
                    "name": "Guy Van den Broeck"
                },
                "author": "Guy Van den Broeck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05960v1",
                "updated": "2025-02-09T17:09:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    9,
                    17,
                    9,
                    20,
                    6,
                    40,
                    0
                ],
                "published": "2025-02-09T17:09:20Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    17,
                    9,
                    20,
                    6,
                    40,
                    0
                ],
                "title": "Electric field control of nonlinear Hall effect in Weyl semimetal\n  TaIrTe4",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electric field control of nonlinear Hall effect in Weyl semimetal\n  TaIrTe4"
                },
                "summary": "The nonlinear Hall effect (NLHE), as an important probe to reveal the\nsymmetry breaking in topological properties of materials, opens up a new\ndimension for exploring the energy band structure and electron transport\nmechanism of quantum materials. Current studies mainly focus on the observation\nof material intrinsic the NLHE or inducing the NLHE response by artificially\nconstructing corrugated/twisted twodimensionalmaterial systems. Notably, the\nmodulation of NLHE signal strength, a core parameter of device performance, has\nattracted much attention, while theoretical predictions suggest that an applied\nelectric field can achieve the NLHE enhancement through modulation of the Berry\ncurvature dipole (BCD). Here we report effective modulation the magnitude and\nsign of the NLHE by applying additional constant electric fields of different\ndirections and magnitudes in the semimetal TaIrTe4. The NLHE response strength\nis enhanced by 168 times compared to the intrinsic one at 4 K when the\nadditional constant electric field of -0.5 kV/cm is applied to the b-axis of\nTaIrTe4 and the through a.c. current is parallel to the TaIrTe4 a-axis. Scaling\nlaw analysis suggests that the enhancement may be the result of the combined\neffect of the electric field on the intrinsic BCD and disorder scattering\neffect of TaIrTe4. This work provides a means to study the properties of\nTaIrTe4, as well as a valuable reference for the study of novel electronic\ndevices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The nonlinear Hall effect (NLHE), as an important probe to reveal the\nsymmetry breaking in topological properties of materials, opens up a new\ndimension for exploring the energy band structure and electron transport\nmechanism of quantum materials. Current studies mainly focus on the observation\nof material intrinsic the NLHE or inducing the NLHE response by artificially\nconstructing corrugated/twisted twodimensionalmaterial systems. Notably, the\nmodulation of NLHE signal strength, a core parameter of device performance, has\nattracted much attention, while theoretical predictions suggest that an applied\nelectric field can achieve the NLHE enhancement through modulation of the Berry\ncurvature dipole (BCD). Here we report effective modulation the magnitude and\nsign of the NLHE by applying additional constant electric fields of different\ndirections and magnitudes in the semimetal TaIrTe4. The NLHE response strength\nis enhanced by 168 times compared to the intrinsic one at 4 K when the\nadditional constant electric field of -0.5 kV/cm is applied to the b-axis of\nTaIrTe4 and the through a.c. current is parallel to the TaIrTe4 a-axis. Scaling\nlaw analysis suggests that the enhancement may be the result of the combined\neffect of the electric field on the intrinsic BCD and disorder scattering\neffect of TaIrTe4. This work provides a means to study the properties of\nTaIrTe4, as well as a valuable reference for the study of novel electronic\ndevices."
                },
                "authors": [
                    {
                        "name": "Jiaju Yang"
                    },
                    {
                        "name": "Lujun Wei"
                    },
                    {
                        "name": "Yanghui Li"
                    },
                    {
                        "name": "Lina Chen"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Feng Li"
                    },
                    {
                        "name": "Ping Liu"
                    },
                    {
                        "name": "Shuang Zhou"
                    },
                    {
                        "name": "Yong Pu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Pu"
                },
                "author": "Yong Pu",
                "arxiv_comment": "19 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05859v1",
                "updated": "2025-02-09T11:36:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    9,
                    11,
                    36,
                    45,
                    6,
                    40,
                    0
                ],
                "published": "2025-02-09T11:36:45Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    11,
                    36,
                    45,
                    6,
                    40,
                    0
                ],
                "title": "SphereFusion: Efficient Panorama Depth Estimation via Gated Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SphereFusion: Efficient Panorama Depth Estimation via Gated Fusion"
                },
                "summary": "Due to the rapid development of panorama cameras, the task of estimating\npanorama depth has attracted significant attention from the computer vision\ncommunity, especially in applications such as robot sensing and autonomous\ndriving. However, existing methods relying on different projection formats\noften encounter challenges, either struggling with distortion and discontinuity\nin the case of equirectangular, cubemap, and tangent projections, or\nexperiencing a loss of texture details with the spherical projection. To tackle\nthese concerns, we present SphereFusion, an end-to-end framework that combines\nthe strengths of various projection methods. Specifically, SphereFusion\ninitially employs 2D image convolution and mesh operations to extract two\ndistinct types of features from the panorama image in both equirectangular and\nspherical projection domains. These features are then projected onto the\nspherical domain, where a gate fusion module selects the most reliable features\nfor fusion. Finally, SphereFusion estimates panorama depth within the spherical\ndomain. Meanwhile, SphereFusion employs a cache strategy to improve the\nefficiency of mesh operation. Extensive experiments on three public panorama\ndatasets demonstrate that SphereFusion achieves competitive results with other\nstate-of-the-art methods, while presenting the fastest inference speed at only\n17 ms on a 512$\\times$1024 panorama image.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the rapid development of panorama cameras, the task of estimating\npanorama depth has attracted significant attention from the computer vision\ncommunity, especially in applications such as robot sensing and autonomous\ndriving. However, existing methods relying on different projection formats\noften encounter challenges, either struggling with distortion and discontinuity\nin the case of equirectangular, cubemap, and tangent projections, or\nexperiencing a loss of texture details with the spherical projection. To tackle\nthese concerns, we present SphereFusion, an end-to-end framework that combines\nthe strengths of various projection methods. Specifically, SphereFusion\ninitially employs 2D image convolution and mesh operations to extract two\ndistinct types of features from the panorama image in both equirectangular and\nspherical projection domains. These features are then projected onto the\nspherical domain, where a gate fusion module selects the most reliable features\nfor fusion. Finally, SphereFusion estimates panorama depth within the spherical\ndomain. Meanwhile, SphereFusion employs a cache strategy to improve the\nefficiency of mesh operation. Extensive experiments on three public panorama\ndatasets demonstrate that SphereFusion achieves competitive results with other\nstate-of-the-art methods, while presenting the fastest inference speed at only\n17 ms on a 512$\\times$1024 panorama image."
                },
                "authors": [
                    {
                        "name": "Qingsong Yan"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Kaiyong Zhao"
                    },
                    {
                        "name": "Jie Chen"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Fei Deng"
                    }
                ],
                "author_detail": {
                    "name": "Fei Deng"
                },
                "author": "Fei Deng",
                "arxiv_comment": "3DV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05763v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05763v1",
                "updated": "2025-02-09T03:49:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    9,
                    3,
                    49,
                    52,
                    6,
                    40,
                    0
                ],
                "published": "2025-02-09T03:49:52Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    3,
                    49,
                    52,
                    6,
                    40,
                    0
                ],
                "title": "Public DNS Resolvers Meet Content Delivery Networks: A Performance\n  Assessment of the Interplay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Public DNS Resolvers Meet Content Delivery Networks: A Performance\n  Assessment of the Interplay"
                },
                "summary": "This paper investigates two key performance aspects of the interplay between\npublic DNS resolution services and content delivery networks -- the latency of\nDNS queries for resolving CDN-accelerated hostnames and the latency between the\nend-user and the CDN's edge server obtained by the user through a given\nresolution service. While these important issues have been considered in the\npast, significant developments, such as the IPv6 finally getting traction, the\nadoption of the ECS extension to DNS by major DNS resolution services, and the\nembracing of anycast by some CDNs warrant a reassessment under these new\nrealities. Among the resolution services we consider, We find Google DNS and\nOpenDNS to lag behind the Cloudflare resolver and, for some CDNs, Quad9 in\nterms of DNS latency, and trace the cause to drastically lower cache hit rates.\nAt the same time, we find that Google and OpenDNS have largely closed the gap\nwith ISP resolvers in the quality of CDNs'client-to-edge-server mappings as\nmeasured by latency, while the Cloudflare resolver still shows some penalty\nwith Akamai, and Quad9 exhibits a noticeable penalty with three of the four\nCDNs in the study, keeping up only for Cloudflare CDN that does not use DNS to\nmap clients to servers. Finally, in several locations, we observe IPv6 penalty\nin the latency of client-to-CDN-edge-server mappings produced by the resolvers.\nMoreover, this penalty does not rise above typical thresholds employed by the\nHappy Eyeballs algorithm for falling back to IPv4 communication. Thus,\ndual-stacked clients in these locations may experience suboptimal performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates two key performance aspects of the interplay between\npublic DNS resolution services and content delivery networks -- the latency of\nDNS queries for resolving CDN-accelerated hostnames and the latency between the\nend-user and the CDN's edge server obtained by the user through a given\nresolution service. While these important issues have been considered in the\npast, significant developments, such as the IPv6 finally getting traction, the\nadoption of the ECS extension to DNS by major DNS resolution services, and the\nembracing of anycast by some CDNs warrant a reassessment under these new\nrealities. Among the resolution services we consider, We find Google DNS and\nOpenDNS to lag behind the Cloudflare resolver and, for some CDNs, Quad9 in\nterms of DNS latency, and trace the cause to drastically lower cache hit rates.\nAt the same time, we find that Google and OpenDNS have largely closed the gap\nwith ISP resolvers in the quality of CDNs'client-to-edge-server mappings as\nmeasured by latency, while the Cloudflare resolver still shows some penalty\nwith Akamai, and Quad9 exhibits a noticeable penalty with three of the four\nCDNs in the study, keeping up only for Cloudflare CDN that does not use DNS to\nmap clients to servers. Finally, in several locations, we observe IPv6 penalty\nin the latency of client-to-CDN-edge-server mappings produced by the resolvers.\nMoreover, this penalty does not rise above typical thresholds employed by the\nHappy Eyeballs algorithm for falling back to IPv4 communication. Thus,\ndual-stacked clients in these locations may experience suboptimal performance."
                },
                "authors": [
                    {
                        "name": "Nicholas Kernan"
                    },
                    {
                        "name": "Joey Li"
                    },
                    {
                        "name": "Rami Al-Dalky"
                    },
                    {
                        "name": "Michael Rabinovich"
                    }
                ],
                "author_detail": {
                    "name": "Michael Rabinovich"
                },
                "author": "Michael Rabinovich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05763v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05763v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05228v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05228v2",
                "updated": "2025-02-08T21:44:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    21,
                    44,
                    24,
                    5,
                    39,
                    0
                ],
                "published": "2024-12-06T17:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    58,
                    57,
                    4,
                    341,
                    0
                ],
                "title": "MC3: Memory Contention based Covert Channel Communication on Shared DRAM\n  System-on-Chips",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MC3: Memory Contention based Covert Channel Communication on Shared DRAM\n  System-on-Chips"
                },
                "summary": "Shared-memory system-on-chips (SM-SoC) are ubiquitously employed by a\nwide-range of mobile computing platforms, including edge/IoT devices,\nautonomous systems and smartphones. In SM-SoCs, system-wide shared physical\nmemory enables a convenient and financially-feasible way to make data\naccessible by dozens of processing units (PUs), such as CPU cores and domain\nspecific accelerators. In this study, we investigate vulnerabilities that stem\nfrom the shared use of physical memory in such systems. Due to the diverse\ncomputational characteristics of the PUs they embed, SM-SoCs often do not\nemploy a shared last level cache (LLC). While the literature proposes covert\nchannel attacks for shared memory systems, high-throughput communication is\ncurrently possible by either relying on an LLC or privileged/physical access to\nthe shared memory subsystem.\n  In this study, we introduce a new memory-contention based covert\ncommunication attack, MC3, which specifically targets the shared system memory\nin mobile SoCs. Different from existing attacks, our approach achieves high\nthroughput communication between applications running on CPU and GPU without\nthe need for an LLC or elevated access to the system. We extensively explore\nthe effectiveness of our methodology by demonstrating the trade-off between the\nchannel transmission rate and the robustness of the communication. We\ndemonstrate the utility of MC3 on NVIDIA Orin AGX, Orin NX, and Orin Nano up to\na transmit rate of 6.4 kbps with less than 1% error rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared-memory system-on-chips (SM-SoC) are ubiquitously employed by a\nwide-range of mobile computing platforms, including edge/IoT devices,\nautonomous systems and smartphones. In SM-SoCs, system-wide shared physical\nmemory enables a convenient and financially-feasible way to make data\naccessible by dozens of processing units (PUs), such as CPU cores and domain\nspecific accelerators. In this study, we investigate vulnerabilities that stem\nfrom the shared use of physical memory in such systems. Due to the diverse\ncomputational characteristics of the PUs they embed, SM-SoCs often do not\nemploy a shared last level cache (LLC). While the literature proposes covert\nchannel attacks for shared memory systems, high-throughput communication is\ncurrently possible by either relying on an LLC or privileged/physical access to\nthe shared memory subsystem.\n  In this study, we introduce a new memory-contention based covert\ncommunication attack, MC3, which specifically targets the shared system memory\nin mobile SoCs. Different from existing attacks, our approach achieves high\nthroughput communication between applications running on CPU and GPU without\nthe need for an LLC or elevated access to the system. We extensively explore\nthe effectiveness of our methodology by demonstrating the trade-off between the\nchannel transmission rate and the robustness of the communication. We\ndemonstrate the utility of MC3 on NVIDIA Orin AGX, Orin NX, and Orin Nano up to\na transmit rate of 6.4 kbps with less than 1% error rate."
                },
                "authors": [
                    {
                        "name": "Ismet Dagli"
                    },
                    {
                        "name": "James Crea"
                    },
                    {
                        "name": "Soner Seckiner"
                    },
                    {
                        "name": "Yuanchao Xu"
                    },
                    {
                        "name": "Seluk Kse"
                    },
                    {
                        "name": "Mehmet E. Belviranli"
                    }
                ],
                "author_detail": {
                    "name": "Mehmet E. Belviranli"
                },
                "author": "Mehmet E. Belviranli",
                "arxiv_comment": "This paper is accepted to 2025 Design, Automation Test in Europe\n  Conference Exhibition (DATE)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05228v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05228v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22134v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22134v2",
                "updated": "2025-02-08T14:11:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    14,
                    11,
                    25,
                    5,
                    39,
                    0
                ],
                "published": "2024-10-29T15:31:27Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "title": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching"
                },
                "summary": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions."
                },
                "authors": [
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Zihang Zhong"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22134v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22134v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09416v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09416v2",
                "updated": "2025-02-08T11:51:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    11,
                    51,
                    57,
                    5,
                    39,
                    0
                ],
                "published": "2024-12-12T16:24:35Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    24,
                    35,
                    3,
                    347,
                    0
                ],
                "title": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors"
                },
                "summary": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusions in the mathematical\ndomain. We release MRBench - a new evaluation benchmark containing 192\nconversations and 1,596 responses from seven state-of-the-art LLM-based and\nhuman tutors, providing gold annotations for eight pedagogical dimensions. We\nassess reliability of the popular Prometheus2 and Llama-3.1-8B LLMs as\nevaluators and analyze each tutor's pedagogical abilities, highlighting which\nLLMs are good tutors and which ones are more suitable as question-answering\nsystems. We believe that the presented taxonomy, benchmark, and human-annotated\nlabels will streamline the evaluation process and help track the progress in AI\ntutors' development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusions in the mathematical\ndomain. We release MRBench - a new evaluation benchmark containing 192\nconversations and 1,596 responses from seven state-of-the-art LLM-based and\nhuman tutors, providing gold annotations for eight pedagogical dimensions. We\nassess reliability of the popular Prometheus2 and Llama-3.1-8B LLMs as\nevaluators and analyze each tutor's pedagogical abilities, highlighting which\nLLMs are good tutors and which ones are more suitable as question-answering\nsystems. We believe that the presented taxonomy, benchmark, and human-annotated\nlabels will streamline the evaluation process and help track the progress in AI\ntutors' development."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kseniia Petukhova"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "9 pages",
                "arxiv_journal_ref": "NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09416v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09416v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05511v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05511v1",
                "updated": "2025-02-08T10:14:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    10,
                    14,
                    21,
                    5,
                    39,
                    0
                ],
                "published": "2025-02-08T10:14:21Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    10,
                    14,
                    21,
                    5,
                    39,
                    0
                ],
                "title": "New and Improved Bounds for Markov Paging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "New and Improved Bounds for Markov Paging"
                },
                "summary": "In the Markov paging model, one assumes that page requests are drawn from a\nMarkov chain over the pages in memory, and the goal is to maintain a fast cache\nthat suffers few page faults in expectation. While computing the optimal online\nalgorithm $(\\mathrm{OPT})$ for this problem naively takes time exponential in\nthe size of the cache, the best-known polynomial-time approximation algorithm\nis the dominating distribution algorithm due to Lund, Phillips and Reingold\n(FOCS 1994), who showed that the algorithm is $4$-competitive against\n$\\mathrm{OPT}$. We substantially improve their analysis and show that the\ndominating distribution algorithm is in fact $2$-competitive against\n$\\mathrm{OPT}$. We also show a lower bound of $1.5907$-competitiveness for this\nalgorithm -- to the best of our knowledge, no such lower bound was previously\nknown.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the Markov paging model, one assumes that page requests are drawn from a\nMarkov chain over the pages in memory, and the goal is to maintain a fast cache\nthat suffers few page faults in expectation. While computing the optimal online\nalgorithm $(\\mathrm{OPT})$ for this problem naively takes time exponential in\nthe size of the cache, the best-known polynomial-time approximation algorithm\nis the dominating distribution algorithm due to Lund, Phillips and Reingold\n(FOCS 1994), who showed that the algorithm is $4$-competitive against\n$\\mathrm{OPT}$. We substantially improve their analysis and show that the\ndominating distribution algorithm is in fact $2$-competitive against\n$\\mathrm{OPT}$. We also show a lower bound of $1.5907$-competitiveness for this\nalgorithm -- to the best of our knowledge, no such lower bound was previously\nknown."
                },
                "authors": [
                    {
                        "name": "Chirag Pabbaraju"
                    },
                    {
                        "name": "Ali Vakilian"
                    }
                ],
                "author_detail": {
                    "name": "Ali Vakilian"
                },
                "author": "Ali Vakilian",
                "arxiv_comment": "26 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05511v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05433v1",
                "updated": "2025-02-08T03:46:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    46,
                    28,
                    5,
                    39,
                    0
                ],
                "published": "2025-02-08T03:46:28Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    46,
                    28,
                    5,
                    39,
                    0
                ],
                "title": "AdaFlow: Efficient Long Video Editing via Adaptive Attention Slimming\n  And Keyframe Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaFlow: Efficient Long Video Editing via Adaptive Attention Slimming\n  And Keyframe Selection"
                },
                "summary": "Despite great progress, text-driven long video editing is still notoriously\nchallenging mainly due to excessive memory overhead. Although recent efforts\nhave simplified this task into a two-step process of keyframe translation and\ninterpolation generation, the token-wise keyframe translation still plagues the\nupper limit of video length. In this paper, we propose a novel and\ntraining-free approach towards efficient and effective long video editing,\ntermed AdaFlow. We first reveal that not all tokens of video frames hold equal\nimportance for keyframe translation, based on which we propose an Adaptive\nAttention Slimming scheme for AdaFlow to squeeze the $KV$ sequence, thus\nincreasing the number of keyframes for translations by an order of magnitude.\nIn addition, an Adaptive Keyframe Selection scheme is also equipped to select\nthe representative frames for joint editing, further improving generation\nquality. With these innovative designs, AdaFlow achieves high-quality long\nvideo editing of minutes in one inference, i.e., more than 1$k$ frames on one\nA800 GPU, which is about ten times longer than the compared methods, e.g.,\nTokenFlow. To validate AdaFlow, we also build a new benchmark for long video\nediting with high-quality annotations, termed LongV-EVAL. Our code is released\nat: https://github.com/jidantang55/AdaFlow.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite great progress, text-driven long video editing is still notoriously\nchallenging mainly due to excessive memory overhead. Although recent efforts\nhave simplified this task into a two-step process of keyframe translation and\ninterpolation generation, the token-wise keyframe translation still plagues the\nupper limit of video length. In this paper, we propose a novel and\ntraining-free approach towards efficient and effective long video editing,\ntermed AdaFlow. We first reveal that not all tokens of video frames hold equal\nimportance for keyframe translation, based on which we propose an Adaptive\nAttention Slimming scheme for AdaFlow to squeeze the $KV$ sequence, thus\nincreasing the number of keyframes for translations by an order of magnitude.\nIn addition, an Adaptive Keyframe Selection scheme is also equipped to select\nthe representative frames for joint editing, further improving generation\nquality. With these innovative designs, AdaFlow achieves high-quality long\nvideo editing of minutes in one inference, i.e., more than 1$k$ frames on one\nA800 GPU, which is about ten times longer than the compared methods, e.g.,\nTokenFlow. To validate AdaFlow, we also build a new benchmark for long video\nediting with high-quality annotations, termed LongV-EVAL. Our code is released\nat: https://github.com/jidantang55/AdaFlow."
                },
                "authors": [
                    {
                        "name": "Shuheng Zhang"
                    },
                    {
                        "name": "Yuqi Liu"
                    },
                    {
                        "name": "Hongbo Zhou"
                    },
                    {
                        "name": "Jun Peng"
                    },
                    {
                        "name": "Yiyi Zhou"
                    },
                    {
                        "name": "Xiaoshuai Sun"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05429v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05429v1",
                "updated": "2025-02-08T03:35:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    35,
                    55,
                    5,
                    39,
                    0
                ],
                "published": "2025-02-08T03:35:55Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    35,
                    55,
                    5,
                    39,
                    0
                ],
                "title": "SMaCk: Efficient Instruction Cache Attacks via Self-Modifying Code\n  Conflicts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SMaCk: Efficient Instruction Cache Attacks via Self-Modifying Code\n  Conflicts"
                },
                "summary": "Self-modifying code (SMC) allows programs to alter their own instructions,\noptimizing performance and functionality on x86 processors. Despite its\nbenefits, SMC introduces unique microarchitectural behaviors that can be\nexploited for malicious purposes. In this paper, we explore the security\nimplications of SMC by examining how specific x86 instructions affecting\ninstruction cache lines lead to measurable timing discrepancies between cache\nhits and misses. These discrepancies facilitate refined cache attacks, making\nthem less noisy and more effective. We introduce novel attack techniques that\nleverage these timing variations to enhance existing methods such as\nPrime+Probe and Flush+Reload. Our advanced techniques allow adversaries to more\nprecisely attack cryptographic keys and create covert channels akin to Spectre\nacross various x86 platforms. Finally, we propose a dynamic detection\nmethodology utilizing hardware performance counters to mitigate these enhanced\nthreats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-modifying code (SMC) allows programs to alter their own instructions,\noptimizing performance and functionality on x86 processors. Despite its\nbenefits, SMC introduces unique microarchitectural behaviors that can be\nexploited for malicious purposes. In this paper, we explore the security\nimplications of SMC by examining how specific x86 instructions affecting\ninstruction cache lines lead to measurable timing discrepancies between cache\nhits and misses. These discrepancies facilitate refined cache attacks, making\nthem less noisy and more effective. We introduce novel attack techniques that\nleverage these timing variations to enhance existing methods such as\nPrime+Probe and Flush+Reload. Our advanced techniques allow adversaries to more\nprecisely attack cryptographic keys and create covert channels akin to Spectre\nacross various x86 platforms. Finally, we propose a dynamic detection\nmethodology utilizing hardware performance counters to mitigate these enhanced\nthreats."
                },
                "authors": [
                    {
                        "name": "Seonghun Son"
                    },
                    {
                        "name": "Daniel Moghimi"
                    },
                    {
                        "name": "Berk Gulmezoglu"
                    }
                ],
                "author_detail": {
                    "name": "Berk Gulmezoglu"
                },
                "author": "Berk Gulmezoglu",
                "arxiv_doi": "10.1145/3676641.3716274",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3716274",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.05429v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05429v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Proceedings of the 30th ACM International Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS) accepted",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12304v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12304v4",
                "updated": "2025-02-07T23:14:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    23,
                    14,
                    10,
                    4,
                    38,
                    0
                ],
                "published": "2024-05-20T18:11:45Z",
                "published_parsed": [
                    2024,
                    5,
                    20,
                    18,
                    11,
                    45,
                    0,
                    141,
                    0
                ],
                "title": "Automatic Hardware Pragma Insertion in High-Level Synthesis: A\n  Non-Linear Programming Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Hardware Pragma Insertion in High-Level Synthesis: A\n  Non-Linear Programming Approach"
                },
                "summary": "High-Level Synthesis enables the rapid prototyping of hardware accelerators,\nby combining a high-level description of the functional behavior of a kernel\nwith a set of micro-architecture optimizations as inputs. Such optimizations\ncan be described by inserting pragmas e.g. pipelining and replication of units,\nor even higher level transformations for HLS such as automatic data caching\nusing the AMD/Xilinx Merlin compiler. Selecting the best combination of\npragmas, even within a restricted set, remains particularly challenging and the\ntypical state-of-practice uses design-space exploration to navigate this space.\nBut due to the highly irregular performance distribution of pragma\nconfigurations, typical DSE approaches are either extremely time consuming, or\noperating on a severely restricted search space. This work proposes a framework\nto automatically insert HLS pragmas in regular loop-based programs, supporting\npipelining, unit replication, and data caching. We develop an analytical\nperformance and resource model as a function of the input program properties\nand pragmas inserted, using non-linear constraints and objectives. We prove\nthis model provides a lower bound on the actual performance after HLS. We then\nencode this model as a Non-Linear Program, by making the pragma configuration\nunknowns of the system, which is computed optimally by solving this NLP. This\napproach can also be used during DSE, to quickly prune points with a (possibly\npartial) pragma configuration, driven by lower bounds on achievable latency. We\nextensively evaluate our end-to-end, fully implemented system, showing it can\neffectively manipulate spaces of billions of designs in seconds to minutes for\nthe kernels evaluated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Level Synthesis enables the rapid prototyping of hardware accelerators,\nby combining a high-level description of the functional behavior of a kernel\nwith a set of micro-architecture optimizations as inputs. Such optimizations\ncan be described by inserting pragmas e.g. pipelining and replication of units,\nor even higher level transformations for HLS such as automatic data caching\nusing the AMD/Xilinx Merlin compiler. Selecting the best combination of\npragmas, even within a restricted set, remains particularly challenging and the\ntypical state-of-practice uses design-space exploration to navigate this space.\nBut due to the highly irregular performance distribution of pragma\nconfigurations, typical DSE approaches are either extremely time consuming, or\noperating on a severely restricted search space. This work proposes a framework\nto automatically insert HLS pragmas in regular loop-based programs, supporting\npipelining, unit replication, and data caching. We develop an analytical\nperformance and resource model as a function of the input program properties\nand pragmas inserted, using non-linear constraints and objectives. We prove\nthis model provides a lower bound on the actual performance after HLS. We then\nencode this model as a Non-Linear Program, by making the pragma configuration\nunknowns of the system, which is computed optimally by solving this NLP. This\napproach can also be used during DSE, to quickly prune points with a (possibly\npartial) pragma configuration, driven by lower bounds on achievable latency. We\nextensively evaluate our end-to-end, fully implemented system, showing it can\neffectively manipulate spaces of billions of designs in seconds to minutes for\nthe kernels evaluated."
                },
                "authors": [
                    {
                        "name": "Stphane Pouget"
                    },
                    {
                        "name": "Louis-Nol Pouchet"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong",
                "arxiv_doi": "10.1145/3711847",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3711847",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.12304v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12304v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05370v1",
                "updated": "2025-02-07T22:51:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    22,
                    51,
                    17,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T22:51:17Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    22,
                    51,
                    17,
                    4,
                    38,
                    0
                ],
                "title": "fMoE: Fine-Grained Expert Offloading for Large Mixture-of-Experts\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "fMoE: Fine-Grained Expert Offloading for Large Mixture-of-Experts\n  Serving"
                },
                "summary": "Large Language Models (LLMs) have gained immense success in revolutionizing\nvarious applications, including content generation, search and recommendation,\nand AI-assisted operation. To reduce high training costs, Mixture-of-Experts\n(MoE) architecture has become a popular backbone for modern LLMs. However,\ndespite the benefits, serving MoE-based LLMs experience severe memory\ninefficiency due to sparsely activated experts. Recent studies propose to\noffload inactive experts from GPU memory to CPU memory to improve the serving\nefficiency of MoE models. However, they either incur high inference latency or\nhigh model memory footprints due to coarse-grained designs. To tame the\nlatency-memory trade-off in MoE serving, we present fMoE, a fine-grained expert\noffloading system for MoE serving that achieves low inference latency with\nmemory efficiency. We design fMoE to extract fine-grained expert selection\npatterns from MoE models and semantic hints from input prompts to efficiently\nguide expert prefetching, caching, and offloading decisions. fMoE is prototyped\non top of HuggingFace Transformers and deployed on a six-GPU testbed.\nExperiments with open-source MoE models and real-world workloads show that fMoE\nreduces inference latency by 47% and improves expert hit rate by 36% over\nstate-of-the-art solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained immense success in revolutionizing\nvarious applications, including content generation, search and recommendation,\nand AI-assisted operation. To reduce high training costs, Mixture-of-Experts\n(MoE) architecture has become a popular backbone for modern LLMs. However,\ndespite the benefits, serving MoE-based LLMs experience severe memory\ninefficiency due to sparsely activated experts. Recent studies propose to\noffload inactive experts from GPU memory to CPU memory to improve the serving\nefficiency of MoE models. However, they either incur high inference latency or\nhigh model memory footprints due to coarse-grained designs. To tame the\nlatency-memory trade-off in MoE serving, we present fMoE, a fine-grained expert\noffloading system for MoE serving that achieves low inference latency with\nmemory efficiency. We design fMoE to extract fine-grained expert selection\npatterns from MoE models and semantic hints from input prompts to efficiently\nguide expert prefetching, caching, and offloading decisions. fMoE is prototyped\non top of HuggingFace Transformers and deployed on a six-GPU testbed.\nExperiments with open-source MoE models and real-world workloads show that fMoE\nreduces inference latency by 47% and improves expert hit rate by 36% over\nstate-of-the-art solutions."
                },
                "authors": [
                    {
                        "name": "Hanfei Yu"
                    },
                    {
                        "name": "Xingqi Cui"
                    },
                    {
                        "name": "Hong Zhang"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06425v2",
                "updated": "2025-02-07T22:00:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    22,
                    0,
                    48,
                    4,
                    38,
                    0
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "summary": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPA's memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPA's memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "arxiv_comment": "31 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06425v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06425v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04923v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04923v1",
                "updated": "2025-02-07T13:41:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    41,
                    51,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T13:41:51Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    41,
                    51,
                    4,
                    38,
                    0
                ],
                "title": "Cached Multi-Lora Composition for Multi-Concept Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cached Multi-Lora Composition for Multi-Concept Image Generation"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a widely adopted technique in\ntext-to-image models, enabling precise rendering of multiple distinct elements,\nsuch as characters and styles, in multi-concept image generation. However,\ncurrent approaches face significant challenges when composing these LoRAs for\nmulti-concept image generation, resulting in diminished generated image\nquality. In this paper, we initially investigate the role of LoRAs in the\ndenoising process through the lens of the Fourier frequency domain. Based on\nthe hypothesis that applying multiple LoRAs could lead to \"semantic conflicts\",\nwe find that certain LoRAs amplify high-frequency features such as edges and\ntextures, whereas others mainly focus on low-frequency elements, including the\noverall structure and smooth color gradients. Building on these insights, we\ndevise a frequency domain based sequencing strategy to determine the optimal\norder in which LoRAs should be integrated during inference. This strategy\noffers a methodical and generalizable solution compared to the naive\nintegration commonly found in existing LoRA fusion techniques. To fully\nleverage our proposed LoRA order sequence determination method in multi-LoRA\ncomposition tasks, we introduce a novel, training-free framework, Cached\nMulti-LoRA (CMLoRA), designed to efficiently integrate multiple LoRAs while\nmaintaining cohesive image generation. With its flexible backbone for\nmulti-LoRA fusion and a non-uniform caching strategy tailored to individual\nLoRAs, CMLoRA has the potential to reduce semantic conflicts in LoRA\ncomposition and improve computational efficiency. Our experimental evaluations\ndemonstrate that CMLoRA outperforms state-of-the-art training-free LoRA fusion\nmethods by a significant margin -- it achieves an average improvement of\n$2.19\\%$ in CLIPScore, and $11.25\\%$ in MLLM win rate compared to LoraHub, LoRA\nComposite, and LoRA Switch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a widely adopted technique in\ntext-to-image models, enabling precise rendering of multiple distinct elements,\nsuch as characters and styles, in multi-concept image generation. However,\ncurrent approaches face significant challenges when composing these LoRAs for\nmulti-concept image generation, resulting in diminished generated image\nquality. In this paper, we initially investigate the role of LoRAs in the\ndenoising process through the lens of the Fourier frequency domain. Based on\nthe hypothesis that applying multiple LoRAs could lead to \"semantic conflicts\",\nwe find that certain LoRAs amplify high-frequency features such as edges and\ntextures, whereas others mainly focus on low-frequency elements, including the\noverall structure and smooth color gradients. Building on these insights, we\ndevise a frequency domain based sequencing strategy to determine the optimal\norder in which LoRAs should be integrated during inference. This strategy\noffers a methodical and generalizable solution compared to the naive\nintegration commonly found in existing LoRA fusion techniques. To fully\nleverage our proposed LoRA order sequence determination method in multi-LoRA\ncomposition tasks, we introduce a novel, training-free framework, Cached\nMulti-LoRA (CMLoRA), designed to efficiently integrate multiple LoRAs while\nmaintaining cohesive image generation. With its flexible backbone for\nmulti-LoRA fusion and a non-uniform caching strategy tailored to individual\nLoRAs, CMLoRA has the potential to reduce semantic conflicts in LoRA\ncomposition and improve computational efficiency. Our experimental evaluations\ndemonstrate that CMLoRA outperforms state-of-the-art training-free LoRA fusion\nmethods by a significant margin -- it achieves an average improvement of\n$2.19\\%$ in CLIPScore, and $11.25\\%$ in MLLM win rate compared to LoraHub, LoRA\nComposite, and LoRA Switch."
                },
                "authors": [
                    {
                        "name": "Xiandong Zou"
                    },
                    {
                        "name": "Mingzhu Shen"
                    },
                    {
                        "name": "Christos-Savvas Bouganis"
                    },
                    {
                        "name": "Yiren Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yiren Zhao"
                },
                "author": "Yiren Zhao",
                "arxiv_comment": "The Thirteenth International Conference on Learning Representations\n  (ICLR 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04923v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14846v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14846v2",
                "updated": "2025-02-07T13:09:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    9,
                    17,
                    4,
                    38,
                    0
                ],
                "published": "2024-09-23T09:22:59Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    9,
                    22,
                    59,
                    0,
                    267,
                    0
                ],
                "title": "A-VL: Adaptive Attention for Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A-VL: Adaptive Attention for Large Vision-Language Models"
                },
                "summary": "The Large Vision-Language Model (LVLM) integrates computer vision and natural\nlanguage processing techniques, offering substantial application potential.\nHowever, these models demand extensive resources during inference. Adaptive\nattention techniques can dynamically reduce computational redundancy and thus\nimprove efficiency. Although current adaptive attention methods significantly\nreduce the memory requirements of Transformer-based language models, they are\nnot tailored for LVLMs. We observe that LVLMs generate responses from both\nremote image tokens and local text tokens, and different modalities have\ndifferent attention patterns. This observation inspires us to manage the\nattention for each modality separately. Specifically, for visual input, we\nstore the cache of potentially useful information but only compute the most\ncritical parts. For language input, we care more about local information. Based\non our observation and analysis of vision-language attention patterns, we\ndevelop A-VL, a plug-and-play adaptive attention tailored for LVLM inference.\nExtensive evaluations on three vision-language tasks and five datasets show the\neffectiveness of our designs. Our approach A-VL outperforms existing adaptive\nattention methods in reducing memory usage and computational load without\ncompromising performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Large Vision-Language Model (LVLM) integrates computer vision and natural\nlanguage processing techniques, offering substantial application potential.\nHowever, these models demand extensive resources during inference. Adaptive\nattention techniques can dynamically reduce computational redundancy and thus\nimprove efficiency. Although current adaptive attention methods significantly\nreduce the memory requirements of Transformer-based language models, they are\nnot tailored for LVLMs. We observe that LVLMs generate responses from both\nremote image tokens and local text tokens, and different modalities have\ndifferent attention patterns. This observation inspires us to manage the\nattention for each modality separately. Specifically, for visual input, we\nstore the cache of potentially useful information but only compute the most\ncritical parts. For language input, we care more about local information. Based\non our observation and analysis of vision-language attention patterns, we\ndevelop A-VL, a plug-and-play adaptive attention tailored for LVLM inference.\nExtensive evaluations on three vision-language tasks and five datasets show the\neffectiveness of our designs. Our approach A-VL outperforms existing adaptive\nattention methods in reducing memory usage and computational load without\ncompromising performance."
                },
                "authors": [
                    {
                        "name": "Junyang Zhang"
                    },
                    {
                        "name": "Mu Yuan"
                    },
                    {
                        "name": "Ruiguang Zhong"
                    },
                    {
                        "name": "Puhan Luo"
                    },
                    {
                        "name": "Huiyou Zhan"
                    },
                    {
                        "name": "Ningkang Zhang"
                    },
                    {
                        "name": "Chengchen Hu"
                    },
                    {
                        "name": "Xiangyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyang Li"
                },
                "author": "Xiangyang Li",
                "arxiv_comment": "AAAI 2025 Accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14846v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14846v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04760v1",
                "updated": "2025-02-07T08:48:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    48,
                    6,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T08:48:06Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    48,
                    6,
                    4,
                    38,
                    0
                ],
                "title": "Graph Federated Learning Based Proactive Content Caching in Edge\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Federated Learning Based Proactive Content Caching in Edge\n  Computing"
                },
                "summary": "With the rapid growth of mobile data traffic and the increasing prevalence of\nvideo streaming, proactive content caching in edge computing has become crucial\nfor reducing latency and alleviating network congestion. However, traditional\ncaching strategies such as FIFO, LRU, and LFU fail to effectively predict\nfuture content popularity, while existing proactive caching approaches often\nrequire users to upload data to a central server, raising concerns regarding\nprivacy and scalability. To address these challenges, this paper proposes a\nGraph Federated Learning-based Proactive Content Caching (GFPCC) scheme that\nenhances caching efficiency while preserving user privacy. The proposed\napproach integrates federated learning and graph neural networks, enabling\nusers to locally train Light Graph Convolutional Networks (LightGCN) to capture\nuser-item relationships and predict content popularity. Instead of sharing raw\ndata, only the trained model parameters are transmitted to the central server,\nwhere a federated averaging algorithm aggregates updates, refines the global\nmodel, and selects the most popular files for proactive caching. Experimental\nevaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC\noutperforms baseline caching algorithms by achieving higher cache efficiency\nthrough more accurate content popularity predictions. Moreover, the federated\nlearning framework strengthens privacy protection while maintaining efficient\nmodel training; however, scalability remains a challenge in large-scale\nnetworks with dynamic user preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of mobile data traffic and the increasing prevalence of\nvideo streaming, proactive content caching in edge computing has become crucial\nfor reducing latency and alleviating network congestion. However, traditional\ncaching strategies such as FIFO, LRU, and LFU fail to effectively predict\nfuture content popularity, while existing proactive caching approaches often\nrequire users to upload data to a central server, raising concerns regarding\nprivacy and scalability. To address these challenges, this paper proposes a\nGraph Federated Learning-based Proactive Content Caching (GFPCC) scheme that\nenhances caching efficiency while preserving user privacy. The proposed\napproach integrates federated learning and graph neural networks, enabling\nusers to locally train Light Graph Convolutional Networks (LightGCN) to capture\nuser-item relationships and predict content popularity. Instead of sharing raw\ndata, only the trained model parameters are transmitted to the central server,\nwhere a federated averaging algorithm aggregates updates, refines the global\nmodel, and selects the most popular files for proactive caching. Experimental\nevaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC\noutperforms baseline caching algorithms by achieving higher cache efficiency\nthrough more accurate content popularity predictions. Moreover, the federated\nlearning framework strengthens privacy protection while maintaining efficient\nmodel training; however, scalability remains a challenge in large-scale\nnetworks with dynamic user preferences."
                },
                "authors": [
                    {
                        "name": "Rui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Wang"
                },
                "author": "Rui Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21035v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21035v2",
                "updated": "2025-02-06T20:26:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    20,
                    26,
                    24,
                    3,
                    37,
                    0
                ],
                "published": "2024-10-28T13:56:30Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    56,
                    30,
                    0,
                    302,
                    0
                ],
                "title": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time"
                },
                "summary": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, at the 1.3B parameters scale,\ndiffusion models, even without caching, can generate tokens at a rate that is\nup to 8 times faster than AR models employing KV-caching, and we anticipate\nfurther improvements with the inclusion of caching. Moreover, we demonstrate\nthe efficacy of our approach for diffusion language models with up to 860M\nparameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, at the 1.3B parameters scale,\ndiffusion models, even without caching, can generate tokens at a rate that is\nup to 8 times faster than AR models employing KV-caching, and we anticipate\nfurther improvements with the inclusion of caching. Moreover, we demonstrate\nthe efficacy of our approach for diffusion language models with up to 860M\nparameters."
                },
                "authors": [
                    {
                        "name": "Justin Deschenaux"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21035v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21035v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04420v1",
                "updated": "2025-02-06T15:26:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    26,
                    26,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T15:26:26Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    26,
                    26,
                    3,
                    37,
                    0
                ],
                "title": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference"
                },
                "summary": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths."
                },
                "authors": [
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zeyu Xing"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Linping Qu"
                    },
                    {
                        "name": "Hui-Ling Zhen"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Sinno Jialin Pan"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04077v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04077v1",
                "updated": "2025-02-06T13:41:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    41,
                    46,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T13:41:46Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    41,
                    46,
                    3,
                    37,
                    0
                ],
                "title": "AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference"
                },
                "summary": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through heuristic ranking with attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the \\textit{temporal patterns} in attention scores,\nresulting in a noticeable degradation in LLM performance. To address this\nchallenge, we propose AttentionPredictor, which is the first learning-based\ncritical token identification approach. Specifically, AttentionPredictor learns\na lightweight convolution model to capture spatiotemporal patterns and predict\nthe next-token attention score. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score while consuming negligible\nmemory. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n16$\\times$ KV cache compression with comparable LLM performance, significantly\noutperforming the state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through heuristic ranking with attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the \\textit{temporal patterns} in attention scores,\nresulting in a noticeable degradation in LLM performance. To address this\nchallenge, we propose AttentionPredictor, which is the first learning-based\ncritical token identification approach. Specifically, AttentionPredictor learns\na lightweight convolution model to capture spatiotemporal patterns and predict\nthe next-token attention score. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score while consuming negligible\nmemory. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n16$\\times$ KV cache compression with comparable LLM performance, significantly\noutperforming the state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Qingyue Yang"
                    },
                    {
                        "name": "Jie Wang"
                    },
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zhihai Wang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Bin Li"
                    }
                ],
                "author_detail": {
                    "name": "Bin Li"
                },
                "author": "Bin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04077v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.06893v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.06893v4",
                "updated": "2025-02-06T12:32:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    32,
                    34,
                    3,
                    37,
                    0
                ],
                "published": "2023-12-11T23:34:23Z",
                "published_parsed": [
                    2023,
                    12,
                    11,
                    23,
                    34,
                    23,
                    0,
                    345,
                    0
                ],
                "title": "Styx: Transactional Stateful Functions on Streaming Dataflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Styx: Transactional Stateful Functions on Streaming Dataflows"
                },
                "summary": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches provide weak transactional\nguarantees or perform expensive external state accesses requiring inefficient\ntransactional protocols that increase execution latency.\n  In this paper, we present Styx, a novel dataflow-based SFaaS runtime that\nexecutes serializable transactions consisting of stateful functions that form\narbitrary call-graphs with exactly-once guarantees. Styx extends a\ndeterministic transactional protocol by contributing: i) a function\nacknowledgment scheme to determine transaction boundaries required in SFaaS\nworkloads, ii) a function-execution caching mechanism, and iii) an early-commit\nreply mechanism that substantially reduces transaction execution latency.\nExperiments with the YCSB, TPC-C, and Deathstar benchmarks show that Styx\noutperforms state-of-the-art approaches by achieving at least one order of\nmagnitude higher throughput while exhibiting near-linear scalability and low\nlatency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches provide weak transactional\nguarantees or perform expensive external state accesses requiring inefficient\ntransactional protocols that increase execution latency.\n  In this paper, we present Styx, a novel dataflow-based SFaaS runtime that\nexecutes serializable transactions consisting of stateful functions that form\narbitrary call-graphs with exactly-once guarantees. Styx extends a\ndeterministic transactional protocol by contributing: i) a function\nacknowledgment scheme to determine transaction boundaries required in SFaaS\nworkloads, ii) a function-execution caching mechanism, and iii) an early-commit\nreply mechanism that substantially reduces transaction execution latency.\nExperiments with the YCSB, TPC-C, and Deathstar benchmarks show that Styx\noutperforms state-of-the-art approaches by achieving at least one order of\nmagnitude higher throughput while exhibiting near-linear scalability and low\nlatency."
                },
                "authors": [
                    {
                        "name": "Kyriakos Psarakis"
                    },
                    {
                        "name": "George Christodoulou"
                    },
                    {
                        "name": "George Siachamis"
                    },
                    {
                        "name": "Marios Fragkoulis"
                    },
                    {
                        "name": "Asterios Katsifodimos"
                    }
                ],
                "author_detail": {
                    "name": "Asterios Katsifodimos"
                },
                "author": "Asterios Katsifodimos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.06893v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.06893v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04018v1",
                "updated": "2025-02-06T12:19:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    19,
                    34,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T12:19:34Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    19,
                    34,
                    3,
                    37,
                    0
                ],
                "title": "PINT: Physics-Informed Neural Time Series Models with Applications to\n  Long-term Inference on WeatherBench 2m-Temperature Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PINT: Physics-Informed Neural Time Series Models with Applications to\n  Long-term Inference on WeatherBench 2m-Temperature Data"
                },
                "summary": "This paper introduces PINT (Physics-Informed Neural Time Series Models), a\nframework that integrates physical constraints into neural time series models\nto improve their ability to capture complex dynamics. We apply PINT to the ERA5\nWeatherBench dataset, focusing on long-term forecasting of 2m-temperature data.\nPINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed\nprior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures.\nThis equation's analytical solutions (sine and cosine functions) facilitate\nrigorous evaluation of the benefits of incorporating physics-informed\nconstraints. By benchmarking against a linear regression baseline derived from\nits exact solutions, we quantify the impact of embedding physical principles in\ndata-driven models. Unlike traditional time series models that rely on future\nobservations, PINT is designed for practical forecasting. Using only the first\n90 days of observed data, it iteratively predicts the next two years,\naddressing challenges posed by limited real-time updates. Experiments on the\nWeatherBench dataset demonstrate PINT's ability to generalize, capture periodic\ntrends, and align with physical principles. This study highlights the potential\nof physics-informed neural models in bridging machine learning and\ninterpretable climate applications.\n  Our models and datasets are publicly available on GitHub:\nhttps://github.com/KV-Park.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces PINT (Physics-Informed Neural Time Series Models), a\nframework that integrates physical constraints into neural time series models\nto improve their ability to capture complex dynamics. We apply PINT to the ERA5\nWeatherBench dataset, focusing on long-term forecasting of 2m-temperature data.\nPINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed\nprior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures.\nThis equation's analytical solutions (sine and cosine functions) facilitate\nrigorous evaluation of the benefits of incorporating physics-informed\nconstraints. By benchmarking against a linear regression baseline derived from\nits exact solutions, we quantify the impact of embedding physical principles in\ndata-driven models. Unlike traditional time series models that rely on future\nobservations, PINT is designed for practical forecasting. Using only the first\n90 days of observed data, it iteratively predicts the next two years,\naddressing challenges posed by limited real-time updates. Experiments on the\nWeatherBench dataset demonstrate PINT's ability to generalize, capture periodic\ntrends, and align with physical principles. This study highlights the potential\nof physics-informed neural models in bridging machine learning and\ninterpretable climate applications.\n  Our models and datasets are publicly available on GitHub:\nhttps://github.com/KV-Park."
                },
                "authors": [
                    {
                        "name": "Keon Vin Park"
                    },
                    {
                        "name": "Jisu Kim"
                    },
                    {
                        "name": "Jaemin Seo"
                    }
                ],
                "author_detail": {
                    "name": "Jaemin Seo"
                },
                "author": "Jaemin Seo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01449v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01449v2",
                "updated": "2025-02-06T08:36:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    8,
                    36,
                    44,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-03T15:38:53Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    15,
                    38,
                    53,
                    0,
                    34,
                    0
                ],
                "title": "PlaceIT: Placement-based Inter-Chiplet Interconnect Topologies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PlaceIT: Placement-based Inter-Chiplet Interconnect Topologies"
                },
                "summary": "2.5D integration technology is gaining traction as it copes with the\nexponentially growing design cost of modern integrated circuits. A crucial part\nof a 2.5D stacked chip is a low-latency and high-throughput inter-chiplet\ninterconnect (ICI). Two major factors affecting the latency and throughput are\nthe topology of links between chiplets and the chiplet placement. In this work,\nwe present PlaceIT, a novel methodology to jointly optimize the ICI topology\nand the chiplet placement. While state-of-the-art methods optimize the chiplet\nplacement for a predetermined ICI topology, or they select one topology out of\na set of candidates, we generate a completely new topology for each placement.\nOur process of inferring placement-based ICI topologies connects chiplets that\nare in close proximity to each other, making it particularly attractive for\nchips with silicon bridges or passive silicon interposers with severely limited\nlink lengths. We provide an open-source implementation of our method that\noptimizes the placement of homogeneously or heterogeneously shaped chiplets and\nthe ICI topology connecting them for a user-defined mix of four different\ntraffic types. We evaluate our methodology using synthetic traffic and traces,\nand we compare our results to a 2D mesh baseline. PlaceIT reduces the latency\nof synthetic L1-to-L2 and L2-to-memory traffic, the two most important types\nfor cache coherency traffic, by up to 28% and 62%, respectively. It also\nachieve an average packet latency reduction of up to 18% on traffic traces.\nPlaceIT enables the construction of 2.5D stacked chips with low-latency ICIs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "2.5D integration technology is gaining traction as it copes with the\nexponentially growing design cost of modern integrated circuits. A crucial part\nof a 2.5D stacked chip is a low-latency and high-throughput inter-chiplet\ninterconnect (ICI). Two major factors affecting the latency and throughput are\nthe topology of links between chiplets and the chiplet placement. In this work,\nwe present PlaceIT, a novel methodology to jointly optimize the ICI topology\nand the chiplet placement. While state-of-the-art methods optimize the chiplet\nplacement for a predetermined ICI topology, or they select one topology out of\na set of candidates, we generate a completely new topology for each placement.\nOur process of inferring placement-based ICI topologies connects chiplets that\nare in close proximity to each other, making it particularly attractive for\nchips with silicon bridges or passive silicon interposers with severely limited\nlink lengths. We provide an open-source implementation of our method that\noptimizes the placement of homogeneously or heterogeneously shaped chiplets and\nthe ICI topology connecting them for a user-defined mix of four different\ntraffic types. We evaluate our methodology using synthetic traffic and traces,\nand we compare our results to a 2D mesh baseline. PlaceIT reduces the latency\nof synthetic L1-to-L2 and L2-to-memory traffic, the two most important types\nfor cache coherency traffic, by up to 28% and 62%, respectively. It also\nachieve an average packet latency reduction of up to 18% on traffic traces.\nPlaceIT enables the construction of 2.5D stacked chips with low-latency ICIs."
                },
                "authors": [
                    {
                        "name": "Patrick Iff"
                    },
                    {
                        "name": "Benigna Bruggmann"
                    },
                    {
                        "name": "Maciej Besta"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01449v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01449v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03805v1",
                "updated": "2025-02-06T06:31:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    6,
                    31,
                    47,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T06:31:47Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    6,
                    31,
                    47,
                    3,
                    37,
                    0
                ],
                "title": "Identify Critical KV Cache in LLM Inference from an Output Perturbation\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identify Critical KV Cache in LLM Inference from an Output Perturbation\n  Perspective"
                },
                "summary": "Large language models have revolutionized natural language processing but\nface significant challenges of high storage and runtime costs, due to the\ntransformer architecture's reliance on self-attention, particularly the large\nKey-Value (KV) cache for long-sequence inference. Recent efforts to reduce KV\ncache size by pruning less critical entries based on attention weights remain\nempirical and lack formal grounding. This paper presents a formal study on\nidentifying critical KV cache entries by analyzing attention output\nperturbation. Our analysis reveals that, beyond attention weights, the value\nstates within KV entries and pretrained parameter matrices are also crucial.\nBased on this, we propose a perturbation-constrained selection algorithm that\noptimizes the worst-case output perturbation to identify critical entries.\nEvaluations on the Needle-in-a-Haystack test and Longbench benchmark show our\nalgorithm enhances state-of-the-art cache eviction methods. Further empirical\nanalysis confirms that our algorithm achieves lower output perturbations in\nover 92% attention heads in Llama model, thereby providing a significant\nimprovement over existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have revolutionized natural language processing but\nface significant challenges of high storage and runtime costs, due to the\ntransformer architecture's reliance on self-attention, particularly the large\nKey-Value (KV) cache for long-sequence inference. Recent efforts to reduce KV\ncache size by pruning less critical entries based on attention weights remain\nempirical and lack formal grounding. This paper presents a formal study on\nidentifying critical KV cache entries by analyzing attention output\nperturbation. Our analysis reveals that, beyond attention weights, the value\nstates within KV entries and pretrained parameter matrices are also crucial.\nBased on this, we propose a perturbation-constrained selection algorithm that\noptimizes the worst-case output perturbation to identify critical entries.\nEvaluations on the Needle-in-a-Haystack test and Longbench benchmark show our\nalgorithm enhances state-of-the-art cache eviction methods. Further empirical\nanalysis confirms that our algorithm achieves lower output perturbations in\nover 92% attention heads in Llama model, thereby providing a significant\nimprovement over existing methods."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S Kevin Zhou"
                },
                "author": "S Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03771v1",
                "updated": "2025-02-06T04:16:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T04:16:20Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "title": "Adaptive Semantic Prompt Caching with VectorQ",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Semantic Prompt Caching with VectorQ"
                },
                "summary": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different prompts. We\npropose VectorQ, a framework to learn embedding-specific threshold regions that\nadapt to the complexity and uncertainty of an embedding. Through evaluations on\na combination of four diverse datasets, we show that VectorQ consistently\noutperforms state-of-the-art systems across all static thresholds, achieving up\nto 12x increases in cache hit rate and error rate reductions up to 92%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different prompts. We\npropose VectorQ, a framework to learn embedding-specific threshold regions that\nadapt to the complexity and uncertainty of an embedding. Through evaluations on\na combination of four diverse datasets, we show that VectorQ consistently\noutperforms state-of-the-art systems across all static thresholds, achieving up\nto 12x increases in cache hit rate and error rate reductions up to 92%."
                },
                "authors": [
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Alejandro Cuadron"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Stephan Krusche"
                    },
                    {
                        "name": "Alfons Kemper"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E. Gonzalez"
                },
                "author": "Joseph E. Gonzalez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04393v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04393v1",
                "updated": "2025-02-06T03:56:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    3,
                    56,
                    11,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T03:56:11Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    3,
                    56,
                    11,
                    3,
                    37,
                    0
                ],
                "title": "UniCP: A Unified Caching and Pruning Framework for Efficient Video\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniCP: A Unified Caching and Pruning Framework for Efficient Video\n  Generation"
                },
                "summary": "Diffusion Transformers (DiT) excel in video generation but encounter\nsignificant computational challenges due to the quadratic complexity of\nattention. Notably, attention differences between adjacent diffusion steps\nfollow a U-shaped pattern. Current methods leverage this property by caching\nattention blocks, however, they still struggle with sudden error spikes and\nlarge discrepancies. To address these issues, we propose UniCP a unified\ncaching and pruning framework for efficient video generation. UniCP optimizes\nboth temporal and spatial dimensions through. Error Aware Dynamic Cache Window\n(EDCW): Dynamically adjusts cache window sizes for different blocks at various\ntimesteps, adapting to abrupt error changes. PCA based Slicing (PCAS) and\nDynamic Weight Shift (DWS): PCAS prunes redundant attention components, and DWS\nintegrates caching and pruning by enabling dynamic switching between pruned and\ncached outputs. By adjusting cache windows and pruning redundant components,\nUniCP enhances computational efficiency and maintains video detail fidelity.\nExperimental results show that UniCP outperforms existing methods in both\nperformance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) excel in video generation but encounter\nsignificant computational challenges due to the quadratic complexity of\nattention. Notably, attention differences between adjacent diffusion steps\nfollow a U-shaped pattern. Current methods leverage this property by caching\nattention blocks, however, they still struggle with sudden error spikes and\nlarge discrepancies. To address these issues, we propose UniCP a unified\ncaching and pruning framework for efficient video generation. UniCP optimizes\nboth temporal and spatial dimensions through. Error Aware Dynamic Cache Window\n(EDCW): Dynamically adjusts cache window sizes for different blocks at various\ntimesteps, adapting to abrupt error changes. PCA based Slicing (PCAS) and\nDynamic Weight Shift (DWS): PCAS prunes redundant attention components, and DWS\nintegrates caching and pruning by enabling dynamic switching between pruned and\ncached outputs. By adjusting cache windows and pruning redundant components,\nUniCP enhances computational efficiency and maintains video detail fidelity.\nExperimental results show that UniCP outperforms existing methods in both\nperformance and efficiency."
                },
                "authors": [
                    {
                        "name": "Wenzhang Sun"
                    },
                    {
                        "name": "Qirui Hou"
                    },
                    {
                        "name": "Donglin Di"
                    },
                    {
                        "name": "Jiahui Yang"
                    },
                    {
                        "name": "Yongjia Ma"
                    },
                    {
                        "name": "Jianxun Cui"
                    }
                ],
                "author_detail": {
                    "name": "Jianxun Cui"
                },
                "author": "Jianxun Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04393v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04393v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02770v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02770v2",
                "updated": "2025-02-06T03:16:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    3,
                    16,
                    0,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-04T23:26:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    23,
                    26,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning"
                },
                "summary": "Leveraging attention sparsity to accelerate long-context large language\nmodels (LLMs) has been a hot research topic. However, current algorithms such\nas sparse attention or key-value (KV) cache compression tend to use a fixed\nbudget, which presents a significant challenge during deployment because it\nfails to account for the dynamic nature of real-world scenarios, where the\noptimal balance between accuracy and efficiency can vary greatly. In this\npaper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse\nattention can surprisingly achieve adaptive budgeting. Based on this, we\npropose Twilight, a framework to bring adaptive sparsity to any existing sparse\nattention algorithm without sacrificing their accuracy. Empirical results show\nthat Twilight can adaptively prune at most 98% of redundant tokens, leading to\n$15.4\\times$ acceleration in self-attention operations and $3.9\\times$\nacceleration in end-to-end per token latency in long context LLM decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging attention sparsity to accelerate long-context large language\nmodels (LLMs) has been a hot research topic. However, current algorithms such\nas sparse attention or key-value (KV) cache compression tend to use a fixed\nbudget, which presents a significant challenge during deployment because it\nfails to account for the dynamic nature of real-world scenarios, where the\noptimal balance between accuracy and efficiency can vary greatly. In this\npaper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse\nattention can surprisingly achieve adaptive budgeting. Based on this, we\npropose Twilight, a framework to bring adaptive sparsity to any existing sparse\nattention algorithm without sacrificing their accuracy. Empirical results show\nthat Twilight can adaptively prune at most 98% of redundant tokens, leading to\n$15.4\\times$ acceleration in self-attention operations and $3.9\\times$\nacceleration in end-to-end per token latency in long context LLM decoding."
                },
                "authors": [
                    {
                        "name": "Chaofan Lin"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Hanshuo Wang"
                    },
                    {
                        "name": "Tian Tang"
                    },
                    {
                        "name": "Boyu Tian"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02770v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02770v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05460v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05460v2",
                "updated": "2025-02-05T22:55:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    22,
                    55,
                    47,
                    2,
                    36,
                    0
                ],
                "published": "2024-12-25T10:11:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    10,
                    11,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "Efficiently Serving Large Multimodal Models Using EPD Disaggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently Serving Large Multimodal Models Using EPD Disaggregation"
                },
                "summary": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step negatively impacting key Service Level Objectives (SLOs)\nlike time to first token (TTFT) and end-to-end throughput (E2ETP). We introduce\nEncode-Prefill-Decode (EPD) Disaggregation, a novel framework that separates\nthe encoding, prefill, and decode stages onto dedicated resources. Unlike\ncurrent systems, which bundle encoding and prefill together, our approach\ndecouple these steps unlocking new opportunities and optimizations. These\ninclude a new mechanism to cache multimedia tokens for efficient transfer, a\nnovel way to parallelize encoding load within a request, a module to find the\noptimal resource allocation for disaggregated serving, and a novel role\nswitching method to handle changing workload characteristics. Experimental\nevaluations with popular LMMs show substantial gains in memory efficiency (up\nto 15$\\times$ less utilization), batch sizes (up to 22$\\times$ larger),\n10$\\times$ more images/request, and 2.2$\\times$ larger KV caches. Further, it\nleads to significant improvements in latency metrics (TTFT up to 71\\%\nreduction) and end-to-end throughput (up to 57\\% reduction), compared to\nsystems that do not disaggregate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step negatively impacting key Service Level Objectives (SLOs)\nlike time to first token (TTFT) and end-to-end throughput (E2ETP). We introduce\nEncode-Prefill-Decode (EPD) Disaggregation, a novel framework that separates\nthe encoding, prefill, and decode stages onto dedicated resources. Unlike\ncurrent systems, which bundle encoding and prefill together, our approach\ndecouple these steps unlocking new opportunities and optimizations. These\ninclude a new mechanism to cache multimedia tokens for efficient transfer, a\nnovel way to parallelize encoding load within a request, a module to find the\noptimal resource allocation for disaggregated serving, and a novel role\nswitching method to handle changing workload characteristics. Experimental\nevaluations with popular LMMs show substantial gains in memory efficiency (up\nto 15$\\times$ less utilization), batch sizes (up to 22$\\times$ larger),\n10$\\times$ more images/request, and 2.2$\\times$ larger KV caches. Further, it\nleads to significant improvements in latency metrics (TTFT up to 71\\%\nreduction) and end-to-end throughput (up to 57\\% reduction), compared to\nsystems that do not disaggregate."
                },
                "authors": [
                    {
                        "name": "Gursimran Singh"
                    },
                    {
                        "name": "Xinglu Wang"
                    },
                    {
                        "name": "Yifan Hu"
                    },
                    {
                        "name": "Timothy Yu"
                    },
                    {
                        "name": "Linzi Xing"
                    },
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "arxiv_comment": "16 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05460v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05460v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04107v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04107v2",
                "updated": "2025-02-05T21:44:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    21,
                    44,
                    56,
                    2,
                    36,
                    0
                ],
                "published": "2024-08-07T22:10:26Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "title": "ZACK: Zero-Overhead LLM Inference Acceleration via Dimensionality\n  Compression of the Key-Value Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZACK: Zero-Overhead LLM Inference Acceleration via Dimensionality\n  Compression of the Key-Value Cache"
                },
                "summary": "In large-language models, memory constraints in the Key-Value Cache (KVC)\npose a challenge during inference. In this work, we propose ZACK, the first KV\ndimensionality compression system that achieves zero-overhead compression and\ndecompression and also reduces attention computation time. It complements and\ncan be combined with eviction-based and quantization-based methods to further\nenhance KV compression. Moreover, ZACK employs adaptive compression, tailoring\nKV compression rates across heads and layers based on their contributions to\ninference to maximize overall compression while maintaining an accuracy loss\nconstraint. Additionally, ZACK enhances the self-attention kernel to balance\nthe uneven workloads caused by the adaptive compression approach to further\nreduce attention computation latency. Comprehensive experiments demonstrate\nthat when combined with ZACK, state-of-the-art eviction-based and\nquantization-based methods for KV compression further reduce KV size by up to\n68%, Time-To-First-Token (TTFT) by up to 44%, and Time-Between-Tokens (TBT) by\nup to 55% and achieve up to 1.72X throughput under the same latency, while\nmaintaining 99% of the baseline accuracy. We open-sourced the code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-language models, memory constraints in the Key-Value Cache (KVC)\npose a challenge during inference. In this work, we propose ZACK, the first KV\ndimensionality compression system that achieves zero-overhead compression and\ndecompression and also reduces attention computation time. It complements and\ncan be combined with eviction-based and quantization-based methods to further\nenhance KV compression. Moreover, ZACK employs adaptive compression, tailoring\nKV compression rates across heads and layers based on their contributions to\ninference to maximize overall compression while maintaining an accuracy loss\nconstraint. Additionally, ZACK enhances the self-attention kernel to balance\nthe uneven workloads caused by the adaptive compression approach to further\nreduce attention computation latency. Comprehensive experiments demonstrate\nthat when combined with ZACK, state-of-the-art eviction-based and\nquantization-based methods for KV compression further reduce KV size by up to\n68%, Time-To-First-Token (TTFT) by up to 44%, and Time-Between-Tokens (TBT) by\nup to 55% and achieve up to 1.72X throughput under the same latency, while\nmaintaining 99% of the baseline accuracy. We open-sourced the code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04107v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04107v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10424v1",
                "updated": "2025-02-05T20:43:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    20,
                    43,
                    48,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T20:43:48Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    20,
                    43,
                    48,
                    2,
                    36,
                    0
                ],
                "title": "QuantSpec: Self-Speculative Decoding with Hierarchical Quantized KV\n  Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuantSpec: Self-Speculative Decoding with Hierarchical Quantized KV\n  Cache"
                },
                "summary": "Large Language Models (LLMs) are increasingly being deployed on edge devices\nfor long-context settings, creating a growing need for fast and efficient\nlong-context inference. In these scenarios, the Key-Value (KV) cache is the\nprimary bottleneck in terms of both GPU memory and latency, as the full KV\ncache must be loaded for each decoding step. While speculative decoding is a\nwidely accepted technique to accelerate autoregressive decoding, existing\nmethods often struggle to achieve significant speedups due to inefficient KV\ncache optimization strategies and result in low acceptance rates. To address\nthese challenges, we propose a novel self-speculative decoding framework,\nQuantSpec, where the draft model shares the architecture of the target model\nbut employs a hierarchical 4-bit quantized KV cache and 4-bit quantized weights\nfor acceleration. QuantSpec maintains high acceptance rates ($>$90%) and\nreliably provides consistent end-to-end speedups upto $\\sim2.5\\times$,\noutperforming other self-speculative decoding methods that use sparse KV cache\nfor long-context LLM inference. QuantSpec also reduces the memory requirements\nby $\\sim 1.3\\times$ compared to these alternatives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly being deployed on edge devices\nfor long-context settings, creating a growing need for fast and efficient\nlong-context inference. In these scenarios, the Key-Value (KV) cache is the\nprimary bottleneck in terms of both GPU memory and latency, as the full KV\ncache must be loaded for each decoding step. While speculative decoding is a\nwidely accepted technique to accelerate autoregressive decoding, existing\nmethods often struggle to achieve significant speedups due to inefficient KV\ncache optimization strategies and result in low acceptance rates. To address\nthese challenges, we propose a novel self-speculative decoding framework,\nQuantSpec, where the draft model shares the architecture of the target model\nbut employs a hierarchical 4-bit quantized KV cache and 4-bit quantized weights\nfor acceleration. QuantSpec maintains high acceptance rates ($>$90%) and\nreliably provides consistent end-to-end speedups upto $\\sim2.5\\times$,\noutperforming other self-speculative decoding methods that use sparse KV cache\nfor long-context LLM inference. QuantSpec also reduces the memory requirements\nby $\\sim 1.3\\times$ compared to these alternatives."
                },
                "authors": [
                    {
                        "name": "Rishabh Tiwari"
                    },
                    {
                        "name": "Haocheng Xi"
                    },
                    {
                        "name": "Aditya Tomar"
                    },
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Maxwell Horton"
                    },
                    {
                        "name": "Mahyar Najibi"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03589v1",
                "updated": "2025-02-05T20:09:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    20,
                    9,
                    51,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T20:09:51Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    20,
                    9,
                    51,
                    2,
                    36,
                    0
                ],
                "title": "HACK: Homomorphic Acceleration via Compression of the Key-Value Cache\n  for Disaggregated LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HACK: Homomorphic Acceleration via Compression of the Key-Value Cache\n  for Disaggregated LLM Inference"
                },
                "summary": "Disaggregated Large Language Model (LLM) inference has gained popularity as\nit separates the computation-intensive prefill stage from the memory-intensive\ndecode stage, avoiding the prefill-decode interference and improving resource\nutilization. However, transmitting Key-Value (KV) data between the two stages\ncan be a bottleneck, especially for long prompts. Additionally, the computation\ntime overhead for prefill and decode is key for optimizing Job Completion Time\n(JCT), and KV data size can become prohibitive for long prompts and sequences.\nExisting KV quantization methods can alleviate the transmission bottleneck and\nreduce memory requirements, but they introduce significant dequantization\noverhead, exacerbating the computation time.\n  We propose Homomorphic Acceleration via Compression of the KV cache (HACK)\nfor disaggregated LLM inference. HACK eliminates the heavy KV dequantization\nstep, and directly performs computations on quantized KV data to approximate\nand reduce the cost of the expensive matrix-multiplication step. Extensive\ntrace-driven experiments show that HACK reduces JCT by up to 70.9% compared to\ndisaggregated LLM inference baseline and by up to 52.3% compared to\nstate-of-the-art KV quantization methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated Large Language Model (LLM) inference has gained popularity as\nit separates the computation-intensive prefill stage from the memory-intensive\ndecode stage, avoiding the prefill-decode interference and improving resource\nutilization. However, transmitting Key-Value (KV) data between the two stages\ncan be a bottleneck, especially for long prompts. Additionally, the computation\ntime overhead for prefill and decode is key for optimizing Job Completion Time\n(JCT), and KV data size can become prohibitive for long prompts and sequences.\nExisting KV quantization methods can alleviate the transmission bottleneck and\nreduce memory requirements, but they introduce significant dequantization\noverhead, exacerbating the computation time.\n  We propose Homomorphic Acceleration via Compression of the KV cache (HACK)\nfor disaggregated LLM inference. HACK eliminates the heavy KV dequantization\nstep, and directly performs computations on quantized KV data to approximate\nand reduce the cost of the expensive matrix-multiplication step. Extensive\ntrace-driven experiments show that HACK reduces JCT by up to 70.9% compared to\ndisaggregated LLM inference baseline and by up to 52.3% compared to\nstate-of-the-art KV quantization methods."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Shay Vargaftik"
                    },
                    {
                        "name": "Ran Ben Basat"
                    },
                    {
                        "name": "Michael Mitzenmacher"
                    },
                    {
                        "name": "Minlan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Minlan Yu"
                },
                "author": "Minlan Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12959v2",
                "updated": "2025-02-05T09:35:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    9,
                    35,
                    38,
                    2,
                    36,
                    0
                ],
                "published": "2025-01-22T15:33:17Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    33,
                    17,
                    2,
                    22,
                    0
                ],
                "title": "Efficient Prompt Compression with Evaluator Heads for Long-Context\n  Transformer Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Prompt Compression with Evaluator Heads for Long-Context\n  Transformer Inference"
                },
                "summary": "Although applications involving long-context inputs are crucial for the\neffective utilization of large language models (LLMs), they also result in\nincreased computational costs and reduced performance. To address this\nchallenge, we propose an efficient, training-free prompt compression method\nthat retains key information within compressed prompts. We identify specific\nattention heads in transformer-based LLMs, which we designate as evaluator\nheads, that are capable of selecting tokens in long inputs that are most\nsignificant for inference. Building on this discovery, we develop EHPC, an\nEvaluator Head-based Prompt Compression method, which enables LLMs to rapidly\n\"skim through\" input prompts by leveraging only the first few layers with\nevaluator heads during the pre-filling stage, subsequently passing only the\nimportant tokens to the model for inference. EHPC achieves state-of-the-art\nresults across two mainstream benchmarks: prompt compression and long-context\ninference acceleration. Consequently, it effectively reduces the complexity and\ncosts associated with commercial API calls. We further demonstrate that EHPC\nattains competitive results compared to key-value cache-based acceleration\nmethods, thereby highlighting its potential to enhance the efficiency of LLMs\nfor long-context tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although applications involving long-context inputs are crucial for the\neffective utilization of large language models (LLMs), they also result in\nincreased computational costs and reduced performance. To address this\nchallenge, we propose an efficient, training-free prompt compression method\nthat retains key information within compressed prompts. We identify specific\nattention heads in transformer-based LLMs, which we designate as evaluator\nheads, that are capable of selecting tokens in long inputs that are most\nsignificant for inference. Building on this discovery, we develop EHPC, an\nEvaluator Head-based Prompt Compression method, which enables LLMs to rapidly\n\"skim through\" input prompts by leveraging only the first few layers with\nevaluator heads during the pre-filling stage, subsequently passing only the\nimportant tokens to the model for inference. EHPC achieves state-of-the-art\nresults across two mainstream benchmarks: prompt compression and long-context\ninference acceleration. Consequently, it effectively reduces the complexity and\ncosts associated with commercial API calls. We further demonstrate that EHPC\nattains competitive results compared to key-value cache-based acceleration\nmethods, thereby highlighting its potential to enhance the efficiency of LLMs\nfor long-context tasks."
                },
                "authors": [
                    {
                        "name": "Weizhi Fei"
                    },
                    {
                        "name": "Xueyan Niu"
                    },
                    {
                        "name": "Guoqing Xie"
                    },
                    {
                        "name": "Yingqing Liu"
                    },
                    {
                        "name": "Bo Bai"
                    },
                    {
                        "name": "Wei Han"
                    }
                ],
                "author_detail": {
                    "name": "Wei Han"
                },
                "author": "Wei Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14442v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14442v2",
                "updated": "2025-02-05T08:22:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    8,
                    22,
                    5,
                    2,
                    36,
                    0
                ],
                "published": "2024-10-18T13:01:14Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    1,
                    14,
                    4,
                    292,
                    0
                ],
                "title": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference"
                },
                "summary": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2$\\times$, most configurations can achieve higher throughput than\nstandard transformers while maintaining competitive performance. When further\nreducing the size of the KV cache, however, pairing queries of all layers with\nKVs of upper layers performs better, at the expense of additional training cost\nand prefilling latency. We hope that this work will help users make more\ninformed choices of cross-layer KV sharing approaches and facilitate future\nresearch on efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2$\\times$, most configurations can achieve higher throughput than\nstandard transformers while maintaining competitive performance. When further\nreducing the size of the KV cache, however, pairing queries of all layers with\nKVs of upper layers performs better, at the expense of additional training cost\nand prefilling latency. We hope that this work will help users make more\ninformed choices of cross-layer KV sharing approaches and facilitate future\nresearch on efficient LLM inference."
                },
                "authors": [
                    {
                        "name": "You Wu"
                    },
                    {
                        "name": "Haoyi Wu"
                    },
                    {
                        "name": "Kewei Tu"
                    }
                ],
                "author_detail": {
                    "name": "Kewei Tu"
                },
                "author": "Kewei Tu",
                "arxiv_comment": "Accepted to NAACL2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14442v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14442v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13331v2",
                "updated": "2025-02-05T08:10:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    8,
                    10,
                    45,
                    2,
                    36,
                    0
                ],
                "published": "2025-01-23T02:20:08Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    2,
                    20,
                    8,
                    3,
                    23,
                    0
                ],
                "title": "Qrazor: Reliable and Effortless 4-bit LLM Quantization by Significant\n  Data Razoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qrazor: Reliable and Effortless 4-bit LLM Quantization by Significant\n  Data Razoring"
                },
                "summary": "Large-scale language models (LLMs) excel in language processing tasks but\nface deployment challenges due to high memory and computational demands. While\nlow-bit quantization, such as 4-bit techniques, offers a potential solution,\nthese methods often suffer from significant accuracy loss or require\nconsiderable effort for implementation such as reordering, rotation, etc. To\naddress these challenges, we propose QRazor, a simple yet effective\nquantization scheme that enables 4-bit quantization of weights, activations,\nand KV cache in transformer-based LLMs. QRazor operates in two stages: first,\nquantizing data using 8 or 16-bit integers as a basis with absolute max scaling\nto preserve accuracy close to full-precision models, and second, compressing\nthe quantized data to 4-bit using our significant data razoring (SDR)\ntechnique, which retains only the four most salient bits. Without any\nadditional requirment of fine-tuning or additional training, QRazor achieves\nperformance similar or better compared to state-of-the-art in 4-bit\nquantization method, surpassing Smoothquant and QLLM by over 12 points and\nQuarot(RTN) by more than 2.9 points in zero-shot reasoning task accuracy on the\nLLaMA2-7B model. Additionally, we introduce an integer-based arithmetic unit\noptimized for QRazor, allowing direct low-precision operations on SDR data\nwithout decompression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale language models (LLMs) excel in language processing tasks but\nface deployment challenges due to high memory and computational demands. While\nlow-bit quantization, such as 4-bit techniques, offers a potential solution,\nthese methods often suffer from significant accuracy loss or require\nconsiderable effort for implementation such as reordering, rotation, etc. To\naddress these challenges, we propose QRazor, a simple yet effective\nquantization scheme that enables 4-bit quantization of weights, activations,\nand KV cache in transformer-based LLMs. QRazor operates in two stages: first,\nquantizing data using 8 or 16-bit integers as a basis with absolute max scaling\nto preserve accuracy close to full-precision models, and second, compressing\nthe quantized data to 4-bit using our significant data razoring (SDR)\ntechnique, which retains only the four most salient bits. Without any\nadditional requirment of fine-tuning or additional training, QRazor achieves\nperformance similar or better compared to state-of-the-art in 4-bit\nquantization method, surpassing Smoothquant and QLLM by over 12 points and\nQuarot(RTN) by more than 2.9 points in zero-shot reasoning task accuracy on the\nLLaMA2-7B model. Additionally, we introduce an integer-based arithmetic unit\noptimized for QRazor, allowing direct low-precision operations on SDR data\nwithout decompression."
                },
                "authors": [
                    {
                        "name": "Dongyoung Lee"
                    },
                    {
                        "name": "Seungkyu Choi"
                    },
                    {
                        "name": "Ik Joon Chang"
                    }
                ],
                "author_detail": {
                    "name": "Ik Joon Chang"
                },
                "author": "Ik Joon Chang",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02818v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02818v1",
                "updated": "2025-02-05T01:36:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    1,
                    36,
                    40,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T01:36:40Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    1,
                    36,
                    40,
                    2,
                    36,
                    0
                ],
                "title": "Accessible and Portable LLM Inference by Compiling Computational Graphs\n  into SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accessible and Portable LLM Inference by Compiling Computational Graphs\n  into SQL"
                },
                "summary": "Serving large language models (LLMs) often demands specialized hardware,\ndedicated frameworks, and substantial development efforts, which restrict their\naccessibility, especially for edge devices and organizations with limited\ntechnical resources. We propose a novel compiler that translates LLM inference\ngraphs into SQL queries, enabling relational databases, one of the most widely\nused and mature software systems globally, to serve as the runtime. By mapping\nneural operators such as matrix multiplication and attention into relational\nprimitives like joins and aggregations, our approach leverages database\ncapabilities, including disk-based data management and native caching.\nSupporting key transformer components, such as attention mechanisms and\nkey-value caching, our system generates SQL pipelines for end-to-end LLM\ninference. Using the Llama3 family as a case study, we demonstrate up to 30x\nspeedup in token generation for memory-constrained scenarios comparable to\ncompetitive CPU-based frameworks. Our work offers an accessible, portable, and\nefficient solution, facilitating the serving of LLMs across diverse deployment\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) often demands specialized hardware,\ndedicated frameworks, and substantial development efforts, which restrict their\naccessibility, especially for edge devices and organizations with limited\ntechnical resources. We propose a novel compiler that translates LLM inference\ngraphs into SQL queries, enabling relational databases, one of the most widely\nused and mature software systems globally, to serve as the runtime. By mapping\nneural operators such as matrix multiplication and attention into relational\nprimitives like joins and aggregations, our approach leverages database\ncapabilities, including disk-based data management and native caching.\nSupporting key transformer components, such as attention mechanisms and\nkey-value caching, our system generates SQL pipelines for end-to-end LLM\ninference. Using the Llama3 family as a case study, we demonstrate up to 30x\nspeedup in token generation for memory-constrained scenarios comparable to\ncompetitive CPU-based frameworks. Our work offers an accessible, portable, and\nefficient solution, facilitating the serving of LLMs across diverse deployment\nenvironments."
                },
                "authors": [
                    {
                        "name": "Wenbo Sun"
                    },
                    {
                        "name": "Qiming Guo"
                    },
                    {
                        "name": "Wenlu Wang"
                    },
                    {
                        "name": "Rihan Hai"
                    }
                ],
                "author_detail": {
                    "name": "Rihan Hai"
                },
                "author": "Rihan Hai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02818v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02818v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02750v1",
                "updated": "2025-02-04T22:37:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    22,
                    37,
                    17,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T22:37:17Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    22,
                    37,
                    17,
                    1,
                    35,
                    0
                ],
                "title": "Cache is King: Smart Page Eviction with eBPF",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache is King: Smart Page Eviction with eBPF"
                },
                "summary": "The page cache is a central part of an OS. It reduces repeated accesses to\nstorage by deciding which pages to retain in memory. As a result, the page\ncache has a significant impact on the performance of many applications.\nHowever, its one-size-fits-all eviction policy performs poorly in many\nworkloads. While the systems community has experimented with a plethora of new\nand adaptive eviction policies in non-OS settings (e.g., key-value stores,\nCDNs), it is very difficult to implement such policies in the page cache, due\nto the complexity of modifying kernel code. To address these shortcomings, we\ndesign a novel eBPF-based framework for the Linux page cache, called\n$\\texttt{cachebpf}$, that allows developers to customize the page cache without\nmodifying the kernel. $\\texttt{cachebpf}$ enables applications to customize the\npage cache policy for their specific needs, while also ensuring that different\napplications' policies do not interfere with each other and preserving the page\ncache's ability to share memory across different processes. We demonstrate the\nflexibility of $\\texttt{cachebpf}$'s interface by using it to implement several\neviction policies. Our evaluation shows that it is indeed beneficial for\napplications to customize the page cache to match their workloads' unique\nproperties, and that they can achieve up to 70% higher throughput and 58% lower\ntail latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The page cache is a central part of an OS. It reduces repeated accesses to\nstorage by deciding which pages to retain in memory. As a result, the page\ncache has a significant impact on the performance of many applications.\nHowever, its one-size-fits-all eviction policy performs poorly in many\nworkloads. While the systems community has experimented with a plethora of new\nand adaptive eviction policies in non-OS settings (e.g., key-value stores,\nCDNs), it is very difficult to implement such policies in the page cache, due\nto the complexity of modifying kernel code. To address these shortcomings, we\ndesign a novel eBPF-based framework for the Linux page cache, called\n$\\texttt{cachebpf}$, that allows developers to customize the page cache without\nmodifying the kernel. $\\texttt{cachebpf}$ enables applications to customize the\npage cache policy for their specific needs, while also ensuring that different\napplications' policies do not interfere with each other and preserving the page\ncache's ability to share memory across different processes. We demonstrate the\nflexibility of $\\texttt{cachebpf}$'s interface by using it to implement several\neviction policies. Our evaluation shows that it is indeed beneficial for\napplications to customize the page cache to match their workloads' unique\nproperties, and that they can achieve up to 70% higher throughput and 58% lower\ntail latency."
                },
                "authors": [
                    {
                        "name": "Tal Zussman"
                    },
                    {
                        "name": "Ioannis Zarkadas"
                    },
                    {
                        "name": "Jeremy Carin"
                    },
                    {
                        "name": "Andrew Cheng"
                    },
                    {
                        "name": "Hubertus Franke"
                    },
                    {
                        "name": "Jonas Pfefferle"
                    },
                    {
                        "name": "Asaf Cidon"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Cidon"
                },
                "author": "Asaf Cidon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02564v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02564v1",
                "updated": "2025-02-04T18:39:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    39,
                    10,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T18:39:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    39,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "CReIS: Computation Reuse through Image Similarity in ICN-Based Edge\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CReIS: Computation Reuse through Image Similarity in ICN-Based Edge\n  Computing"
                },
                "summary": "At the edge, there is a high level of similarity in computing. One approach\nthat has been proposed to enhance the efficiency of edge computing is\ncomputation reuse, which eliminates redundant computations. Edge computing is\nintegrated with the ICN architecture, capitalizing on its inherent intelligence\nto facilitate computation reuse and reduce redundancies in computing\noperations. In many past works, ICN's ability to enable computation reuse\nthrough caching has been limited. In this context, a new approach is proposed\nthat considers computation requests with similar input data, which yield\nidentical results, as equivalent. This method facilitates computation reuse\nthrough caching in ICN. The use of approximate results to reduce redundant\ncomputations without requiring high accuracy in input matching is provided.\nThis concept is termed the Similarity Index, which effectively considers images\nto be similar despite minor changes in the angle of photography. The Similarity\nIndex is determined through an algorithm known as HNSW and utilizes the SIFT\ndescriptor to identify similar data. This approach helps reduce user latency\ntimes by providing quick access to results. The evaluation, simulated using the\nndnSIM tool, showed an 86% improvement in completion time compared to scenarios\nwithout computation reuse, whereas previous works reported only a 70%\nimprovement. To strengthen this method, an analytical model for computing\nrequest transfer considering computation reuse in ICN-based edge computing is\nprovided. To assess the accuracy of the model, several evaluations have been\nconducted in the simulator by varying the parameters, resulting in a maximum\nerror percentage of approximately 16%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At the edge, there is a high level of similarity in computing. One approach\nthat has been proposed to enhance the efficiency of edge computing is\ncomputation reuse, which eliminates redundant computations. Edge computing is\nintegrated with the ICN architecture, capitalizing on its inherent intelligence\nto facilitate computation reuse and reduce redundancies in computing\noperations. In many past works, ICN's ability to enable computation reuse\nthrough caching has been limited. In this context, a new approach is proposed\nthat considers computation requests with similar input data, which yield\nidentical results, as equivalent. This method facilitates computation reuse\nthrough caching in ICN. The use of approximate results to reduce redundant\ncomputations without requiring high accuracy in input matching is provided.\nThis concept is termed the Similarity Index, which effectively considers images\nto be similar despite minor changes in the angle of photography. The Similarity\nIndex is determined through an algorithm known as HNSW and utilizes the SIFT\ndescriptor to identify similar data. This approach helps reduce user latency\ntimes by providing quick access to results. The evaluation, simulated using the\nndnSIM tool, showed an 86% improvement in completion time compared to scenarios\nwithout computation reuse, whereas previous works reported only a 70%\nimprovement. To strengthen this method, an analytical model for computing\nrequest transfer considering computation reuse in ICN-based edge computing is\nprovided. To assess the accuracy of the model, several evaluations have been\nconducted in the simulator by varying the parameters, resulting in a maximum\nerror percentage of approximately 16%."
                },
                "authors": [
                    {
                        "name": "Atiyeh Javaheri"
                    },
                    {
                        "name": "Ali Bohlooli"
                    },
                    {
                        "name": "Kamal Jamshidi"
                    }
                ],
                "author_detail": {
                    "name": "Kamal Jamshidi"
                },
                "author": "Kamal Jamshidi",
                "arxiv_comment": "18 pages, 14 figures, submit to Digital Communications and Networks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02564v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14001v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14001v2",
                "updated": "2025-02-04T17:14:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    14,
                    22,
                    1,
                    35,
                    0
                ],
                "published": "2024-08-26T03:58:20Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    58,
                    20,
                    0,
                    239,
                    0
                ],
                "title": "Decentralized Federated Learning with Model Caching on Mobile Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Federated Learning with Model Caching on Mobile Agents"
                },
                "summary": "Federated Learning (FL) trains a shared model using data and computation\npower on distributed agents coordinated by a central server. Decentralized FL\n(DFL) utilizes local model exchange and aggregation between agents to reduce\nthe communication and computation overheads on the central server. However,\nwhen agents are mobile, the communication opportunity between agents can be\nsporadic, largely hindering the convergence and accuracy of DFL. In this paper,\nwe propose Cached Decentralized Federated Learning (Cached-DFL) to investigate\ndelay-tolerant model spreading and aggregation enabled by model caching on\nmobile agents. Each agent stores not only its own model, but also models of\nagents encountered in the recent past. When two agents meet, they exchange\ntheir own models as well as the cached models. Local model aggregation utilizes\nall models stored in the cache. We theoretically analyze the convergence of\nCached-DFL, explicitly taking into account the model staleness introduced by\ncaching. We design and compare different model caching algorithms for different\nDFL and mobility scenarios. We conduct detailed case studies in a vehicular\nnetwork to systematically investigate the interplay between agent mobility,\ncache staleness, and model convergence. In our experiments, Cached-DFL\nconverges quickly, and significantly outperforms DFL without caching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) trains a shared model using data and computation\npower on distributed agents coordinated by a central server. Decentralized FL\n(DFL) utilizes local model exchange and aggregation between agents to reduce\nthe communication and computation overheads on the central server. However,\nwhen agents are mobile, the communication opportunity between agents can be\nsporadic, largely hindering the convergence and accuracy of DFL. In this paper,\nwe propose Cached Decentralized Federated Learning (Cached-DFL) to investigate\ndelay-tolerant model spreading and aggregation enabled by model caching on\nmobile agents. Each agent stores not only its own model, but also models of\nagents encountered in the recent past. When two agents meet, they exchange\ntheir own models as well as the cached models. Local model aggregation utilizes\nall models stored in the cache. We theoretically analyze the convergence of\nCached-DFL, explicitly taking into account the model staleness introduced by\ncaching. We design and compare different model caching algorithms for different\nDFL and mobility scenarios. We conduct detailed case studies in a vehicular\nnetwork to systematically investigate the interplay between agent mobility,\ncache staleness, and model convergence. In our experiments, Cached-DFL\nconverges quickly, and significantly outperforms DFL without caching."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Wang"
                    },
                    {
                        "name": "Guojun Xiong"
                    },
                    {
                        "name": "Houwei Cao"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "Oral Presentation at AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14001v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14001v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02493v1",
                "updated": "2025-02-04T17:09:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    9,
                    21,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T17:09:21Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    9,
                    21,
                    1,
                    35,
                    0
                ],
                "title": "EasySpec: Layer-Parallel Speculative Decoding for Efficient Multi-GPU\n  Utilization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EasySpec: Layer-Parallel Speculative Decoding for Efficient Multi-GPU\n  Utilization"
                },
                "summary": "Speculative decoding is an effective and lossless method for Large Language\nModel (LLM) inference acceleration. It employs a smaller model to generate a\ndraft token sequence, which is then verified by the original base model. In\nmulti-GPU systems, inference latency can be further reduced through tensor\nparallelism (TP), while the optimal TP size of the draft model is typically\nsmaller than that of the base model, leading to GPU idling during the drafting\nstage. To solve this problem, we propose EasySpec, a layer-parallel speculation\nstrategy that optimizes the efficiency of multi-GPU utilization.EasySpec breaks\nthe sequential execution order of layers in the drafting model, enabling\nmulti-layer parallelization across devices, albeit with some induced\napproximation errors. After each drafting-and-verification iteration, the draft\nmodel's key-value (KV) cache is calibrated in a single forward pass, preventing\nlong-term error accumulation at minimal additional latency. We evaluated\nEasySpec on several mainstream open-source LLMs, using smaller versions of\nmodels from the same series as drafters. The results demonstrate that EasySpec\ncan achieve a peak speedup of 4.17x compared to vanilla decoding, while\npreserving the original distribution of the base LLMs. Specifically, the\ndrafting stage can be accelerated by up to 1.62x with a maximum accuracy drop\nof only 7%, requiring no training or fine-tuning on the draft models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is an effective and lossless method for Large Language\nModel (LLM) inference acceleration. It employs a smaller model to generate a\ndraft token sequence, which is then verified by the original base model. In\nmulti-GPU systems, inference latency can be further reduced through tensor\nparallelism (TP), while the optimal TP size of the draft model is typically\nsmaller than that of the base model, leading to GPU idling during the drafting\nstage. To solve this problem, we propose EasySpec, a layer-parallel speculation\nstrategy that optimizes the efficiency of multi-GPU utilization.EasySpec breaks\nthe sequential execution order of layers in the drafting model, enabling\nmulti-layer parallelization across devices, albeit with some induced\napproximation errors. After each drafting-and-verification iteration, the draft\nmodel's key-value (KV) cache is calibrated in a single forward pass, preventing\nlong-term error accumulation at minimal additional latency. We evaluated\nEasySpec on several mainstream open-source LLMs, using smaller versions of\nmodels from the same series as drafters. The results demonstrate that EasySpec\ncan achieve a peak speedup of 4.17x compared to vanilla decoding, while\npreserving the original distribution of the base LLMs. Specifically, the\ndrafting stage can be accelerated by up to 1.62x with a maximum accuracy drop\nof only 7%, requiring no training or fine-tuning on the draft models."
                },
                "authors": [
                    {
                        "name": "Yize Wu"
                    },
                    {
                        "name": "Ke Gao"
                    },
                    {
                        "name": "Yanjun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yanjun Wu"
                },
                "author": "Yanjun Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02437v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02437v1",
                "updated": "2025-02-04T16:03:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    3,
                    52,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T16:03:52Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    3,
                    52,
                    1,
                    35,
                    0
                ],
                "title": "H-MBR: Hypervisor-level Memory Bandwidth Reservation for Mixed\n  Criticality Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "H-MBR: Hypervisor-level Memory Bandwidth Reservation for Mixed\n  Criticality Systems"
                },
                "summary": "Recent advancements in fields such as automotive and aerospace have driven a\ngrowing demand for robust computational resources. Applications that were once\ndesigned for basic MCUs are now deployed on highly heterogeneous SoC platforms.\nWhile these platforms deliver the necessary computational performance, they\nalso present challenges related to resource sharing and predictability. These\nchallenges are particularly pronounced when consolidating safety and\nnon-safety-critical systems, the so-called Mixed-Criticality Systems (MCS) to\nadhere to strict SWaP-C requirements. MCS consolidation on shared platforms\nrequires stringent spatial and temporal isolation to comply with functional\nsafety standards. Virtualization, mainly leveraged by hypervisors, is a key\ntechnology that ensures spatial isolation across multiple OSes and\napplications; however, ensuring temporal isolation remains challenging due to\ncontention on shared hardwar resources, which impacts real-time performance and\npredictability. To mitigate this problem, several strategies as cache coloring\nand memory bandwidth reservation have been proposed. Although cache coloring is\ntypically implemented on state-of-the-art hypervisors, memory bandwidth\nreservation approaches are commonly implemented at the Linux kernel level or\nrely on dedicated hardware and typically do not consider the concept of VMs\nthat can run different OSes. To fill the gap between current memory bandwidth\nreservation solutions and the deployment of MCSs that operate on a hypervisor,\nthis work introduces H-MBR, an open-source VM-centric memory bandwidth\nreservation mechanism. H-MBR features (i) VM-centric bandwidth reservation,\n(ii) OS and platform agnosticism, and (iii) reduced overhead. Empirical results\nevidenced no overhead on non-regulated workloads, and negligible overhead (<1%)\nfor regulated workloads for regulation periods of 2 us or higher.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in fields such as automotive and aerospace have driven a\ngrowing demand for robust computational resources. Applications that were once\ndesigned for basic MCUs are now deployed on highly heterogeneous SoC platforms.\nWhile these platforms deliver the necessary computational performance, they\nalso present challenges related to resource sharing and predictability. These\nchallenges are particularly pronounced when consolidating safety and\nnon-safety-critical systems, the so-called Mixed-Criticality Systems (MCS) to\nadhere to strict SWaP-C requirements. MCS consolidation on shared platforms\nrequires stringent spatial and temporal isolation to comply with functional\nsafety standards. Virtualization, mainly leveraged by hypervisors, is a key\ntechnology that ensures spatial isolation across multiple OSes and\napplications; however, ensuring temporal isolation remains challenging due to\ncontention on shared hardwar resources, which impacts real-time performance and\npredictability. To mitigate this problem, several strategies as cache coloring\nand memory bandwidth reservation have been proposed. Although cache coloring is\ntypically implemented on state-of-the-art hypervisors, memory bandwidth\nreservation approaches are commonly implemented at the Linux kernel level or\nrely on dedicated hardware and typically do not consider the concept of VMs\nthat can run different OSes. To fill the gap between current memory bandwidth\nreservation solutions and the deployment of MCSs that operate on a hypervisor,\nthis work introduces H-MBR, an open-source VM-centric memory bandwidth\nreservation mechanism. H-MBR features (i) VM-centric bandwidth reservation,\n(ii) OS and platform agnosticism, and (iii) reduced overhead. Empirical results\nevidenced no overhead on non-regulated workloads, and negligible overhead (<1%)\nfor regulated workloads for regulation periods of 2 us or higher."
                },
                "authors": [
                    {
                        "name": "Afonso Oliveira"
                    },
                    {
                        "name": "Diogo Costa"
                    },
                    {
                        "name": "Gonalo Moreira"
                    },
                    {
                        "name": "Jos Martins"
                    },
                    {
                        "name": "Sandro Pinto"
                    }
                ],
                "author_detail": {
                    "name": "Sandro Pinto"
                },
                "author": "Sandro Pinto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02437v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02437v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02430v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02430v1",
                "updated": "2025-02-04T15:55:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    55,
                    10,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T15:55:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    55,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "A Scalable Crawling Algorithm Utilizing Noisy Change-Indicating Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Scalable Crawling Algorithm Utilizing Noisy Change-Indicating Signals"
                },
                "summary": "Web refresh crawling is the problem of keeping a cache of web pages fresh,\nthat is, having the most recent copy available when a page is requested, given\na limited bandwidth available to the crawler. Under the assumption that the\nchange and request events, resp., to each web page follow independent Poisson\nprocesses, the optimal scheduling policy was derived by Azar et al. 2018. In\nthis paper, we study an extension of this problem where side information\nindicating content changes, such as various types of web pings, for example,\nsignals from sitemaps, content delivery networks, etc., is available.\nIncorporating such side information into the crawling policy is challenging,\nbecause (i) the signals can be noisy with false positive events and with\nmissing change events; and (ii) the crawler should achieve a fair performance\nover web pages regardless of the quality of the side information, which might\ndiffer from web page to web page. We propose a scalable crawling algorithm\nwhich (i) uses the noisy side information in an optimal way under mild\nassumptions; (ii) can be deployed without heavy centralized computation; (iii)\nis able to crawl web pages at a constant total rate without spikes in the total\nbandwidth usage over any time interval, and automatically adapt to the new\noptimal solution when the total bandwidth changes without centralized\ncomputation. Experiments clearly demonstrate the versatility of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web refresh crawling is the problem of keeping a cache of web pages fresh,\nthat is, having the most recent copy available when a page is requested, given\na limited bandwidth available to the crawler. Under the assumption that the\nchange and request events, resp., to each web page follow independent Poisson\nprocesses, the optimal scheduling policy was derived by Azar et al. 2018. In\nthis paper, we study an extension of this problem where side information\nindicating content changes, such as various types of web pings, for example,\nsignals from sitemaps, content delivery networks, etc., is available.\nIncorporating such side information into the crawling policy is challenging,\nbecause (i) the signals can be noisy with false positive events and with\nmissing change events; and (ii) the crawler should achieve a fair performance\nover web pages regardless of the quality of the side information, which might\ndiffer from web page to web page. We propose a scalable crawling algorithm\nwhich (i) uses the noisy side information in an optimal way under mild\nassumptions; (ii) can be deployed without heavy centralized computation; (iii)\nis able to crawl web pages at a constant total rate without spikes in the total\nbandwidth usage over any time interval, and automatically adapt to the new\noptimal solution when the total bandwidth changes without centralized\ncomputation. Experiments clearly demonstrate the versatility of our approach."
                },
                "authors": [
                    {
                        "name": "Rbert Busa-Fekete"
                    },
                    {
                        "name": "Julian Zimmert"
                    },
                    {
                        "name": "Andrs Gyrgy"
                    },
                    {
                        "name": "Linhai Qiu"
                    },
                    {
                        "name": "Tzu-Wei Sung"
                    },
                    {
                        "name": "Hao Shen"
                    },
                    {
                        "name": "Hyomin Choi"
                    },
                    {
                        "name": "Sharmila Subramaniam"
                    },
                    {
                        "name": "Li Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Li Xiao"
                },
                "author": "Li Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02430v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02349v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02349v1",
                "updated": "2025-02-04T14:33:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    33,
                    44,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T14:33:44Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    33,
                    44,
                    1,
                    35,
                    0
                ],
                "title": "Random Adaptive Cache Placement Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Random Adaptive Cache Placement Policy"
                },
                "summary": "This paper presents a new hybrid cache replacement algorithm that combines\nrandom allocation with a modified V-Way cache implementation. Our RAC adapts to\ncomplex cache access patterns and optimizes cache usage by improving the\nutilization of cache sets, unlike traditional cache policies. The algorithm\nutilizes a 16-way set-associative cache with 2048 sets, incorporating dynamic\nallocation and flexible tag management. RAC extends the V-Way cache design and\nits variants by optimizing tag and data storage for enhanced efficiency.\n  We evaluated the algorithm using the ChampSim simulator with four diverse\nbenchmark traces and observed significant improvements in cache hit rates up to\n80.82% hit rate. Although the improvements in the instructions per cycle (IPC)\nwere moderate, our findings emphasize the algorithm's potential to enhance\ncache utilization and reduce memory access times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a new hybrid cache replacement algorithm that combines\nrandom allocation with a modified V-Way cache implementation. Our RAC adapts to\ncomplex cache access patterns and optimizes cache usage by improving the\nutilization of cache sets, unlike traditional cache policies. The algorithm\nutilizes a 16-way set-associative cache with 2048 sets, incorporating dynamic\nallocation and flexible tag management. RAC extends the V-Way cache design and\nits variants by optimizing tag and data storage for enhanced efficiency.\n  We evaluated the algorithm using the ChampSim simulator with four diverse\nbenchmark traces and observed significant improvements in cache hit rates up to\n80.82% hit rate. Although the improvements in the instructions per cycle (IPC)\nwere moderate, our findings emphasize the algorithm's potential to enhance\ncache utilization and reduce memory access times."
                },
                "authors": [
                    {
                        "name": "Vrushank Ahire"
                    },
                    {
                        "name": "Pranav Menon"
                    },
                    {
                        "name": "Aniruddh Muley"
                    },
                    {
                        "name": "Abhinandan S. Prasad"
                    }
                ],
                "author_detail": {
                    "name": "Abhinandan S. Prasad"
                },
                "author": "Abhinandan S. Prasad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02349v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02349v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13846v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13846v2",
                "updated": "2025-02-04T13:45:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    13,
                    45,
                    37,
                    1,
                    35,
                    0
                ],
                "published": "2024-10-17T17:58:14Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    58,
                    14,
                    3,
                    291,
                    0
                ],
                "title": "LightTransfer: Your Long-Context LLM is Secretly a Hybrid Model with\n  Effortless Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightTransfer: Your Long-Context LLM is Secretly a Hybrid Model with\n  Effortless Adaptation"
                },
                "summary": "Scaling language models to handle longer contexts introduces substantial\nmemory challenges due to the growing cost of key-value (KV) caches. Motivated\nby the efficiency gains of hybrid models and the broad availability of\npretrained large transformer backbones, we explore transitioning transformer\nmodels into hybrid architectures for a more efficient generation. In this work,\nwe propose LightTransfer, a lightweight method that transforms models such as\nLLaMA into hybrid variants. Our approach identifies lazy layers -- those\nfocusing on recent or initial tokens -- and replaces their full attention with\nstreaming attention. This transformation can be performed without any training\nfor long-context understanding tasks or with minimal fine-tuning for o1-like\nlong reasoning generation tasks that require stronger reasoning capabilities.\nExperiments across diverse benchmarks and models (e.g., LLaMA, Mistral,\nQwQ-STILL) demonstrate that, even when half of the layers are identified as\nlazy, LightTransfer achieves up to 2.17$\\times$ throughput improvement with\nminimal performance loss ($<1.5\\%$ on LongBench) and achieves 53.3\\% on math\nbenchmark AIME24 of advanced o1-like long reasoning model QwQ-STILL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer contexts introduces substantial\nmemory challenges due to the growing cost of key-value (KV) caches. Motivated\nby the efficiency gains of hybrid models and the broad availability of\npretrained large transformer backbones, we explore transitioning transformer\nmodels into hybrid architectures for a more efficient generation. In this work,\nwe propose LightTransfer, a lightweight method that transforms models such as\nLLaMA into hybrid variants. Our approach identifies lazy layers -- those\nfocusing on recent or initial tokens -- and replaces their full attention with\nstreaming attention. This transformation can be performed without any training\nfor long-context understanding tasks or with minimal fine-tuning for o1-like\nlong reasoning generation tasks that require stronger reasoning capabilities.\nExperiments across diverse benchmarks and models (e.g., LLaMA, Mistral,\nQwQ-STILL) demonstrate that, even when half of the layers are identified as\nlazy, LightTransfer achieves up to 2.17$\\times$ throughput improvement with\nminimal performance loss ($<1.5\\%$ on LongBench) and achieves 53.3\\% on math\nbenchmark AIME24 of advanced o1-like long reasoning model QwQ-STILL."
                },
                "authors": [
                    {
                        "name": "Xuan Zhang"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13846v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13846v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02175v1",
                "updated": "2025-02-04T09:48:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    48,
                    14,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T09:48:14Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    48,
                    14,
                    1,
                    35,
                    0
                ],
                "title": "VLA-Cache: Towards Efficient Vision-Language-Action Model via Adaptive\n  Token Caching in Robotic Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLA-Cache: Towards Efficient Vision-Language-Action Model via Adaptive\n  Token Caching in Robotic Manipulation"
                },
                "summary": "Vision-Language-Action (VLA) model can process instructions and visual\nperception to directly generate actions as output in an end-to-end fashion due\nto its strong multi-modal reasoning capabilities. While the performance of VLA\nmodels is promising, their computational cost can be substantial. This raises\nchallenge for applying them on robotics tasks, which requires real-time\ndecision-making to respond quickly to environmental changes. Since robotic\ncontrol involves sequential decision-making, the visual input often exhibits\nminimal variation between successive steps. A natural idea is to reuse the\ncomputational results of unchanged visual tokens from the last step. Motivated\nby this idea, we propose VLA-Cache, an efficient vision-language-action model.\nVLA-Cache incorporates a token-selection mechanism that compares the visual\ninput at each step with the input from the previous step, adaptively\nidentifying visual tokens with minimal changes. The computational results for\nthese unchanged tokens are then reused in subsequent steps via KV-cache,\nthereby significantly improving the efficiency of the VLA-Cache model.\nExperimental results on both simulation (e.g., LIBERO benchmark and SIMPLER)\nand real-world robot valid VLA-Cache can achieve practical acceleration with\nminimal sacrifice in success rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) model can process instructions and visual\nperception to directly generate actions as output in an end-to-end fashion due\nto its strong multi-modal reasoning capabilities. While the performance of VLA\nmodels is promising, their computational cost can be substantial. This raises\nchallenge for applying them on robotics tasks, which requires real-time\ndecision-making to respond quickly to environmental changes. Since robotic\ncontrol involves sequential decision-making, the visual input often exhibits\nminimal variation between successive steps. A natural idea is to reuse the\ncomputational results of unchanged visual tokens from the last step. Motivated\nby this idea, we propose VLA-Cache, an efficient vision-language-action model.\nVLA-Cache incorporates a token-selection mechanism that compares the visual\ninput at each step with the input from the previous step, adaptively\nidentifying visual tokens with minimal changes. The computational results for\nthese unchanged tokens are then reused in subsequent steps via KV-cache,\nthereby significantly improving the efficiency of the VLA-Cache model.\nExperimental results on both simulation (e.g., LIBERO benchmark and SIMPLER)\nand real-world robot valid VLA-Cache can achieve practical acceleration with\nminimal sacrifice in success rate."
                },
                "authors": [
                    {
                        "name": "Siyu Xu"
                    },
                    {
                        "name": "Yunke Wang"
                    },
                    {
                        "name": "Chenghao Xia"
                    },
                    {
                        "name": "Dihao Zhu"
                    },
                    {
                        "name": "Tao Huang"
                    },
                    {
                        "name": "Chang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Xu"
                },
                "author": "Chang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02617v1",
                "updated": "2025-02-04T08:52:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    8,
                    52,
                    13,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T08:52:13Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    8,
                    52,
                    13,
                    1,
                    35,
                    0
                ],
                "title": "PolarQuant: Quantizing KV Caches with Polar Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PolarQuant: Quantizing KV Caches with Polar Transformation"
                },
                "summary": "Large language models (LLMs) require significant memory to store Key-Value\n(KV) embeddings in their KV cache, especially when handling long-range\ncontexts. Quantization of these KV embeddings is a common technique to reduce\nmemory consumption. This work introduces PolarQuant, a novel quantization\nmethod employing random preconditioning and polar transformation. Our method\ntransforms the KV embeddings into polar coordinates using an efficient\nrecursive algorithm and then quantizes resulting angles. Our key insight is\nthat, after random preconditioning, the angles in the polar representation\nexhibit a tightly bounded and highly concentrated distribution with an\nanalytically computable form. This nice distribution eliminates the need for\nexplicit normalization, a step required by traditional quantization methods\nwhich introduces significant memory overhead because quantization parameters\n(e.g., zero point and scale) must be stored in full precision per each data\nblock. PolarQuant bypasses this normalization step, enabling substantial memory\nsavings. The long-context evaluation demonstrates that PolarQuant compresses\nthe KV cache by over x4.2 while achieving the best quality scores compared to\nthe state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) require significant memory to store Key-Value\n(KV) embeddings in their KV cache, especially when handling long-range\ncontexts. Quantization of these KV embeddings is a common technique to reduce\nmemory consumption. This work introduces PolarQuant, a novel quantization\nmethod employing random preconditioning and polar transformation. Our method\ntransforms the KV embeddings into polar coordinates using an efficient\nrecursive algorithm and then quantizes resulting angles. Our key insight is\nthat, after random preconditioning, the angles in the polar representation\nexhibit a tightly bounded and highly concentrated distribution with an\nanalytically computable form. This nice distribution eliminates the need for\nexplicit normalization, a step required by traditional quantization methods\nwhich introduces significant memory overhead because quantization parameters\n(e.g., zero point and scale) must be stored in full precision per each data\nblock. PolarQuant bypasses this normalization step, enabling substantial memory\nsavings. The long-context evaluation demonstrates that PolarQuant compresses\nthe KV cache by over x4.2 while achieving the best quality scores compared to\nthe state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Insu Han"
                    },
                    {
                        "name": "Praneeth Kacham"
                    },
                    {
                        "name": "Amin Karbasi"
                    },
                    {
                        "name": "Vahab Mirrokni"
                    },
                    {
                        "name": "Amir Zandieh"
                    }
                ],
                "author_detail": {
                    "name": "Amir Zandieh"
                },
                "author": "Amir Zandieh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v4",
                "updated": "2025-02-04T08:16:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    8,
                    16,
                    31,
                    1,
                    35,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on\n  GitHub^_^. Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02069v1",
                "updated": "2025-02-04T07:40:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    7,
                    40,
                    26,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T07:40:26Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    7,
                    40,
                    26,
                    1,
                    35,
                    0
                ],
                "title": "LoRA-TTT: Low-Rank Test-Time Training for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRA-TTT: Low-Rank Test-Time Training for Vision-Language Models"
                },
                "summary": "The rapid advancements in vision-language models (VLMs), such as CLIP, have\nintensified the need to address distribution shifts between training and\ntesting datasets. Although prior Test-Time Training (TTT) techniques for VLMs\nhave demonstrated robust performance, they predominantly rely on tuning text\nprompts, a process that demands substantial computational resources and is\nheavily dependent on entropy-based loss. In this paper, we propose LoRA-TTT, a\nnovel TTT method that leverages Low-Rank Adaptation (LoRA), applied exclusively\nto the image encoder of VLMs. By introducing LoRA and updating only its\nparameters during test time, our method offers a simple yet effective TTT\napproach, retaining the model's initial generalization capability while\nachieving substantial performance gains with minimal memory and runtime\noverhead. Additionally, we introduce a highly efficient reconstruction loss\ntailored for TTT. Our method can adapt to diverse domains by combining these\ntwo losses, without increasing memory consumption or runtime. Extensive\nexperiments on two benchmarks, covering 15 datasets, demonstrate that our\nmethod improves the zero-shot top-1 accuracy of CLIP-ViT-B/16 by an average of\n5.79% on the OOD benchmark and 1.36% on the fine-grained benchmark, efficiently\nsurpassing test-time prompt tuning, without relying on any external models or\ncache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancements in vision-language models (VLMs), such as CLIP, have\nintensified the need to address distribution shifts between training and\ntesting datasets. Although prior Test-Time Training (TTT) techniques for VLMs\nhave demonstrated robust performance, they predominantly rely on tuning text\nprompts, a process that demands substantial computational resources and is\nheavily dependent on entropy-based loss. In this paper, we propose LoRA-TTT, a\nnovel TTT method that leverages Low-Rank Adaptation (LoRA), applied exclusively\nto the image encoder of VLMs. By introducing LoRA and updating only its\nparameters during test time, our method offers a simple yet effective TTT\napproach, retaining the model's initial generalization capability while\nachieving substantial performance gains with minimal memory and runtime\noverhead. Additionally, we introduce a highly efficient reconstruction loss\ntailored for TTT. Our method can adapt to diverse domains by combining these\ntwo losses, without increasing memory consumption or runtime. Extensive\nexperiments on two benchmarks, covering 15 datasets, demonstrate that our\nmethod improves the zero-shot top-1 accuracy of CLIP-ViT-B/16 by an average of\n5.79% on the OOD benchmark and 1.36% on the fine-grained benchmark, efficiently\nsurpassing test-time prompt tuning, without relying on any external models or\ncache."
                },
                "authors": [
                    {
                        "name": "Yuto Kojima"
                    },
                    {
                        "name": "Jiarui Xu"
                    },
                    {
                        "name": "Xueyan Zou"
                    },
                    {
                        "name": "Xiaolong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaolong Wang"
                },
                "author": "Xiaolong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2502.13145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13145v1",
                "updated": "2025-02-18T18:59:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    59,
                    57,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T18:59:57Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    59,
                    57,
                    1,
                    49,
                    0
                ],
                "title": "Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation"
                },
                "summary": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba"
                },
                "authors": [
                    {
                        "name": "Bencheng Liao"
                    },
                    {
                        "name": "Hongyuan Tao"
                    },
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Tianheng Cheng"
                    },
                    {
                        "name": "Yingyue Li"
                    },
                    {
                        "name": "Haoran Yin"
                    },
                    {
                        "name": "Wenyu Liu"
                    },
                    {
                        "name": "Xinggang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinggang Wang"
                },
                "author": "Xinggang Wang",
                "arxiv_comment": "Code and model are available at https://github.com/hustvl/mmMamba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13146v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13146v1",
                "updated": "2025-02-18T18:59:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    59,
                    57,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T18:59:57Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    59,
                    57,
                    1,
                    49,
                    0
                ],
                "title": "Re-Align: Aligning Vision Language Models via Retrieval-Augmented Direct\n  Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re-Align: Aligning Vision Language Models via Retrieval-Augmented Direct\n  Preference Optimization"
                },
                "summary": "The emergence of large Vision Language Models (VLMs) has broadened the scope\nand capabilities of single-modal Large Language Models (LLMs) by integrating\nvisual modalities, thereby unlocking transformative cross-modal applications in\na variety of real-world scenarios. Despite their impressive performance, VLMs\nare prone to significant hallucinations, particularly in the form of\ncross-modal inconsistencies. Building on the success of Reinforcement Learning\nfrom Human Feedback (RLHF) in aligning LLMs, recent advancements have focused\non applying direct preference optimization (DPO) on carefully curated datasets\nto mitigate these issues. Yet, such approaches typically introduce preference\nsignals in a brute-force manner, neglecting the crucial role of visual\ninformation in the alignment process. In this paper, we introduce Re-Align, a\nnovel alignment framework that leverages image retrieval to construct a\ndual-preference dataset, effectively incorporating both textual and visual\npreference signals. We further introduce rDPO, an extension of the standard\ndirect preference optimization that incorporates an additional visual\npreference objective during fine-tuning. Our experimental results demonstrate\nthat Re-Align not only mitigates hallucinations more effectively than previous\nmethods but also yields significant performance gains in general visual\nquestion-answering (VQA) tasks. Moreover, we show that Re-Align maintains\nrobustness and scalability across a wide range of VLM sizes and architectures.\nThis work represents a significant step forward in aligning multimodal LLMs,\npaving the way for more reliable and effective cross-modal applications. We\nrelease all the code in https://github.com/taco-group/Re-Align.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large Vision Language Models (VLMs) has broadened the scope\nand capabilities of single-modal Large Language Models (LLMs) by integrating\nvisual modalities, thereby unlocking transformative cross-modal applications in\na variety of real-world scenarios. Despite their impressive performance, VLMs\nare prone to significant hallucinations, particularly in the form of\ncross-modal inconsistencies. Building on the success of Reinforcement Learning\nfrom Human Feedback (RLHF) in aligning LLMs, recent advancements have focused\non applying direct preference optimization (DPO) on carefully curated datasets\nto mitigate these issues. Yet, such approaches typically introduce preference\nsignals in a brute-force manner, neglecting the crucial role of visual\ninformation in the alignment process. In this paper, we introduce Re-Align, a\nnovel alignment framework that leverages image retrieval to construct a\ndual-preference dataset, effectively incorporating both textual and visual\npreference signals. We further introduce rDPO, an extension of the standard\ndirect preference optimization that incorporates an additional visual\npreference objective during fine-tuning. Our experimental results demonstrate\nthat Re-Align not only mitigates hallucinations more effectively than previous\nmethods but also yields significant performance gains in general visual\nquestion-answering (VQA) tasks. Moreover, we show that Re-Align maintains\nrobustness and scalability across a wide range of VLM sizes and architectures.\nThis work represents a significant step forward in aligning multimodal LLMs,\npaving the way for more reliable and effective cross-modal applications. We\nrelease all the code in https://github.com/taco-group/Re-Align."
                },
                "authors": [
                    {
                        "name": "Shuo Xing"
                    },
                    {
                        "name": "Yuping Wang"
                    },
                    {
                        "name": "Peiran Li"
                    },
                    {
                        "name": "Ruizheng Bai"
                    },
                    {
                        "name": "Yueqi Wang"
                    },
                    {
                        "name": "Chengxuan Qian"
                    },
                    {
                        "name": "Huaxiu Yao"
                    },
                    {
                        "name": "Zhengzhong Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhengzhong Tu"
                },
                "author": "Zhengzhong Tu",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13146v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13146v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13141v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13141v1",
                "updated": "2025-02-18T18:59:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    59,
                    0,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T18:59:00Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    59,
                    0,
                    1,
                    49,
                    0
                ],
                "title": "UniGuardian: A Unified Defense for Detecting Prompt Injection, Backdoor\n  Attacks and Adversarial Attacks in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniGuardian: A Unified Defense for Detecting Prompt Injection, Backdoor\n  Attacks and Adversarial Attacks in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are vulnerable to attacks like prompt injection,\nbackdoor attacks, and adversarial attacks, which manipulate prompts or models\nto generate harmful outputs. In this paper, departing from traditional deep\nlearning attack paradigms, we explore their intrinsic relationship and\ncollectively term them Prompt Trigger Attacks (PTA). This raises a key\nquestion: Can we determine if a prompt is benign or poisoned? To address this,\nwe propose UniGuardian, the first unified defense mechanism designed to detect\nprompt injection, backdoor attacks, and adversarial attacks in LLMs.\nAdditionally, we introduce a single-forward strategy to optimize the detection\npipeline, enabling simultaneous attack detection and text generation within a\nsingle forward pass. Our experiments confirm that UniGuardian accurately and\nefficiently identifies malicious prompts in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are vulnerable to attacks like prompt injection,\nbackdoor attacks, and adversarial attacks, which manipulate prompts or models\nto generate harmful outputs. In this paper, departing from traditional deep\nlearning attack paradigms, we explore their intrinsic relationship and\ncollectively term them Prompt Trigger Attacks (PTA). This raises a key\nquestion: Can we determine if a prompt is benign or poisoned? To address this,\nwe propose UniGuardian, the first unified defense mechanism designed to detect\nprompt injection, backdoor attacks, and adversarial attacks in LLMs.\nAdditionally, we introduce a single-forward strategy to optimize the detection\npipeline, enabling simultaneous attack detection and text generation within a\nsingle forward pass. Our experiments confirm that UniGuardian accurately and\nefficiently identifies malicious prompts in LLMs."
                },
                "authors": [
                    {
                        "name": "Huawei Lin"
                    },
                    {
                        "name": "Yingjie Lao"
                    },
                    {
                        "name": "Tong Geng"
                    },
                    {
                        "name": "Tan Yu"
                    },
                    {
                        "name": "Weijie Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Weijie Zhao"
                },
                "author": "Weijie Zhao",
                "arxiv_comment": "18 Pages, 8 Figures, 5 Tables, Keywords: Attack Defending, Security,\n  Prompt Injection, Backdoor Attacks, Adversarial Attacks, Prompt Trigger\n  Attacks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13141v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13141v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13138v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13138v1",
                "updated": "2025-02-18T18:57:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    57,
                    21,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T18:57:21Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    57,
                    21,
                    1,
                    49,
                    0
                ],
                "title": "AIDE: AI-Driven Exploration in the Space of Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIDE: AI-Driven Exploration in the Space of Code"
                },
                "summary": "Machine learning, the foundation of modern artificial intelligence, has\ndriven innovations that have fundamentally transformed the world. Yet, behind\nadvancements lies a complex and often tedious process requiring labor and\ncompute intensive iteration and experimentation. Engineers and scientists\ndeveloping machine learning models spend much of their time on trial-and-error\ntasks instead of conceptualizing innovative solutions or research hypotheses.\nTo address this challenge, we introduce AI-Driven Exploration (AIDE), a machine\nlearning engineering agent powered by large language models (LLMs). AIDE frames\nmachine learning engineering as a code optimization problem, and formulates\ntrial-and-error as a tree search in the space of potential solutions. By\nstrategically reusing and refining promising solutions, AIDE effectively trades\ncomputational resources for enhanced performance, achieving state-of-the-art\nresults on multiple machine learning engineering benchmarks, including our\nKaggle evaluations, OpenAI MLE-Bench and METRs RE-Bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning, the foundation of modern artificial intelligence, has\ndriven innovations that have fundamentally transformed the world. Yet, behind\nadvancements lies a complex and often tedious process requiring labor and\ncompute intensive iteration and experimentation. Engineers and scientists\ndeveloping machine learning models spend much of their time on trial-and-error\ntasks instead of conceptualizing innovative solutions or research hypotheses.\nTo address this challenge, we introduce AI-Driven Exploration (AIDE), a machine\nlearning engineering agent powered by large language models (LLMs). AIDE frames\nmachine learning engineering as a code optimization problem, and formulates\ntrial-and-error as a tree search in the space of potential solutions. By\nstrategically reusing and refining promising solutions, AIDE effectively trades\ncomputational resources for enhanced performance, achieving state-of-the-art\nresults on multiple machine learning engineering benchmarks, including our\nKaggle evaluations, OpenAI MLE-Bench and METRs RE-Bench."
                },
                "authors": [
                    {
                        "name": "Zhengyao Jiang"
                    },
                    {
                        "name": "Dominik Schmidt"
                    },
                    {
                        "name": "Dhruv Srikanth"
                    },
                    {
                        "name": "Dixing Xu"
                    },
                    {
                        "name": "Ian Kaplan"
                    },
                    {
                        "name": "Deniss Jacenko"
                    },
                    {
                        "name": "Yuxiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiang Wu"
                },
                "author": "Yuxiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13138v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13138v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13137v1",
                "updated": "2025-02-18T18:57:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    57,
                    9,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T18:57:09Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    57,
                    9,
                    1,
                    49,
                    0
                ],
                "title": "Theorem Prover as a Judge for Synthetic Data Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theorem Prover as a Judge for Synthetic Data Generation"
                },
                "summary": "The demand for synthetic data in mathematical reasoning has increased due to\nits potential to enhance the mathematical capabilities of large language models\n(LLMs). However, ensuring the validity of intermediate reasoning steps remains\na significant challenge, affecting data quality. While formal verification via\ntheorem provers effectively validates LLM reasoning, the autoformalisation of\nmathematical proofs remains error-prone. In response, we introduce iterative\nautoformalisation, an approach that iteratively refines theorem prover\nformalisation to mitigate errors, thereby increasing the execution rate on the\nLean prover from 60% to 87%. Building upon that, we introduce Theorem Prover as\na Judge (TP-as-a-Judge), a method that employs theorem prover formalisation to\nrigorously assess LLM intermediate reasoning, effectively integrating\nautoformalisation with synthetic data generation. Finally, we present\nReinforcement Learning from Theorem Prover Feedback (RLTPF), a framework that\nreplaces human annotation with theorem prover feedback in Reinforcement\nLearning from Human Feedback (RLHF). Across multiple LLMs, applying\nTP-as-a-Judge and RLTPF improves benchmarks with only 3,508 samples, achieving\n5.56% accuracy gain on Mistral-7B for MultiArith, 6.00% on Llama-2-7B for\nSVAMP, and 3.55% on Llama-3.1-8B for AQUA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The demand for synthetic data in mathematical reasoning has increased due to\nits potential to enhance the mathematical capabilities of large language models\n(LLMs). However, ensuring the validity of intermediate reasoning steps remains\na significant challenge, affecting data quality. While formal verification via\ntheorem provers effectively validates LLM reasoning, the autoformalisation of\nmathematical proofs remains error-prone. In response, we introduce iterative\nautoformalisation, an approach that iteratively refines theorem prover\nformalisation to mitigate errors, thereby increasing the execution rate on the\nLean prover from 60% to 87%. Building upon that, we introduce Theorem Prover as\na Judge (TP-as-a-Judge), a method that employs theorem prover formalisation to\nrigorously assess LLM intermediate reasoning, effectively integrating\nautoformalisation with synthetic data generation. Finally, we present\nReinforcement Learning from Theorem Prover Feedback (RLTPF), a framework that\nreplaces human annotation with theorem prover feedback in Reinforcement\nLearning from Human Feedback (RLHF). Across multiple LLMs, applying\nTP-as-a-Judge and RLTPF improves benchmarks with only 3,508 samples, achieving\n5.56% accuracy gain on Mistral-7B for MultiArith, 6.00% on Llama-2-7B for\nSVAMP, and 3.55% on Llama-3.1-8B for AQUA."
                },
                "authors": [
                    {
                        "name": "Joshua Ong Jun Leang"
                    },
                    {
                        "name": "Giwon Hong"
                    },
                    {
                        "name": "Wenda Li"
                    },
                    {
                        "name": "Shay B. Cohen"
                    }
                ],
                "author_detail": {
                    "name": "Shay B. Cohen"
                },
                "author": "Shay B. Cohen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13134v1",
                "updated": "2025-02-18T18:56:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    56,
                    41,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T18:56:41Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    56,
                    41,
                    1,
                    49,
                    0
                ],
                "title": "RHINO: Learning Real-Time Humanoid-Human-Object Interaction from Human\n  Demonstrations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RHINO: Learning Real-Time Humanoid-Human-Object Interaction from Human\n  Demonstrations"
                },
                "summary": "Humanoid robots have shown success in locomotion and manipulation. Despite\nthese basic abilities, humanoids are still required to quickly understand human\ninstructions and react based on human interaction signals to become valuable\nassistants in human daily life. Unfortunately, most existing works only focus\non multi-stage interactions, treating each task separately, and neglecting\nreal-time feedback. In this work, we aim to empower humanoid robots with\nreal-time reaction abilities to achieve various tasks, allowing human to\ninterrupt robots at any time, and making robots respond to humans immediately.\nTo support such abilities, we propose a general humanoid-human-object\ninteraction framework, named RHINO, i.e., Real-time Humanoid-human Interaction\nand Object manipulation. RHINO provides a unified view of reactive motion,\ninstruction-based manipulation, and safety concerns, over multiple human signal\nmodalities, such as languages, images, and motions. RHINO is a hierarchical\nlearning framework, enabling humanoids to learn reaction skills from\nhuman-human-object demonstrations and teleoperation data. In particular, it\ndecouples the interaction process into two levels: 1) a high-level planner\ninferring human intentions from real-time human behaviors; and 2) a low-level\ncontroller achieving reactive motion behaviors and object manipulation skills\nbased on the predicted intentions. We evaluate the proposed framework on a real\nhumanoid robot and demonstrate its effectiveness, flexibility, and safety in\nvarious scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humanoid robots have shown success in locomotion and manipulation. Despite\nthese basic abilities, humanoids are still required to quickly understand human\ninstructions and react based on human interaction signals to become valuable\nassistants in human daily life. Unfortunately, most existing works only focus\non multi-stage interactions, treating each task separately, and neglecting\nreal-time feedback. In this work, we aim to empower humanoid robots with\nreal-time reaction abilities to achieve various tasks, allowing human to\ninterrupt robots at any time, and making robots respond to humans immediately.\nTo support such abilities, we propose a general humanoid-human-object\ninteraction framework, named RHINO, i.e., Real-time Humanoid-human Interaction\nand Object manipulation. RHINO provides a unified view of reactive motion,\ninstruction-based manipulation, and safety concerns, over multiple human signal\nmodalities, such as languages, images, and motions. RHINO is a hierarchical\nlearning framework, enabling humanoids to learn reaction skills from\nhuman-human-object demonstrations and teleoperation data. In particular, it\ndecouples the interaction process into two levels: 1) a high-level planner\ninferring human intentions from real-time human behaviors; and 2) a low-level\ncontroller achieving reactive motion behaviors and object manipulation skills\nbased on the predicted intentions. We evaluate the proposed framework on a real\nhumanoid robot and demonstrate its effectiveness, flexibility, and safety in\nvarious scenarios."
                },
                "authors": [
                    {
                        "name": "Jingxiao Chen"
                    },
                    {
                        "name": "Xinyao Li"
                    },
                    {
                        "name": "Jiahang Cao"
                    },
                    {
                        "name": "Zhengbang Zhu"
                    },
                    {
                        "name": "Wentao Dong"
                    },
                    {
                        "name": "Minghuan Liu"
                    },
                    {
                        "name": "Ying Wen"
                    },
                    {
                        "name": "Yong Yu"
                    },
                    {
                        "name": "Liqing Zhang"
                    },
                    {
                        "name": "Weinan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weinan Zhang"
                },
                "author": "Weinan Zhang",
                "arxiv_comment": "Project website: https://humanoid-interaction.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13133v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13133v1",
                "updated": "2025-02-18T18:56:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    56,
                    18,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T18:56:18Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    56,
                    18,
                    1,
                    49,
                    0
                ],
                "title": "AV-Flow: Transforming Text to Audio-Visual Human-like Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AV-Flow: Transforming Text to Audio-Visual Human-like Interactions"
                },
                "summary": "We introduce AV-Flow, an audio-visual generative model that animates\nphoto-realistic 4D talking avatars given only text input. In contrast to prior\nwork that assumes an existing speech signal, we synthesize speech and vision\njointly. We demonstrate human-like speech synthesis, synchronized lip motion,\nlively facial expressions and head pose; all generated from just text\ncharacters. The core premise of our approach lies in the architecture of our\ntwo parallel diffusion transformers. Intermediate highway connections ensure\ncommunication between the audio and visual modalities, and thus, synchronized\nspeech intonation and facial dynamics (e.g., eyebrow motion). Our model is\ntrained with flow matching, leading to expressive results and fast inference.\nIn case of dyadic conversations, AV-Flow produces an always-on avatar, that\nactively listens and reacts to the audio-visual input of a user. Through\nextensive experiments, we show that our method outperforms prior work,\nsynthesizing natural-looking 4D talking avatars. Project page:\nhttps://aggelinacha.github.io/AV-Flow/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce AV-Flow, an audio-visual generative model that animates\nphoto-realistic 4D talking avatars given only text input. In contrast to prior\nwork that assumes an existing speech signal, we synthesize speech and vision\njointly. We demonstrate human-like speech synthesis, synchronized lip motion,\nlively facial expressions and head pose; all generated from just text\ncharacters. The core premise of our approach lies in the architecture of our\ntwo parallel diffusion transformers. Intermediate highway connections ensure\ncommunication between the audio and visual modalities, and thus, synchronized\nspeech intonation and facial dynamics (e.g., eyebrow motion). Our model is\ntrained with flow matching, leading to expressive results and fast inference.\nIn case of dyadic conversations, AV-Flow produces an always-on avatar, that\nactively listens and reacts to the audio-visual input of a user. Through\nextensive experiments, we show that our method outperforms prior work,\nsynthesizing natural-looking 4D talking avatars. Project page:\nhttps://aggelinacha.github.io/AV-Flow/"
                },
                "authors": [
                    {
                        "name": "Aggelina Chatziagapi"
                    },
                    {
                        "name": "Louis-Philippe Morency"
                    },
                    {
                        "name": "Hongyu Gong"
                    },
                    {
                        "name": "Michael Zollhoefer"
                    },
                    {
                        "name": "Dimitris Samaras"
                    },
                    {
                        "name": "Alexander Richard"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Richard"
                },
                "author": "Alexander Richard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13133v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13133v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13131v1",
                "updated": "2025-02-18T18:55:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    55,
                    26,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T18:55:26Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    55,
                    26,
                    1,
                    49,
                    0
                ],
                "title": "Rethinking Diverse Human Preference Learning through Principal Component\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Diverse Human Preference Learning through Principal Component\n  Analysis"
                },
                "summary": "Understanding human preferences is crucial for improving foundation models\nand building personalized AI systems. However, preferences are inherently\ndiverse and complex, making it difficult for traditional reward models to\ncapture their full range. While fine-grained preference data can help,\ncollecting it is expensive and hard to scale. In this paper, we introduce\nDecomposed Reward Models (DRMs), a novel approach that extracts diverse human\npreferences from binary comparisons without requiring fine-grained annotations.\nOur key insight is to represent human preferences as vectors and analyze them\nusing Principal Component Analysis (PCA). By constructing a dataset of\nembedding differences between preferred and rejected responses, DRMs identify\northogonal basis vectors that capture distinct aspects of preference. These\ndecomposed rewards can be flexibly combined to align with different user needs,\noffering an interpretable and scalable alternative to traditional reward\nmodels. We demonstrate that DRMs effectively extract meaningful preference\ndimensions (e.g., helpfulness, safety, humor) and adapt to new users without\nadditional training. Our results highlight DRMs as a powerful framework for\npersonalized and interpretable LLM alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding human preferences is crucial for improving foundation models\nand building personalized AI systems. However, preferences are inherently\ndiverse and complex, making it difficult for traditional reward models to\ncapture their full range. While fine-grained preference data can help,\ncollecting it is expensive and hard to scale. In this paper, we introduce\nDecomposed Reward Models (DRMs), a novel approach that extracts diverse human\npreferences from binary comparisons without requiring fine-grained annotations.\nOur key insight is to represent human preferences as vectors and analyze them\nusing Principal Component Analysis (PCA). By constructing a dataset of\nembedding differences between preferred and rejected responses, DRMs identify\northogonal basis vectors that capture distinct aspects of preference. These\ndecomposed rewards can be flexibly combined to align with different user needs,\noffering an interpretable and scalable alternative to traditional reward\nmodels. We demonstrate that DRMs effectively extract meaningful preference\ndimensions (e.g., helpfulness, safety, humor) and adapt to new users without\nadditional training. Our results highlight DRMs as a powerful framework for\npersonalized and interpretable LLM alignment."
                },
                "authors": [
                    {
                        "name": "Feng Luo"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Chunyuan Deng"
                    },
                    {
                        "name": "Jiarui Yao"
                    },
                    {
                        "name": "Jingyan Shen"
                    },
                    {
                        "name": "Huan Zhang"
                    },
                    {
                        "name": "Hanjie Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hanjie Chen"
                },
                "author": "Hanjie Chen",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12118v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12118v2",
                "updated": "2025-02-18T18:54:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    54,
                    12,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-17T18:43:24Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    18,
                    43,
                    24,
                    0,
                    48,
                    0
                ],
                "title": "Scaling Test-Time Compute Without Verification or RL is Suboptimal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Test-Time Compute Without Verification or RL is Suboptimal"
                },
                "summary": "Despite substantial advances in scaling test-time compute, an ongoing debate\nin the community is how it should be scaled up to enable continued and\nefficient improvements with scaling. There are largely two approaches: first,\ndistilling successful search or thinking traces; and second, using verification\n(e.g., 0/1 outcome rewards, reward models, or verifiers) to guide reinforcement\nlearning (RL) and search algorithms. In this paper, we prove that finetuning\nLLMs with verifier-based (VB) methods based on RL or search is far superior to\nverifier-free (VF) approaches based on distilling or cloning search traces,\ngiven a fixed amount of compute/data budget. Further, we show that as we scale\ntest-time compute (measured as the output token length) and training data,\nsuboptimality of VF methods scales poorly compared to VB when the base\npre-trained LLM presents a heterogeneous distribution over correct solution\ntraces (e.g., different lengths, styles, etc.) and admits a non-sharp\ndistribution over rewards on traces sampled from it. We formalize this\ncondition using anti-concentration [Erd\\H{o}s, 1945]. This implies a stronger\nresult that VB methods scale better asymptotically, with the performance gap\nbetween VB and VF methods widening as test-time budget grows. We corroborate\nour theory empirically on both didactic and math reasoning problems with\n3/8/32B-sized pre-trained LLMs, where we find verification is crucial for\nscaling test-time compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite substantial advances in scaling test-time compute, an ongoing debate\nin the community is how it should be scaled up to enable continued and\nefficient improvements with scaling. There are largely two approaches: first,\ndistilling successful search or thinking traces; and second, using verification\n(e.g., 0/1 outcome rewards, reward models, or verifiers) to guide reinforcement\nlearning (RL) and search algorithms. In this paper, we prove that finetuning\nLLMs with verifier-based (VB) methods based on RL or search is far superior to\nverifier-free (VF) approaches based on distilling or cloning search traces,\ngiven a fixed amount of compute/data budget. Further, we show that as we scale\ntest-time compute (measured as the output token length) and training data,\nsuboptimality of VF methods scales poorly compared to VB when the base\npre-trained LLM presents a heterogeneous distribution over correct solution\ntraces (e.g., different lengths, styles, etc.) and admits a non-sharp\ndistribution over rewards on traces sampled from it. We formalize this\ncondition using anti-concentration [Erd\\H{o}s, 1945]. This implies a stronger\nresult that VB methods scale better asymptotically, with the performance gap\nbetween VB and VF methods widening as test-time budget grows. We corroborate\nour theory empirically on both didactic and math reasoning problems with\n3/8/32B-sized pre-trained LLMs, where we find verification is crucial for\nscaling test-time compute."
                },
                "authors": [
                    {
                        "name": "Amrith Setlur"
                    },
                    {
                        "name": "Nived Rajaraman"
                    },
                    {
                        "name": "Sergey Levine"
                    },
                    {
                        "name": "Aviral Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Aviral Kumar"
                },
                "author": "Aviral Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12118v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12118v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13128v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13128v1",
                "updated": "2025-02-18T18:52:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    52,
                    21,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T18:52:21Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    52,
                    21,
                    1,
                    49,
                    0
                ],
                "title": "SongGen: A Single Stage Auto-regressive Transformer for Text-to-Song\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SongGen: A Single Stage Auto-regressive Transformer for Text-to-Song\n  Generation"
                },
                "summary": "Text-to-song generation, the task of creating vocals and accompaniment from\ntextual inputs, poses significant challenges due to domain complexity and data\nscarcity. Existing approaches often employ multi-stage generation procedures,\nresulting in cumbersome training and inference pipelines. In this paper, we\npropose SongGen, a fully open-source, single-stage auto-regressive transformer\ndesigned for controllable song generation. The proposed model facilitates\nfine-grained control over diverse musical attributes, including lyrics and\ntextual descriptions of instrumentation, genre, mood, and timbre, while also\noffering an optional three-second reference clip for voice cloning. Within a\nunified auto-regressive framework, SongGen supports two output modes: mixed\nmode, which generates a mixture of vocals and accompaniment directly, and\ndual-track mode, which synthesizes them separately for greater flexibility in\ndownstream applications. We explore diverse token pattern strategies for each\nmode, leading to notable improvements and valuable insights. Furthermore, we\ndesign an automated data preprocessing pipeline with effective quality control.\nTo foster community engagement and future research, we will release our model\nweights, training code, annotated data, and preprocessing pipeline. The\ngenerated samples are showcased on our project page at\nhttps://liuzh-19.github.io/SongGen/ , and the code will be available at\nhttps://github.com/LiuZH-19/SongGen .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-song generation, the task of creating vocals and accompaniment from\ntextual inputs, poses significant challenges due to domain complexity and data\nscarcity. Existing approaches often employ multi-stage generation procedures,\nresulting in cumbersome training and inference pipelines. In this paper, we\npropose SongGen, a fully open-source, single-stage auto-regressive transformer\ndesigned for controllable song generation. The proposed model facilitates\nfine-grained control over diverse musical attributes, including lyrics and\ntextual descriptions of instrumentation, genre, mood, and timbre, while also\noffering an optional three-second reference clip for voice cloning. Within a\nunified auto-regressive framework, SongGen supports two output modes: mixed\nmode, which generates a mixture of vocals and accompaniment directly, and\ndual-track mode, which synthesizes them separately for greater flexibility in\ndownstream applications. We explore diverse token pattern strategies for each\nmode, leading to notable improvements and valuable insights. Furthermore, we\ndesign an automated data preprocessing pipeline with effective quality control.\nTo foster community engagement and future research, we will release our model\nweights, training code, annotated data, and preprocessing pipeline. The\ngenerated samples are showcased on our project page at\nhttps://liuzh-19.github.io/SongGen/ , and the code will be available at\nhttps://github.com/LiuZH-19/SongGen ."
                },
                "authors": [
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Shuangrui Ding"
                    },
                    {
                        "name": "Zhixiong Zhang"
                    },
                    {
                        "name": "Xiaoyi Dong"
                    },
                    {
                        "name": "Pan Zhang"
                    },
                    {
                        "name": "Yuhang Zang"
                    },
                    {
                        "name": "Yuhang Cao"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiaqi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaqi Wang"
                },
                "author": "Jiaqi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13128v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13128v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13127v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13127v1",
                "updated": "2025-02-18T18:50:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    50,
                    6,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T18:50:06Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    50,
                    6,
                    1,
                    49,
                    0
                ],
                "title": "Facilitating Long Context Understanding via Supervised Chain-of-Thought\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Facilitating Long Context Understanding via Supervised Chain-of-Thought\n  Reasoning"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have enabled them to process\nincreasingly longer sequences, ranging from 2K to 2M tokens and even beyond.\nHowever, simply extending the input sequence length does not necessarily lead\nto effective long-context understanding. In this study, we integrate\nChain-of-Thought (CoT) reasoning into LLMs in a supervised manner to facilitate\neffective long-context understanding. To achieve this, we introduce\nLongFinanceQA, a synthetic dataset in the financial domain designed to improve\nlong-context reasoning. Unlike existing long-context synthetic data,\nLongFinanceQA includes intermediate CoT reasoning before the final conclusion,\nwhich encourages LLMs to perform explicit reasoning, improving accuracy and\ninterpretability in long-context understanding. To generate synthetic CoT\nreasoning, we propose Property-driven Agentic Inference (PAI), an agentic\nframework that simulates human-like reasoning steps, including property\nextraction, retrieval, and summarization. We evaluate PAI's reasoning\ncapabilities by assessing GPT-4o-mini w/ PAI on the Loong benchmark,\noutperforming standard GPT-4o-mini by 20.0%. Furthermore, we fine-tune\nLLaMA-3.1-8B-Instruct on LongFinanceQA, achieving a 24.6% gain on Loong's\nfinancial subset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have enabled them to process\nincreasingly longer sequences, ranging from 2K to 2M tokens and even beyond.\nHowever, simply extending the input sequence length does not necessarily lead\nto effective long-context understanding. In this study, we integrate\nChain-of-Thought (CoT) reasoning into LLMs in a supervised manner to facilitate\neffective long-context understanding. To achieve this, we introduce\nLongFinanceQA, a synthetic dataset in the financial domain designed to improve\nlong-context reasoning. Unlike existing long-context synthetic data,\nLongFinanceQA includes intermediate CoT reasoning before the final conclusion,\nwhich encourages LLMs to perform explicit reasoning, improving accuracy and\ninterpretability in long-context understanding. To generate synthetic CoT\nreasoning, we propose Property-driven Agentic Inference (PAI), an agentic\nframework that simulates human-like reasoning steps, including property\nextraction, retrieval, and summarization. We evaluate PAI's reasoning\ncapabilities by assessing GPT-4o-mini w/ PAI on the Loong benchmark,\noutperforming standard GPT-4o-mini by 20.0%. Furthermore, we fine-tune\nLLaMA-3.1-8B-Instruct on LongFinanceQA, achieving a 24.6% gain on Loong's\nfinancial subset."
                },
                "authors": [
                    {
                        "name": "Jingyang Lin"
                    },
                    {
                        "name": "Andy Wong"
                    },
                    {
                        "name": "Tian Xia"
                    },
                    {
                        "name": "Shenghua He"
                    },
                    {
                        "name": "Hui Wei"
                    },
                    {
                        "name": "Mei Han"
                    },
                    {
                        "name": "Jiebo Luo"
                    }
                ],
                "author_detail": {
                    "name": "Jiebo Luo"
                },
                "author": "Jiebo Luo",
                "arxiv_comment": "15 Pages, 6 Tables, 8 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13127v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13127v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13125v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13125v1",
                "updated": "2025-02-18T18:47:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    47,
                    11,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T18:47:11Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    47,
                    11,
                    1,
                    49,
                    0
                ],
                "title": "RuozhiBench: Evaluating LLMs with Logical Fallacies and Misleading\n  Premises",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RuozhiBench: Evaluating LLMs with Logical Fallacies and Misleading\n  Premises"
                },
                "summary": "Recent advances in large language models (LLMs) have shown that they can\nanswer questions requiring complex reasoning. However, their ability to\nidentify and respond to text containing logical fallacies or deliberately\nmisleading premises remains less studied. To address this gap, we introduce\nRuozhiBench, a bilingual dataset comprising 677 carefully curated questions\nthat contain various forms of deceptive reasoning, meticulously crafted through\nextensive human effort and expert review. In a comprehensive evaluation of 17\nLLMs from 5 Series over RuozhiBench using both open-ended and two-choice\nformats, we conduct extensive analyses on evaluation protocols and result\npatterns. Despite their high scores on conventional benchmarks, these models\nshowed limited ability to detect and reason correctly about logical fallacies,\nwith even the best-performing model, Claude-3-haiku, achieving only 62%\naccuracy compared to the human of more than 90%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have shown that they can\nanswer questions requiring complex reasoning. However, their ability to\nidentify and respond to text containing logical fallacies or deliberately\nmisleading premises remains less studied. To address this gap, we introduce\nRuozhiBench, a bilingual dataset comprising 677 carefully curated questions\nthat contain various forms of deceptive reasoning, meticulously crafted through\nextensive human effort and expert review. In a comprehensive evaluation of 17\nLLMs from 5 Series over RuozhiBench using both open-ended and two-choice\nformats, we conduct extensive analyses on evaluation protocols and result\npatterns. Despite their high scores on conventional benchmarks, these models\nshowed limited ability to detect and reason correctly about logical fallacies,\nwith even the best-performing model, Claude-3-haiku, achieving only 62%\naccuracy compared to the human of more than 90%."
                },
                "authors": [
                    {
                        "name": "Zenan Zhai"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Xudong Han"
                    },
                    {
                        "name": "Zhenxuan Zhang"
                    },
                    {
                        "name": "Yixuan Zhang"
                    },
                    {
                        "name": "Timothy Baldwin"
                    },
                    {
                        "name": "Haonan Li"
                    }
                ],
                "author_detail": {
                    "name": "Haonan Li"
                },
                "author": "Haonan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13125v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13125v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13120v1",
                "updated": "2025-02-18T18:42:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    42,
                    11,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T18:42:11Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    42,
                    11,
                    1,
                    49,
                    0
                ],
                "title": "Adapting Psycholinguistic Research for LLMs: Gender-inclusive Language\n  in a Coreference Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting Psycholinguistic Research for LLMs: Gender-inclusive Language\n  in a Coreference Context"
                },
                "summary": "Gender-inclusive language is often used with the aim of ensuring that all\nindividuals, regardless of gender, can be associated with certain concepts.\nWhile psycholinguistic studies have examined its effects in relation to human\ncognition, it remains unclear how Large Language Models (LLMs) process\ngender-inclusive language. Given that commercial LLMs are gaining an\nincreasingly strong foothold in everyday applications, it is crucial to examine\nwhether LLMs in fact interpret gender-inclusive language neutrally, because the\nlanguage they generate has the potential to influence the language of their\nusers. This study examines whether LLM-generated coreferent terms align with a\ngiven gender expression or reflect model biases. Adapting psycholinguistic\nmethods from French to English and German, we find that in English, LLMs\ngenerally maintain the antecedent's gender but exhibit underlying masculine\nbias. In German, this bias is much stronger, overriding all tested\ngender-neutralization strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gender-inclusive language is often used with the aim of ensuring that all\nindividuals, regardless of gender, can be associated with certain concepts.\nWhile psycholinguistic studies have examined its effects in relation to human\ncognition, it remains unclear how Large Language Models (LLMs) process\ngender-inclusive language. Given that commercial LLMs are gaining an\nincreasingly strong foothold in everyday applications, it is crucial to examine\nwhether LLMs in fact interpret gender-inclusive language neutrally, because the\nlanguage they generate has the potential to influence the language of their\nusers. This study examines whether LLM-generated coreferent terms align with a\ngiven gender expression or reflect model biases. Adapting psycholinguistic\nmethods from French to English and German, we find that in English, LLMs\ngenerally maintain the antecedent's gender but exhibit underlying masculine\nbias. In German, this bias is much stronger, overriding all tested\ngender-neutralization strategies."
                },
                "authors": [
                    {
                        "name": "Marion Bartl"
                    },
                    {
                        "name": "Thomas Brendan Murphy"
                    },
                    {
                        "name": "Susan Leavy"
                    }
                ],
                "author_detail": {
                    "name": "Susan Leavy"
                },
                "author": "Susan Leavy",
                "arxiv_comment": "9 pages, 7 figures, submitted to ACL 2025 (ARR February 2025 cycle)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13119v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13119v1",
                "updated": "2025-02-18T18:42:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    42,
                    9,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T18:42:09Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    42,
                    9,
                    1,
                    49,
                    0
                ],
                "title": "STEER-ME: Assessing the Microeconomic Reasoning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STEER-ME: Assessing the Microeconomic Reasoning of Large Language Models"
                },
                "summary": "How should one judge whether a given large language model (LLM) can reliably\nperform economic reasoning? Most existing LLM benchmarks focus on specific\napplications and fail to present the model with a rich variety of economic\ntasks. A notable exception is Raman et al. [2024], who offer an approach for\ncomprehensively benchmarking strategic decision-making; however, this approach\nfails to address the non-strategic settings prevalent in microeconomics, such\nas supply-and-demand analysis. We address this gap by taxonomizing\nmicroeconomic reasoning into $58$ distinct elements, focusing on the logic of\nsupply and demand, each grounded in up to $10$ distinct domains, $5$\nperspectives, and $3$ types. The generation of benchmark data across this\ncombinatorial space is powered by a novel LLM-assisted data generation protocol\nthat we dub auto-STEER, which generates a set of questions by adapting\nhandwritten templates to target new domains and perspectives. Because it offers\nan automated way of generating fresh questions, auto-STEER mitigates the risk\nthat LLMs will be trained to over-fit evaluation benchmarks; we thus hope that\nit will serve as a useful tool both for evaluating and fine-tuning models for\nyears to come. We demonstrate the usefulness of our benchmark via a case study\non $27$ LLMs, ranging from small open-source models to the current state of the\nart. We examined each model's ability to solve microeconomic problems across\nour whole taxonomy and present the results across a range of prompting\nstrategies and scoring metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How should one judge whether a given large language model (LLM) can reliably\nperform economic reasoning? Most existing LLM benchmarks focus on specific\napplications and fail to present the model with a rich variety of economic\ntasks. A notable exception is Raman et al. [2024], who offer an approach for\ncomprehensively benchmarking strategic decision-making; however, this approach\nfails to address the non-strategic settings prevalent in microeconomics, such\nas supply-and-demand analysis. We address this gap by taxonomizing\nmicroeconomic reasoning into $58$ distinct elements, focusing on the logic of\nsupply and demand, each grounded in up to $10$ distinct domains, $5$\nperspectives, and $3$ types. The generation of benchmark data across this\ncombinatorial space is powered by a novel LLM-assisted data generation protocol\nthat we dub auto-STEER, which generates a set of questions by adapting\nhandwritten templates to target new domains and perspectives. Because it offers\nan automated way of generating fresh questions, auto-STEER mitigates the risk\nthat LLMs will be trained to over-fit evaluation benchmarks; we thus hope that\nit will serve as a useful tool both for evaluating and fine-tuning models for\nyears to come. We demonstrate the usefulness of our benchmark via a case study\non $27$ LLMs, ranging from small open-source models to the current state of the\nart. We examined each model's ability to solve microeconomic problems across\nour whole taxonomy and present the results across a range of prompting\nstrategies and scoring metrics."
                },
                "authors": [
                    {
                        "name": "Narun Raman"
                    },
                    {
                        "name": "Taylor Lundy"
                    },
                    {
                        "name": "Thiago Amin"
                    },
                    {
                        "name": "Jesse Perla"
                    },
                    {
                        "name": "Kevin-Leyton Brown"
                    }
                ],
                "author_detail": {
                    "name": "Kevin-Leyton Brown"
                },
                "author": "Kevin-Leyton Brown",
                "arxiv_comment": "18 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13119v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13119v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13117v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13117v1",
                "updated": "2025-02-18T18:37:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    37,
                    15,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T18:37:15Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    37,
                    15,
                    1,
                    49,
                    0
                ],
                "title": "Performance Evaluation of Large Language Models in Statistical\n  Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Evaluation of Large Language Models in Statistical\n  Programming"
                },
                "summary": "The programming capabilities of large language models (LLMs) have\nrevolutionized automatic code generation and opened new avenues for automatic\nstatistical analysis. However, the validity and quality of these generated\ncodes need to be systematically evaluated before they can be widely adopted.\nDespite their growing prominence, a comprehensive evaluation of statistical\ncode generated by LLMs remains scarce in the literature. In this paper, we\nassess the performance of LLMs, including two versions of ChatGPT and one\nversion of Llama, in the domain of SAS programming for statistical analysis.\nOur study utilizes a set of statistical analysis tasks encompassing diverse\nstatistical topics and datasets. Each task includes a problem description,\ndataset information, and human-verified SAS code. We conduct a comprehensive\nassessment of the quality of SAS code generated by LLMs through human expert\nevaluation based on correctness, effectiveness, readability, executability, and\nthe accuracy of output results. The analysis of rating scores reveals that\nwhile LLMs demonstrate usefulness in generating syntactically correct code,\nthey struggle with tasks requiring deep domain understanding and may produce\nredundant or incorrect results. This study offers valuable insights into the\ncapabilities and limitations of LLMs in statistical programming, providing\nguidance for future advancements in AI-assisted coding systems for statistical\nanalysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The programming capabilities of large language models (LLMs) have\nrevolutionized automatic code generation and opened new avenues for automatic\nstatistical analysis. However, the validity and quality of these generated\ncodes need to be systematically evaluated before they can be widely adopted.\nDespite their growing prominence, a comprehensive evaluation of statistical\ncode generated by LLMs remains scarce in the literature. In this paper, we\nassess the performance of LLMs, including two versions of ChatGPT and one\nversion of Llama, in the domain of SAS programming for statistical analysis.\nOur study utilizes a set of statistical analysis tasks encompassing diverse\nstatistical topics and datasets. Each task includes a problem description,\ndataset information, and human-verified SAS code. We conduct a comprehensive\nassessment of the quality of SAS code generated by LLMs through human expert\nevaluation based on correctness, effectiveness, readability, executability, and\nthe accuracy of output results. The analysis of rating scores reveals that\nwhile LLMs demonstrate usefulness in generating syntactically correct code,\nthey struggle with tasks requiring deep domain understanding and may produce\nredundant or incorrect results. This study offers valuable insights into the\ncapabilities and limitations of LLMs in statistical programming, providing\nguidance for future advancements in AI-assisted coding systems for statistical\nanalysis."
                },
                "authors": [
                    {
                        "name": "Xinyi Song"
                    },
                    {
                        "name": "Kexin Xie"
                    },
                    {
                        "name": "Lina Lee"
                    },
                    {
                        "name": "Ruizhe Chen"
                    },
                    {
                        "name": "Jared M. Clark"
                    },
                    {
                        "name": "Hao He"
                    },
                    {
                        "name": "Haoran He"
                    },
                    {
                        "name": "Jie Min"
                    },
                    {
                        "name": "Xinlei Zhang"
                    },
                    {
                        "name": "Simin Zheng"
                    },
                    {
                        "name": "Zhiyang Zhang"
                    },
                    {
                        "name": "Xinwei Deng"
                    },
                    {
                        "name": "Yili Hong"
                    }
                ],
                "author_detail": {
                    "name": "Yili Hong"
                },
                "author": "Yili Hong",
                "arxiv_comment": "27 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13117v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01171v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01171v2",
                "updated": "2025-02-18T18:32:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    32,
                    25,
                    1,
                    49,
                    0
                ],
                "published": "2024-10-02T01:59:07Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    1,
                    59,
                    7,
                    2,
                    276,
                    0
                ],
                "title": "Multilingual Retrieval Augmented Generation for Culturally-Sensitive\n  Tasks: A Benchmark for Cross-lingual Robustness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Retrieval Augmented Generation for Culturally-Sensitive\n  Tasks: A Benchmark for Cross-lingual Robustness"
                },
                "summary": "The paradigm of retrieval-augmented generated (RAG) helps mitigate\nhallucinations of large language models (LLMs). However, RAG also introduces\nbiases contained within the retrieved documents. These biases can be amplified\nin scenarios which are multilingual and culturally-sensitive, such as\nterritorial disputes. In this paper, we introduce BordIRLines, a benchmark\nconsisting of 720 territorial dispute queries paired with 14k Wikipedia\ndocuments across 49 languages. To evaluate LLMs' cross-lingual robustness for\nthis task, we formalize several modes for multilingual retrieval. Our\nexperiments on several LLMs reveal that retrieving multilingual documents best\nimproves response consistency and decreases geopolitical bias over using purely\nin-language documents, showing how incorporating diverse perspectives improves\nrobustness. Also, querying in low-resource languages displays a much wider\nvariance in the linguistic distribution of response citations. Our further\nexperiments and case studies investigate how cross-lingual RAG is affected by\naspects from IR to document contents. We release our benchmark and code to\nsupport further research towards ensuring equitable information access across\nlanguages at https://huggingface.co/datasets/borderlines/bordirlines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The paradigm of retrieval-augmented generated (RAG) helps mitigate\nhallucinations of large language models (LLMs). However, RAG also introduces\nbiases contained within the retrieved documents. These biases can be amplified\nin scenarios which are multilingual and culturally-sensitive, such as\nterritorial disputes. In this paper, we introduce BordIRLines, a benchmark\nconsisting of 720 territorial dispute queries paired with 14k Wikipedia\ndocuments across 49 languages. To evaluate LLMs' cross-lingual robustness for\nthis task, we formalize several modes for multilingual retrieval. Our\nexperiments on several LLMs reveal that retrieving multilingual documents best\nimproves response consistency and decreases geopolitical bias over using purely\nin-language documents, showing how incorporating diverse perspectives improves\nrobustness. Also, querying in low-resource languages displays a much wider\nvariance in the linguistic distribution of response citations. Our further\nexperiments and case studies investigate how cross-lingual RAG is affected by\naspects from IR to document contents. We release our benchmark and code to\nsupport further research towards ensuring equitable information access across\nlanguages at https://huggingface.co/datasets/borderlines/bordirlines."
                },
                "authors": [
                    {
                        "name": "Bryan Li"
                    },
                    {
                        "name": "Fiona Luo"
                    },
                    {
                        "name": "Samar Haider"
                    },
                    {
                        "name": "Adwait Agashe"
                    },
                    {
                        "name": "Tammy Li"
                    },
                    {
                        "name": "Runqi Liu"
                    },
                    {
                        "name": "Muqing Miao"
                    },
                    {
                        "name": "Shriya Ramakrishnan"
                    },
                    {
                        "name": "Yuan Yuan"
                    },
                    {
                        "name": "Chris Callison-Burch"
                    }
                ],
                "author_detail": {
                    "name": "Chris Callison-Burch"
                },
                "author": "Chris Callison-Burch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01171v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01171v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13107v1",
                "updated": "2025-02-18T18:19:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    19,
                    36,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T18:19:36Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    19,
                    36,
                    1,
                    49,
                    0
                ],
                "title": "MatterChat: A Multi-Modal LLM for Material Science",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MatterChat: A Multi-Modal LLM for Material Science"
                },
                "summary": "Understanding and predicting the properties of inorganic materials is crucial\nfor accelerating advancements in materials science and driving applications in\nenergy, electronics, and beyond. Integrating material structure data with\nlanguage-based information through multi-modal large language models (LLMs)\noffers great potential to support these efforts by enhancing human-AI\ninteraction. However, a key challenge lies in integrating atomic structures at\nfull resolution into LLMs. In this work, we introduce MatterChat, a versatile\nstructure-aware multi-modal LLM that unifies material structural data and\ntextual inputs into a single cohesive model. MatterChat employs a bridging\nmodule to effectively align a pretrained machine learning interatomic potential\nwith a pretrained LLM, reducing training costs and enhancing flexibility. Our\nresults demonstrate that MatterChat significantly improves performance in\nmaterial property prediction and human-AI interaction, surpassing\ngeneral-purpose LLMs such as GPT-4. We also demonstrate its usefulness in\napplications such as more advanced scientific reasoning and step-by-step\nmaterial synthesis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and predicting the properties of inorganic materials is crucial\nfor accelerating advancements in materials science and driving applications in\nenergy, electronics, and beyond. Integrating material structure data with\nlanguage-based information through multi-modal large language models (LLMs)\noffers great potential to support these efforts by enhancing human-AI\ninteraction. However, a key challenge lies in integrating atomic structures at\nfull resolution into LLMs. In this work, we introduce MatterChat, a versatile\nstructure-aware multi-modal LLM that unifies material structural data and\ntextual inputs into a single cohesive model. MatterChat employs a bridging\nmodule to effectively align a pretrained machine learning interatomic potential\nwith a pretrained LLM, reducing training costs and enhancing flexibility. Our\nresults demonstrate that MatterChat significantly improves performance in\nmaterial property prediction and human-AI interaction, surpassing\ngeneral-purpose LLMs such as GPT-4. We also demonstrate its usefulness in\napplications such as more advanced scientific reasoning and step-by-step\nmaterial synthesis."
                },
                "authors": [
                    {
                        "name": "Yingheng Tang"
                    },
                    {
                        "name": "Wenbin Xu"
                    },
                    {
                        "name": "Jie Cao"
                    },
                    {
                        "name": "Jianzhu Ma"
                    },
                    {
                        "name": "Weilu Gao"
                    },
                    {
                        "name": "Steve Farrell"
                    },
                    {
                        "name": "Benjamin Erichson"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Andy Nonaka"
                    },
                    {
                        "name": "Zhi Yao"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Yao"
                },
                "author": "Zhi Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08820v2",
                "updated": "2025-02-18T18:08:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    8,
                    56,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-12T22:18:34Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    22,
                    18,
                    34,
                    2,
                    43,
                    0
                ],
                "title": "Can a Single Model Master Both Multi-turn Conversations and Tool Use?\n  CoALM: A Unified Conversational Agentic Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can a Single Model Master Both Multi-turn Conversations and Tool Use?\n  CoALM: A Unified Conversational Agentic Language Model"
                },
                "summary": "Large Language Models (LLMs) with API-calling capabilities enabled building\neffective Language Agents (LA), while also revolutionizing the conventional\ntask-oriented dialogue (TOD) paradigm. However, current approaches face a\ncritical dilemma: TOD systems are often trained on a limited set of target\nAPIs, requiring new data to maintain their quality when interfacing with new\nservices, while LAs are not trained to maintain user intent over multi-turn\nconversations. Because both robust multi-turn management and advanced function\ncalling are crucial for effective conversational agents, we evaluate these\nskills on three popular benchmarks: MultiWOZ 2.4 (TOD), BFCL V3 (LA), and\nAPI-Bank (LA), and our analyses reveal that specialized approaches excel in one\ndomain but underperform in the other. To bridge this chasm, we introduce CoALM\n(Conversational Agentic Language Model), a unified approach that integrates\nboth conversational and agentic capabilities. We created CoALM-IT, a carefully\nconstructed multi-task dataset that interleave multi-turn ReAct reasoning with\ncomplex API usage. Using CoALM-IT, we train three models CoALM 8B, CoALM 70B,\nand CoALM 405B, which outperform top domain-specific models, including GPT-4o,\nacross all three benchmarks.This demonstrates the feasibility of a single model\napproach for both TOD and LA, setting a new standard for conversational agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with API-calling capabilities enabled building\neffective Language Agents (LA), while also revolutionizing the conventional\ntask-oriented dialogue (TOD) paradigm. However, current approaches face a\ncritical dilemma: TOD systems are often trained on a limited set of target\nAPIs, requiring new data to maintain their quality when interfacing with new\nservices, while LAs are not trained to maintain user intent over multi-turn\nconversations. Because both robust multi-turn management and advanced function\ncalling are crucial for effective conversational agents, we evaluate these\nskills on three popular benchmarks: MultiWOZ 2.4 (TOD), BFCL V3 (LA), and\nAPI-Bank (LA), and our analyses reveal that specialized approaches excel in one\ndomain but underperform in the other. To bridge this chasm, we introduce CoALM\n(Conversational Agentic Language Model), a unified approach that integrates\nboth conversational and agentic capabilities. We created CoALM-IT, a carefully\nconstructed multi-task dataset that interleave multi-turn ReAct reasoning with\ncomplex API usage. Using CoALM-IT, we train three models CoALM 8B, CoALM 70B,\nand CoALM 405B, which outperform top domain-specific models, including GPT-4o,\nacross all three benchmarks.This demonstrates the feasibility of a single model\napproach for both TOD and LA, setting a new standard for conversational agents."
                },
                "authors": [
                    {
                        "name": "Emre Can Acikgoz"
                    },
                    {
                        "name": "Jeremiah Greer"
                    },
                    {
                        "name": "Akul Datta"
                    },
                    {
                        "name": "Ze Yang"
                    },
                    {
                        "name": "William Zeng"
                    },
                    {
                        "name": "Oussama Elachqar"
                    },
                    {
                        "name": "Emmanouil Koukoumidis"
                    },
                    {
                        "name": "Dilek Hakkani-Tr"
                    },
                    {
                        "name": "Gokhan Tur"
                    }
                ],
                "author_detail": {
                    "name": "Gokhan Tur"
                },
                "author": "Gokhan Tur",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05935v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05935v2",
                "updated": "2025-02-18T18:07:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    7,
                    42,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-09T15:32:46Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    15,
                    32,
                    46,
                    6,
                    40,
                    0
                ],
                "title": "Interactive Inference: A Neuromorphic Theory of Human-Computer\n  Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interactive Inference: A Neuromorphic Theory of Human-Computer\n  Interaction"
                },
                "summary": "Neuromorphic HCI is a new theoretical approach to designing better UX\ninspired by the neurophysiology of the brain. Here, we apply the\nneuroscientific theory of Active Inference to HCI, postulating that users\nperform Bayesian inference on progress and goal distributions to predict their\nnext action (Interactive Inference). We show how Bayesian surprise between goal\nand progress distributions follows a mean square error function of the\nsignal-to-noise ratio (SNR) of the task. However, capacity to process Bayesian\nsurprise follows the logarithm of SNR, and errors occur when average capacity\nis exceeded. Our model allows the quantitative analysis of performance and\nerror in one framework with real-time estimation of mental load. We show\nthrough mathematical theorems how three basic laws of HCI, Hick's Law, Fitts'\nLaw and the Power Law fit our model. We then test the validity of the general\nmodel by empirically measuring how well it predicts human performance in a car\nfollowing task. Results suggest that driver processing capacity indeed is a\nlogarithmic function of the SNR of the distance to a lead car. This positive\nresult provides initial evidence that Interactive Interference can work as a\nnew theoretical underpinning for HCI, deserving further exploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuromorphic HCI is a new theoretical approach to designing better UX\ninspired by the neurophysiology of the brain. Here, we apply the\nneuroscientific theory of Active Inference to HCI, postulating that users\nperform Bayesian inference on progress and goal distributions to predict their\nnext action (Interactive Inference). We show how Bayesian surprise between goal\nand progress distributions follows a mean square error function of the\nsignal-to-noise ratio (SNR) of the task. However, capacity to process Bayesian\nsurprise follows the logarithm of SNR, and errors occur when average capacity\nis exceeded. Our model allows the quantitative analysis of performance and\nerror in one framework with real-time estimation of mental load. We show\nthrough mathematical theorems how three basic laws of HCI, Hick's Law, Fitts'\nLaw and the Power Law fit our model. We then test the validity of the general\nmodel by empirically measuring how well it predicts human performance in a car\nfollowing task. Results suggest that driver processing capacity indeed is a\nlogarithmic function of the SNR of the distance to a lead car. This positive\nresult provides initial evidence that Interactive Interference can work as a\nnew theoretical underpinning for HCI, deserving further exploration."
                },
                "authors": [
                    {
                        "name": "Roel Vertegaal"
                    },
                    {
                        "name": "Timothy Merritt"
                    },
                    {
                        "name": "Saul Greenberg"
                    },
                    {
                        "name": "Aneesh P. Tarun"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Zafeiros Fountas"
                    }
                ],
                "author_detail": {
                    "name": "Zafeiros Fountas"
                },
                "author": "Zafeiros Fountas",
                "arxiv_comment": "18 pages, 7 figures, 1 table, 35 mathematical formulas, submitted for\n  review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05935v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05935v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13095v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13095v1",
                "updated": "2025-02-18T18:06:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    6,
                    48,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T18:06:48Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    6,
                    48,
                    1,
                    49,
                    0
                ],
                "title": "Understanding and Rectifying Safety Perception Distortion in VLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and Rectifying Safety Perception Distortion in VLMs"
                },
                "summary": "Recent studies reveal that vision-language models (VLMs) become more\nsusceptible to harmful requests and jailbreak attacks after integrating the\nvision modality, exhibiting greater vulnerability than their text-only LLM\nbackbones. To uncover the root cause of this phenomenon, we conduct an in-depth\nanalysis and identify a key issue: multimodal inputs introduce an\nmodality-induced activation shift toward a \"safer\" direction compared to their\ntext-only counterparts, leading VLMs to systematically overestimate the safety\nof harmful inputs. We refer to this issue as safety perception distortion. To\nmitigate such distortion, we propose Activation Shift Disentanglement and\nCalibration (ShiftDC), a training-free method that decomposes and calibrates\nthe modality-induced activation shift to reduce the impact of modality on\nsafety. By isolating and removing the safety-relevant component, ShiftDC\nrestores the inherent safety alignment of the LLM backbone while preserving the\nvision-language capabilities of VLMs. Empirical results demonstrate that\nShiftDC significantly enhances alignment performance on safety benchmarks\nwithout impairing model utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies reveal that vision-language models (VLMs) become more\nsusceptible to harmful requests and jailbreak attacks after integrating the\nvision modality, exhibiting greater vulnerability than their text-only LLM\nbackbones. To uncover the root cause of this phenomenon, we conduct an in-depth\nanalysis and identify a key issue: multimodal inputs introduce an\nmodality-induced activation shift toward a \"safer\" direction compared to their\ntext-only counterparts, leading VLMs to systematically overestimate the safety\nof harmful inputs. We refer to this issue as safety perception distortion. To\nmitigate such distortion, we propose Activation Shift Disentanglement and\nCalibration (ShiftDC), a training-free method that decomposes and calibrates\nthe modality-induced activation shift to reduce the impact of modality on\nsafety. By isolating and removing the safety-relevant component, ShiftDC\nrestores the inherent safety alignment of the LLM backbone while preserving the\nvision-language capabilities of VLMs. Empirical results demonstrate that\nShiftDC significantly enhances alignment performance on safety benchmarks\nwithout impairing model utility."
                },
                "authors": [
                    {
                        "name": "Xiaohan Zou"
                    },
                    {
                        "name": "Jian Kang"
                    },
                    {
                        "name": "George Kesidis"
                    },
                    {
                        "name": "Lu Lin"
                    }
                ],
                "author_detail": {
                    "name": "Lu Lin"
                },
                "author": "Lu Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13095v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13095v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13092v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13092v1",
                "updated": "2025-02-18T17:59:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    59,
                    48,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T17:59:48Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    59,
                    48,
                    1,
                    49,
                    0
                ],
                "title": "Text2World: Benchmarking Large Language Models for Symbolic World Model\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text2World: Benchmarking Large Language Models for Symbolic World Model\n  Generation"
                },
                "summary": "Recently, there has been growing interest in leveraging large language models\n(LLMs) to generate symbolic world models from textual descriptions. Although\nLLMs have been extensively explored in the context of world modeling, prior\nstudies encountered several challenges, including evaluation randomness,\ndependence on indirect metrics, and a limited domain scope. To address these\nlimitations, we introduce a novel benchmark, Text2World, based on planning\ndomain definition language (PDDL), featuring hundreds of diverse domains and\nemploying multi-criteria, execution-based metrics for a more robust evaluation.\nWe benchmark current LLMs using Text2World and find that reasoning models\ntrained with large-scale reinforcement learning outperform others. However,\neven the best-performing model still demonstrates limited capabilities in world\nmodeling. Building on these insights, we examine several promising strategies\nto enhance the world modeling capabilities of LLMs, including test-time\nscaling, agent training, and more. We hope that Text2World can serve as a\ncrucial resource, laying the groundwork for future research in leveraging LLMs\nas world models. The project page is available at\nhttps://text-to-world.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, there has been growing interest in leveraging large language models\n(LLMs) to generate symbolic world models from textual descriptions. Although\nLLMs have been extensively explored in the context of world modeling, prior\nstudies encountered several challenges, including evaluation randomness,\ndependence on indirect metrics, and a limited domain scope. To address these\nlimitations, we introduce a novel benchmark, Text2World, based on planning\ndomain definition language (PDDL), featuring hundreds of diverse domains and\nemploying multi-criteria, execution-based metrics for a more robust evaluation.\nWe benchmark current LLMs using Text2World and find that reasoning models\ntrained with large-scale reinforcement learning outperform others. However,\neven the best-performing model still demonstrates limited capabilities in world\nmodeling. Building on these insights, we examine several promising strategies\nto enhance the world modeling capabilities of LLMs, including test-time\nscaling, agent training, and more. We hope that Text2World can serve as a\ncrucial resource, laying the groundwork for future research in leveraging LLMs\nas world models. The project page is available at\nhttps://text-to-world.github.io/."
                },
                "authors": [
                    {
                        "name": "Mengkang Hu"
                    },
                    {
                        "name": "Tianxing Chen"
                    },
                    {
                        "name": "Yude Zou"
                    },
                    {
                        "name": "Yuheng Lei"
                    },
                    {
                        "name": "Qiguang Chen"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Hongyuan Zhang"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Ping Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ping Luo"
                },
                "author": "Ping Luo",
                "arxiv_comment": "Project page: https://text-to-world.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13092v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01077v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01077v2",
                "updated": "2025-02-18T17:57:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    57,
                    26,
                    1,
                    49,
                    0
                ],
                "published": "2024-11-01T23:18:32Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    23,
                    18,
                    32,
                    4,
                    306,
                    0
                ],
                "title": "Emoji Attack: Enhancing Jailbreak Attacks Against Judge LLM Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emoji Attack: Enhancing Jailbreak Attacks Against Judge LLM Detection"
                },
                "summary": "Jailbreaking techniques trick Large Language Models (LLMs) into producing\nrestricted outputs, posing a serious threat. One line of defense is to use\nanother LLM as a Judge to evaluate the harmfulness of generated text. However,\nwe reveal that these Judge LLMs are vulnerable to token segmentation bias, an\nissue that arises when delimiters alter the tokenization process, splitting\nwords into smaller sub-tokens. This disrupts the embeddings of the entire\nsequence, reducing detection accuracy and allowing harmful content to be\nmisclassified as safe. In this paper, we introduce Emoji Attack, a novel\nstrategy that amplifies existing jailbreak prompts by exploiting token\nsegmentation bias. Our method leverages in-context learning to systematically\ninsert emojis into text before it is evaluated by a Judge LLM, inducing\nembedding distortions that significantly lower the likelihood of detecting\nunsafe content. Unlike traditional delimiters, emojis also introduce semantic\nambiguity, making them particularly effective in this attack. Through\nexperiments on state-of-the-art Judge LLMs, we demonstrate that Emoji Attack\nsubstantially reduces the \"unsafe\" prediction rate, bypassing existing\nsafeguards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreaking techniques trick Large Language Models (LLMs) into producing\nrestricted outputs, posing a serious threat. One line of defense is to use\nanother LLM as a Judge to evaluate the harmfulness of generated text. However,\nwe reveal that these Judge LLMs are vulnerable to token segmentation bias, an\nissue that arises when delimiters alter the tokenization process, splitting\nwords into smaller sub-tokens. This disrupts the embeddings of the entire\nsequence, reducing detection accuracy and allowing harmful content to be\nmisclassified as safe. In this paper, we introduce Emoji Attack, a novel\nstrategy that amplifies existing jailbreak prompts by exploiting token\nsegmentation bias. Our method leverages in-context learning to systematically\ninsert emojis into text before it is evaluated by a Judge LLM, inducing\nembedding distortions that significantly lower the likelihood of detecting\nunsafe content. Unlike traditional delimiters, emojis also introduce semantic\nambiguity, making them particularly effective in this attack. Through\nexperiments on state-of-the-art Judge LLMs, we demonstrate that Emoji Attack\nsubstantially reduces the \"unsafe\" prediction rate, bypassing existing\nsafeguards."
                },
                "authors": [
                    {
                        "name": "Zhipeng Wei"
                    },
                    {
                        "name": "Yuqi Liu"
                    },
                    {
                        "name": "N. Benjamin Erichson"
                    }
                ],
                "author_detail": {
                    "name": "N. Benjamin Erichson"
                },
                "author": "N. Benjamin Erichson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01077v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01077v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2302.14728v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2302.14728v2",
                "updated": "2025-02-18T17:48:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    48,
                    45,
                    1,
                    49,
                    0
                ],
                "published": "2023-02-28T16:34:55Z",
                "published_parsed": [
                    2023,
                    2,
                    28,
                    16,
                    34,
                    55,
                    1,
                    59,
                    0
                ],
                "title": "Semantically Consistent Person Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantically Consistent Person Image Generation"
                },
                "summary": "We propose a data-driven approach for context-aware person image generation.\nSpecifically, we attempt to generate a person image such that the synthesized\ninstance can blend into a complex scene. In our method, the position, scale,\nand appearance of the generated person are semantically conditioned on the\nexisting persons in the scene. The proposed technique is divided into three\nsequential steps. At first, we employ a Pix2PixHD model to infer a coarse\nsemantic mask that represents the new person's spatial location, scale, and\npotential pose. Next, we use a data-centric approach to select the closest\nrepresentation from a precomputed cluster of fine semantic masks. Finally, we\nadopt a multi-scale, attention-guided architecture to transfer the appearance\nattributes from an exemplar image. The proposed strategy enables us to\nsynthesize semantically coherent realistic persons that can blend into an\nexisting scene without altering the global context. We conclude our findings\nwith relevant qualitative and quantitative evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a data-driven approach for context-aware person image generation.\nSpecifically, we attempt to generate a person image such that the synthesized\ninstance can blend into a complex scene. In our method, the position, scale,\nand appearance of the generated person are semantically conditioned on the\nexisting persons in the scene. The proposed technique is divided into three\nsequential steps. At first, we employ a Pix2PixHD model to infer a coarse\nsemantic mask that represents the new person's spatial location, scale, and\npotential pose. Next, we use a data-centric approach to select the closest\nrepresentation from a precomputed cluster of fine semantic masks. Finally, we\nadopt a multi-scale, attention-guided architecture to transfer the appearance\nattributes from an exemplar image. The proposed strategy enables us to\nsynthesize semantically coherent realistic persons that can blend into an\nexisting scene without altering the global context. We conclude our findings\nwith relevant qualitative and quantitative evaluations."
                },
                "authors": [
                    {
                        "name": "Prasun Roy"
                    },
                    {
                        "name": "Saumik Bhattacharya"
                    },
                    {
                        "name": "Subhankar Ghosh"
                    },
                    {
                        "name": "Umapada Pal"
                    },
                    {
                        "name": "Michael Blumenstein"
                    }
                ],
                "author_detail": {
                    "name": "Michael Blumenstein"
                },
                "author": "Michael Blumenstein",
                "arxiv_comment": "Accepted in The International Conference on Pattern Recognition\n  (ICPR) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2302.14728v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2302.14728v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03371v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03371v3",
                "updated": "2025-02-18T17:31:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    31,
                    48,
                    1,
                    49,
                    0
                ],
                "published": "2024-05-06T11:24:13Z",
                "published_parsed": [
                    2024,
                    5,
                    6,
                    11,
                    24,
                    13,
                    0,
                    127,
                    0
                ],
                "title": "Explainable Fake News Detection With Large Language Model via Defense\n  Among Competing Wisdom",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable Fake News Detection With Large Language Model via Defense\n  Among Competing Wisdom"
                },
                "summary": "Most fake news detection methods learn latent feature representations based\non neural networks, which makes them black boxes to classify a piece of news\nwithout giving any justification. Existing explainable systems generate\nveracity justifications from investigative journalism, which suffer from\ndebunking delayed and low efficiency. Recent studies simply assume that the\njustification is equivalent to the majority opinions expressed in the wisdom of\ncrowds. However, the opinions typically contain some inaccurate or biased\ninformation since the wisdom of crowds is uncensored. To detect fake news from\na sea of diverse, crowded and even competing narratives, in this paper, we\npropose a novel defense-based explainable fake news detection framework.\nSpecifically, we first propose an evidence extraction module to split the\nwisdom of crowds into two competing parties and respectively detect salient\nevidences. To gain concise insights from evidences, we then design a\nprompt-based module that utilizes a large language model to generate\njustifications by inferring reasons towards two possible veracities. Finally,\nwe propose a defense-based inference module to determine veracity via modeling\nthe defense among these justifications. Extensive experiments conducted on two\nreal-world benchmarks demonstrate that our proposed method outperforms\nstate-of-the-art baselines in terms of fake news detection and provides\nhigh-quality justifications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most fake news detection methods learn latent feature representations based\non neural networks, which makes them black boxes to classify a piece of news\nwithout giving any justification. Existing explainable systems generate\nveracity justifications from investigative journalism, which suffer from\ndebunking delayed and low efficiency. Recent studies simply assume that the\njustification is equivalent to the majority opinions expressed in the wisdom of\ncrowds. However, the opinions typically contain some inaccurate or biased\ninformation since the wisdom of crowds is uncensored. To detect fake news from\na sea of diverse, crowded and even competing narratives, in this paper, we\npropose a novel defense-based explainable fake news detection framework.\nSpecifically, we first propose an evidence extraction module to split the\nwisdom of crowds into two competing parties and respectively detect salient\nevidences. To gain concise insights from evidences, we then design a\nprompt-based module that utilizes a large language model to generate\njustifications by inferring reasons towards two possible veracities. Finally,\nwe propose a defense-based inference module to determine veracity via modeling\nthe defense among these justifications. Extensive experiments conducted on two\nreal-world benchmarks demonstrate that our proposed method outperforms\nstate-of-the-art baselines in terms of fake news detection and provides\nhigh-quality justifications."
                },
                "authors": [
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Jing Ma"
                    },
                    {
                        "name": "Hongzhan Lin"
                    },
                    {
                        "name": "Zhiwei Yang"
                    },
                    {
                        "name": "Ruichao Yang"
                    },
                    {
                        "name": "Yuan Tian"
                    },
                    {
                        "name": "Yi Chang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Chang"
                },
                "author": "Yi Chang",
                "arxiv_comment": "12 pages, WWW'2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03371v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03371v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19437v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19437v2",
                "updated": "2025-02-18T17:26:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    26,
                    38,
                    1,
                    49,
                    0
                ],
                "published": "2024-12-27T04:03:16Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    4,
                    3,
                    16,
                    4,
                    362,
                    0
                ],
                "title": "DeepSeek-V3 Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSeek-V3 Technical Report"
                },
                "summary": "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with\n671B total parameters with 37B activated for each token. To achieve efficient\ninference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent\nAttention (MLA) and DeepSeekMoE architectures, which were thoroughly validated\nin DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free\nstrategy for load balancing and sets a multi-token prediction training\nobjective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion\ndiverse and high-quality tokens, followed by Supervised Fine-Tuning and\nReinforcement Learning stages to fully harness its capabilities. Comprehensive\nevaluations reveal that DeepSeek-V3 outperforms other open-source models and\nachieves performance comparable to leading closed-source models. Despite its\nexcellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its\nfull training. In addition, its training process is remarkably stable.\nThroughout the entire training process, we did not experience any irrecoverable\nloss spikes or perform any rollbacks. The model checkpoints are available at\nhttps://github.com/deepseek-ai/DeepSeek-V3.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with\n671B total parameters with 37B activated for each token. To achieve efficient\ninference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent\nAttention (MLA) and DeepSeekMoE architectures, which were thoroughly validated\nin DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free\nstrategy for load balancing and sets a multi-token prediction training\nobjective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion\ndiverse and high-quality tokens, followed by Supervised Fine-Tuning and\nReinforcement Learning stages to fully harness its capabilities. Comprehensive\nevaluations reveal that DeepSeek-V3 outperforms other open-source models and\nachieves performance comparable to leading closed-source models. Despite its\nexcellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its\nfull training. In addition, its training process is remarkably stable.\nThroughout the entire training process, we did not experience any irrecoverable\nloss spikes or perform any rollbacks. The model checkpoints are available at\nhttps://github.com/deepseek-ai/DeepSeek-V3."
                },
                "authors": [
                    {
                        "name": "DeepSeek-AI"
                    },
                    {
                        "name": "Aixin Liu"
                    },
                    {
                        "name": "Bei Feng"
                    },
                    {
                        "name": "Bing Xue"
                    },
                    {
                        "name": "Bingxuan Wang"
                    },
                    {
                        "name": "Bochao Wu"
                    },
                    {
                        "name": "Chengda Lu"
                    },
                    {
                        "name": "Chenggang Zhao"
                    },
                    {
                        "name": "Chengqi Deng"
                    },
                    {
                        "name": "Chenyu Zhang"
                    },
                    {
                        "name": "Chong Ruan"
                    },
                    {
                        "name": "Damai Dai"
                    },
                    {
                        "name": "Daya Guo"
                    },
                    {
                        "name": "Dejian Yang"
                    },
                    {
                        "name": "Deli Chen"
                    },
                    {
                        "name": "Dongjie Ji"
                    },
                    {
                        "name": "Erhang Li"
                    },
                    {
                        "name": "Fangyun Lin"
                    },
                    {
                        "name": "Fucong Dai"
                    },
                    {
                        "name": "Fuli Luo"
                    },
                    {
                        "name": "Guangbo Hao"
                    },
                    {
                        "name": "Guanting Chen"
                    },
                    {
                        "name": "Guowei Li"
                    },
                    {
                        "name": "H. Zhang"
                    },
                    {
                        "name": "Han Bao"
                    },
                    {
                        "name": "Hanwei Xu"
                    },
                    {
                        "name": "Haocheng Wang"
                    },
                    {
                        "name": "Haowei Zhang"
                    },
                    {
                        "name": "Honghui Ding"
                    },
                    {
                        "name": "Huajian Xin"
                    },
                    {
                        "name": "Huazuo Gao"
                    },
                    {
                        "name": "Hui Li"
                    },
                    {
                        "name": "Hui Qu"
                    },
                    {
                        "name": "J. L. Cai"
                    },
                    {
                        "name": "Jian Liang"
                    },
                    {
                        "name": "Jianzhong Guo"
                    },
                    {
                        "name": "Jiaqi Ni"
                    },
                    {
                        "name": "Jiashi Li"
                    },
                    {
                        "name": "Jiawei Wang"
                    },
                    {
                        "name": "Jin Chen"
                    },
                    {
                        "name": "Jingchang Chen"
                    },
                    {
                        "name": "Jingyang Yuan"
                    },
                    {
                        "name": "Junjie Qiu"
                    },
                    {
                        "name": "Junlong Li"
                    },
                    {
                        "name": "Junxiao Song"
                    },
                    {
                        "name": "Kai Dong"
                    },
                    {
                        "name": "Kai Hu"
                    },
                    {
                        "name": "Kaige Gao"
                    },
                    {
                        "name": "Kang Guan"
                    },
                    {
                        "name": "Kexin Huang"
                    },
                    {
                        "name": "Kuai Yu"
                    },
                    {
                        "name": "Lean Wang"
                    },
                    {
                        "name": "Lecong Zhang"
                    },
                    {
                        "name": "Lei Xu"
                    },
                    {
                        "name": "Leyi Xia"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Litong Wang"
                    },
                    {
                        "name": "Liyue Zhang"
                    },
                    {
                        "name": "Meng Li"
                    },
                    {
                        "name": "Miaojun Wang"
                    },
                    {
                        "name": "Mingchuan Zhang"
                    },
                    {
                        "name": "Minghua Zhang"
                    },
                    {
                        "name": "Minghui Tang"
                    },
                    {
                        "name": "Mingming Li"
                    },
                    {
                        "name": "Ning Tian"
                    },
                    {
                        "name": "Panpan Huang"
                    },
                    {
                        "name": "Peiyi Wang"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Qiancheng Wang"
                    },
                    {
                        "name": "Qihao Zhu"
                    },
                    {
                        "name": "Qinyu Chen"
                    },
                    {
                        "name": "Qiushi Du"
                    },
                    {
                        "name": "R. J. Chen"
                    },
                    {
                        "name": "R. L. Jin"
                    },
                    {
                        "name": "Ruiqi Ge"
                    },
                    {
                        "name": "Ruisong Zhang"
                    },
                    {
                        "name": "Ruizhe Pan"
                    },
                    {
                        "name": "Runji Wang"
                    },
                    {
                        "name": "Runxin Xu"
                    },
                    {
                        "name": "Ruoyu Zhang"
                    },
                    {
                        "name": "Ruyi Chen"
                    },
                    {
                        "name": "S. S. Li"
                    },
                    {
                        "name": "Shanghao Lu"
                    },
                    {
                        "name": "Shangyan Zhou"
                    },
                    {
                        "name": "Shanhuang Chen"
                    },
                    {
                        "name": "Shaoqing Wu"
                    },
                    {
                        "name": "Shengfeng Ye"
                    },
                    {
                        "name": "Shengfeng Ye"
                    },
                    {
                        "name": "Shirong Ma"
                    },
                    {
                        "name": "Shiyu Wang"
                    },
                    {
                        "name": "Shuang Zhou"
                    },
                    {
                        "name": "Shuiping Yu"
                    },
                    {
                        "name": "Shunfeng Zhou"
                    },
                    {
                        "name": "Shuting Pan"
                    },
                    {
                        "name": "T. Wang"
                    },
                    {
                        "name": "Tao Yun"
                    },
                    {
                        "name": "Tian Pei"
                    },
                    {
                        "name": "Tianyu Sun"
                    },
                    {
                        "name": "W. L. Xiao"
                    },
                    {
                        "name": "Wangding Zeng"
                    },
                    {
                        "name": "Wanjia Zhao"
                    },
                    {
                        "name": "Wei An"
                    },
                    {
                        "name": "Wen Liu"
                    },
                    {
                        "name": "Wenfeng Liang"
                    },
                    {
                        "name": "Wenjun Gao"
                    },
                    {
                        "name": "Wenqin Yu"
                    },
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "X. Q. Li"
                    },
                    {
                        "name": "Xiangyue Jin"
                    },
                    {
                        "name": "Xianzu Wang"
                    },
                    {
                        "name": "Xiao Bi"
                    },
                    {
                        "name": "Xiaodong Liu"
                    },
                    {
                        "name": "Xiaohan Wang"
                    },
                    {
                        "name": "Xiaojin Shen"
                    },
                    {
                        "name": "Xiaokang Chen"
                    },
                    {
                        "name": "Xiaokang Zhang"
                    },
                    {
                        "name": "Xiaosha Chen"
                    },
                    {
                        "name": "Xiaotao Nie"
                    },
                    {
                        "name": "Xiaowen Sun"
                    },
                    {
                        "name": "Xiaoxiang Wang"
                    },
                    {
                        "name": "Xin Cheng"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Xin Xie"
                    },
                    {
                        "name": "Xingchao Liu"
                    },
                    {
                        "name": "Xingkai Yu"
                    },
                    {
                        "name": "Xinnan Song"
                    },
                    {
                        "name": "Xinxia Shan"
                    },
                    {
                        "name": "Xinyi Zhou"
                    },
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Xinyuan Li"
                    },
                    {
                        "name": "Xuecheng Su"
                    },
                    {
                        "name": "Xuheng Lin"
                    },
                    {
                        "name": "Y. K. Li"
                    },
                    {
                        "name": "Y. Q. Wang"
                    },
                    {
                        "name": "Y. X. Wei"
                    },
                    {
                        "name": "Y. X. Zhu"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Yanhong Xu"
                    },
                    {
                        "name": "Yanhong Xu"
                    },
                    {
                        "name": "Yanping Huang"
                    },
                    {
                        "name": "Yao Li"
                    },
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Yaofeng Sun"
                    },
                    {
                        "name": "Yaohui Li"
                    },
                    {
                        "name": "Yaohui Wang"
                    },
                    {
                        "name": "Yi Yu"
                    },
                    {
                        "name": "Yi Zheng"
                    },
                    {
                        "name": "Yichao Zhang"
                    },
                    {
                        "name": "Yifan Shi"
                    },
                    {
                        "name": "Yiliang Xiong"
                    },
                    {
                        "name": "Ying He"
                    },
                    {
                        "name": "Ying Tang"
                    },
                    {
                        "name": "Yishi Piao"
                    },
                    {
                        "name": "Yisong Wang"
                    },
                    {
                        "name": "Yixuan Tan"
                    },
                    {
                        "name": "Yiyang Ma"
                    },
                    {
                        "name": "Yiyuan Liu"
                    },
                    {
                        "name": "Yongqiang Guo"
                    },
                    {
                        "name": "Yu Wu"
                    },
                    {
                        "name": "Yuan Ou"
                    },
                    {
                        "name": "Yuchen Zhu"
                    },
                    {
                        "name": "Yuduan Wang"
                    },
                    {
                        "name": "Yue Gong"
                    },
                    {
                        "name": "Yuheng Zou"
                    },
                    {
                        "name": "Yujia He"
                    },
                    {
                        "name": "Yukun Zha"
                    },
                    {
                        "name": "Yunfan Xiong"
                    },
                    {
                        "name": "Yunxian Ma"
                    },
                    {
                        "name": "Yuting Yan"
                    },
                    {
                        "name": "Yuxiang Luo"
                    },
                    {
                        "name": "Yuxiang You"
                    },
                    {
                        "name": "Yuxuan Liu"
                    },
                    {
                        "name": "Yuyang Zhou"
                    },
                    {
                        "name": "Z. F. Wu"
                    },
                    {
                        "name": "Z. Z. Ren"
                    },
                    {
                        "name": "Zehui Ren"
                    },
                    {
                        "name": "Zhangli Sha"
                    },
                    {
                        "name": "Zhe Fu"
                    },
                    {
                        "name": "Zhean Xu"
                    },
                    {
                        "name": "Zhen Huang"
                    },
                    {
                        "name": "Zhen Zhang"
                    },
                    {
                        "name": "Zhenda Xie"
                    },
                    {
                        "name": "Zhengyan Zhang"
                    },
                    {
                        "name": "Zhewen Hao"
                    },
                    {
                        "name": "Zhibin Gou"
                    },
                    {
                        "name": "Zhicheng Ma"
                    },
                    {
                        "name": "Zhigang Yan"
                    },
                    {
                        "name": "Zhihong Shao"
                    },
                    {
                        "name": "Zhipeng Xu"
                    },
                    {
                        "name": "Zhiyu Wu"
                    },
                    {
                        "name": "Zhongyu Zhang"
                    },
                    {
                        "name": "Zhuoshu Li"
                    },
                    {
                        "name": "Zihui Gu"
                    },
                    {
                        "name": "Zijia Zhu"
                    },
                    {
                        "name": "Zijun Liu"
                    },
                    {
                        "name": "Zilin Li"
                    },
                    {
                        "name": "Ziwei Xie"
                    },
                    {
                        "name": "Ziyang Song"
                    },
                    {
                        "name": "Ziyi Gao"
                    },
                    {
                        "name": "Zizheng Pan"
                    }
                ],
                "author_detail": {
                    "name": "Zizheng Pan"
                },
                "author": "Zizheng Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19437v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19437v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15939v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15939v2",
                "updated": "2025-02-18T17:19:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    19,
                    24,
                    1,
                    49,
                    0
                ],
                "published": "2024-10-21T12:12:21Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    12,
                    12,
                    21,
                    0,
                    295,
                    0
                ],
                "title": "CausalGraph2LLM: Evaluating LLMs for Causal Queries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CausalGraph2LLM: Evaluating LLMs for Causal Queries"
                },
                "summary": "Causality is essential in scientific research, enabling researchers to\ninterpret true relationships between variables. These causal relationships are\noften represented by causal graphs, which are directed acyclic graphs. With the\nrecent advancements in Large Language Models (LLMs), there is an increasing\ninterest in exploring their capabilities in causal reasoning and their\npotential use to hypothesize causal graphs. These tasks necessitate the LLMs to\nencode the causal graph effectively for subsequent downstream tasks. In this\npaper, we introduce CausalGraph2LLM, a comprehensive benchmark comprising over\n700k queries across diverse causal graph settings to evaluate the causal\nreasoning capabilities of LLMs. We categorize the causal queries into two\ntypes: graph-level and node-level queries. We benchmark both open-sourced and\npropriety models for our study. Our findings reveal that while LLMs show\npromise in this domain, they are highly sensitive to the encoding used. Even\ncapable models like GPT-4 and Gemini-1.5 exhibit sensitivity to encoding, with\ndeviations of about $60\\%$. We further demonstrate this sensitivity for\ndownstream causal intervention tasks. Moreover, we observe that LLMs can often\ndisplay biases when presented with contextual information about a causal graph,\npotentially stemming from their parametric memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causality is essential in scientific research, enabling researchers to\ninterpret true relationships between variables. These causal relationships are\noften represented by causal graphs, which are directed acyclic graphs. With the\nrecent advancements in Large Language Models (LLMs), there is an increasing\ninterest in exploring their capabilities in causal reasoning and their\npotential use to hypothesize causal graphs. These tasks necessitate the LLMs to\nencode the causal graph effectively for subsequent downstream tasks. In this\npaper, we introduce CausalGraph2LLM, a comprehensive benchmark comprising over\n700k queries across diverse causal graph settings to evaluate the causal\nreasoning capabilities of LLMs. We categorize the causal queries into two\ntypes: graph-level and node-level queries. We benchmark both open-sourced and\npropriety models for our study. Our findings reveal that while LLMs show\npromise in this domain, they are highly sensitive to the encoding used. Even\ncapable models like GPT-4 and Gemini-1.5 exhibit sensitivity to encoding, with\ndeviations of about $60\\%$. We further demonstrate this sensitivity for\ndownstream causal intervention tasks. Moreover, we observe that LLMs can often\ndisplay biases when presented with contextual information about a causal graph,\npotentially stemming from their parametric memory."
                },
                "authors": [
                    {
                        "name": "Ivaxi Sheth"
                    },
                    {
                        "name": "Bahare Fatemi"
                    },
                    {
                        "name": "Mario Fritz"
                    }
                ],
                "author_detail": {
                    "name": "Mario Fritz"
                },
                "author": "Mario Fritz",
                "arxiv_comment": "NAACL'25 Findings, Code - https://github.com/ivaxi0s/CausalGraph2LLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15939v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15939v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13069v1",
                "updated": "2025-02-18T17:12:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    12,
                    26,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T17:12:26Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    12,
                    26,
                    1,
                    49,
                    0
                ],
                "title": "Interactive Agents to Overcome Ambiguity in Software Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interactive Agents to Overcome Ambiguity in Software Engineering"
                },
                "summary": "AI agents are increasingly being deployed to automate tasks, often based on\nambiguous and underspecified user instructions. Making unwarranted assumptions\nand failing to ask clarifying questions can lead to suboptimal outcomes, safety\nrisks due to tool misuse, and wasted computational resources. In this work, we\nstudy the ability of LLM agents to handle ambiguous instructions in interactive\ncode generation settings by evaluating proprietary and open-weight models on\ntheir performance across three key steps: (a) leveraging interactivity to\nimprove performance in ambiguous scenarios, (b) detecting ambiguity, and (c)\nasking targeted questions. Our findings reveal that models struggle to\ndistinguish between well-specified and underspecified instructions. However,\nwhen models interact for underspecified inputs, they effectively obtain vital\ninformation from the user, leading to significant improvements in performance\nand underscoring the value of effective interaction. Our study highlights\ncritical gaps in how current state-of-the-art models handle ambiguity in\ncomplex software engineering tasks and structures the evaluation into distinct\nsteps to enable targeted improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI agents are increasingly being deployed to automate tasks, often based on\nambiguous and underspecified user instructions. Making unwarranted assumptions\nand failing to ask clarifying questions can lead to suboptimal outcomes, safety\nrisks due to tool misuse, and wasted computational resources. In this work, we\nstudy the ability of LLM agents to handle ambiguous instructions in interactive\ncode generation settings by evaluating proprietary and open-weight models on\ntheir performance across three key steps: (a) leveraging interactivity to\nimprove performance in ambiguous scenarios, (b) detecting ambiguity, and (c)\nasking targeted questions. Our findings reveal that models struggle to\ndistinguish between well-specified and underspecified instructions. However,\nwhen models interact for underspecified inputs, they effectively obtain vital\ninformation from the user, leading to significant improvements in performance\nand underscoring the value of effective interaction. Our study highlights\ncritical gaps in how current state-of-the-art models handle ambiguity in\ncomplex software engineering tasks and structures the evaluation into distinct\nsteps to enable targeted improvements."
                },
                "authors": [
                    {
                        "name": "Sanidhya Vijayvargiya"
                    },
                    {
                        "name": "Xuhui Zhou"
                    },
                    {
                        "name": "Akhila Yerukola"
                    },
                    {
                        "name": "Maarten Sap"
                    },
                    {
                        "name": "Graham Neubig"
                    }
                ],
                "author_detail": {
                    "name": "Graham Neubig"
                },
                "author": "Graham Neubig",
                "arxiv_comment": "15 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06254v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06254v2",
                "updated": "2025-02-18T17:10:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    10,
                    39,
                    1,
                    49,
                    0
                ],
                "published": "2025-01-09T02:54:19Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    2,
                    54,
                    19,
                    3,
                    9,
                    0
                ],
                "title": "Rethinking Evaluation of Sparse Autoencoders through the Representation\n  of Polysemous Words",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Evaluation of Sparse Autoencoders through the Representation\n  of Polysemous Words"
                },
                "summary": "Sparse autoencoders (SAEs) have gained a lot of attention as a promising tool\nto improve the interpretability of large language models (LLMs) by mapping the\ncomplex superposition of polysemantic neurons into monosemantic features and\ncomposing a sparse dictionary of words. However, traditional performance\nmetrics like Mean Squared Error and L0 sparsity ignore the evaluation of the\nsemantic representational power of SAEs -- whether they can acquire\ninterpretable monosemantic features while preserving the semantic relationship\nof words. For instance, it is not obvious whether a learned sparse feature\ncould distinguish different meanings in one word. In this paper, we propose a\nsuite of evaluations for SAEs to analyze the quality of monosemantic features\nby focusing on polysemous words. Our findings reveal that SAEs developed to\nimprove the MSE-L0 Pareto frontier may confuse interpretability, which does not\nnecessarily enhance the extraction of monosemantic features. The analysis of\nSAEs with polysemous words can also figure out the internal mechanism of LLMs;\ndeeper layers and the Attention module contribute to distinguishing polysemy in\na word. Our semantics focused evaluation offers new insights into the polysemy\nand the existing SAE objective and contributes to the development of more\npractical SAEs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse autoencoders (SAEs) have gained a lot of attention as a promising tool\nto improve the interpretability of large language models (LLMs) by mapping the\ncomplex superposition of polysemantic neurons into monosemantic features and\ncomposing a sparse dictionary of words. However, traditional performance\nmetrics like Mean Squared Error and L0 sparsity ignore the evaluation of the\nsemantic representational power of SAEs -- whether they can acquire\ninterpretable monosemantic features while preserving the semantic relationship\nof words. For instance, it is not obvious whether a learned sparse feature\ncould distinguish different meanings in one word. In this paper, we propose a\nsuite of evaluations for SAEs to analyze the quality of monosemantic features\nby focusing on polysemous words. Our findings reveal that SAEs developed to\nimprove the MSE-L0 Pareto frontier may confuse interpretability, which does not\nnecessarily enhance the extraction of monosemantic features. The analysis of\nSAEs with polysemous words can also figure out the internal mechanism of LLMs;\ndeeper layers and the Attention module contribute to distinguishing polysemy in\na word. Our semantics focused evaluation offers new insights into the polysemy\nand the existing SAE objective and contributes to the development of more\npractical SAEs."
                },
                "authors": [
                    {
                        "name": "Gouki Minegishi"
                    },
                    {
                        "name": "Hiroki Furuta"
                    },
                    {
                        "name": "Yusuke Iwasawa"
                    },
                    {
                        "name": "Yutaka Matsuo"
                    }
                ],
                "author_detail": {
                    "name": "Yutaka Matsuo"
                },
                "author": "Yutaka Matsuo",
                "arxiv_comment": "Published at ICLR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06254v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06254v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13059v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13059v1",
                "updated": "2025-02-18T17:04:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    4,
                    26,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T17:04:26Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    4,
                    26,
                    1,
                    49,
                    0
                ],
                "title": "SimpleVQA: Multimodal Factuality Evaluation for Multimodal Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimpleVQA: Multimodal Factuality Evaluation for Multimodal Large\n  Language Models"
                },
                "summary": "The increasing application of multi-modal large language models (MLLMs)\nacross various sectors have spotlighted the essence of their output reliability\nand accuracy, particularly their ability to produce content grounded in factual\ninformation (e.g. common and domain-specific knowledge). In this work, we\nintroduce SimpleVQA, the first comprehensive multi-modal benchmark to evaluate\nthe factuality ability of MLLMs to answer natural language short questions.\nSimpleVQA is characterized by six key features: it covers multiple tasks and\nmultiple scenarios, ensures high quality and challenging queries, maintains\nstatic and timeless reference answers, and is straightforward to evaluate. Our\napproach involves categorizing visual question-answering items into 9 different\ntasks around objective events or common knowledge and situating these within 9\ntopics. Rigorous quality control processes are implemented to guarantee\nhigh-quality, concise, and clear answers, facilitating evaluation with minimal\nvariance via an LLM-as-a-judge scoring system. Using SimpleVQA, we perform a\ncomprehensive assessment of leading 18 MLLMs and 8 text-only LLMs, delving into\ntheir image comprehension and text generation abilities by identifying and\nanalyzing error cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing application of multi-modal large language models (MLLMs)\nacross various sectors have spotlighted the essence of their output reliability\nand accuracy, particularly their ability to produce content grounded in factual\ninformation (e.g. common and domain-specific knowledge). In this work, we\nintroduce SimpleVQA, the first comprehensive multi-modal benchmark to evaluate\nthe factuality ability of MLLMs to answer natural language short questions.\nSimpleVQA is characterized by six key features: it covers multiple tasks and\nmultiple scenarios, ensures high quality and challenging queries, maintains\nstatic and timeless reference answers, and is straightforward to evaluate. Our\napproach involves categorizing visual question-answering items into 9 different\ntasks around objective events or common knowledge and situating these within 9\ntopics. Rigorous quality control processes are implemented to guarantee\nhigh-quality, concise, and clear answers, facilitating evaluation with minimal\nvariance via an LLM-as-a-judge scoring system. Using SimpleVQA, we perform a\ncomprehensive assessment of leading 18 MLLMs and 8 text-only LLMs, delving into\ntheir image comprehension and text generation abilities by identifying and\nanalyzing error cases."
                },
                "authors": [
                    {
                        "name": "Xianfu Cheng"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Shiwei Zhang"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Xiangyuan Guan"
                    },
                    {
                        "name": "Xianjie Wu"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Yuying Mai"
                    },
                    {
                        "name": "Yutao Zeng"
                    },
                    {
                        "name": "Zhoufutu Wen"
                    },
                    {
                        "name": "Ke Jin"
                    },
                    {
                        "name": "Baorui Wang"
                    },
                    {
                        "name": "Weixiao Zhou"
                    },
                    {
                        "name": "Yunhong Lu"
                    },
                    {
                        "name": "Tongliang Li"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Zhoujun Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhoujun Li"
                },
                "author": "Zhoujun Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13059v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13059v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13056v1",
                "updated": "2025-02-18T17:02:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    2,
                    41,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T17:02:41Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    2,
                    41,
                    1,
                    49,
                    0
                ],
                "title": "Benchmarking MedMNIST dataset on real quantum hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking MedMNIST dataset on real quantum hardware"
                },
                "summary": "Quantum machine learning (QML) has emerged as a promising domain to leverage\nthe computational capabilities of quantum systems to solve complex\nclassification tasks. In this work, we present first comprehensive QML study by\nbenchmarking the MedMNIST-a diverse collection of medical imaging datasets on a\n127-qubit real IBM quantum hardware, to evaluate the feasibility and\nperformance of quantum models (without any classical neural networks) in\npractical applications. This study explore recent advancements in quantum\ncomputing such as device-aware quantum circuits, error suppression and\nmitigation for medical image classification. Our methodology comprised of three\nstages: preprocessing, generation of noise-resilient and hardware-efficient\nquantum circuits, optimizing/training of quantum circuits on classical\nhardware, and inference on real IBM quantum hardware. Firstly, we process all\ninput images in the preprocessing stage to reduce the spatial dimension due to\nthe quantum hardware limitations. We generate hardware-efficient quantum\ncircuits using backend properties expressible to learn complex patterns for\nmedical image classification. After classical optimization of QML models, we\nperform the inference on real quantum hardware. We also incorporates advanced\nerror suppression and mitigation techniques in our QML workflow including\ndynamical decoupling (DD), gate twirling, and matrix-free measurement\nmitigation (M3) to mitigate the effects of noise and improve classification\nperformance. The experimental results showcase the potential of quantum\ncomputing for medical imaging and establishes a benchmark for future\nadvancements in QML applied to healthcare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum machine learning (QML) has emerged as a promising domain to leverage\nthe computational capabilities of quantum systems to solve complex\nclassification tasks. In this work, we present first comprehensive QML study by\nbenchmarking the MedMNIST-a diverse collection of medical imaging datasets on a\n127-qubit real IBM quantum hardware, to evaluate the feasibility and\nperformance of quantum models (without any classical neural networks) in\npractical applications. This study explore recent advancements in quantum\ncomputing such as device-aware quantum circuits, error suppression and\nmitigation for medical image classification. Our methodology comprised of three\nstages: preprocessing, generation of noise-resilient and hardware-efficient\nquantum circuits, optimizing/training of quantum circuits on classical\nhardware, and inference on real IBM quantum hardware. Firstly, we process all\ninput images in the preprocessing stage to reduce the spatial dimension due to\nthe quantum hardware limitations. We generate hardware-efficient quantum\ncircuits using backend properties expressible to learn complex patterns for\nmedical image classification. After classical optimization of QML models, we\nperform the inference on real quantum hardware. We also incorporates advanced\nerror suppression and mitigation techniques in our QML workflow including\ndynamical decoupling (DD), gate twirling, and matrix-free measurement\nmitigation (M3) to mitigate the effects of noise and improve classification\nperformance. The experimental results showcase the potential of quantum\ncomputing for medical imaging and establishes a benchmark for future\nadvancements in QML applied to healthcare."
                },
                "authors": [
                    {
                        "name": "Gurinder Singh"
                    },
                    {
                        "name": "Hongni Jin"
                    },
                    {
                        "name": "Kenneth M. Merz Jr"
                    }
                ],
                "author_detail": {
                    "name": "Kenneth M. Merz Jr"
                },
                "author": "Kenneth M. Merz Jr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13055v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13055v1",
                "updated": "2025-02-18T17:01:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    1,
                    37,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T17:01:37Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    1,
                    37,
                    1,
                    49,
                    0
                ],
                "title": "LAMD: Context-driven Android Malware Detection and Classification with\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAMD: Context-driven Android Malware Detection and Classification with\n  LLMs"
                },
                "summary": "The rapid growth of mobile applications has escalated Android malware\nthreats. Although there are numerous detection methods, they often struggle\nwith evolving attacks, dataset biases, and limited explainability. Large\nLanguage Models (LLMs) offer a promising alternative with their zero-shot\ninference and reasoning capabilities. However, applying LLMs to Android malware\ndetection presents two key challenges: (1)the extensive support code in Android\napplications, often spanning thousands of classes, exceeds LLMs' context limits\nand obscures malicious behavior within benign functionality; (2)the structural\ncomplexity and interdependencies of Android applications surpass LLMs'\nsequence-based reasoning, fragmenting code analysis and hindering malicious\nintent inference. To address these challenges, we propose LAMD, a practical\ncontext-driven framework to enable LLM-based Android malware detection. LAMD\nintegrates key context extraction to isolate security-critical code regions and\nconstruct program structures, then applies tier-wise code reasoning to analyze\napplication behavior progressively, from low-level instructions to high-level\nsemantics, providing final prediction and explanation. A well-designed factual\nconsistency verification mechanism is equipped to mitigate LLM hallucinations\nfrom the first tier. Evaluation in real-world settings demonstrates LAMD's\neffectiveness over conventional detectors, establishing a feasible basis for\nLLM-driven malware analysis in dynamic threat landscapes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of mobile applications has escalated Android malware\nthreats. Although there are numerous detection methods, they often struggle\nwith evolving attacks, dataset biases, and limited explainability. Large\nLanguage Models (LLMs) offer a promising alternative with their zero-shot\ninference and reasoning capabilities. However, applying LLMs to Android malware\ndetection presents two key challenges: (1)the extensive support code in Android\napplications, often spanning thousands of classes, exceeds LLMs' context limits\nand obscures malicious behavior within benign functionality; (2)the structural\ncomplexity and interdependencies of Android applications surpass LLMs'\nsequence-based reasoning, fragmenting code analysis and hindering malicious\nintent inference. To address these challenges, we propose LAMD, a practical\ncontext-driven framework to enable LLM-based Android malware detection. LAMD\nintegrates key context extraction to isolate security-critical code regions and\nconstruct program structures, then applies tier-wise code reasoning to analyze\napplication behavior progressively, from low-level instructions to high-level\nsemantics, providing final prediction and explanation. A well-designed factual\nconsistency verification mechanism is equipped to mitigate LLM hallucinations\nfrom the first tier. Evaluation in real-world settings demonstrates LAMD's\neffectiveness over conventional detectors, establishing a feasible basis for\nLLM-driven malware analysis in dynamic threat landscapes."
                },
                "authors": [
                    {
                        "name": "Xingzhi Qian"
                    },
                    {
                        "name": "Xinran Zheng"
                    },
                    {
                        "name": "Yiling He"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Lorenzo Cavallaro"
                    }
                ],
                "author_detail": {
                    "name": "Lorenzo Cavallaro"
                },
                "author": "Lorenzo Cavallaro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13055v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13055v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13054v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13054v1",
                "updated": "2025-02-18T17:01:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    1,
                    32,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T17:01:32Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    1,
                    32,
                    1,
                    49,
                    0
                ],
                "title": "QZO: A Catalog of 5 Million Quasars from the Zwicky Transient Facility",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QZO: A Catalog of 5 Million Quasars from the Zwicky Transient Facility"
                },
                "summary": "Machine learning methods are well established in the classification of\nquasars (QSOs). However, the advent of light curve observations adds a great\namount of complexity to the problem. Our goal is to use the Zwicky Transient\nFacility (ZTF) to create a catalog of QSOs. We process the ZTF DR20 light\ncurves with a transformer artificial neural network and combine the Pan-STARRS\n(PS), AllWISE, and Gaia surveys with extreme gradient boosting. Using ZTF\ng-band data with at least 100 observational epochs per light curve, we obtain\n97% F1 score for QSOs. We find that with 3 day median cadence, a survey time\nspan of at least 900 days is required to achieve 90% QSO F1 score. However, one\ncan obtain the same score with a survey time span of 1800 days and the median\ncadence prolonged to 12 days. We find that ZTF classification is superior to\nthe PS static bands, and on par with WISE and Gaia measurements. Additionally,\nwe find that the light curves provide the most important features for QSO\nclassification in the ZTF dataset. We robustly classify objects fainter than\nthe $5\\sigma$ SNR limit at $g=20.8$ by requiring $g < \\mathrm{n_{obs}} / 80 +\n20.375$. For this sample, we run inference with added WISE observations, and\nfind 4,849,574 objects classified as QSOs. For 33% of QZO objects, with\navailable WISE data, we publish redshifts with estimated error $\\Delta z/(1 +\nz) = 0.14$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning methods are well established in the classification of\nquasars (QSOs). However, the advent of light curve observations adds a great\namount of complexity to the problem. Our goal is to use the Zwicky Transient\nFacility (ZTF) to create a catalog of QSOs. We process the ZTF DR20 light\ncurves with a transformer artificial neural network and combine the Pan-STARRS\n(PS), AllWISE, and Gaia surveys with extreme gradient boosting. Using ZTF\ng-band data with at least 100 observational epochs per light curve, we obtain\n97% F1 score for QSOs. We find that with 3 day median cadence, a survey time\nspan of at least 900 days is required to achieve 90% QSO F1 score. However, one\ncan obtain the same score with a survey time span of 1800 days and the median\ncadence prolonged to 12 days. We find that ZTF classification is superior to\nthe PS static bands, and on par with WISE and Gaia measurements. Additionally,\nwe find that the light curves provide the most important features for QSO\nclassification in the ZTF dataset. We robustly classify objects fainter than\nthe $5\\sigma$ SNR limit at $g=20.8$ by requiring $g < \\mathrm{n_{obs}} / 80 +\n20.375$. For this sample, we run inference with added WISE observations, and\nfind 4,849,574 objects classified as QSOs. For 33% of QZO objects, with\navailable WISE data, we publish redshifts with estimated error $\\Delta z/(1 +\nz) = 0.14$."
                },
                "authors": [
                    {
                        "name": "S. J. Nakoneczny"
                    },
                    {
                        "name": "M. J. Graham"
                    },
                    {
                        "name": "D. Stern"
                    },
                    {
                        "name": "G. Helou"
                    },
                    {
                        "name": "S. G. Djorgovski"
                    },
                    {
                        "name": "E. C. Bellm"
                    },
                    {
                        "name": "T. X. Chen"
                    },
                    {
                        "name": "R. Dekany"
                    },
                    {
                        "name": "A. Drake"
                    },
                    {
                        "name": "A. A. Mahabal"
                    },
                    {
                        "name": "T. A. Prince"
                    },
                    {
                        "name": "R. Riddle"
                    },
                    {
                        "name": "B. Rusholme"
                    },
                    {
                        "name": "N. Sravan"
                    }
                ],
                "author_detail": {
                    "name": "N. Sravan"
                },
                "author": "N. Sravan",
                "arxiv_comment": "We will release the catalog upon acceptance in a journal. The code is\n  available at https://github.com/snakoneczny/ztf-agn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13054v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13054v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13053v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13053v1",
                "updated": "2025-02-18T17:01:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    1,
                    28,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T17:01:28Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    1,
                    28,
                    1,
                    49,
                    0
                ],
                "title": "AEIA-MN: Evaluating the Robustness of Multimodal LLM-Powered Mobile\n  Agents Against Active Environmental Injection Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AEIA-MN: Evaluating the Robustness of Multimodal LLM-Powered Mobile\n  Agents Against Active Environmental Injection Attacks"
                },
                "summary": "As researchers continuously optimize AI agents to perform tasks more\neffectively within operating systems, they often neglect to address the\ncritical need for enabling these agents to identify \"impostors\" within the\nsystem. Through an analysis of the agents' operating environment, we identified\na potential threat: attackers can disguise their attack methods as\nenvironmental elements, injecting active disturbances into the agents'\nexecution process, thereby disrupting their decision-making. We define this\ntype of attack as Active Environment Injection Attack (AEIA). Based on this, we\npropose AEIA-MN, an active environment injection attack scheme that exploits\ninteraction vulnerabilities in the mobile operating system to evaluate the\nrobustness of MLLM-based agents against such threats. Experimental results show\nthat even advanced MLLMs are highly vulnerable to this attack, achieving a\nmaximum attack success rate of 93% in the AndroidWorld benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As researchers continuously optimize AI agents to perform tasks more\neffectively within operating systems, they often neglect to address the\ncritical need for enabling these agents to identify \"impostors\" within the\nsystem. Through an analysis of the agents' operating environment, we identified\na potential threat: attackers can disguise their attack methods as\nenvironmental elements, injecting active disturbances into the agents'\nexecution process, thereby disrupting their decision-making. We define this\ntype of attack as Active Environment Injection Attack (AEIA). Based on this, we\npropose AEIA-MN, an active environment injection attack scheme that exploits\ninteraction vulnerabilities in the mobile operating system to evaluate the\nrobustness of MLLM-based agents against such threats. Experimental results show\nthat even advanced MLLMs are highly vulnerable to this attack, achieving a\nmaximum attack success rate of 93% in the AndroidWorld benchmark."
                },
                "authors": [
                    {
                        "name": "Yurun Chen"
                    },
                    {
                        "name": "Xueyu Hu"
                    },
                    {
                        "name": "Keting Yin"
                    },
                    {
                        "name": "Juncheng Li"
                    },
                    {
                        "name": "Shengyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shengyu Zhang"
                },
                "author": "Shengyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13053v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13053v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13047v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13047v1",
                "updated": "2025-02-18T16:57:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    57,
                    22,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T16:57:22Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    57,
                    22,
                    1,
                    49,
                    0
                ],
                "title": "Development of systematic uncertainty-aware neural network trainings for\n  binned-likelihood analyses at the LHC",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development of systematic uncertainty-aware neural network trainings for\n  binned-likelihood analyses at the LHC"
                },
                "summary": "We propose a neural network training method capable of accounting for the\neffects of systematic variations of the data model in the training process and\ndescribe its extension towards neural network multiclass classification. The\nprocedure is evaluated on the realistic case of the measurement of Higgs boson\nproduction via gluon fusion and vector boson fusion in the $\\tau\\tau$ decay\nchannel at the CMS experiment. The neural network output functions are used to\ninfer the signal strengths for inclusive production of Higgs bosons as well as\nfor their production via gluon fusion and vector boson fusion. We observe\nimprovements of 12 and 16% in the uncertainty in the signal strengths for gluon\nand vector-boson fusion, respectively, compared with a conventional neural\nnetwork training based on cross-entropy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a neural network training method capable of accounting for the\neffects of systematic variations of the data model in the training process and\ndescribe its extension towards neural network multiclass classification. The\nprocedure is evaluated on the realistic case of the measurement of Higgs boson\nproduction via gluon fusion and vector boson fusion in the $\\tau\\tau$ decay\nchannel at the CMS experiment. The neural network output functions are used to\ninfer the signal strengths for inclusive production of Higgs bosons as well as\nfor their production via gluon fusion and vector boson fusion. We observe\nimprovements of 12 and 16% in the uncertainty in the signal strengths for gluon\nand vector-boson fusion, respectively, compared with a conventional neural\nnetwork training based on cross-entropy."
                },
                "authors": [
                    {
                        "name": "CMS Collaboration"
                    }
                ],
                "author_detail": {
                    "name": "CMS Collaboration"
                },
                "author": "CMS Collaboration",
                "arxiv_comment": "Submitted to Computing and Software for Big Science. All figures and\n  tables can be found at\n  http://cms-results.web.cern.ch/cms-results/public-results/publications/MLG-23-005\n  (CMS Public Pages)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13047v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13047v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13044v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13044v1",
                "updated": "2025-02-18T16:56:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    56,
                    15,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T16:56:15Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    56,
                    15,
                    1,
                    49,
                    0
                ],
                "title": "Do we still need Human Annotators? Prompting Large Language Models for\n  Aspect Sentiment Quad Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do we still need Human Annotators? Prompting Large Language Models for\n  Aspect Sentiment Quad Prediction"
                },
                "summary": "Aspect sentiment quadruple prediction (ASQP) facilitates a detailed\nunderstanding of opinions expressed in a text by identifying the opinion term,\naspect term, aspect category and sentiment polarity for each opinion. However,\nannotating a full set of training examples to fine-tune models for ASQP is a\nresource-intensive process. In this study, we explore the capabilities of large\nlanguage models (LLMs) for zero- and few-shot learning on the ASQP task across\nfive diverse datasets. We report F1 scores slightly below those obtained with\nstate-of-the-art fine-tuned models but exceeding previously reported zero- and\nfew-shot performance. In the 40-shot setting on the Rest16 restaurant domain\ndataset, LLMs achieved an F1 score of 52.46, compared to 60.39 by the\nbest-performing fine-tuned method MVP. Additionally, we report the performance\nof LLMs in target aspect sentiment detection (TASD), where the F1 scores were\nalso close to fine-tuned models, achieving 66.03 on Rest16 in the 40-shot\nsetting, compared to 72.76 with MVP. While human annotators remain essential\nfor achieving optimal performance, LLMs can reduce the need for extensive\nmanual annotation in ASQP tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aspect sentiment quadruple prediction (ASQP) facilitates a detailed\nunderstanding of opinions expressed in a text by identifying the opinion term,\naspect term, aspect category and sentiment polarity for each opinion. However,\nannotating a full set of training examples to fine-tune models for ASQP is a\nresource-intensive process. In this study, we explore the capabilities of large\nlanguage models (LLMs) for zero- and few-shot learning on the ASQP task across\nfive diverse datasets. We report F1 scores slightly below those obtained with\nstate-of-the-art fine-tuned models but exceeding previously reported zero- and\nfew-shot performance. In the 40-shot setting on the Rest16 restaurant domain\ndataset, LLMs achieved an F1 score of 52.46, compared to 60.39 by the\nbest-performing fine-tuned method MVP. Additionally, we report the performance\nof LLMs in target aspect sentiment detection (TASD), where the F1 scores were\nalso close to fine-tuned models, achieving 66.03 on Rest16 in the 40-shot\nsetting, compared to 72.76 with MVP. While human annotators remain essential\nfor achieving optimal performance, LLMs can reduce the need for extensive\nmanual annotation in ASQP tasks."
                },
                "authors": [
                    {
                        "name": "Nils Constantin Hellwig"
                    },
                    {
                        "name": "Jakob Fehle"
                    },
                    {
                        "name": "Udo Kruschwitz"
                    },
                    {
                        "name": "Christian Wolff"
                    }
                ],
                "author_detail": {
                    "name": "Christian Wolff"
                },
                "author": "Christian Wolff",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13044v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13044v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18623v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18623v2",
                "updated": "2025-02-18T16:53:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    53,
                    58,
                    1,
                    49,
                    0
                ],
                "published": "2025-01-27T00:20:48Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    0,
                    20,
                    48,
                    0,
                    27,
                    0
                ],
                "title": "VLMaterial: Procedural Material Generation with Large Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLMaterial: Procedural Material Generation with Large Vision-Language\n  Models"
                },
                "summary": "Procedural materials, represented as functional node graphs, are ubiquitous\nin computer graphics for photorealistic material appearance design. They allow\nusers to perform intuitive and precise editing to achieve desired visual\nappearances. However, creating a procedural material given an input image\nrequires professional knowledge and significant effort. In this work, we\nleverage the ability to convert procedural materials into standard Python\nprograms and fine-tune a large pre-trained vision-language model (VLM) to\ngenerate such programs from input images. To enable effective fine-tuning, we\nalso contribute an open-source procedural material dataset and propose to\nperform program-level augmentation by prompting another pre-trained large\nlanguage model (LLM). Through extensive evaluation, we show that our method\noutperforms previous methods on both synthetic and real-world examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Procedural materials, represented as functional node graphs, are ubiquitous\nin computer graphics for photorealistic material appearance design. They allow\nusers to perform intuitive and precise editing to achieve desired visual\nappearances. However, creating a procedural material given an input image\nrequires professional knowledge and significant effort. In this work, we\nleverage the ability to convert procedural materials into standard Python\nprograms and fine-tune a large pre-trained vision-language model (VLM) to\ngenerate such programs from input images. To enable effective fine-tuning, we\nalso contribute an open-source procedural material dataset and propose to\nperform program-level augmentation by prompting another pre-trained large\nlanguage model (LLM). Through extensive evaluation, we show that our method\noutperforms previous methods on both synthetic and real-world examples."
                },
                "authors": [
                    {
                        "name": "Beichen Li"
                    },
                    {
                        "name": "Rundi Wu"
                    },
                    {
                        "name": "Armando Solar-Lezama"
                    },
                    {
                        "name": "Changxi Zheng"
                    },
                    {
                        "name": "Liang Shi"
                    },
                    {
                        "name": "Bernd Bickel"
                    },
                    {
                        "name": "Wojciech Matusik"
                    }
                ],
                "author_detail": {
                    "name": "Wojciech Matusik"
                },
                "author": "Wojciech Matusik",
                "arxiv_comment": "ICLR 2025 Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18623v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18623v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18585v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18585v2",
                "updated": "2025-02-18T16:51:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    51,
                    53,
                    1,
                    49,
                    0
                ],
                "published": "2025-01-30T18:58:18Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    58,
                    18,
                    3,
                    30,
                    0
                ],
                "title": "Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs"
                },
                "summary": "Large language models (LLMs) such as OpenAI's o1 have demonstrated remarkable\nabilities in complex reasoning tasks by scaling test-time compute and\nexhibiting human-like deep thinking. However, we identify a phenomenon we term\nunderthinking, where o1-like LLMs frequently switch between different reasoning\nthoughts without sufficiently exploring promising paths to reach a correct\nsolution. This behavior leads to inadequate depth of reasoning and decreased\nperformance, particularly on challenging mathematical problems. To\nsystematically analyze this issue, we conduct experiments on three challenging\ntest sets and two representative open-source o1-like models, revealing that\nfrequent thought switching correlates with incorrect responses. We introduce a\nnovel metric to quantify underthinking by measuring token efficiency in\nincorrect answers. To address underthinking, we propose a decoding strategy\nwith thought switching penalty TIP that discourages premature transitions\nbetween thoughts, encouraging deeper exploration of each reasoning path.\nExperimental results demonstrate that our approach improves accuracy across\nchallenging datasets without requiring model fine-tuning. Our findings\ncontribute to understanding reasoning inefficiencies in o1-like LLMs and offer\na practical solution to enhance their problem-solving capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) such as OpenAI's o1 have demonstrated remarkable\nabilities in complex reasoning tasks by scaling test-time compute and\nexhibiting human-like deep thinking. However, we identify a phenomenon we term\nunderthinking, where o1-like LLMs frequently switch between different reasoning\nthoughts without sufficiently exploring promising paths to reach a correct\nsolution. This behavior leads to inadequate depth of reasoning and decreased\nperformance, particularly on challenging mathematical problems. To\nsystematically analyze this issue, we conduct experiments on three challenging\ntest sets and two representative open-source o1-like models, revealing that\nfrequent thought switching correlates with incorrect responses. We introduce a\nnovel metric to quantify underthinking by measuring token efficiency in\nincorrect answers. To address underthinking, we propose a decoding strategy\nwith thought switching penalty TIP that discourages premature transitions\nbetween thoughts, encouraging deeper exploration of each reasoning path.\nExperimental results demonstrate that our approach improves accuracy across\nchallenging datasets without requiring model fine-tuning. Our findings\ncontribute to understanding reasoning inefficiencies in o1-like LLMs and offer\na practical solution to enhance their problem-solving capabilities."
                },
                "authors": [
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Qiuzhi Liu"
                    },
                    {
                        "name": "Jiahao Xu"
                    },
                    {
                        "name": "Tian Liang"
                    },
                    {
                        "name": "Xingyu Chen"
                    },
                    {
                        "name": "Zhiwei He"
                    },
                    {
                        "name": "Linfeng Song"
                    },
                    {
                        "name": "Dian Yu"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Zhuosheng Zhang"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "arxiv_comment": "1. We have updated the results for DeepSeek-R1, and all of our\n  original conclusions remain valid. 2. Our proposed Tip approach remains\n  effective in Best-of-N scenarios (e.g., self-consistency and Laconic\n  Decoding) when built on DeepSeek-R1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18585v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18585v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13031v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13031v1",
                "updated": "2025-02-18T16:46:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    46,
                    47,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T16:46:47Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    46,
                    47,
                    1,
                    49,
                    0
                ],
                "title": "HPSS: Heuristic Prompting Strategy Search for LLM Evaluators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HPSS: Heuristic Prompting Strategy Search for LLM Evaluators"
                },
                "summary": "Since the adoption of large language models (LLMs) for text evaluation has\nbecome increasingly prevalent in the field of natural language processing\n(NLP), a series of existing works attempt to optimize the prompts for LLM\nevaluators to improve their alignment with human judgment. However, their\nefforts are limited to optimizing individual factors of evaluation prompts,\nsuch as evaluation criteria or output formats, neglecting the combinatorial\nimpact of multiple factors, which leads to insufficient optimization of the\nevaluation pipeline. Nevertheless, identifying well-behaved prompting\nstrategies for adjusting multiple factors requires extensive enumeration. To\nthis end, we comprehensively integrate 8 key factors for evaluation prompts and\npropose a novel automatic prompting strategy optimization method called\nHeuristic Prompting Strategy Search (HPSS). Inspired by the genetic algorithm,\nHPSS conducts an iterative search to find well-behaved prompting strategies for\nLLM evaluators. A heuristic function is employed to guide the search process,\nenhancing the performance of our algorithm. Extensive experiments across four\nevaluation tasks demonstrate the effectiveness of HPSS, consistently\noutperforming both human-designed evaluation prompts and existing automatic\nprompt optimization methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the adoption of large language models (LLMs) for text evaluation has\nbecome increasingly prevalent in the field of natural language processing\n(NLP), a series of existing works attempt to optimize the prompts for LLM\nevaluators to improve their alignment with human judgment. However, their\nefforts are limited to optimizing individual factors of evaluation prompts,\nsuch as evaluation criteria or output formats, neglecting the combinatorial\nimpact of multiple factors, which leads to insufficient optimization of the\nevaluation pipeline. Nevertheless, identifying well-behaved prompting\nstrategies for adjusting multiple factors requires extensive enumeration. To\nthis end, we comprehensively integrate 8 key factors for evaluation prompts and\npropose a novel automatic prompting strategy optimization method called\nHeuristic Prompting Strategy Search (HPSS). Inspired by the genetic algorithm,\nHPSS conducts an iterative search to find well-behaved prompting strategies for\nLLM evaluators. A heuristic function is employed to guide the search process,\nenhancing the performance of our algorithm. Extensive experiments across four\nevaluation tasks demonstrate the effectiveness of HPSS, consistently\noutperforming both human-designed evaluation prompts and existing automatic\nprompt optimization methods."
                },
                "authors": [
                    {
                        "name": "Bosi Wen"
                    },
                    {
                        "name": "Pei Ke"
                    },
                    {
                        "name": "Yufei Sun"
                    },
                    {
                        "name": "Cunxiang Wang"
                    },
                    {
                        "name": "Xiaotao Gu"
                    },
                    {
                        "name": "Jinfeng Zhou"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Hongning Wang"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang",
                "arxiv_comment": "32 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13031v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13031v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13029v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13029v1",
                "updated": "2025-02-18T16:46:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    46,
                    11,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T16:46:11Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    46,
                    11,
                    1,
                    49,
                    0
                ],
                "title": "Galactic magnetic fields II. Applying the model to nearby galaxies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Galactic magnetic fields II. Applying the model to nearby galaxies"
                },
                "summary": "Many spiral galaxies host magnetic fields with energy densities comparable to\nthose of the turbulent and thermal motions of their interstellar gas. However,\nquantitative comparison between magnetic field properties inferred from\nobservation and those obtained from theoretical modeling has been lacking. In\nPaper I we developed a simple, axisymmetric galactic dynamo model that uses\nvarious observational data as input. Here we apply our model to calculate\nradial profiles of azimuthally and vertically averaged magnetic field strength\nand pitch angle, gas velocity dispersion and scale height, turbulent\ncorrelation time and length, and the sizes of supernova remnants for the\ngalaxies M31, M33, M51, and NGC 6946, using input data collected from the\nliterature. Scaling factors are introduced to account for a lack of precision\nin both theory and observation. Despite the simplicity of our model, its\noutputs agree fairly well with galaxy properties inferred from observation.\nAdditionally, we find that most of the parameter values are similar between\ngalaxies. We extend the model to predict the magnetic field pitch angles\narising from a combination of mean-field dynamo action and the winding up of\nthe random small-scale field owing to the large-scale radial shear. We find\ntheir magnitudes to be much smaller than those of the pitch angles measured in\npolarized radio and far infrared emission. This suggests that effects not\nincluded in our model, such as effects associated with spiral arms, are needed\nto explain the pitch angle values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many spiral galaxies host magnetic fields with energy densities comparable to\nthose of the turbulent and thermal motions of their interstellar gas. However,\nquantitative comparison between magnetic field properties inferred from\nobservation and those obtained from theoretical modeling has been lacking. In\nPaper I we developed a simple, axisymmetric galactic dynamo model that uses\nvarious observational data as input. Here we apply our model to calculate\nradial profiles of azimuthally and vertically averaged magnetic field strength\nand pitch angle, gas velocity dispersion and scale height, turbulent\ncorrelation time and length, and the sizes of supernova remnants for the\ngalaxies M31, M33, M51, and NGC 6946, using input data collected from the\nliterature. Scaling factors are introduced to account for a lack of precision\nin both theory and observation. Despite the simplicity of our model, its\noutputs agree fairly well with galaxy properties inferred from observation.\nAdditionally, we find that most of the parameter values are similar between\ngalaxies. We extend the model to predict the magnetic field pitch angles\narising from a combination of mean-field dynamo action and the winding up of\nthe random small-scale field owing to the large-scale radial shear. We find\ntheir magnitudes to be much smaller than those of the pitch angles measured in\npolarized radio and far infrared emission. This suggests that effects not\nincluded in our model, such as effects associated with spiral arms, are needed\nto explain the pitch angle values."
                },
                "authors": [
                    {
                        "name": "Rion Glenn Nazareth"
                    },
                    {
                        "name": "Gayathri Santhosh"
                    },
                    {
                        "name": "Luke Chamandy"
                    }
                ],
                "author_detail": {
                    "name": "Luke Chamandy"
                },
                "author": "Luke Chamandy",
                "arxiv_comment": "27 pages, 13 figures, 5 tables, submitted to the Astrophysical\n  Journal (ApJ)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13029v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13029v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13028v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13028v1",
                "updated": "2025-02-18T16:45:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    45,
                    41,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T16:45:41Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    45,
                    41,
                    1,
                    49,
                    0
                ],
                "title": "Whose story is it? Personalizing story generation by inferring author\n  styles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Whose story is it? Personalizing story generation by inferring author\n  styles"
                },
                "summary": "Personalization has become essential for improving user experience in\ninteractive writing and educational applications, yet its potential in story\ngeneration remains largely unexplored. In this work, we propose a novel\ntwo-stage pipeline for personalized story generation. Our approach first infers\nan author's implicit story-writing characteristics from their past work and\norganizes them into an Author Writing Sheet, inspired by narrative theory. The\nsecond stage uses this sheet to simulate the author's persona through tailored\npersona descriptions and personalized story writing rules. To enable and\nvalidate our approach, we construct Mythos, a dataset of 590 stories from 64\nauthors across five distinct sources that reflect diverse story-writing\nsettings. A head-to-head comparison with a non-personalized baseline\ndemonstrates our pipeline's effectiveness in generating high-quality\npersonalized stories. Our personalized stories achieve a 75 percent win rate\n(versus 14 percent for the baseline and 11 percent ties) in capturing authors'\nwriting style based on their past works. Human evaluation highlights the high\nquality of our Author Writing Sheet and provides valuable insights into the\npersonalized story generation task. Notable takeaways are that writings from\ncertain sources, such as Reddit, are easier to personalize than others, like\nAO3, while narrative aspects, like Creativity and Language Use, are easier to\npersonalize than others, like Plot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalization has become essential for improving user experience in\ninteractive writing and educational applications, yet its potential in story\ngeneration remains largely unexplored. In this work, we propose a novel\ntwo-stage pipeline for personalized story generation. Our approach first infers\nan author's implicit story-writing characteristics from their past work and\norganizes them into an Author Writing Sheet, inspired by narrative theory. The\nsecond stage uses this sheet to simulate the author's persona through tailored\npersona descriptions and personalized story writing rules. To enable and\nvalidate our approach, we construct Mythos, a dataset of 590 stories from 64\nauthors across five distinct sources that reflect diverse story-writing\nsettings. A head-to-head comparison with a non-personalized baseline\ndemonstrates our pipeline's effectiveness in generating high-quality\npersonalized stories. Our personalized stories achieve a 75 percent win rate\n(versus 14 percent for the baseline and 11 percent ties) in capturing authors'\nwriting style based on their past works. Human evaluation highlights the high\nquality of our Author Writing Sheet and provides valuable insights into the\npersonalized story generation task. Notable takeaways are that writings from\ncertain sources, such as Reddit, are easier to personalize than others, like\nAO3, while narrative aspects, like Creativity and Language Use, are easier to\npersonalize than others, like Plot."
                },
                "authors": [
                    {
                        "name": "Nischal Ashok Kumar"
                    },
                    {
                        "name": "Chau Minh Pham"
                    },
                    {
                        "name": "Mohit Iyyer"
                    },
                    {
                        "name": "Andrew Lan"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Lan"
                },
                "author": "Andrew Lan",
                "arxiv_comment": "preprint 52 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13028v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13028v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13023v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13023v1",
                "updated": "2025-02-18T16:43:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    43,
                    11,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T16:43:11Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    43,
                    11,
                    1,
                    49,
                    0
                ],
                "title": "Detection and Geographic Localization of Natural Objects in the Wild: A\n  Case Study on Palms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detection and Geographic Localization of Natural Objects in the Wild: A\n  Case Study on Palms"
                },
                "summary": "Palms are ecologically and economically indicators of tropical forest health,\nbiodiversity, and human impact that support local economies and global forest\nproduct supply chains. While palm detection in plantations is well-studied,\nefforts to map naturally occurring palms in dense forests remain limited by\noverlapping crowns, uneven shading, and heterogeneous landscapes. We develop\nPRISM (Processing, Inference, Segmentation, and Mapping), a flexible pipeline\nfor detecting and localizing palms in dense tropical forests using large\northomosaic images. Orthomosaics are created from thousands of aerial images\nand spanning several to hundreds of gigabytes. Our contributions are threefold.\nFirst, we construct a large UAV-derived orthomosaic dataset collected across 21\necologically diverse sites in western Ecuador, annotated with 8,830 bounding\nboxes and 5,026 palm center points. Second, we evaluate multiple\nstate-of-the-art object detectors based on efficiency and performance,\nintegrating zero-shot SAM 2 as the segmentation backbone, and refining the\nresults for precise geographic mapping. Third, we apply calibration methods to\nalign confidence scores with IoU and explore saliency maps for feature\nexplainability. Though optimized for palms, PRISM is adaptable for identifying\nother natural objects, such as eastern white pines. Future work will explore\ntransfer learning for lower-resolution datasets (0.5 to 1m).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Palms are ecologically and economically indicators of tropical forest health,\nbiodiversity, and human impact that support local economies and global forest\nproduct supply chains. While palm detection in plantations is well-studied,\nefforts to map naturally occurring palms in dense forests remain limited by\noverlapping crowns, uneven shading, and heterogeneous landscapes. We develop\nPRISM (Processing, Inference, Segmentation, and Mapping), a flexible pipeline\nfor detecting and localizing palms in dense tropical forests using large\northomosaic images. Orthomosaics are created from thousands of aerial images\nand spanning several to hundreds of gigabytes. Our contributions are threefold.\nFirst, we construct a large UAV-derived orthomosaic dataset collected across 21\necologically diverse sites in western Ecuador, annotated with 8,830 bounding\nboxes and 5,026 palm center points. Second, we evaluate multiple\nstate-of-the-art object detectors based on efficiency and performance,\nintegrating zero-shot SAM 2 as the segmentation backbone, and refining the\nresults for precise geographic mapping. Third, we apply calibration methods to\nalign confidence scores with IoU and explore saliency maps for feature\nexplainability. Though optimized for palms, PRISM is adaptable for identifying\nother natural objects, such as eastern white pines. Future work will explore\ntransfer learning for lower-resolution datasets (0.5 to 1m)."
                },
                "authors": [
                    {
                        "name": "Kangning Cui"
                    },
                    {
                        "name": "Rongkun Zhu"
                    },
                    {
                        "name": "Manqi Wang"
                    },
                    {
                        "name": "Wei Tang"
                    },
                    {
                        "name": "Gregory D. Larsen"
                    },
                    {
                        "name": "Victor P. Pauca"
                    },
                    {
                        "name": "Sarra Alqahtani"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "David Segurado"
                    },
                    {
                        "name": "David Lutz"
                    },
                    {
                        "name": "Jean-Michel Morel"
                    },
                    {
                        "name": "Miles R. Silman"
                    }
                ],
                "author_detail": {
                    "name": "Miles R. Silman"
                },
                "author": "Miles R. Silman",
                "arxiv_comment": "15 pages, 8 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13023v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13023v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13019v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13019v1",
                "updated": "2025-02-18T16:38:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    38,
                    39,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T16:38:39Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    38,
                    39,
                    1,
                    49,
                    0
                ],
                "title": "Oreo: A Plug-in Context Reconstructor to Enhance Retrieval-Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oreo: A Plug-in Context Reconstructor to Enhance Retrieval-Augmented\n  Generation"
                },
                "summary": "Despite the remarkable capabilities of Large Language Models (LLMs) in\nvarious NLP tasks, they remain vulnerable to hallucinations due to their\nlimited parametric knowledge and lack of domain-specific expertise.\nRetrieval-Augmented Generation (RAG) addresses this challenge by incorporating\nexternal document retrieval to augment the knowledge base of LLMs. In this\napproach, RAG retrieves document chunks from an external corpus in response to\na query, which are then used as context for the downstream language model to\ngenerate an answer. However, these retrieved knowledge sources often include\nirrelevant or erroneous information, undermining the effectiveness of RAG in\ndownstream tasks. To overcome this limitation, we introduce a compact,\nefficient, and pluggable module designed to refine external knowledge sources\nbefore feeding them to the generator. The module reconstructs retrieved content\nby extracting the most relevant and supportive information and reorganising it\ninto a concise, query-specific format. Through a three-stage training paradigm\n- comprising supervised fine-tuning, contrastive multi-task learning, and\nreinforcement learning-based alignment - it prioritises critical knowledge and\naligns it with the generator's preferences. This method enables LLMs to produce\noutputs that are more accurate, reliable, and contextually appropriate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable capabilities of Large Language Models (LLMs) in\nvarious NLP tasks, they remain vulnerable to hallucinations due to their\nlimited parametric knowledge and lack of domain-specific expertise.\nRetrieval-Augmented Generation (RAG) addresses this challenge by incorporating\nexternal document retrieval to augment the knowledge base of LLMs. In this\napproach, RAG retrieves document chunks from an external corpus in response to\na query, which are then used as context for the downstream language model to\ngenerate an answer. However, these retrieved knowledge sources often include\nirrelevant or erroneous information, undermining the effectiveness of RAG in\ndownstream tasks. To overcome this limitation, we introduce a compact,\nefficient, and pluggable module designed to refine external knowledge sources\nbefore feeding them to the generator. The module reconstructs retrieved content\nby extracting the most relevant and supportive information and reorganising it\ninto a concise, query-specific format. Through a three-stage training paradigm\n- comprising supervised fine-tuning, contrastive multi-task learning, and\nreinforcement learning-based alignment - it prioritises critical knowledge and\naligns it with the generator's preferences. This method enables LLMs to produce\noutputs that are more accurate, reliable, and contextually appropriate."
                },
                "authors": [
                    {
                        "name": "Sha Li"
                    },
                    {
                        "name": "Naren Ramarkrishnan"
                    }
                ],
                "author_detail": {
                    "name": "Naren Ramarkrishnan"
                },
                "author": "Naren Ramarkrishnan",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13019v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13019v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16813v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16813v2",
                "updated": "2025-02-18T16:36:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    36,
                    25,
                    1,
                    49,
                    0
                ],
                "published": "2024-09-25T11:09:39Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    11,
                    9,
                    39,
                    2,
                    269,
                    0
                ],
                "title": "PeerArg: Argumentative Peer Review with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PeerArg: Argumentative Peer Review with LLMs"
                },
                "summary": "Peer review is an essential process to determine the quality of papers\nsubmitted to scientific conferences or journals. However, it is subjective and\nprone to biases. Several studies have been conducted to apply techniques from\nNLP to support peer review, but they are based on black-box techniques and\ntheir outputs are difficult to interpret and trust. In this paper, we propose a\nnovel pipeline to support and understand the reviewing and decision-making\nprocesses of peer review: the PeerArg system combining LLMs with methods from\nknowledge representation. PeerArg takes in input a set of reviews for a paper\nand outputs the paper acceptance prediction. We evaluate the performance of the\nPeerArg pipeline on three different datasets, in comparison with a novel\nend-2-end LLM that uses few-shot learning to predict paper acceptance given\nreviews. The results indicate that the end-2-end LLM is capable of predicting\npaper acceptance from reviews, but a variant of the PeerArg pipeline\noutperforms this LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Peer review is an essential process to determine the quality of papers\nsubmitted to scientific conferences or journals. However, it is subjective and\nprone to biases. Several studies have been conducted to apply techniques from\nNLP to support peer review, but they are based on black-box techniques and\ntheir outputs are difficult to interpret and trust. In this paper, we propose a\nnovel pipeline to support and understand the reviewing and decision-making\nprocesses of peer review: the PeerArg system combining LLMs with methods from\nknowledge representation. PeerArg takes in input a set of reviews for a paper\nand outputs the paper acceptance prediction. We evaluate the performance of the\nPeerArg pipeline on three different datasets, in comparison with a novel\nend-2-end LLM that uses few-shot learning to predict paper acceptance given\nreviews. The results indicate that the end-2-end LLM is capable of predicting\npaper acceptance from reviews, but a variant of the PeerArg pipeline\noutperforms this LLM."
                },
                "authors": [
                    {
                        "name": "Purin Sukpanichnant"
                    },
                    {
                        "name": "Anna Rapberger"
                    },
                    {
                        "name": "Francesca Toni"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Toni"
                },
                "author": "Francesca Toni",
                "arxiv_comment": "Presented at NeLaMKRR@KR, 2024 (arXiv:2410.05339)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16813v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16813v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13016v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13016v1",
                "updated": "2025-02-18T16:34:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    34,
                    45,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T16:34:45Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    34,
                    45,
                    1,
                    49,
                    0
                ],
                "title": "LLM-Powered Proactive Data Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Powered Proactive Data Systems"
                },
                "summary": "With the power of LLMs, we now have the ability to query data that was\npreviously impossible to query, including text, images, and video. However,\ndespite this enormous potential, most present-day data systems that leverage\nLLMs are reactive, reflecting our community's desire to map LLMs to known\nabstractions. Most data systems treat LLMs as an opaque black box that operates\non user inputs and data as is, optimizing them much like any other approximate,\nexpensive UDFs, in conjunction with other relational operators. Such data\nsystems do as they are told, but fail to understand and leverage what the LLM\nis being asked to do (i.e. the underlying operations, which may be\nerror-prone), the data the LLM is operating on (e.g., long, complex documents),\nor what the user really needs. They don't take advantage of the characteristics\nof the operations and/or the data at hand, or ensure correctness of results\nwhen there are imprecisions and ambiguities. We argue that data systems instead\nneed to be proactive: they need to be given more agency -- armed with the power\nof LLMs -- to understand and rework the user inputs and the data and to make\ndecisions on how the operations and the data should be represented and\nprocessed. By allowing the data system to parse, rewrite, and decompose user\ninputs and data, or to interact with the user in ways that go beyond the\nstandard single-shot query-result paradigm, the data system is able to address\nuser needs more efficiently and effectively. These new capabilities lead to a\nrich design space where the data system takes more initiative: they are\nempowered to perform optimization based on the transformation operations, data\ncharacteristics, and user intent. We discuss various successful examples of how\nthis framework has been and can be applied in real-world tasks, and present\nfuture directions for this ambitious research agenda.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the power of LLMs, we now have the ability to query data that was\npreviously impossible to query, including text, images, and video. However,\ndespite this enormous potential, most present-day data systems that leverage\nLLMs are reactive, reflecting our community's desire to map LLMs to known\nabstractions. Most data systems treat LLMs as an opaque black box that operates\non user inputs and data as is, optimizing them much like any other approximate,\nexpensive UDFs, in conjunction with other relational operators. Such data\nsystems do as they are told, but fail to understand and leverage what the LLM\nis being asked to do (i.e. the underlying operations, which may be\nerror-prone), the data the LLM is operating on (e.g., long, complex documents),\nor what the user really needs. They don't take advantage of the characteristics\nof the operations and/or the data at hand, or ensure correctness of results\nwhen there are imprecisions and ambiguities. We argue that data systems instead\nneed to be proactive: they need to be given more agency -- armed with the power\nof LLMs -- to understand and rework the user inputs and the data and to make\ndecisions on how the operations and the data should be represented and\nprocessed. By allowing the data system to parse, rewrite, and decompose user\ninputs and data, or to interact with the user in ways that go beyond the\nstandard single-shot query-result paradigm, the data system is able to address\nuser needs more efficiently and effectively. These new capabilities lead to a\nrich design space where the data system takes more initiative: they are\nempowered to perform optimization based on the transformation operations, data\ncharacteristics, and user intent. We discuss various successful examples of how\nthis framework has been and can be applied in real-world tasks, and present\nfuture directions for this ambitious research agenda."
                },
                "authors": [
                    {
                        "name": "Sepanta Zeighami"
                    },
                    {
                        "name": "Yiming Lin"
                    },
                    {
                        "name": "Shreya Shankar"
                    },
                    {
                        "name": "Aditya Parameswaran"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Parameswaran"
                },
                "author": "Aditya Parameswaran",
                "arxiv_journal_ref": "IEEE Data Engineering Bulletin March 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13016v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13016v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13012v1",
                "updated": "2025-02-18T16:33:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    33,
                    33,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T16:33:33Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    33,
                    33,
                    1,
                    49,
                    0
                ],
                "title": "Towards a Design Guideline for RPA Evaluation: A Survey of Large\n  Language Model-Based Role-Playing Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a Design Guideline for RPA Evaluation: A Survey of Large\n  Language Model-Based Role-Playing Agents"
                },
                "summary": "Role-Playing Agent (RPA) is an increasingly popular type of LLM Agent that\nsimulates human-like behaviors in a variety of tasks. However, evaluating RPAs\nis challenging due to diverse task requirements and agent designs. This paper\nproposes an evidence-based, actionable, and generalizable evaluation design\nguideline for LLM-based RPA by systematically reviewing 1,676 papers published\nbetween Jan. 2021 and Dec. 2024. Our analysis identifies six agent attributes,\nseven task attributes, and seven evaluation metrics from existing literature.\nBased on these findings, we present an RPA evaluation design guideline to help\nresearchers develop more systematic and consistent evaluation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role-Playing Agent (RPA) is an increasingly popular type of LLM Agent that\nsimulates human-like behaviors in a variety of tasks. However, evaluating RPAs\nis challenging due to diverse task requirements and agent designs. This paper\nproposes an evidence-based, actionable, and generalizable evaluation design\nguideline for LLM-based RPA by systematically reviewing 1,676 papers published\nbetween Jan. 2021 and Dec. 2024. Our analysis identifies six agent attributes,\nseven task attributes, and seven evaluation metrics from existing literature.\nBased on these findings, we present an RPA evaluation design guideline to help\nresearchers develop more systematic and consistent evaluation methods."
                },
                "authors": [
                    {
                        "name": "Chaoran Chen"
                    },
                    {
                        "name": "Bingsheng Yao"
                    },
                    {
                        "name": "Ruishi Zou"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Weimin Lyu"
                    },
                    {
                        "name": "Toby Jia-Jun Li"
                    },
                    {
                        "name": "Dakuo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Dakuo Wang"
                },
                "author": "Dakuo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13010v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13010v1",
                "updated": "2025-02-18T16:29:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    29,
                    45,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T16:29:45Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    29,
                    45,
                    1,
                    49,
                    0
                ],
                "title": "Adaptive Knowledge Graphs Enhance Medical Question Answering: Bridging\n  the Gap Between LLMs and Evolving Medical Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Knowledge Graphs Enhance Medical Question Answering: Bridging\n  the Gap Between LLMs and Evolving Medical Knowledge"
                },
                "summary": "Large Language Models (LLMs) have significantly advanced medical\nquestion-answering by leveraging extensive clinical data and medical\nliterature. However, the rapid evolution of medical knowledge and the\nlabor-intensive process of manually updating domain-specific resources pose\nchallenges to the reliability of these systems. To address this, we introduce\nAdaptive Medical Graph-RAG (AMG-RAG), a comprehensive framework that automates\nthe construction and continuous updating of medical knowledge graphs,\nintegrates reasoning, and retrieves current external evidence, such as PubMed\nand WikiSearch. By dynamically linking new findings and complex medical\nconcepts, AMG-RAG not only improves accuracy but also enhances interpretability\nin medical queries.\n  Evaluations on the MEDQA and MEDMCQA benchmarks demonstrate the effectiveness\nof AMG-RAG, achieving an F1 score of 74.1 percent on MEDQA and an accuracy of\n66.34 percent on MEDMCQA, outperforming both comparable models and those 10 to\n100 times larger. Notably, these improvements are achieved without increasing\ncomputational overhead, highlighting the critical role of automated knowledge\ngraph generation and external evidence retrieval in delivering up-to-date,\ntrustworthy medical insights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have significantly advanced medical\nquestion-answering by leveraging extensive clinical data and medical\nliterature. However, the rapid evolution of medical knowledge and the\nlabor-intensive process of manually updating domain-specific resources pose\nchallenges to the reliability of these systems. To address this, we introduce\nAdaptive Medical Graph-RAG (AMG-RAG), a comprehensive framework that automates\nthe construction and continuous updating of medical knowledge graphs,\nintegrates reasoning, and retrieves current external evidence, such as PubMed\nand WikiSearch. By dynamically linking new findings and complex medical\nconcepts, AMG-RAG not only improves accuracy but also enhances interpretability\nin medical queries.\n  Evaluations on the MEDQA and MEDMCQA benchmarks demonstrate the effectiveness\nof AMG-RAG, achieving an F1 score of 74.1 percent on MEDQA and an accuracy of\n66.34 percent on MEDMCQA, outperforming both comparable models and those 10 to\n100 times larger. Notably, these improvements are achieved without increasing\ncomputational overhead, highlighting the critical role of automated knowledge\ngraph generation and external evidence retrieval in delivering up-to-date,\ntrustworthy medical insights."
                },
                "authors": [
                    {
                        "name": "Mohammad Reza Rezaei"
                    },
                    {
                        "name": "Reza Saadati Fard"
                    },
                    {
                        "name": "Jayson Parker"
                    },
                    {
                        "name": "Rahul G. Krishnan"
                    },
                    {
                        "name": "Milad Lankarany"
                    }
                ],
                "author_detail": {
                    "name": "Milad Lankarany"
                },
                "author": "Milad Lankarany",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13010v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13010v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13931v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13931v3",
                "updated": "2025-02-18T16:27:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    27,
                    26,
                    1,
                    49,
                    0
                ],
                "published": "2024-09-20T22:34:37Z",
                "published_parsed": [
                    2024,
                    9,
                    20,
                    22,
                    34,
                    37,
                    4,
                    264,
                    0
                ],
                "title": "On-Device Collaborative Language Modeling via a Mixture of Generalists\n  and Specialists",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-Device Collaborative Language Modeling via a Mixture of Generalists\n  and Specialists"
                },
                "summary": "On-device LLMs have gained increasing attention for their ability to enhance\nprivacy and provide a personalized user experience. To facilitate private\nlearning with scarce data, Federated Learning has become a standard approach.\nHowever, it faces challenges such as computational resource heterogeneity and\ndata heterogeneity among end users. We propose CoMiGS ($\\textbf{Co}$llaborative\nlearning with a $\\textbf{Mi}$xture of $\\textbf{G}$eneralists and\n$\\textbf{S}$pecialists), the first approach to address both challenges. A key\ninnovation of our method is the bi-level optimization formulation of the\nMixture-of-Experts learning objective, where the router is optimized using a\nseparate validation set to ensure alignment with the target distribution. We\nsolve our objective with alternating minimization, for which we provide a\ntheoretical analysis. Our method shares generalist experts across users while\nlocalizing a varying number of specialist experts, thereby adapting to users'\ncomputational resources and preserving privacy. Through extensive experiments,\nwe show CoMiGS effectively balances general and personalized knowledge for each\ntoken generation. We demonstrate that CoMiGS remains robust against\noverfitting-due to the generalists' regularizing effect-while adapting to local\ndata through specialist expertise. We open source our codebase for\ncollaborative LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-device LLMs have gained increasing attention for their ability to enhance\nprivacy and provide a personalized user experience. To facilitate private\nlearning with scarce data, Federated Learning has become a standard approach.\nHowever, it faces challenges such as computational resource heterogeneity and\ndata heterogeneity among end users. We propose CoMiGS ($\\textbf{Co}$llaborative\nlearning with a $\\textbf{Mi}$xture of $\\textbf{G}$eneralists and\n$\\textbf{S}$pecialists), the first approach to address both challenges. A key\ninnovation of our method is the bi-level optimization formulation of the\nMixture-of-Experts learning objective, where the router is optimized using a\nseparate validation set to ensure alignment with the target distribution. We\nsolve our objective with alternating minimization, for which we provide a\ntheoretical analysis. Our method shares generalist experts across users while\nlocalizing a varying number of specialist experts, thereby adapting to users'\ncomputational resources and preserving privacy. Through extensive experiments,\nwe show CoMiGS effectively balances general and personalized knowledge for each\ntoken generation. We demonstrate that CoMiGS remains robust against\noverfitting-due to the generalists' regularizing effect-while adapting to local\ndata through specialist expertise. We open source our codebase for\ncollaborative LLMs."
                },
                "authors": [
                    {
                        "name": "Dongyang Fan"
                    },
                    {
                        "name": "Bettina Messmer"
                    },
                    {
                        "name": "Nikita Doikov"
                    },
                    {
                        "name": "Martin Jaggi"
                    }
                ],
                "author_detail": {
                    "name": "Martin Jaggi"
                },
                "author": "Martin Jaggi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13931v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13931v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09962v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09962v2",
                "updated": "2025-02-18T16:24:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    24,
                    59,
                    1,
                    49,
                    0
                ],
                "published": "2024-04-15T17:39:44Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    17,
                    39,
                    44,
                    0,
                    106,
                    0
                ],
                "title": "Invariant Subspace Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Invariant Subspace Decomposition"
                },
                "summary": "We consider the task of predicting a response Y from a set of covariates X in\nsettings where the conditional distribution of Y given X changes over time. For\nthis to be feasible, assumptions on how the conditional distribution changes\nover time are required. Existing approaches assume, for example, that changes\noccur smoothly over time so that short-term prediction using only the recent\npast becomes feasible. To additionally exploit observations further in the\npast, we propose a novel invariance-based framework for linear conditionals,\ncalled Invariant Subspace Decomposition (ISD), that splits the conditional\ndistribution into a time-invariant and a residual time-dependent component. As\nwe show, this decomposition can be utilized both for zero-shot and\ntime-adaptation prediction tasks, that is, settings where either no or a small\namount of training data is available at the time points we want to predict Y\nat, respectively. We propose a practical estimation procedure, which\nautomatically infers the decomposition using tools from approximate joint\nmatrix diagonalization. Furthermore, we provide finite sample guarantees for\nthe proposed estimator and demonstrate empirically that it indeed improves on\napproaches that do not use the additional invariant structure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the task of predicting a response Y from a set of covariates X in\nsettings where the conditional distribution of Y given X changes over time. For\nthis to be feasible, assumptions on how the conditional distribution changes\nover time are required. Existing approaches assume, for example, that changes\noccur smoothly over time so that short-term prediction using only the recent\npast becomes feasible. To additionally exploit observations further in the\npast, we propose a novel invariance-based framework for linear conditionals,\ncalled Invariant Subspace Decomposition (ISD), that splits the conditional\ndistribution into a time-invariant and a residual time-dependent component. As\nwe show, this decomposition can be utilized both for zero-shot and\ntime-adaptation prediction tasks, that is, settings where either no or a small\namount of training data is available at the time points we want to predict Y\nat, respectively. We propose a practical estimation procedure, which\nautomatically infers the decomposition using tools from approximate joint\nmatrix diagonalization. Furthermore, we provide finite sample guarantees for\nthe proposed estimator and demonstrate empirically that it indeed improves on\napproaches that do not use the additional invariant structure."
                },
                "authors": [
                    {
                        "name": "Margherita Lazzaretto"
                    },
                    {
                        "name": "Jonas Peters"
                    },
                    {
                        "name": "Niklas Pfister"
                    }
                ],
                "author_detail": {
                    "name": "Niklas Pfister"
                },
                "author": "Niklas Pfister",
                "arxiv_comment": "60 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09962v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09962v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13001v1",
                "updated": "2025-02-18T16:21:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    21,
                    22,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T16:21:22Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    21,
                    22,
                    1,
                    49,
                    0
                ],
                "title": "You need to MIMIC to get FAME: Solving Meeting Transcript Scarcity with\n  a Multi-Agent Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "You need to MIMIC to get FAME: Solving Meeting Transcript Scarcity with\n  a Multi-Agent Conversations"
                },
                "summary": "Meeting summarization suffers from limited high-quality data, mainly due to\nprivacy restrictions and expensive collection processes. We address this gap\nwith FAME, a dataset of 500 meetings in English and 300 in German produced by\nMIMIC, our new multi-agent meeting synthesis framework that generates meeting\ntranscripts on a given knowledge source by defining psychologically grounded\nparticipant profiles, outlining the conversation, and orchestrating a large\nlanguage model (LLM) debate. A modular post-processing step refines these\noutputs, mitigating potential repetitiveness and overly formal tones, ensuring\ncoherent, credible dialogues at scale. We also propose a psychologically\ngrounded evaluation framework assessing naturalness, social behavior\nauthenticity, and transcript difficulties. Human assessments show that FAME\napproximates real-meeting spontaneity (4.5/5 in naturalness), preserves\nspeaker-centric challenges (3/5 in spoken language), and introduces richer\ninformation-oriented difficulty (4/5 in difficulty). These findings highlight\nthat FAME is a good and scalable proxy for real-world meeting conditions. It\nenables new test scenarios for meeting summarization research and other\nconversation-centric applications in tasks requiring conversation data or\nsimulating social scenarios under behavioral constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meeting summarization suffers from limited high-quality data, mainly due to\nprivacy restrictions and expensive collection processes. We address this gap\nwith FAME, a dataset of 500 meetings in English and 300 in German produced by\nMIMIC, our new multi-agent meeting synthesis framework that generates meeting\ntranscripts on a given knowledge source by defining psychologically grounded\nparticipant profiles, outlining the conversation, and orchestrating a large\nlanguage model (LLM) debate. A modular post-processing step refines these\noutputs, mitigating potential repetitiveness and overly formal tones, ensuring\ncoherent, credible dialogues at scale. We also propose a psychologically\ngrounded evaluation framework assessing naturalness, social behavior\nauthenticity, and transcript difficulties. Human assessments show that FAME\napproximates real-meeting spontaneity (4.5/5 in naturalness), preserves\nspeaker-centric challenges (3/5 in spoken language), and introduces richer\ninformation-oriented difficulty (4/5 in difficulty). These findings highlight\nthat FAME is a good and scalable proxy for real-world meeting conditions. It\nenables new test scenarios for meeting summarization research and other\nconversation-centric applications in tasks requiring conversation data or\nsimulating social scenarios under behavioral constraints."
                },
                "authors": [
                    {
                        "name": "Frederic Kirstein"
                    },
                    {
                        "name": "Muneeb Khan"
                    },
                    {
                        "name": "Jan Philip Wahle"
                    },
                    {
                        "name": "Terry Ruas"
                    },
                    {
                        "name": "Bela Gipp"
                    }
                ],
                "author_detail": {
                    "name": "Bela Gipp"
                },
                "author": "Bela Gipp",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12998v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12998v1",
                "updated": "2025-02-18T16:19:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    19,
                    8,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T16:19:08Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    19,
                    8,
                    1,
                    49,
                    0
                ],
                "title": "Personalized Top-k Set Queries Over Predicted Scores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Top-k Set Queries Over Predicted Scores"
                },
                "summary": "This work studies the applicability of expensive external oracles such as\nlarge language models in answering top-k queries over predicted scores. Such\nscores are incurred by user-defined functions to answer personalized queries\nover multi-modal data. We propose a generic computational framework that\nhandles arbitrary set-based scoring functions, as long as the functions could\nbe decomposed into constructs, each of which sent to an oracle (in our case an\nLLM) to predict partial scores. At a given point in time, the framework assumes\na set of responses and their partial predicted scores, and it maintains a\ncollection of possible sets that are likely to be the true top-k. Since calling\noracles is costly, our framework judiciously identifies the next construct,\ni.e., the next best question to ask the oracle so as to maximize the likelihood\nof identifying the true top-k. We present a principled probabilistic model that\nquantifies that likelihood. We study efficiency opportunities in designing\nalgorithms. We run an evaluation with three large scale datasets, scoring\nfunctions, and baselines. Experiments indicate the efficacy of our framework,\nas it achieves an order of magnitude improvement over baselines in requiring\nLLM calls while ensuring result accuracy. Scalability experiments further\nindicate that our framework could be used in large-scale applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work studies the applicability of expensive external oracles such as\nlarge language models in answering top-k queries over predicted scores. Such\nscores are incurred by user-defined functions to answer personalized queries\nover multi-modal data. We propose a generic computational framework that\nhandles arbitrary set-based scoring functions, as long as the functions could\nbe decomposed into constructs, each of which sent to an oracle (in our case an\nLLM) to predict partial scores. At a given point in time, the framework assumes\na set of responses and their partial predicted scores, and it maintains a\ncollection of possible sets that are likely to be the true top-k. Since calling\noracles is costly, our framework judiciously identifies the next construct,\ni.e., the next best question to ask the oracle so as to maximize the likelihood\nof identifying the true top-k. We present a principled probabilistic model that\nquantifies that likelihood. We study efficiency opportunities in designing\nalgorithms. We run an evaluation with three large scale datasets, scoring\nfunctions, and baselines. Experiments indicate the efficacy of our framework,\nas it achieves an order of magnitude improvement over baselines in requiring\nLLM calls while ensuring result accuracy. Scalability experiments further\nindicate that our framework could be used in large-scale applications."
                },
                "authors": [
                    {
                        "name": "Sohrab Namazi Nia"
                    },
                    {
                        "name": "Subhodeep Ghosh"
                    },
                    {
                        "name": "Senjuti Basu Roy"
                    },
                    {
                        "name": "Sihem Amer-Yahia"
                    }
                ],
                "author_detail": {
                    "name": "Sihem Amer-Yahia"
                },
                "author": "Sihem Amer-Yahia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12998v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12998v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07832v9",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07832v9",
                "updated": "2025-02-18T16:14:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    14,
                    2,
                    1,
                    49,
                    0
                ],
                "published": "2024-07-31T14:49:35Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    49,
                    35,
                    2,
                    213,
                    0
                ],
                "title": "LADDER: Language Driven Slice Discovery and Error Rectification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LADDER: Language Driven Slice Discovery and Error Rectification"
                },
                "summary": "Error slice discovery is crucial to diagnose and mitigate model errors.\nCurrent clustering or discrete attribute-based slice discovery methods face key\nlimitations: 1) clustering results in incoherent slices, while assigning\ndiscrete attributes to slices leads to incomplete coverage of error patterns\ndue to missing or insufficient attributes; 2) these methods lack complex\nreasoning, preventing them from fully explaining model biases; 3) they fail to\nintegrate \\textit{domain knowledge}, limiting their usage in specialized fields\n\\eg radiology. We propose\\ladder (\\underline{La}nguage-\\underline{D}riven\n\\underline{D}iscovery and \\underline{E}rror \\underline{R}ectification), to\naddress the limitations by: (1) leveraging the flexibility of natural language\nto address incompleteness, (2) employing LLM's latent \\textit{domain knowledge}\nand advanced reasoning to analyze sentences and derive testable hypotheses\ndirectly, identifying biased attributes, and form coherent error slices without\nclustering. Existing mitigation methods typically address only the\nworst-performing group, often amplifying errors in other subgroups. In\ncontrast,\\ladder generates pseudo attributes from the discovered hypotheses to\nmitigate errors across all biases without explicit attribute annotations or\nprior knowledge of bias. Rigorous evaluations on 6 datasets spanning natural\nand medical images -- comparing 200+ classifiers with diverse architectures,\npretraining strategies, and LLMs -- show that\\ladder consistently outperforms\nexisting baselines in discovering and mitigating biases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Error slice discovery is crucial to diagnose and mitigate model errors.\nCurrent clustering or discrete attribute-based slice discovery methods face key\nlimitations: 1) clustering results in incoherent slices, while assigning\ndiscrete attributes to slices leads to incomplete coverage of error patterns\ndue to missing or insufficient attributes; 2) these methods lack complex\nreasoning, preventing them from fully explaining model biases; 3) they fail to\nintegrate \\textit{domain knowledge}, limiting their usage in specialized fields\n\\eg radiology. We propose\\ladder (\\underline{La}nguage-\\underline{D}riven\n\\underline{D}iscovery and \\underline{E}rror \\underline{R}ectification), to\naddress the limitations by: (1) leveraging the flexibility of natural language\nto address incompleteness, (2) employing LLM's latent \\textit{domain knowledge}\nand advanced reasoning to analyze sentences and derive testable hypotheses\ndirectly, identifying biased attributes, and form coherent error slices without\nclustering. Existing mitigation methods typically address only the\nworst-performing group, often amplifying errors in other subgroups. In\ncontrast,\\ladder generates pseudo attributes from the discovered hypotheses to\nmitigate errors across all biases without explicit attribute annotations or\nprior knowledge of bias. Rigorous evaluations on 6 datasets spanning natural\nand medical images -- comparing 200+ classifiers with diverse architectures,\npretraining strategies, and LLMs -- show that\\ladder consistently outperforms\nexisting baselines in discovering and mitigating biases."
                },
                "authors": [
                    {
                        "name": "Shantanu Ghosh"
                    },
                    {
                        "name": "Rayan Syed"
                    },
                    {
                        "name": "Chenyu Wang"
                    },
                    {
                        "name": "Clare B. Poynton"
                    },
                    {
                        "name": "Shyam Visweswaran"
                    },
                    {
                        "name": "Kayhan Batmanghelich"
                    }
                ],
                "author_detail": {
                    "name": "Kayhan Batmanghelich"
                },
                "author": "Kayhan Batmanghelich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07832v9",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07832v9",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.19100v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.19100v2",
                "updated": "2025-02-18T16:12:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    12,
                    14,
                    1,
                    49,
                    0
                ],
                "published": "2024-02-29T12:29:34Z",
                "published_parsed": [
                    2024,
                    2,
                    29,
                    12,
                    29,
                    34,
                    3,
                    60,
                    0
                ],
                "title": "Bayesian distances for quantifying tensions in cosmological inference\n  and the surprise statistic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian distances for quantifying tensions in cosmological inference\n  and the surprise statistic"
                },
                "summary": "Tensions between cosmological parameters derived through different channels\ncan be a genuine signature of new physics that $\\Lambda$CDM as the standard\nmodel is not able to reproduce, in particular in the missing consistency\nbetween parameter estimates from measurements the early and late Universe. Or,\nthey could be caused by yet to be understood systematics in the measurements as\na more mundane explanation. Commonly, cosmological tensions are stated in terms\nof mismatches of the posterior parameter distributions, often assuming Gaussian\nstatistics. More importantly, though, would be a quantification if two data\nsets are consistent to each other before combining them into a joint\nmeasurement, ideally isolating hints at individual data points that have a\nstrong influence in generating the tension. For this purpose, we start with\nstatistical divergences applied to posterior distributions following from\ndifferent data sets and develop the theory of a Fisher metric between two data\nsets, in analogy to the Fisher metric for different parameter choices. As a\ntopical example, we consider the tension in the Hubble-Lema\\^itre constant\n$H_0$ from supernova and measurements of the cosmic microwave background,\nderive a ranking of data points in order of their influence on the tension on\n$H_0$. For this particular example, we compute Bayesian distance measures and\nshow that in the light of CMB data, supernovae are commonly too bright, whereas\nthe low-$\\ell$ CMB spectrum is too high, in agreement with intuition about the\nparameter sensitivity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensions between cosmological parameters derived through different channels\ncan be a genuine signature of new physics that $\\Lambda$CDM as the standard\nmodel is not able to reproduce, in particular in the missing consistency\nbetween parameter estimates from measurements the early and late Universe. Or,\nthey could be caused by yet to be understood systematics in the measurements as\na more mundane explanation. Commonly, cosmological tensions are stated in terms\nof mismatches of the posterior parameter distributions, often assuming Gaussian\nstatistics. More importantly, though, would be a quantification if two data\nsets are consistent to each other before combining them into a joint\nmeasurement, ideally isolating hints at individual data points that have a\nstrong influence in generating the tension. For this purpose, we start with\nstatistical divergences applied to posterior distributions following from\ndifferent data sets and develop the theory of a Fisher metric between two data\nsets, in analogy to the Fisher metric for different parameter choices. As a\ntopical example, we consider the tension in the Hubble-Lema\\^itre constant\n$H_0$ from supernova and measurements of the cosmic microwave background,\nderive a ranking of data points in order of their influence on the tension on\n$H_0$. For this particular example, we compute Bayesian distance measures and\nshow that in the light of CMB data, supernovae are commonly too bright, whereas\nthe low-$\\ell$ CMB spectrum is too high, in agreement with intuition about the\nparameter sensitivity."
                },
                "authors": [
                    {
                        "name": "Benedikt Schosser"
                    },
                    {
                        "name": "Pedro Riba Mello"
                    },
                    {
                        "name": "Miguel Quartin"
                    },
                    {
                        "name": "Bjoern Malte Schaefer"
                    }
                ],
                "author_detail": {
                    "name": "Bjoern Malte Schaefer"
                },
                "author": "Bjoern Malte Schaefer",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.19100v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.19100v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12989v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12989v1",
                "updated": "2025-02-18T16:12:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    12,
                    4,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T16:12:04Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    12,
                    4,
                    1,
                    49,
                    0
                ],
                "title": "Identifying rapid changes in the hemodynamic response in event-related\n  functional magnetic resonance imaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying rapid changes in the hemodynamic response in event-related\n  functional magnetic resonance imaging"
                },
                "summary": "The hemodynamic response (HR) in event-related functional magnetic resonance\nimaging is typically assumed to be stationary. While there are some approaches\nin the literature to model nonstationary HRs, few focus on rapid changes. In\nthis work, we propose two procedures to investigate rapid changes in the HR.\nBoth procedures make inference on the existence of rapid changes for\nmulti-subject data. We allow the change point locations to vary between\nsubjects, conditions and brain regions. The first procedure utilizes available\ninformation about the change point locations to compare multiple shape\nparameters of the HR over time. In the second procedure, the change point\nlocations are determined for each subject separately. To account for the\nestimation of the change point locations, we propose the notion of post\nselection variance. The power of the proposed procedures is assessed in\nsimulation studies. We apply the procedure for pre-specified change point\nlocations to data from a category learning experiment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The hemodynamic response (HR) in event-related functional magnetic resonance\nimaging is typically assumed to be stationary. While there are some approaches\nin the literature to model nonstationary HRs, few focus on rapid changes. In\nthis work, we propose two procedures to investigate rapid changes in the HR.\nBoth procedures make inference on the existence of rapid changes for\nmulti-subject data. We allow the change point locations to vary between\nsubjects, conditions and brain regions. The first procedure utilizes available\ninformation about the change point locations to compare multiple shape\nparameters of the HR over time. In the second procedure, the change point\nlocations are determined for each subject separately. To account for the\nestimation of the change point locations, we propose the notion of post\nselection variance. The power of the proposed procedures is assessed in\nsimulation studies. We apply the procedure for pre-specified change point\nlocations to data from a category learning experiment."
                },
                "authors": [
                    {
                        "name": "Friederike Preusse"
                    },
                    {
                        "name": "Thorsten Dickhaus"
                    },
                    {
                        "name": "Andr Brechmann"
                    }
                ],
                "author_detail": {
                    "name": "Andr Brechmann"
                },
                "author": "Andr Brechmann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12989v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12989v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12988v1",
                "updated": "2025-02-18T16:11:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    11,
                    54,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T16:11:54Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    11,
                    54,
                    1,
                    49,
                    0
                ],
                "title": "Beyond Profile: From Surface-Level Facts to Deep Persona Simulation in\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Profile: From Surface-Level Facts to Deep Persona Simulation in\n  LLMs"
                },
                "summary": "Previous approaches to persona simulation large language models (LLMs) have\ntypically relied on learning basic biographical information, or using limited\nrole-play dialogue datasets to capture a character's responses. However, a\nholistic representation of an individual goes beyond surface-level facts or\nconversations to deeper thoughts and thinking. In this work, we introduce\nCharacterBot, a model designed to replicate both the linguistic patterns and\ndistinctive thought processes of a character. Using Lu Xun, a renowned Chinese\nwriter, as a case study, we propose four training tasks derived from his 17\nessay collections. These include a pre-training task focused on mastering\nexternal linguistic structures and knowledge, as well as three fine-tuning\ntasks: multiple-choice question answering, generative question answering, and\nstyle transfer, each aligning the LLM with Lu Xun's internal ideation and\nwriting style. To optimize learning across these tasks, we introduce a CharLoRA\nparameter updating mechanism, where a general linguistic style expert\ncollaborates with other task-specific experts to better study both the language\nstyle and the understanding of deeper thoughts. We evaluate CharacterBot on\nthree tasks for linguistic accuracy and opinion comprehension, demonstrating\nthat it significantly outperforms the baselines on our adapted metrics. We hope\nthat this work inspires future research on deep character persona simulation\nLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous approaches to persona simulation large language models (LLMs) have\ntypically relied on learning basic biographical information, or using limited\nrole-play dialogue datasets to capture a character's responses. However, a\nholistic representation of an individual goes beyond surface-level facts or\nconversations to deeper thoughts and thinking. In this work, we introduce\nCharacterBot, a model designed to replicate both the linguistic patterns and\ndistinctive thought processes of a character. Using Lu Xun, a renowned Chinese\nwriter, as a case study, we propose four training tasks derived from his 17\nessay collections. These include a pre-training task focused on mastering\nexternal linguistic structures and knowledge, as well as three fine-tuning\ntasks: multiple-choice question answering, generative question answering, and\nstyle transfer, each aligning the LLM with Lu Xun's internal ideation and\nwriting style. To optimize learning across these tasks, we introduce a CharLoRA\nparameter updating mechanism, where a general linguistic style expert\ncollaborates with other task-specific experts to better study both the language\nstyle and the understanding of deeper thoughts. We evaluate CharacterBot on\nthree tasks for linguistic accuracy and opinion comprehension, demonstrating\nthat it significantly outperforms the baselines on our adapted metrics. We hope\nthat this work inspires future research on deep character persona simulation\nLLM."
                },
                "authors": [
                    {
                        "name": "Zixiao Wang"
                    },
                    {
                        "name": "Duzhen Zhang"
                    },
                    {
                        "name": "Ishita Agrawal"
                    },
                    {
                        "name": "Shen Gao"
                    },
                    {
                        "name": "Le Song"
                    },
                    {
                        "name": "Xiuying Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiuying Chen"
                },
                "author": "Xiuying Chen",
                "arxiv_comment": "19 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09992v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09992v2",
                "updated": "2025-02-18T16:08:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    8,
                    59,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-14T08:23:51Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    8,
                    23,
                    51,
                    4,
                    45,
                    0
                ],
                "title": "Large Language Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Diffusion Models"
                },
                "summary": "Autoregressive models (ARMs) are widely regarded as the cornerstone of large\nlanguage models (LLMs). We challenge this notion by introducing LLaDA, a\ndiffusion model trained from scratch under the pre-training and supervised\nfine-tuning (SFT) paradigm. LLaDA models distributions through a forward data\nmasking process and a reverse process, parameterized by a vanilla Transformer\nto predict masked tokens. By optimizing a likelihood bound, it provides a\nprincipled generative approach for probabilistic inference. Across extensive\nbenchmarks, LLaDA demonstrates strong scalability, outperforming our\nself-constructed ARM baselines. Remarkably, LLaDA 8B is competitive with strong\nLLMs like LLaMA3 8B in in-context learning and, after SFT, exhibits impressive\ninstruction-following abilities in case studies such as multi-turn dialogue.\nMoreover, LLaDA addresses the reversal curse, surpassing GPT-4o in a reversal\npoem completion task. Our findings establish diffusion models as a viable and\npromising alternative to ARMs, challenging the assumption that key LLM\ncapabilities discussed above are inherently tied to ARMs. Project page and\ncodes: https://ml-gsai.github.io/LLaDA-demo/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive models (ARMs) are widely regarded as the cornerstone of large\nlanguage models (LLMs). We challenge this notion by introducing LLaDA, a\ndiffusion model trained from scratch under the pre-training and supervised\nfine-tuning (SFT) paradigm. LLaDA models distributions through a forward data\nmasking process and a reverse process, parameterized by a vanilla Transformer\nto predict masked tokens. By optimizing a likelihood bound, it provides a\nprincipled generative approach for probabilistic inference. Across extensive\nbenchmarks, LLaDA demonstrates strong scalability, outperforming our\nself-constructed ARM baselines. Remarkably, LLaDA 8B is competitive with strong\nLLMs like LLaMA3 8B in in-context learning and, after SFT, exhibits impressive\ninstruction-following abilities in case studies such as multi-turn dialogue.\nMoreover, LLaDA addresses the reversal curse, surpassing GPT-4o in a reversal\npoem completion task. Our findings establish diffusion models as a viable and\npromising alternative to ARMs, challenging the assumption that key LLM\ncapabilities discussed above are inherently tied to ARMs. Project page and\ncodes: https://ml-gsai.github.io/LLaDA-demo/."
                },
                "authors": [
                    {
                        "name": "Shen Nie"
                    },
                    {
                        "name": "Fengqi Zhu"
                    },
                    {
                        "name": "Zebin You"
                    },
                    {
                        "name": "Xiaolu Zhang"
                    },
                    {
                        "name": "Jingyang Ou"
                    },
                    {
                        "name": "Jun Hu"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Yankai Lin"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    },
                    {
                        "name": "Chongxuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Chongxuan Li"
                },
                "author": "Chongxuan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09992v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09992v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12982v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12982v1",
                "updated": "2025-02-18T16:04:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    4,
                    57,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T16:04:57Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    4,
                    57,
                    1,
                    49,
                    0
                ],
                "title": "Sailor2: Sailing in South-East Asia with Inclusive Multilingual LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sailor2: Sailing in South-East Asia with Inclusive Multilingual LLMs"
                },
                "summary": "Sailor2 is a family of cutting-edge multilingual language models for\nSouth-East Asian (SEA) languages, available in 1B, 8B, and 20B sizes to suit\ndiverse applications. Building on Qwen2.5, Sailor2 undergoes continuous\npre-training on 500B tokens (400B SEA-specific and 100B replay tokens) to\nsupport 13 SEA languages while retaining proficiency in Chinese and English.\nSailor2-20B model achieves a 50-50 win rate against GPT-4o across SEA\nlanguages. We also deliver a comprehensive cookbook on how to develop the\nmultilingual model in an efficient manner, including five key aspects: data\ncuration, pre-training, post-training, model customization and evaluation. We\nhope that Sailor2 model (Apache 2.0 license) will drive language development in\nthe SEA region, and Sailor2 cookbook will inspire researchers to build more\ninclusive LLMs for other under-served languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sailor2 is a family of cutting-edge multilingual language models for\nSouth-East Asian (SEA) languages, available in 1B, 8B, and 20B sizes to suit\ndiverse applications. Building on Qwen2.5, Sailor2 undergoes continuous\npre-training on 500B tokens (400B SEA-specific and 100B replay tokens) to\nsupport 13 SEA languages while retaining proficiency in Chinese and English.\nSailor2-20B model achieves a 50-50 win rate against GPT-4o across SEA\nlanguages. We also deliver a comprehensive cookbook on how to develop the\nmultilingual model in an efficient manner, including five key aspects: data\ncuration, pre-training, post-training, model customization and evaluation. We\nhope that Sailor2 model (Apache 2.0 license) will drive language development in\nthe SEA region, and Sailor2 cookbook will inspire researchers to build more\ninclusive LLMs for other under-served languages."
                },
                "authors": [
                    {
                        "name": "Longxu Dou"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Changyu Chen"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Ziqi Jin"
                    },
                    {
                        "name": "Zichen Liu"
                    },
                    {
                        "name": "Tongyao Zhu"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Penghui Yang"
                    },
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Yongchi Zhao"
                    },
                    {
                        "name": "Xiachong Feng"
                    },
                    {
                        "name": "Xin Mao"
                    },
                    {
                        "name": "Man Tsung Yeung"
                    },
                    {
                        "name": "Kunat Pipatanakul"
                    },
                    {
                        "name": "Fajri Koto"
                    },
                    {
                        "name": "Min Si Thu"
                    },
                    {
                        "name": "Hynek Kydlek"
                    },
                    {
                        "name": "Zeyi Liu"
                    },
                    {
                        "name": "Qunshu Lin"
                    },
                    {
                        "name": "Sittipong Sripaisarnmongkol"
                    },
                    {
                        "name": "Kridtaphad Sae-Khow"
                    },
                    {
                        "name": "Nirattisai Thongchim"
                    },
                    {
                        "name": "Taechawat Konkaew"
                    },
                    {
                        "name": "Narong Borijindargoon"
                    },
                    {
                        "name": "Anh Dao"
                    },
                    {
                        "name": "Matichon Maneegard"
                    },
                    {
                        "name": "Phakphum Artkaew"
                    },
                    {
                        "name": "Zheng-Xin Yong"
                    },
                    {
                        "name": "Quan Nguyen"
                    },
                    {
                        "name": "Wannaphong Phatthiyaphaibun"
                    },
                    {
                        "name": "Hoang H. Tran"
                    },
                    {
                        "name": "Mike Zhang"
                    },
                    {
                        "name": "Shiqi Chen"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Xinyi Wan"
                    },
                    {
                        "name": "Wei Lu"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "arxiv_comment": "49 pages, 16 figures. Technical Report of Sailor2:\n  https://sea-sailor.github.io/blog/sailor2/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12982v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12978v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12978v1",
                "updated": "2025-02-18T15:58:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    58,
                    58,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T15:58:58Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    58,
                    58,
                    1,
                    49,
                    0
                ],
                "title": "Statistically Significant $k$NNAD by Selective Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistically Significant $k$NNAD by Selective Inference"
                },
                "summary": "In this paper, we investigate the problem of unsupervised anomaly detection\nusing the k-Nearest Neighbor method. The k-Nearest Neighbor Anomaly Detection\n(kNNAD) is a simple yet effective approach for identifying anomalies across\nvarious domains and fields. A critical challenge in anomaly detection,\nincluding kNNAD, is appropriately quantifying the reliability of detected\nanomalies. To address this, we formulate kNNAD as a statistical hypothesis test\nand quantify the probability of false detection using $p$-values. The main\ntechnical challenge lies in performing both anomaly detection and statistical\ntesting on the same data, which hinders correct $p$-value calculation within\nthe conventional statistical testing framework. To resolve this issue, we\nintroduce a statistical hypothesis testing framework called Selective Inference\n(SI) and propose a method named Statistically Significant NNAD (Stat-kNNAD). By\nleveraging SI, the Stat-kNNAD method ensures that detected anomalies are\nstatistically significant with theoretical guarantees. The proposed Stat-kNNAD\nmethod is applicable to anomaly detection in both the original feature space\nand latent feature spaces derived from deep learning models. Through numerical\nexperiments on synthetic data and applications to industrial product anomaly\ndetection, we demonstrate the validity and effectiveness of the Stat-kNNAD\nmethod.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate the problem of unsupervised anomaly detection\nusing the k-Nearest Neighbor method. The k-Nearest Neighbor Anomaly Detection\n(kNNAD) is a simple yet effective approach for identifying anomalies across\nvarious domains and fields. A critical challenge in anomaly detection,\nincluding kNNAD, is appropriately quantifying the reliability of detected\nanomalies. To address this, we formulate kNNAD as a statistical hypothesis test\nand quantify the probability of false detection using $p$-values. The main\ntechnical challenge lies in performing both anomaly detection and statistical\ntesting on the same data, which hinders correct $p$-value calculation within\nthe conventional statistical testing framework. To resolve this issue, we\nintroduce a statistical hypothesis testing framework called Selective Inference\n(SI) and propose a method named Statistically Significant NNAD (Stat-kNNAD). By\nleveraging SI, the Stat-kNNAD method ensures that detected anomalies are\nstatistically significant with theoretical guarantees. The proposed Stat-kNNAD\nmethod is applicable to anomaly detection in both the original feature space\nand latent feature spaces derived from deep learning models. Through numerical\nexperiments on synthetic data and applications to industrial product anomaly\ndetection, we demonstrate the validity and effectiveness of the Stat-kNNAD\nmethod."
                },
                "authors": [
                    {
                        "name": "Mizuki Niihori"
                    },
                    {
                        "name": "Teruyuki Katsuoka"
                    },
                    {
                        "name": "Tomohiro Shiraishi"
                    },
                    {
                        "name": "Shuichi Nishino"
                    },
                    {
                        "name": "Ichiro Takeuchi"
                    }
                ],
                "author_detail": {
                    "name": "Ichiro Takeuchi"
                },
                "author": "Ichiro Takeuchi",
                "arxiv_comment": "40 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12978v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12978v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13203v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13203v4",
                "updated": "2025-02-18T15:58:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    58,
                    8,
                    1,
                    49,
                    0
                ],
                "published": "2024-09-20T04:17:13Z",
                "published_parsed": [
                    2024,
                    9,
                    20,
                    4,
                    17,
                    13,
                    4,
                    264,
                    0
                ],
                "title": "Neural-Symbolic Collaborative Distillation: Advancing Small Language\n  Models for Complex Reasoning Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural-Symbolic Collaborative Distillation: Advancing Small Language\n  Models for Complex Reasoning Tasks"
                },
                "summary": "In this paper, we propose $\\textbf{Ne}$ural-$\\textbf{Sy}$mbolic\n$\\textbf{C}$ollaborative $\\textbf{D}$istillation ($\\textbf{NesyCD}$), a novel\nknowledge distillation method for learning the complex reasoning abilities of\nLarge Language Models (LLMs, e.g., \\textgreater 13B). We argue that complex\nreasoning tasks are difficult for Small Language Models (SLMs, e.g., $\\leq$\n7B), as these tasks demand not only general cognitive abilities but also\nspecialized knowledge, which is often sparse and difficult for these\nneural-based SLMs to effectively capture. Therefore, NesyCD distills the\ngeneral capabilities and specialized knowledge in LLMs using different manners.\nOn the one hand, we distill only general abilities from teacher LLMs into the\nstudent SLMs of parameterized neural networks. On the other hand, for the\nspecialized abilities and uncommon knowledge of a complex reasoning task, we\nemploy a symbolic knowledge distillation approach to obtain and store the\nspecialized knowledge within a symbolic knowledge base (KB). By decoupling\ngeneral and specialized capabilities, the proposed NesyCD can achieve superior\nperformance cost-effectively, utilizing smaller models and blending\nparameterized neural networks with symbolic KB. Moreover, the specialized KB\ngeneralizes well and is comprehended and manipulated by humans. Our experiments\nshow that NesyCD significantly boosts SLMs' complex reasoning performance on\nin-domain (BBH, GSM8K) and out-of-domain (AGIEval, ARC) datasets. Notably, our\napproach enabled the LLaMA3-8B and Qwen2-7B to surpass GPT-3.5-turbo in\nperformance and come close to matching LLaMA3-70B, despite the latter having\nnine times more parameters. Our code will be available at\nhttps://github.com/Xnhyacinth/NesyCD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose $\\textbf{Ne}$ural-$\\textbf{Sy}$mbolic\n$\\textbf{C}$ollaborative $\\textbf{D}$istillation ($\\textbf{NesyCD}$), a novel\nknowledge distillation method for learning the complex reasoning abilities of\nLarge Language Models (LLMs, e.g., \\textgreater 13B). We argue that complex\nreasoning tasks are difficult for Small Language Models (SLMs, e.g., $\\leq$\n7B), as these tasks demand not only general cognitive abilities but also\nspecialized knowledge, which is often sparse and difficult for these\nneural-based SLMs to effectively capture. Therefore, NesyCD distills the\ngeneral capabilities and specialized knowledge in LLMs using different manners.\nOn the one hand, we distill only general abilities from teacher LLMs into the\nstudent SLMs of parameterized neural networks. On the other hand, for the\nspecialized abilities and uncommon knowledge of a complex reasoning task, we\nemploy a symbolic knowledge distillation approach to obtain and store the\nspecialized knowledge within a symbolic knowledge base (KB). By decoupling\ngeneral and specialized capabilities, the proposed NesyCD can achieve superior\nperformance cost-effectively, utilizing smaller models and blending\nparameterized neural networks with symbolic KB. Moreover, the specialized KB\ngeneralizes well and is comprehended and manipulated by humans. Our experiments\nshow that NesyCD significantly boosts SLMs' complex reasoning performance on\nin-domain (BBH, GSM8K) and out-of-domain (AGIEval, ARC) datasets. Notably, our\napproach enabled the LLaMA3-8B and Qwen2-7B to surpass GPT-3.5-turbo in\nperformance and come close to matching LLaMA3-70B, despite the latter having\nnine times more parameters. Our code will be available at\nhttps://github.com/Xnhyacinth/NesyCD."
                },
                "authors": [
                    {
                        "name": "Huanxuan Liao"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Yao Xu"
                    },
                    {
                        "name": "Yuanzhe Zhang"
                    },
                    {
                        "name": "Kang Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhao"
                },
                "author": "Jun Zhao",
                "arxiv_comment": "Accepted to AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13203v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13203v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12974v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12974v1",
                "updated": "2025-02-18T15:56:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    56,
                    34,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T15:56:34Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    56,
                    34,
                    1,
                    49,
                    0
                ],
                "title": "Learning More Effective Representations for Dense Retrieval through\n  Deliberate Thinking Before Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning More Effective Representations for Dense Retrieval through\n  Deliberate Thinking Before Search"
                },
                "summary": "Recent dense retrievers usually thrive on the emergency capabilities of Large\nLanguage Models (LLMs), using them to encode queries and documents into an\nembedding space for retrieval. These LLM-based dense retrievers have shown\npromising performance across various retrieval scenarios. However, relying on a\nsingle embedding to represent documents proves less effective in capturing\ndifferent perspectives of documents for matching. In this paper, we propose\nDeliberate Thinking based Dense Retriever (DEBATER), which enhances these\nLLM-based retrievers by enabling them to learn more effective document\nrepresentations through a step-by-step thinking process. DEBATER introduces the\nChain-of-Deliberation mechanism to iteratively optimize document\nrepresentations using a continuous chain of thought. To consolidate information\nfrom various thinking steps, DEBATER also incorporates the Self Distillation\nmechanism, which identifies the most informative thinking steps and integrates\nthem into a unified text embedding. Experimental results show that DEBATER\nsignificantly outperforms existing methods across several retrieval benchmarks,\ndemonstrating superior accuracy and robustness. All codes are available at\nhttps://github.com/OpenBMB/DEBATER.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent dense retrievers usually thrive on the emergency capabilities of Large\nLanguage Models (LLMs), using them to encode queries and documents into an\nembedding space for retrieval. These LLM-based dense retrievers have shown\npromising performance across various retrieval scenarios. However, relying on a\nsingle embedding to represent documents proves less effective in capturing\ndifferent perspectives of documents for matching. In this paper, we propose\nDeliberate Thinking based Dense Retriever (DEBATER), which enhances these\nLLM-based retrievers by enabling them to learn more effective document\nrepresentations through a step-by-step thinking process. DEBATER introduces the\nChain-of-Deliberation mechanism to iteratively optimize document\nrepresentations using a continuous chain of thought. To consolidate information\nfrom various thinking steps, DEBATER also incorporates the Self Distillation\nmechanism, which identifies the most informative thinking steps and integrates\nthem into a unified text embedding. Experimental results show that DEBATER\nsignificantly outperforms existing methods across several retrieval benchmarks,\ndemonstrating superior accuracy and robustness. All codes are available at\nhttps://github.com/OpenBMB/DEBATER."
                },
                "authors": [
                    {
                        "name": "Yifan Ji"
                    },
                    {
                        "name": "Zhipeng Xu"
                    },
                    {
                        "name": "Zhenghao Liu"
                    },
                    {
                        "name": "Yukun Yan"
                    },
                    {
                        "name": "Shi Yu"
                    },
                    {
                        "name": "Yishan Li"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Yu Gu"
                    },
                    {
                        "name": "Ge Yu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12974v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12974v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12382v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12382v5",
                "updated": "2025-02-18T15:54:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    54,
                    28,
                    1,
                    49,
                    0
                ],
                "published": "2024-06-18T08:14:28Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    8,
                    14,
                    28,
                    1,
                    170,
                    0
                ],
                "title": "From Instance Training to Instruction Learning: Task Adapters Generation\n  from Instructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Instance Training to Instruction Learning: Task Adapters Generation\n  from Instructions"
                },
                "summary": "Large language models (LLMs) have acquired the ability to solve general tasks\nby utilizing instruction finetuning (IFT). However, IFT still relies heavily on\ninstance training of extensive task data, which greatly limits the adaptability\nof LLMs to real-world scenarios where labeled task instances are scarce and\nbroader task generalization becomes paramount. Contrary to LLMs, humans acquire\nskills and complete tasks not merely through repeated practice but also by\nunderstanding and following instructional guidelines. This paper is dedicated\nto simulating human learning to address the shortcomings of instance training,\nfocusing on instruction learning to enhance cross-task generalization. Within\nthis context, we introduce Task Adapters Generation from Instructions (TAGI),\nwhich automatically constructs the task-specific model in a parameter\ngeneration manner based on the given task instructions without retraining for\nunseen tasks. Specifically, we utilize knowledge distillation to enhance the\nconsistency between TAGI developed through Learning with Instruction and\ntask-specific models developed through Training with Instance, by aligning the\nlabels, output logits, and adapter parameters between them. TAGI is endowed\nwith cross-task generalization capabilities through a two-stage training\nprocess that includes hypernetwork pretraining and finetuning. We evaluate TAGI\non the Super-Natural Instructions and P3 datasets. The experimental results\ndemonstrate that TAGI can match or even outperform traditional meta-trained\nmodels and other hypernetwork models, while significantly reducing\ncomputational requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have acquired the ability to solve general tasks\nby utilizing instruction finetuning (IFT). However, IFT still relies heavily on\ninstance training of extensive task data, which greatly limits the adaptability\nof LLMs to real-world scenarios where labeled task instances are scarce and\nbroader task generalization becomes paramount. Contrary to LLMs, humans acquire\nskills and complete tasks not merely through repeated practice but also by\nunderstanding and following instructional guidelines. This paper is dedicated\nto simulating human learning to address the shortcomings of instance training,\nfocusing on instruction learning to enhance cross-task generalization. Within\nthis context, we introduce Task Adapters Generation from Instructions (TAGI),\nwhich automatically constructs the task-specific model in a parameter\ngeneration manner based on the given task instructions without retraining for\nunseen tasks. Specifically, we utilize knowledge distillation to enhance the\nconsistency between TAGI developed through Learning with Instruction and\ntask-specific models developed through Training with Instance, by aligning the\nlabels, output logits, and adapter parameters between them. TAGI is endowed\nwith cross-task generalization capabilities through a two-stage training\nprocess that includes hypernetwork pretraining and finetuning. We evaluate TAGI\non the Super-Natural Instructions and P3 datasets. The experimental results\ndemonstrate that TAGI can match or even outperform traditional meta-trained\nmodels and other hypernetwork models, while significantly reducing\ncomputational requirements."
                },
                "authors": [
                    {
                        "name": "Huanxuan Liao"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Yao Xu"
                    },
                    {
                        "name": "Yuanzhe Zhang"
                    },
                    {
                        "name": "Yanchao Hao"
                    },
                    {
                        "name": "Shengping Liu"
                    },
                    {
                        "name": "Kang Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhao"
                },
                "author": "Jun Zhao",
                "arxiv_comment": "accepted to NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12382v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12382v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06253v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06253v3",
                "updated": "2025-02-18T15:53:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    53,
                    11,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-10T08:37:21Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    8,
                    37,
                    21,
                    0,
                    41,
                    0
                ],
                "title": "Find Central Dogma Again: Leveraging Multilingual Transfer in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Find Central Dogma Again: Leveraging Multilingual Transfer in Large\n  Language Models"
                },
                "summary": "In recent years, large language models (LLMs) have achieved state-of-the-art\nresults in various biological sequence analysis tasks, such as sequence\nclassification, structure prediction, and function prediction. Similar to\nadvancements in AI for other scientific fields, deeper research into biological\nLLMs has begun to focus on using these models to rediscover important existing\nbiological laws or uncover entirely new patterns in biological sequences. This\nstudy leverages GPT-like LLMs to utilize language transfer capabilities to\nrediscover the genetic code rules of the central dogma. In our experimental\ndesign, we transformed the central dogma into a binary classification problem\nof aligning DNA sequences with protein sequences, where positive examples are\nmatching DNA and protein sequences, and negative examples are non-matching\npairs. We first trained a GPT-2 model from scratch using a dataset comprising\nprotein sequences, DNA sequences, and sequences from languages such as English\nand Chinese. Subsequently, we fine-tuned the model using the natural language\nsentences similarity judgment dataset from PAWS-X. When tested on a dataset for\nDNA and protein sequence alignment judgment, the fine-tuned model achieved a\nclassification accuracy of 81%. The study also analyzed factors contributing to\nthis zero-shot capability, including model training stability and types of\ntraining data. This research demonstrates that LLMs can, through the transfer\nof natural language capabilities and solely relying on the analysis of\nsequences themselves, rediscover the central dogma without prior knowledge of\nit. This study bridges natural language and genetic language, opening a new\ndoor for AI-driven biological research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, large language models (LLMs) have achieved state-of-the-art\nresults in various biological sequence analysis tasks, such as sequence\nclassification, structure prediction, and function prediction. Similar to\nadvancements in AI for other scientific fields, deeper research into biological\nLLMs has begun to focus on using these models to rediscover important existing\nbiological laws or uncover entirely new patterns in biological sequences. This\nstudy leverages GPT-like LLMs to utilize language transfer capabilities to\nrediscover the genetic code rules of the central dogma. In our experimental\ndesign, we transformed the central dogma into a binary classification problem\nof aligning DNA sequences with protein sequences, where positive examples are\nmatching DNA and protein sequences, and negative examples are non-matching\npairs. We first trained a GPT-2 model from scratch using a dataset comprising\nprotein sequences, DNA sequences, and sequences from languages such as English\nand Chinese. Subsequently, we fine-tuned the model using the natural language\nsentences similarity judgment dataset from PAWS-X. When tested on a dataset for\nDNA and protein sequence alignment judgment, the fine-tuned model achieved a\nclassification accuracy of 81%. The study also analyzed factors contributing to\nthis zero-shot capability, including model training stability and types of\ntraining data. This research demonstrates that LLMs can, through the transfer\nof natural language capabilities and solely relying on the analysis of\nsequences themselves, rediscover the central dogma without prior knowledge of\nit. This study bridges natural language and genetic language, opening a new\ndoor for AI-driven biological research."
                },
                "authors": [
                    {
                        "name": "Wang Liang"
                    }
                ],
                "author_detail": {
                    "name": "Wang Liang"
                },
                "author": "Wang Liang",
                "arxiv_comment": "31 pages,8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06253v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06253v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "92-10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22071v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22071v2",
                "updated": "2025-02-18T15:52:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    52,
                    52,
                    1,
                    49,
                    0
                ],
                "published": "2024-10-29T14:31:33Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    14,
                    31,
                    33,
                    1,
                    303,
                    0
                ],
                "title": "Distinguishing Ignorance from Error in LLM Hallucinations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distinguishing Ignorance from Error in LLM Hallucinations"
                },
                "summary": "Large language models (LLMs) are susceptible to hallucinations -- factually\nincorrect outputs -- leading to a large body of work on detecting and\nmitigating such cases. We argue that it is important to distinguish between two\ntypes of hallucinations: ones where the model does not hold the correct answer\nin its parameters, which we term HK-, and ones where the model answers\nincorrectly despite having the required knowledge, termed HK+. We first find\nthat HK+ hallucinations are prevalent and occur across models and datasets.\nThen, we demonstrate that distinguishing between these two cases is beneficial\nfor mitigating hallucinations. Importantly, we show that different models\nhallucinate on different examples, which motivates constructing model-specific\nhallucination datasets for training detectors. Overall, our findings draw\nattention to classifying types of hallucinations and provide means to handle\nthem more effectively. The code is available at\nhttps://github.com/technion-cs-nlp/hallucination-mitigation .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are susceptible to hallucinations -- factually\nincorrect outputs -- leading to a large body of work on detecting and\nmitigating such cases. We argue that it is important to distinguish between two\ntypes of hallucinations: ones where the model does not hold the correct answer\nin its parameters, which we term HK-, and ones where the model answers\nincorrectly despite having the required knowledge, termed HK+. We first find\nthat HK+ hallucinations are prevalent and occur across models and datasets.\nThen, we demonstrate that distinguishing between these two cases is beneficial\nfor mitigating hallucinations. Importantly, we show that different models\nhallucinate on different examples, which motivates constructing model-specific\nhallucination datasets for training detectors. Overall, our findings draw\nattention to classifying types of hallucinations and provide means to handle\nthem more effectively. The code is available at\nhttps://github.com/technion-cs-nlp/hallucination-mitigation ."
                },
                "authors": [
                    {
                        "name": "Adi Simhi"
                    },
                    {
                        "name": "Jonathan Herzig"
                    },
                    {
                        "name": "Idan Szpektor"
                    },
                    {
                        "name": "Yonatan Belinkov"
                    }
                ],
                "author_detail": {
                    "name": "Yonatan Belinkov"
                },
                "author": "Yonatan Belinkov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22071v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22071v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12970v1",
                "updated": "2025-02-18T15:48:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    48,
                    46,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T15:48:46Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    48,
                    46,
                    1,
                    49,
                    0
                ],
                "title": "Reasoning-to-Defend: Safety-Aware Reasoning Can Defend Large Language\n  Models from Jailbreaking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning-to-Defend: Safety-Aware Reasoning Can Defend Large Language\n  Models from Jailbreaking"
                },
                "summary": "The reasoning abilities of Large Language Models (LLMs) have demonstrated\nremarkable advancement and exceptional performance across diverse domains.\nHowever, leveraging these reasoning capabilities to enhance LLM safety against\nadversarial attacks and jailbreak queries remains largely unexplored. To bridge\nthis gap, we propose Reasoning-to-Defend (R2D), a novel training paradigm that\nintegrates safety reflections of queries and responses into LLMs' generation\nprocess, unlocking a safety-aware reasoning mechanism. This approach enables\nself-evaluation at each reasoning step to create safety pivot tokens as\nindicators of the response's safety status. Furthermore, in order to improve\nthe learning efficiency of pivot token prediction, we propose Contrastive Pivot\nOptimization(CPO), which enhances the model's ability to perceive the safety\nstatus of dialogues. Through this mechanism, LLMs dynamically adjust their\nresponse strategies during reasoning, significantly enhancing their defense\ncapabilities against jailbreak attacks. Extensive experimental results\ndemonstrate that R2D effectively mitigates various attacks and improves overall\nsafety, highlighting the substantial potential of safety-aware reasoning in\nstrengthening LLMs' robustness against jailbreaks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reasoning abilities of Large Language Models (LLMs) have demonstrated\nremarkable advancement and exceptional performance across diverse domains.\nHowever, leveraging these reasoning capabilities to enhance LLM safety against\nadversarial attacks and jailbreak queries remains largely unexplored. To bridge\nthis gap, we propose Reasoning-to-Defend (R2D), a novel training paradigm that\nintegrates safety reflections of queries and responses into LLMs' generation\nprocess, unlocking a safety-aware reasoning mechanism. This approach enables\nself-evaluation at each reasoning step to create safety pivot tokens as\nindicators of the response's safety status. Furthermore, in order to improve\nthe learning efficiency of pivot token prediction, we propose Contrastive Pivot\nOptimization(CPO), which enhances the model's ability to perceive the safety\nstatus of dialogues. Through this mechanism, LLMs dynamically adjust their\nresponse strategies during reasoning, significantly enhancing their defense\ncapabilities against jailbreak attacks. Extensive experimental results\ndemonstrate that R2D effectively mitigates various attacks and improves overall\nsafety, highlighting the substantial potential of safety-aware reasoning in\nstrengthening LLMs' robustness against jailbreaks."
                },
                "authors": [
                    {
                        "name": "Junda Zhu"
                    },
                    {
                        "name": "Lingyong Yan"
                    },
                    {
                        "name": "Shuaiqiang Wang"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Lei Sha"
                    }
                ],
                "author_detail": {
                    "name": "Lei Sha"
                },
                "author": "Lei Sha",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12964v1",
                "updated": "2025-02-18T15:46:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    46,
                    31,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T15:46:31Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    46,
                    31,
                    1,
                    49,
                    0
                ],
                "title": "Trust Me, I'm Wrong: High-Certainty Hallucinations in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trust Me, I'm Wrong: High-Certainty Hallucinations in LLMs"
                },
                "summary": "Large Language Models (LLMs) often generate outputs that lack grounding in\nreal-world facts, a phenomenon known as hallucinations. Prior research has\nassociated hallucinations with model uncertainty, leveraging this relationship\nfor hallucination detection and mitigation. In this paper, we challenge the\nunderlying assumption that all hallucinations are associated with uncertainty.\nUsing knowledge detection and uncertainty measurement methods, we demonstrate\nthat models can hallucinate with high certainty even when they have the correct\nknowledge. We further show that high-certainty hallucinations are consistent\nacross models and datasets, distinctive enough to be singled out, and challenge\nexisting mitigation methods. Our findings reveal an overlooked aspect of\nhallucinations, emphasizing the need to understand their origins and improve\nmitigation strategies to enhance LLM safety. The code is available at\nhttps://github.com/technion-cs-nlp/Trust_me_Im_wrong .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often generate outputs that lack grounding in\nreal-world facts, a phenomenon known as hallucinations. Prior research has\nassociated hallucinations with model uncertainty, leveraging this relationship\nfor hallucination detection and mitigation. In this paper, we challenge the\nunderlying assumption that all hallucinations are associated with uncertainty.\nUsing knowledge detection and uncertainty measurement methods, we demonstrate\nthat models can hallucinate with high certainty even when they have the correct\nknowledge. We further show that high-certainty hallucinations are consistent\nacross models and datasets, distinctive enough to be singled out, and challenge\nexisting mitigation methods. Our findings reveal an overlooked aspect of\nhallucinations, emphasizing the need to understand their origins and improve\nmitigation strategies to enhance LLM safety. The code is available at\nhttps://github.com/technion-cs-nlp/Trust_me_Im_wrong ."
                },
                "authors": [
                    {
                        "name": "Adi Simhi"
                    },
                    {
                        "name": "Itay Itzhak"
                    },
                    {
                        "name": "Fazl Barez"
                    },
                    {
                        "name": "Gabriel Stanovsky"
                    },
                    {
                        "name": "Yonatan Belinkov"
                    }
                ],
                "author_detail": {
                    "name": "Yonatan Belinkov"
                },
                "author": "Yonatan Belinkov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12962v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12962v1",
                "updated": "2025-02-18T15:45:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    45,
                    36,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T15:45:36Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    45,
                    36,
                    1,
                    49,
                    0
                ],
                "title": "Infinite Retrieval: Attention Enhanced LLMs in Long-Context Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Infinite Retrieval: Attention Enhanced LLMs in Long-Context Processing"
                },
                "summary": "Limited by the context window size of Large Language Models(LLMs), handling\nvarious tasks with input tokens exceeding the upper limit has been challenging,\nwhether it is a simple direct retrieval task or a complex multi-hop reasoning\ntask. Although various methods have been proposed to enhance the long-context\nprocessing capabilities of LLMs, they either incur substantial post-training\ncosts, or require additional tool modules(e.g.,RAG), or have not shown\nsignificant improvement in realistic tasks. Our work observes the correlation\nbetween the attention distribution and generated answers across each layer, and\nestablishes the attention allocation aligns with retrieval-augmented\ncapabilities through experiments. Drawing on the above insights, we propose a\nnovel method InfiniRetri that leverages the LLMs's own attention information to\nenable accurate retrieval across inputs of infinitely length. Our evaluations\nindicate that InfiniRetri achieves 100% accuracy in the\nNeedle-In-a-Haystack(NIH) test over 1M tokens using a 0.5B parameter model,\nsurpassing other method or larger models and setting a new\nstate-of-the-art(SOTA). Moreover, our method achieves significant performance\nimprovements on real-world benchmarks, with a maximum 288% improvement. In\naddition, InfiniRetri can be applied to any Transformer-based LLMs without\nadditional training and substantially reduces inference latency and compute\noverhead in long texts. In summary, our comprehensive studies show\nInfiniRetri's potential for practical applications and creates a paradigm for\nretrievaling information using LLMs own capabilities under infinite-length\ntokens. Code will be released in link.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Limited by the context window size of Large Language Models(LLMs), handling\nvarious tasks with input tokens exceeding the upper limit has been challenging,\nwhether it is a simple direct retrieval task or a complex multi-hop reasoning\ntask. Although various methods have been proposed to enhance the long-context\nprocessing capabilities of LLMs, they either incur substantial post-training\ncosts, or require additional tool modules(e.g.,RAG), or have not shown\nsignificant improvement in realistic tasks. Our work observes the correlation\nbetween the attention distribution and generated answers across each layer, and\nestablishes the attention allocation aligns with retrieval-augmented\ncapabilities through experiments. Drawing on the above insights, we propose a\nnovel method InfiniRetri that leverages the LLMs's own attention information to\nenable accurate retrieval across inputs of infinitely length. Our evaluations\nindicate that InfiniRetri achieves 100% accuracy in the\nNeedle-In-a-Haystack(NIH) test over 1M tokens using a 0.5B parameter model,\nsurpassing other method or larger models and setting a new\nstate-of-the-art(SOTA). Moreover, our method achieves significant performance\nimprovements on real-world benchmarks, with a maximum 288% improvement. In\naddition, InfiniRetri can be applied to any Transformer-based LLMs without\nadditional training and substantially reduces inference latency and compute\noverhead in long texts. In summary, our comprehensive studies show\nInfiniRetri's potential for practical applications and creates a paradigm for\nretrievaling information using LLMs own capabilities under infinite-length\ntokens. Code will be released in link."
                },
                "authors": [
                    {
                        "name": "Xiaoju Ye"
                    },
                    {
                        "name": "Zhichun Wang"
                    },
                    {
                        "name": "Jingyuan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jingyuan Wang"
                },
                "author": "Jingyuan Wang",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12962v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12962v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12961v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12961v1",
                "updated": "2025-02-18T15:45:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    45,
                    1,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T15:45:01Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    45,
                    1,
                    1,
                    49,
                    0
                ],
                "title": "Adaptive Tool Use in Large Language Models with Meta-Cognition Trigger",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Tool Use in Large Language Models with Meta-Cognition Trigger"
                },
                "summary": "Large language models (LLMs) have shown remarkable emergent capabilities,\ntransforming the execution of functional tasks by leveraging external tools for\ncomplex problems that require specialized processing or real-time data. While\nexisting research expands LLMs access to diverse tools (e.g., program\ninterpreters, search engines, weather/map apps), the necessity of using these\ntools is often overlooked, leading to indiscriminate tool invocation. This\nnaive approach raises two key issues:(1) increased delays due to unnecessary\ntool calls, and (2) potential errors resulting from faulty interactions with\nexternal tools. In this paper, we introduce meta-cognition as a proxy for LLMs\nself-assessment of their capabilities, representing the model's awareness of\nits own limitations. Based on this, we propose MeCo, an adaptive\ndecision-making strategy for external tool use. MeCo quantifies metacognitive\nscores by capturing high-level cognitive signals in the representation space,\nguiding when to invoke tools. Notably, MeCo is fine-tuning-free and incurs\nminimal cost. Our experiments show that MeCo accurately detects LLMs' internal\ncognitive signals and significantly improves tool-use decision-making across\nmultiple base models and benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable emergent capabilities,\ntransforming the execution of functional tasks by leveraging external tools for\ncomplex problems that require specialized processing or real-time data. While\nexisting research expands LLMs access to diverse tools (e.g., program\ninterpreters, search engines, weather/map apps), the necessity of using these\ntools is often overlooked, leading to indiscriminate tool invocation. This\nnaive approach raises two key issues:(1) increased delays due to unnecessary\ntool calls, and (2) potential errors resulting from faulty interactions with\nexternal tools. In this paper, we introduce meta-cognition as a proxy for LLMs\nself-assessment of their capabilities, representing the model's awareness of\nits own limitations. Based on this, we propose MeCo, an adaptive\ndecision-making strategy for external tool use. MeCo quantifies metacognitive\nscores by capturing high-level cognitive signals in the representation space,\nguiding when to invoke tools. Notably, MeCo is fine-tuning-free and incurs\nminimal cost. Our experiments show that MeCo accurately detects LLMs' internal\ncognitive signals and significantly improves tool-use decision-making across\nmultiple base models and benchmarks."
                },
                "authors": [
                    {
                        "name": "Wenjun Li"
                    },
                    {
                        "name": "Dexun Li"
                    },
                    {
                        "name": "Kuicai Dong"
                    },
                    {
                        "name": "Cong Zhang"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12961v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12961v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05193v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05193v2",
                "updated": "2025-02-18T15:38:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    38,
                    18,
                    1,
                    49,
                    0
                ],
                "published": "2024-10-07T16:50:47Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    16,
                    50,
                    47,
                    0,
                    281,
                    0
                ],
                "title": "RevisEval: Improving LLM-as-a-Judge via Response-Adapted References",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RevisEval: Improving LLM-as-a-Judge via Response-Adapted References"
                },
                "summary": "With significant efforts in recent studies, LLM-as-a-Judge has become a\ncost-effective alternative to human evaluation for assessing text generation\nquality in a wide range of tasks. However, there still remains a reliability\ngap between LLM-as-a-Judge and human evaluation. One important reason is the\nlack of guided oracles in the evaluation process. Motivated by the role of\nreference pervasively used in classic text evaluation, we introduce RevisEval,\na novel text generation evaluation paradigm via the response-adapted\nreferences. RevisEval is driven by the key observation that an ideal reference\nshould maintain the necessary relevance to the response to be evaluated.\nSpecifically, RevisEval leverages the text revision capabilities of large\nlanguage models (LLMs) to adaptively revise the response, then treat the\nrevised text as the reference (response-adapted reference) for the subsequent\nevaluation. Extensive experiments demonstrate that RevisEval outperforms\ntraditional reference-free and reference-based evaluation paradigms that use\nLLM-as-a-Judge across NLG tasks and open-ended instruction-following tasks.\nMore importantly, our response-adapted references can further boost the\nclassical text metrics, e.g., BLEU and BERTScore, compared to traditional\nreferences and even rival the LLM-as-a-Judge. A detailed analysis is also\nconducted to confirm RevisEval's effectiveness in bias reduction, the impact of\ninference cost, and reference relevance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With significant efforts in recent studies, LLM-as-a-Judge has become a\ncost-effective alternative to human evaluation for assessing text generation\nquality in a wide range of tasks. However, there still remains a reliability\ngap between LLM-as-a-Judge and human evaluation. One important reason is the\nlack of guided oracles in the evaluation process. Motivated by the role of\nreference pervasively used in classic text evaluation, we introduce RevisEval,\na novel text generation evaluation paradigm via the response-adapted\nreferences. RevisEval is driven by the key observation that an ideal reference\nshould maintain the necessary relevance to the response to be evaluated.\nSpecifically, RevisEval leverages the text revision capabilities of large\nlanguage models (LLMs) to adaptively revise the response, then treat the\nrevised text as the reference (response-adapted reference) for the subsequent\nevaluation. Extensive experiments demonstrate that RevisEval outperforms\ntraditional reference-free and reference-based evaluation paradigms that use\nLLM-as-a-Judge across NLG tasks and open-ended instruction-following tasks.\nMore importantly, our response-adapted references can further boost the\nclassical text metrics, e.g., BLEU and BERTScore, compared to traditional\nreferences and even rival the LLM-as-a-Judge. A detailed analysis is also\nconducted to confirm RevisEval's effectiveness in bias reduction, the impact of\ninference cost, and reference relevance."
                },
                "authors": [
                    {
                        "name": "Qiyuan Zhang"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Tiezheng YU"
                    },
                    {
                        "name": "Yuxin Jiang"
                    },
                    {
                        "name": "Chuhan Wu"
                    },
                    {
                        "name": "Liangyou Li"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Fuyuan Lyu"
                    },
                    {
                        "name": "Chen Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ma"
                },
                "author": "Chen Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05193v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05193v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12945v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12945v1",
                "updated": "2025-02-18T15:29:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    29,
                    5,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T15:29:05Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    29,
                    5,
                    1,
                    49,
                    0
                ],
                "title": "LLMPopcorn: An Empirical Study of LLMs as Assistants for Popular\n  Micro-video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMPopcorn: An Empirical Study of LLMs as Assistants for Popular\n  Micro-video Generation"
                },
                "summary": "Popular Micro-videos, dominant on platforms like TikTok and YouTube, hold\nsignificant commercial value. The rise of high-quality AI-generated content has\nspurred interest in AI-driven micro-video creation. However, despite the\nadvanced capabilities of large language models (LLMs) like ChatGPT and DeepSeek\nin text generation and reasoning, their potential to assist the creation of\npopular micro-videos remains largely unexplored.\n  In this paper, we conduct an empirical study on LLM-assisted popular\nmicro-video generation (LLMPopcorn). Specifically, we investigate the following\nresearch questions: (i) How can LLMs be effectively utilized to assist popular\nmicro-video generation? (ii) To what extent can prompt-based enhancements\noptimize the LLM-generated content for higher popularity? (iii) How well do\nvarious LLMs and video generators perform in the popular micro-video generation\ntask? By exploring these questions, we show that advanced LLMs like DeepSeek-V3\nenable micro-video generation to achieve popularity comparable to human-created\ncontent. Prompt enhancements further boost popularity, and benchmarking\nhighlights DeepSeek-V3 and DeepSeek-R1 among LLMs, while LTX-Video and\nHunyuanVideo lead in video generation. This pioneering work advances\nAI-assisted micro-video creation, uncovering new research opportunities. We\nwill release the code and datasets to support future studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Popular Micro-videos, dominant on platforms like TikTok and YouTube, hold\nsignificant commercial value. The rise of high-quality AI-generated content has\nspurred interest in AI-driven micro-video creation. However, despite the\nadvanced capabilities of large language models (LLMs) like ChatGPT and DeepSeek\nin text generation and reasoning, their potential to assist the creation of\npopular micro-videos remains largely unexplored.\n  In this paper, we conduct an empirical study on LLM-assisted popular\nmicro-video generation (LLMPopcorn). Specifically, we investigate the following\nresearch questions: (i) How can LLMs be effectively utilized to assist popular\nmicro-video generation? (ii) To what extent can prompt-based enhancements\noptimize the LLM-generated content for higher popularity? (iii) How well do\nvarious LLMs and video generators perform in the popular micro-video generation\ntask? By exploring these questions, we show that advanced LLMs like DeepSeek-V3\nenable micro-video generation to achieve popularity comparable to human-created\ncontent. Prompt enhancements further boost popularity, and benchmarking\nhighlights DeepSeek-V3 and DeepSeek-R1 among LLMs, while LTX-Video and\nHunyuanVideo lead in video generation. This pioneering work advances\nAI-assisted micro-video creation, uncovering new research opportunities. We\nwill release the code and datasets to support future studies."
                },
                "authors": [
                    {
                        "name": "Junchen Fu"
                    },
                    {
                        "name": "Xuri Ge"
                    },
                    {
                        "name": "Kaiwen Zheng"
                    },
                    {
                        "name": "Ioannis Arapakis"
                    },
                    {
                        "name": "Xin Xin"
                    },
                    {
                        "name": "Joemon M. Jose"
                    }
                ],
                "author_detail": {
                    "name": "Joemon M. Jose"
                },
                "author": "Joemon M. Jose",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12945v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12945v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12509v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12509v2",
                "updated": "2025-02-18T15:24:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    24,
                    25,
                    1,
                    49,
                    0
                ],
                "published": "2024-12-17T03:37:31Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    3,
                    37,
                    31,
                    1,
                    352,
                    0
                ],
                "title": "Can You Trust LLM Judgments? Reliability of LLM-as-a-Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can You Trust LLM Judgments? Reliability of LLM-as-a-Judge"
                },
                "summary": "Large Language Models (LLMs) have become increasingly powerful and\nubiquitous, but their stochastic nature poses challenges to the reliability of\ntheir outputs. While deterministic settings can improve consistency, they do\nnot guarantee reliability, as a single sample from the model's probability\ndistribution can still be misleading. Building upon the concept of\nLLM-as-a-judge, we introduce a novel framework for rigorously evaluating the\nreliability of LLM judgments, leveraging McDonald's omega. We evaluate the\nreliability of LLMs when judging the outputs of other LLMs on standard\nsingle-turn and multi-turn benchmarks, simultaneously investigating the impact\nof temperature on reliability. By analyzing these results, we demonstrate the\nlimitations of fixed randomness and the importance of considering multiple\nsamples, which we show has significant implications for downstream\napplications. Our findings highlight the need for a nuanced understanding of\nLLM reliability and the potential risks associated with over-reliance on\nsingle-shot evaluations. This work provides a crucial step towards building\nmore trustworthy and reliable LLM-based systems and applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become increasingly powerful and\nubiquitous, but their stochastic nature poses challenges to the reliability of\ntheir outputs. While deterministic settings can improve consistency, they do\nnot guarantee reliability, as a single sample from the model's probability\ndistribution can still be misleading. Building upon the concept of\nLLM-as-a-judge, we introduce a novel framework for rigorously evaluating the\nreliability of LLM judgments, leveraging McDonald's omega. We evaluate the\nreliability of LLMs when judging the outputs of other LLMs on standard\nsingle-turn and multi-turn benchmarks, simultaneously investigating the impact\nof temperature on reliability. By analyzing these results, we demonstrate the\nlimitations of fixed randomness and the importance of considering multiple\nsamples, which we show has significant implications for downstream\napplications. Our findings highlight the need for a nuanced understanding of\nLLM reliability and the potential risks associated with over-reliance on\nsingle-shot evaluations. This work provides a crucial step towards building\nmore trustworthy and reliable LLM-based systems and applications."
                },
                "authors": [
                    {
                        "name": "Kayla Schroeder"
                    },
                    {
                        "name": "Zach Wood-Doughty"
                    }
                ],
                "author_detail": {
                    "name": "Zach Wood-Doughty"
                },
                "author": "Zach Wood-Doughty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12509v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12509v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15175v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15175v3",
                "updated": "2025-02-18T15:22:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    22,
                    49,
                    1,
                    49,
                    0
                ],
                "published": "2024-11-18T00:21:14Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    0,
                    21,
                    14,
                    0,
                    323,
                    0
                ],
                "title": "ToxiLab: How Well Do Open-Source LLMs Generate Synthetic Toxicity Data?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToxiLab: How Well Do Open-Source LLMs Generate Synthetic Toxicity Data?"
                },
                "summary": "Effective toxic content detection relies heavily on high-quality and diverse\ndata, which serve as the foundation for robust content moderation models.\nSynthetic data has become a common approach for training models across various\nNLP tasks. However, its effectiveness remains uncertain for highly subjective\ntasks like hate speech detection, with previous research yielding mixed\nresults. This study explores the potential of open-source LLMs for harmful data\nsynthesis, utilizing controlled prompting and supervised fine-tuning techniques\nto enhance data quality and diversity. We systematically evaluated 6 open\nsource LLMs on 5 datasets, assessing their ability to generate diverse,\nhigh-quality harmful data while minimizing hallucination and duplication. Our\nresults show that Mistral consistently outperforms other open models, and\nsupervised fine-tuning significantly enhances data reliability and diversity.\nWe further analyze the trade-offs between prompt-based vs. fine-tuned toxic\ndata synthesis, discuss real-world deployment challenges, and highlight ethical\nconsiderations. Our findings demonstrate that fine-tuned open source LLMs\nprovide scalable and cost-effective solutions to augment toxic content\ndetection datasets, paving the way for more accessible and transparent content\nmoderation tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective toxic content detection relies heavily on high-quality and diverse\ndata, which serve as the foundation for robust content moderation models.\nSynthetic data has become a common approach for training models across various\nNLP tasks. However, its effectiveness remains uncertain for highly subjective\ntasks like hate speech detection, with previous research yielding mixed\nresults. This study explores the potential of open-source LLMs for harmful data\nsynthesis, utilizing controlled prompting and supervised fine-tuning techniques\nto enhance data quality and diversity. We systematically evaluated 6 open\nsource LLMs on 5 datasets, assessing their ability to generate diverse,\nhigh-quality harmful data while minimizing hallucination and duplication. Our\nresults show that Mistral consistently outperforms other open models, and\nsupervised fine-tuning significantly enhances data reliability and diversity.\nWe further analyze the trade-offs between prompt-based vs. fine-tuned toxic\ndata synthesis, discuss real-world deployment challenges, and highlight ethical\nconsiderations. Our findings demonstrate that fine-tuned open source LLMs\nprovide scalable and cost-effective solutions to augment toxic content\ndetection datasets, paving the way for more accessible and transparent content\nmoderation tools."
                },
                "authors": [
                    {
                        "name": "Zheng Hui"
                    },
                    {
                        "name": "Zhaoxiao Guo"
                    },
                    {
                        "name": "Hang Zhao"
                    },
                    {
                        "name": "Juanyong Duan"
                    },
                    {
                        "name": "Lin Ai"
                    },
                    {
                        "name": "Yinheng Li"
                    },
                    {
                        "name": "Julia Hirschberg"
                    },
                    {
                        "name": "Congrui Huang"
                    }
                ],
                "author_detail": {
                    "name": "Congrui Huang"
                },
                "author": "Congrui Huang",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15175v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15175v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12940v1",
                "updated": "2025-02-18T15:21:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    21,
                    26,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T15:21:26Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    21,
                    26,
                    1,
                    49,
                    0
                ],
                "title": "Tensor cross interpolation for global discrete optimization with\n  application to Bayesian network inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor cross interpolation for global discrete optimization with\n  application to Bayesian network inference"
                },
                "summary": "Global discrete optimization is notoriously difficult due to the lack of\ngradient information and the curse of dimensionality, making exhaustive search\ninfeasible. Tensor cross approximation is an efficient technique to approximate\nmultivariate tensors (and discretized functions) by tensor product\ndecompositions based on a small number of tensor elements, evaluated on\nadaptively selected fibers of the tensor, that intersect on submatrices of\n(nearly) maximum volume. The submatrices of maximum volume are empirically\nknown to contain large elements, hence the entries selected for cross\ninterpolation can also be good candidates for the globally maximal element\nwithin the tensor. In this paper we consider evolution of epidemics on\nnetworks, and infer the contact network from observations of network nodal\nstates over time. By numerical experiments we demonstrate that the contact\nnetwork can be inferred accurately by finding the global maximum of the\nlikelihood using tensor cross interpolation. The proposed tensor product\napproach is flexible and can be applied to global discrete optimization for\nother problems, e.g. discrete hyperparameter tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Global discrete optimization is notoriously difficult due to the lack of\ngradient information and the curse of dimensionality, making exhaustive search\ninfeasible. Tensor cross approximation is an efficient technique to approximate\nmultivariate tensors (and discretized functions) by tensor product\ndecompositions based on a small number of tensor elements, evaluated on\nadaptively selected fibers of the tensor, that intersect on submatrices of\n(nearly) maximum volume. The submatrices of maximum volume are empirically\nknown to contain large elements, hence the entries selected for cross\ninterpolation can also be good candidates for the globally maximal element\nwithin the tensor. In this paper we consider evolution of epidemics on\nnetworks, and infer the contact network from observations of network nodal\nstates over time. By numerical experiments we demonstrate that the contact\nnetwork can be inferred accurately by finding the global maximum of the\nlikelihood using tensor cross interpolation. The proposed tensor product\napproach is flexible and can be applied to global discrete optimization for\nother problems, e.g. discrete hyperparameter tuning."
                },
                "authors": [
                    {
                        "name": "Sergey Dolgov"
                    },
                    {
                        "name": "Dmitry Savostyanov"
                    }
                ],
                "author_detail": {
                    "name": "Dmitry Savostyanov"
                },
                "author": "Dmitry Savostyanov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "15A69, 34A30, 37N25, 60J28, 62F15, 65F55, 90B15, 95C42",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12935v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12935v1",
                "updated": "2025-02-18T15:16:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    16,
                    18,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T15:16:18Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    16,
                    18,
                    1,
                    49,
                    0
                ],
                "title": "Neuro-oscillatory models of cortical speech processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuro-oscillatory models of cortical speech processing"
                },
                "summary": "In this review, we examine computational models that explore the role of\nneural oscillations in speech perception, spanning from early auditory\nprocessing to higher cognitive stages. We focus on models that use rhythmic\nbrain activities, such as gamma, theta, and delta oscillations, to encode\nphonemes, segment speech into syllables and words, and integrate linguistic\nelements to infer meaning. We analyze the mechanisms underlying these models,\ntheir biological plausibility, and their potential applications in processing\nand understanding speech in real time, a computational feature that is achieved\nby the human brain but not yet implemented in speech recognition models.\nReal-time processing enables dynamic adaptation to incoming speech, allowing\nsystems to handle the rapid and continuous flow of auditory information\nrequired for effective communication, interactive applications, and accurate\nspeech recognition in a variety of real-world settings. While significant\nprogress has been made in modeling the neural basis of speech perception,\nchallenges remain, particularly in accounting for the complexity of semantic\nprocessing and the integration of contextual influences. Moreover, the high\ncomputational demands of biologically realistic models pose practical\ndifficulties for their implementation and analysis. Despite these limitations,\nthese models provide valuable insights into the neural mechanisms of speech\nperception. We conclude by identifying current limitations, proposing future\nresearch directions, and suggesting how these models can be further developed\nto achieve a more comprehensive understanding of speech processing in the human\nbrain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this review, we examine computational models that explore the role of\nneural oscillations in speech perception, spanning from early auditory\nprocessing to higher cognitive stages. We focus on models that use rhythmic\nbrain activities, such as gamma, theta, and delta oscillations, to encode\nphonemes, segment speech into syllables and words, and integrate linguistic\nelements to infer meaning. We analyze the mechanisms underlying these models,\ntheir biological plausibility, and their potential applications in processing\nand understanding speech in real time, a computational feature that is achieved\nby the human brain but not yet implemented in speech recognition models.\nReal-time processing enables dynamic adaptation to incoming speech, allowing\nsystems to handle the rapid and continuous flow of auditory information\nrequired for effective communication, interactive applications, and accurate\nspeech recognition in a variety of real-world settings. While significant\nprogress has been made in modeling the neural basis of speech perception,\nchallenges remain, particularly in accounting for the complexity of semantic\nprocessing and the integration of contextual influences. Moreover, the high\ncomputational demands of biologically realistic models pose practical\ndifficulties for their implementation and analysis. Despite these limitations,\nthese models provide valuable insights into the neural mechanisms of speech\nperception. We conclude by identifying current limitations, proposing future\nresearch directions, and suggesting how these models can be further developed\nto achieve a more comprehensive understanding of speech processing in the human\nbrain."
                },
                "authors": [
                    {
                        "name": "Olesia Dogonasheva"
                    },
                    {
                        "name": "Denis Zakharov"
                    },
                    {
                        "name": "Anne-Lise Giraud"
                    },
                    {
                        "name": "Boris Gutkin"
                    }
                ],
                "author_detail": {
                    "name": "Boris Gutkin"
                },
                "author": "Boris Gutkin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12935v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12935v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12932v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12932v1",
                "updated": "2025-02-18T15:14:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    14,
                    58,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T15:14:58Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    14,
                    58,
                    1,
                    49,
                    0
                ],
                "title": "Synthetic Data Generation for Culturally Nuanced Commonsense Reasoning\n  in Low-Resource Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic Data Generation for Culturally Nuanced Commonsense Reasoning\n  in Low-Resource Languages"
                },
                "summary": "Quantifying reasoning capability in low-resource languages remains a\nchallenge in NLP due to data scarcity and limited access to annotators. While\nLLM-assisted dataset construction has proven useful for medium- and\nhigh-resource languages, its effectiveness in low-resource languages,\nparticularly for commonsense reasoning, is still unclear. In this paper, we\ncompare three dataset creation strategies: (1) LLM-assisted dataset generation,\n(2) machine translation, and (3) human-written data by native speakers, to\nbuild a culturally nuanced story comprehension dataset. We focus on Javanese\nand Sundanese, two major local languages in Indonesia, and evaluate the\neffectiveness of open-weight and closed-weight LLMs in assisting dataset\ncreation through extensive manual validation. To assess the utility of\nsynthetic data, we fine-tune language models on classification and generation\ntasks using this data and evaluate performance on a human-written test set. Our\nfindings indicate that LLM-assisted data creation outperforms machine\ntranslation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying reasoning capability in low-resource languages remains a\nchallenge in NLP due to data scarcity and limited access to annotators. While\nLLM-assisted dataset construction has proven useful for medium- and\nhigh-resource languages, its effectiveness in low-resource languages,\nparticularly for commonsense reasoning, is still unclear. In this paper, we\ncompare three dataset creation strategies: (1) LLM-assisted dataset generation,\n(2) machine translation, and (3) human-written data by native speakers, to\nbuild a culturally nuanced story comprehension dataset. We focus on Javanese\nand Sundanese, two major local languages in Indonesia, and evaluate the\neffectiveness of open-weight and closed-weight LLMs in assisting dataset\ncreation through extensive manual validation. To assess the utility of\nsynthetic data, we fine-tune language models on classification and generation\ntasks using this data and evaluate performance on a human-written test set. Our\nfindings indicate that LLM-assisted data creation outperforms machine\ntranslation."
                },
                "authors": [
                    {
                        "name": "Salsabila Zahirah Pranida"
                    },
                    {
                        "name": "Rifo Ahmad Genadi"
                    },
                    {
                        "name": "Fajri Koto"
                    }
                ],
                "author_detail": {
                    "name": "Fajri Koto"
                },
                "author": "Fajri Koto",
                "arxiv_comment": "18 pages total: 8 pages of main body, 6 pages of appendix. 4 figures\n  in main body, 6 figures in appendix. Submitted to ARR on February 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12932v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12932v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12929v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12929v1",
                "updated": "2025-02-18T15:11:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    11,
                    46,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T15:11:46Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    11,
                    46,
                    1,
                    49,
                    0
                ],
                "title": "Flow-of-Options: Diversified and Improved LLM Reasoning by Thinking\n  Through Options",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow-of-Options: Diversified and Improved LLM Reasoning by Thinking\n  Through Options"
                },
                "summary": "We present a novel reasoning approach called Flow-of-Options (FoO), designed\nto address intrinsic biases in Large Language Models (LLMs). FoO enables LLMs\nto systematically explore a diverse range of possibilities in their reasoning,\nas demonstrated by an FoO-based agentic system for autonomously solving Machine\nLearning tasks (AutoML). Our framework outperforms state-of-the-art baselines,\nachieving improvements of 38.2% - 69.2% on standard data science tasks, and\n37.4% - 47.9% on therapeutic chemistry tasks. With an overall operation cost\nunder $1 per task, our framework is well-suited for cost-sensitive\napplications. Beyond classification and regression, we illustrate the broader\napplicability of our FoO-based agentic system to tasks such as reinforcement\nlearning and image generation. Our framework presents significant advancements\ncompared to current state-of-the-art agentic systems for AutoML, due to the\nbenefits of FoO in enforcing diversity in LLM solutions through compressed,\nexplainable representations that also support long-term memory when combined\nwith case-based reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel reasoning approach called Flow-of-Options (FoO), designed\nto address intrinsic biases in Large Language Models (LLMs). FoO enables LLMs\nto systematically explore a diverse range of possibilities in their reasoning,\nas demonstrated by an FoO-based agentic system for autonomously solving Machine\nLearning tasks (AutoML). Our framework outperforms state-of-the-art baselines,\nachieving improvements of 38.2% - 69.2% on standard data science tasks, and\n37.4% - 47.9% on therapeutic chemistry tasks. With an overall operation cost\nunder $1 per task, our framework is well-suited for cost-sensitive\napplications. Beyond classification and regression, we illustrate the broader\napplicability of our FoO-based agentic system to tasks such as reinforcement\nlearning and image generation. Our framework presents significant advancements\ncompared to current state-of-the-art agentic systems for AutoML, due to the\nbenefits of FoO in enforcing diversity in LLM solutions through compressed,\nexplainable representations that also support long-term memory when combined\nwith case-based reasoning."
                },
                "authors": [
                    {
                        "name": "Lakshmi Nair"
                    },
                    {
                        "name": "Ian Trase"
                    },
                    {
                        "name": "Mark Kim"
                    }
                ],
                "author_detail": {
                    "name": "Mark Kim"
                },
                "author": "Mark Kim",
                "arxiv_comment": "Github code: https://github.com/flagshippioneering/Flow-of-Options",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12929v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12929v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12928v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12928v1",
                "updated": "2025-02-18T15:09:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    9,
                    58,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T15:09:58Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    9,
                    58,
                    1,
                    49,
                    0
                ],
                "title": "Finedeep: Mitigating Sparse Activation in Dense LLMs via Multi-Layer\n  Fine-Grained Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finedeep: Mitigating Sparse Activation in Dense LLMs via Multi-Layer\n  Fine-Grained Experts"
                },
                "summary": "Large language models have demonstrated exceptional performance across a wide\nrange of tasks. However, dense models usually suffer from sparse activation,\nwhere many activation values tend towards zero (i.e., being inactivated). We\nargue that this could restrict the efficient exploration of model\nrepresentation space. To mitigate this issue, we propose Finedeep, a\ndeep-layered fine-grained expert architecture for dense models. Our framework\npartitions the feed-forward neural network layers of traditional dense models\ninto small experts, arranges them across multiple sub-layers. A novel routing\nmechanism is proposed to determine each expert's contribution. We conduct\nextensive experiments across various model sizes, demonstrating that our\napproach significantly outperforms traditional dense architectures in terms of\nperplexity and benchmark performance while maintaining a comparable number of\nparameters and floating-point operations. Moreover, we find that Finedeep\nachieves optimal results when balancing depth and width, specifically by\nadjusting the number of expert sub-layers and the number of experts per\nsub-layer. Empirical results confirm that Finedeep effectively alleviates\nsparse activation and efficiently utilizes representation capacity in dense\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have demonstrated exceptional performance across a wide\nrange of tasks. However, dense models usually suffer from sparse activation,\nwhere many activation values tend towards zero (i.e., being inactivated). We\nargue that this could restrict the efficient exploration of model\nrepresentation space. To mitigate this issue, we propose Finedeep, a\ndeep-layered fine-grained expert architecture for dense models. Our framework\npartitions the feed-forward neural network layers of traditional dense models\ninto small experts, arranges them across multiple sub-layers. A novel routing\nmechanism is proposed to determine each expert's contribution. We conduct\nextensive experiments across various model sizes, demonstrating that our\napproach significantly outperforms traditional dense architectures in terms of\nperplexity and benchmark performance while maintaining a comparable number of\nparameters and floating-point operations. Moreover, we find that Finedeep\nachieves optimal results when balancing depth and width, specifically by\nadjusting the number of expert sub-layers and the number of experts per\nsub-layer. Empirical results confirm that Finedeep effectively alleviates\nsparse activation and efficiently utilizes representation capacity in dense\nmodels."
                },
                "authors": [
                    {
                        "name": "Leiyu Pan"
                    },
                    {
                        "name": "Zhenpeng Su"
                    },
                    {
                        "name": "Minxuan Lv"
                    },
                    {
                        "name": "Yizhe Xiong"
                    },
                    {
                        "name": "Xiangwen Zhang"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    },
                    {
                        "name": "Cheng Luo"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Deyi Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Deyi Xiong"
                },
                "author": "Deyi Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12928v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12928v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12927v1",
                "updated": "2025-02-18T15:09:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    9,
                    29,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T15:09:29Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    9,
                    29,
                    1,
                    49,
                    0
                ],
                "title": "SEFL: Harnessing Large Language Model Agents to Improve Educational\n  Feedback Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEFL: Harnessing Large Language Model Agents to Improve Educational\n  Feedback Systems"
                },
                "summary": "Providing high-quality feedback is crucial for student success but is\nconstrained by time, cost, and limited data availability. We introduce\nSynthetic Educational Feedback Loops (SEFL), a novel framework designed to\ndeliver immediate, on-demand feedback at scale without relying on extensive,\nreal-world student data. In SEFL, two large language models (LLMs) operate in\nteacher--student roles to simulate assignment completion and formative\nfeedback, generating abundant synthetic pairs of student work and corresponding\ncritiques. We then fine-tune smaller, more computationally efficient LLMs on\nthese synthetic pairs, enabling them to replicate key features of high-quality,\ngoal-oriented feedback. Unlike personalized tutoring approaches that offer\nmulti-turn, individualized instruction, SEFL specifically focuses on\nreplicating the teacher-->student feedback loop for diverse assignments.\nThrough both LLM-as-a-judge and human evaluations, we demonstrate that\nSEFL-tuned models outperform their non-tuned counterparts in feedback quality,\nclarity, and timeliness. These findings reveal SEFL's potential to transform\nfeedback processes for higher education and beyond, offering an ethical and\nscalable alternative to conventional manual feedback cycles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Providing high-quality feedback is crucial for student success but is\nconstrained by time, cost, and limited data availability. We introduce\nSynthetic Educational Feedback Loops (SEFL), a novel framework designed to\ndeliver immediate, on-demand feedback at scale without relying on extensive,\nreal-world student data. In SEFL, two large language models (LLMs) operate in\nteacher--student roles to simulate assignment completion and formative\nfeedback, generating abundant synthetic pairs of student work and corresponding\ncritiques. We then fine-tune smaller, more computationally efficient LLMs on\nthese synthetic pairs, enabling them to replicate key features of high-quality,\ngoal-oriented feedback. Unlike personalized tutoring approaches that offer\nmulti-turn, individualized instruction, SEFL specifically focuses on\nreplicating the teacher-->student feedback loop for diverse assignments.\nThrough both LLM-as-a-judge and human evaluations, we demonstrate that\nSEFL-tuned models outperform their non-tuned counterparts in feedback quality,\nclarity, and timeliness. These findings reveal SEFL's potential to transform\nfeedback processes for higher education and beyond, offering an ethical and\nscalable alternative to conventional manual feedback cycles."
                },
                "authors": [
                    {
                        "name": "Mike Zhang"
                    },
                    {
                        "name": "Amalie Pernille Dilling"
                    },
                    {
                        "name": "Lon Gondelman"
                    },
                    {
                        "name": "Niels Erik Ruan Lyngdorf"
                    },
                    {
                        "name": "Euan D. Lindsay"
                    },
                    {
                        "name": "Johannes Bjerva"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Bjerva"
                },
                "author": "Johannes Bjerva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05759v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05759v2",
                "updated": "2025-02-18T15:07:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    7,
                    53,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-09T03:37:06Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    3,
                    37,
                    6,
                    6,
                    40,
                    0
                ],
                "title": "Reinforced Lifelong Editing for Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforced Lifelong Editing for Language Models"
                },
                "summary": "Large language models (LLMs) acquire information from pre-training corpora,\nbut their stored knowledge can become inaccurate or outdated over time. Model\nediting addresses this challenge by modifying model parameters without\nretraining, and prevalent approaches leverage hypernetworks to generate these\nparameter updates. However, they face significant challenges in lifelong\nediting due to their incompatibility with LLM parameters that dynamically\nchange during the editing process. To address this, we observed that\nhypernetwork-based lifelong editing aligns with reinforcement learning modeling\nand proposed RLEdit, an RL-based editing method. By treating editing losses as\nrewards and optimizing hypernetwork parameters at the full knowledge sequence\nlevel, we enable it to precisely capture LLM changes and generate appropriate\nparameter updates. Our extensive empirical evaluation across several LLMs\ndemonstrates that RLEdit outperforms existing methods in lifelong editing with\nsuperior effectiveness and efficiency, achieving a 59.24% improvement while\nrequiring only 2.11% of the time compared to most approaches. Our code is\navailable at: https://github.com/zhrli324/RLEdit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) acquire information from pre-training corpora,\nbut their stored knowledge can become inaccurate or outdated over time. Model\nediting addresses this challenge by modifying model parameters without\nretraining, and prevalent approaches leverage hypernetworks to generate these\nparameter updates. However, they face significant challenges in lifelong\nediting due to their incompatibility with LLM parameters that dynamically\nchange during the editing process. To address this, we observed that\nhypernetwork-based lifelong editing aligns with reinforcement learning modeling\nand proposed RLEdit, an RL-based editing method. By treating editing losses as\nrewards and optimizing hypernetwork parameters at the full knowledge sequence\nlevel, we enable it to precisely capture LLM changes and generate appropriate\nparameter updates. Our extensive empirical evaluation across several LLMs\ndemonstrates that RLEdit outperforms existing methods in lifelong editing with\nsuperior effectiveness and efficiency, achieving a 59.24% improvement while\nrequiring only 2.11% of the time compared to most approaches. Our code is\navailable at: https://github.com/zhrli324/RLEdit."
                },
                "authors": [
                    {
                        "name": "Zherui Li"
                    },
                    {
                        "name": "Houcheng Jiang"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Baolong Bi"
                    },
                    {
                        "name": "Zhenhong Zhou"
                    },
                    {
                        "name": "Fei Sun"
                    },
                    {
                        "name": "Junfeng Fang"
                    },
                    {
                        "name": "Xiang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Wang"
                },
                "author": "Xiang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05759v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05759v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12926v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12926v1",
                "updated": "2025-02-18T15:07:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    7,
                    6,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T15:07:06Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    7,
                    6,
                    1,
                    49,
                    0
                ],
                "title": "Towards more Contextual Agents: An extractor-Generator Optimization\n  Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards more Contextual Agents: An extractor-Generator Optimization\n  Framework"
                },
                "summary": "Large Language Model (LLM)-based agents have demonstrated remarkable success\nin solving complex tasks across a wide range of general-purpose applications.\nHowever, their performance often degrades in context-specific scenarios, such\nas specialized industries or research domains, where the absence of\ndomain-relevant knowledge leads to imprecise or suboptimal outcomes. To address\nthis challenge, our work introduces a systematic approach to enhance the\ncontextual adaptability of LLM-based agents by optimizing their underlying\nprompts-critical components that govern agent behavior, roles, and\ninteractions. Manually crafting optimized prompts for context-specific tasks is\nlabor-intensive, error-prone, and lacks scalability. In this work, we introduce\nan Extractor-Generator framework designed to automate the optimization of\ncontextual LLM-based agents. Our method operates through two key stages: (i)\nfeature extraction from a dataset of gold-standard input-output examples, and\n(ii) prompt generation via a high-level optimization strategy that iteratively\nidentifies underperforming cases and applies self-improvement techniques. This\nframework substantially improves prompt adaptability by enabling more precise\ngeneralization across diverse inputs, particularly in context-specific tasks\nwhere maintaining semantic consistency and minimizing error propagation are\ncritical for reliable performance. Although developed with single-stage\nworkflows in mind, the approach naturally extends to multi-stage workflows,\noffering broad applicability across various agent-based systems. Empirical\nevaluations demonstrate that our framework significantly enhances the\nperformance of prompt-optimized agents, providing a structured and efficient\napproach to contextual LLM-based agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based agents have demonstrated remarkable success\nin solving complex tasks across a wide range of general-purpose applications.\nHowever, their performance often degrades in context-specific scenarios, such\nas specialized industries or research domains, where the absence of\ndomain-relevant knowledge leads to imprecise or suboptimal outcomes. To address\nthis challenge, our work introduces a systematic approach to enhance the\ncontextual adaptability of LLM-based agents by optimizing their underlying\nprompts-critical components that govern agent behavior, roles, and\ninteractions. Manually crafting optimized prompts for context-specific tasks is\nlabor-intensive, error-prone, and lacks scalability. In this work, we introduce\nan Extractor-Generator framework designed to automate the optimization of\ncontextual LLM-based agents. Our method operates through two key stages: (i)\nfeature extraction from a dataset of gold-standard input-output examples, and\n(ii) prompt generation via a high-level optimization strategy that iteratively\nidentifies underperforming cases and applies self-improvement techniques. This\nframework substantially improves prompt adaptability by enabling more precise\ngeneralization across diverse inputs, particularly in context-specific tasks\nwhere maintaining semantic consistency and minimizing error propagation are\ncritical for reliable performance. Although developed with single-stage\nworkflows in mind, the approach naturally extends to multi-stage workflows,\noffering broad applicability across various agent-based systems. Empirical\nevaluations demonstrate that our framework significantly enhances the\nperformance of prompt-optimized agents, providing a structured and efficient\napproach to contextual LLM-based agents."
                },
                "authors": [
                    {
                        "name": "Mourad Aouini"
                    },
                    {
                        "name": "Jinan Loubani"
                    }
                ],
                "author_detail": {
                    "name": "Jinan Loubani"
                },
                "author": "Jinan Loubani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12926v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12926v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12924v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12924v1",
                "updated": "2025-02-18T15:04:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    4,
                    13,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T15:04:13Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    4,
                    13,
                    1,
                    49,
                    0
                ],
                "title": "Conditioning LLMs to Generate Code-Switched Text: A Methodology Grounded\n  in Naturally Occurring Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conditioning LLMs to Generate Code-Switched Text: A Methodology Grounded\n  in Naturally Occurring Data"
                },
                "summary": "Code-switching (CS) is still a critical challenge in Natural Language\nProcessing (NLP). Current Large Language Models (LLMs) struggle to interpret\nand generate code-switched text, primarily due to the scarcity of large-scale\nCS datasets for training. This paper presents a novel methodology to generate\nCS data using LLMs, and test it on the English-Spanish language pair. We\npropose back-translating natural CS sentences into monolingual English, and\nusing the resulting parallel corpus to fine-tune LLMs to turn monolingual\nsentences into CS. Unlike previous approaches to CS generation, our methodology\nuses natural CS data as a starting point, allowing models to learn its natural\ndistribution beyond grammatical patterns. We thoroughly analyse the models'\nperformance through a study on human preferences, a qualitative error analysis\nand an evaluation with popular automatic metrics. Results show that our\nmethodology generates fluent code-switched text, expanding research\nopportunities in CS communication, and that traditional metrics do not\ncorrelate with human judgement when assessing the quality of the generated CS\ndata. We release our code and generated dataset under a CC-BY-NC-SA license.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code-switching (CS) is still a critical challenge in Natural Language\nProcessing (NLP). Current Large Language Models (LLMs) struggle to interpret\nand generate code-switched text, primarily due to the scarcity of large-scale\nCS datasets for training. This paper presents a novel methodology to generate\nCS data using LLMs, and test it on the English-Spanish language pair. We\npropose back-translating natural CS sentences into monolingual English, and\nusing the resulting parallel corpus to fine-tune LLMs to turn monolingual\nsentences into CS. Unlike previous approaches to CS generation, our methodology\nuses natural CS data as a starting point, allowing models to learn its natural\ndistribution beyond grammatical patterns. We thoroughly analyse the models'\nperformance through a study on human preferences, a qualitative error analysis\nand an evaluation with popular automatic metrics. Results show that our\nmethodology generates fluent code-switched text, expanding research\nopportunities in CS communication, and that traditional metrics do not\ncorrelate with human judgement when assessing the quality of the generated CS\ndata. We release our code and generated dataset under a CC-BY-NC-SA license."
                },
                "authors": [
                    {
                        "name": "Maite Heredia"
                    },
                    {
                        "name": "Gorka Labaka"
                    },
                    {
                        "name": "Jeremy Barnes"
                    },
                    {
                        "name": "Aitor Soroa"
                    }
                ],
                "author_detail": {
                    "name": "Aitor Soroa"
                },
                "author": "Aitor Soroa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12924v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12924v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12923v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12923v1",
                "updated": "2025-02-18T15:03:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    3,
                    17,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T15:03:17Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    3,
                    17,
                    1,
                    49,
                    0
                ],
                "title": "On-Device LLMs for Home Assistant: Dual Role in Intent Detection and\n  Response Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-Device LLMs for Home Assistant: Dual Role in Intent Detection and\n  Response Generation"
                },
                "summary": "This paper investigates whether Large Language Models (LLMs), fine-tuned on\nsynthetic but domain-representative data, can perform the twofold task of (i)\nslot and intent detection and (ii) natural language response generation for a\nsmart home assistant, while running solely on resource-limited, CPU-only edge\nhardware. We fine-tune LLMs to produce both JSON action calls and text\nresponses. Our experiments show that 16-bit and 8-bit quantized variants\npreserve high accuracy on slot and intent detection and maintain strong\nsemantic coherence in generated text, while the 4-bit model, while retaining\ngenerative fluency, suffers a noticeable drop in device-service classification\naccuracy. Further evaluations on noisy human (non-synthetic) prompts and\nout-of-domain intents confirm the models' generalization ability, obtaining\naround 80--86\\% accuracy. While the average inference time is 5--6 seconds per\nquery -- acceptable for one-shot commands but suboptimal for multi-turn\ndialogue -- our results affirm that an on-device LLM can effectively unify\ncommand interpretation and flexible response generation for home automation\nwithout relying on specialized hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates whether Large Language Models (LLMs), fine-tuned on\nsynthetic but domain-representative data, can perform the twofold task of (i)\nslot and intent detection and (ii) natural language response generation for a\nsmart home assistant, while running solely on resource-limited, CPU-only edge\nhardware. We fine-tune LLMs to produce both JSON action calls and text\nresponses. Our experiments show that 16-bit and 8-bit quantized variants\npreserve high accuracy on slot and intent detection and maintain strong\nsemantic coherence in generated text, while the 4-bit model, while retaining\ngenerative fluency, suffers a noticeable drop in device-service classification\naccuracy. Further evaluations on noisy human (non-synthetic) prompts and\nout-of-domain intents confirm the models' generalization ability, obtaining\naround 80--86\\% accuracy. While the average inference time is 5--6 seconds per\nquery -- acceptable for one-shot commands but suboptimal for multi-turn\ndialogue -- our results affirm that an on-device LLM can effectively unify\ncommand interpretation and flexible response generation for home automation\nwithout relying on specialized hardware."
                },
                "authors": [
                    {
                        "name": "Rune Birkmose"
                    },
                    {
                        "name": "Nathan Mrkeberg Reece"
                    },
                    {
                        "name": "Esben Hofstedt Norvin"
                    },
                    {
                        "name": "Johannes Bjerva"
                    },
                    {
                        "name": "Mike Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zhang"
                },
                "author": "Mike Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12923v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12921v1",
                "updated": "2025-02-18T15:01:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    1,
                    30,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T15:01:30Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    1,
                    30,
                    1,
                    49,
                    0
                ],
                "title": "Q-STRUM Debate: Query-Driven Contrastive Summarization for\n  Recommendation Comparison",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q-STRUM Debate: Query-Driven Contrastive Summarization for\n  Recommendation Comparison"
                },
                "summary": "Query-driven recommendation with unknown items poses a challenge for users to\nunderstand why certain items are appropriate for their needs. Query-driven\nContrastive Summarization (QCS) is a methodology designed to address this issue\nby leveraging language-based item descriptions to clarify contrasts between\nthem. However, existing state-of-the-art contrastive summarization methods such\nas STRUM-LLM fall short of this goal. To overcome these limitations, we\nintroduce Q-STRUM Debate, a novel extension of STRUM-LLM that employs\ndebate-style prompting to generate focused and contrastive summarizations of\nitem aspects relevant to a query. Leveraging modern large language models\n(LLMs) as powerful tools for generating debates, Q-STRUM Debate provides\nenhanced contrastive summaries. Experiments across three datasets demonstrate\nthat Q-STRUM Debate yields significant performance improvements over existing\nmethods on key contrastive summarization criteria, thus introducing a novel and\nperformant debate prompting methodology for QCS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query-driven recommendation with unknown items poses a challenge for users to\nunderstand why certain items are appropriate for their needs. Query-driven\nContrastive Summarization (QCS) is a methodology designed to address this issue\nby leveraging language-based item descriptions to clarify contrasts between\nthem. However, existing state-of-the-art contrastive summarization methods such\nas STRUM-LLM fall short of this goal. To overcome these limitations, we\nintroduce Q-STRUM Debate, a novel extension of STRUM-LLM that employs\ndebate-style prompting to generate focused and contrastive summarizations of\nitem aspects relevant to a query. Leveraging modern large language models\n(LLMs) as powerful tools for generating debates, Q-STRUM Debate provides\nenhanced contrastive summaries. Experiments across three datasets demonstrate\nthat Q-STRUM Debate yields significant performance improvements over existing\nmethods on key contrastive summarization criteria, thus introducing a novel and\nperformant debate prompting methodology for QCS."
                },
                "authors": [
                    {
                        "name": "George-Kirollos Saad"
                    },
                    {
                        "name": "Scott Sanner"
                    }
                ],
                "author_detail": {
                    "name": "Scott Sanner"
                },
                "author": "Scott Sanner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12918v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12918v1",
                "updated": "2025-02-18T14:59:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    59,
                    37,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T14:59:37Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    59,
                    37,
                    1,
                    49,
                    0
                ],
                "title": "Query Rewriting via LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query Rewriting via LLMs"
                },
                "summary": "Query rewriting is a classical technique for transforming complex declarative\nSQL queries into ``lean'' equivalents that are conducive to (a) faster\nexecution from a performance perspective, and (b) better understanding from a\ndeveloper perspective. The rewriting is typically achieved via transformation\nrules, but these rules are limited in scope and difficult to update in a\nproduction system. In recent times, LLM-based techniques have also been mooted,\nbut they are prone to both semantic and syntactic errors.\n  We investigate here, how the remarkable cognitive capabilities of LLMs can be\nleveraged for performant query rewriting while incorporating safeguards and\noptimizations to ensure correctness and efficiency. Our study shows that these\ngoals can be progressively achieved through incorporation of (a) an ensemble\nsuite of basic prompts, (b) database-sensitive prompts via redundancy removal\nand selectivity-based rewriting rules, and (c) LLM token probability-guided\nrewrite paths. Further, a suite of statistical and logic-based tools can be\nused to guard against errors produced by the model.\n  We have implemented the above LLM-infused techniques in the LITHE system, and\nevaluated complex analytic queries from multiple benchmarks on contemporary\ndatabase platforms. The results show significant improvements over SOTA\nrewriting techniques -- for instance, on TPC-DS, LITHE constructed productive\n(>1.5x speedup) rewrites for \\emph{two-thirds} of the query suite, delivering\nfour times more coverage than SOTA. Further, the geometric mean of its\nestimated execution speedups was an \\emph{order-of-magnitude} jump over SOTA\nperformance. In essence, LITHE offers a potent and robust LLM-based\nintermediary between enterprise applications and database engines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query rewriting is a classical technique for transforming complex declarative\nSQL queries into ``lean'' equivalents that are conducive to (a) faster\nexecution from a performance perspective, and (b) better understanding from a\ndeveloper perspective. The rewriting is typically achieved via transformation\nrules, but these rules are limited in scope and difficult to update in a\nproduction system. In recent times, LLM-based techniques have also been mooted,\nbut they are prone to both semantic and syntactic errors.\n  We investigate here, how the remarkable cognitive capabilities of LLMs can be\nleveraged for performant query rewriting while incorporating safeguards and\noptimizations to ensure correctness and efficiency. Our study shows that these\ngoals can be progressively achieved through incorporation of (a) an ensemble\nsuite of basic prompts, (b) database-sensitive prompts via redundancy removal\nand selectivity-based rewriting rules, and (c) LLM token probability-guided\nrewrite paths. Further, a suite of statistical and logic-based tools can be\nused to guard against errors produced by the model.\n  We have implemented the above LLM-infused techniques in the LITHE system, and\nevaluated complex analytic queries from multiple benchmarks on contemporary\ndatabase platforms. The results show significant improvements over SOTA\nrewriting techniques -- for instance, on TPC-DS, LITHE constructed productive\n(>1.5x speedup) rewrites for \\emph{two-thirds} of the query suite, delivering\nfour times more coverage than SOTA. Further, the geometric mean of its\nestimated execution speedups was an \\emph{order-of-magnitude} jump over SOTA\nperformance. In essence, LITHE offers a potent and robust LLM-based\nintermediary between enterprise applications and database engines."
                },
                "authors": [
                    {
                        "name": "Sriram Dharwada"
                    },
                    {
                        "name": "Himanshu Devrani"
                    },
                    {
                        "name": "Jayant Haritsa"
                    },
                    {
                        "name": "Harish Doraiswamy"
                    }
                ],
                "author_detail": {
                    "name": "Harish Doraiswamy"
                },
                "author": "Harish Doraiswamy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12918v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12918v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12913v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12913v1",
                "updated": "2025-02-18T14:54:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    54,
                    55,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T14:54:55Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    54,
                    55,
                    1,
                    49,
                    0
                ],
                "title": "GSQ-Tuning: Group-Shared Exponents Integer in Fully Quantized Training\n  for LLMs On-Device Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GSQ-Tuning: Group-Shared Exponents Integer in Fully Quantized Training\n  for LLMs On-Device Fine-tuning"
                },
                "summary": "Large Language Models (LLMs) fine-tuning technologies have achieved\nremarkable results. However, traditional LLM fine-tuning approaches face\nsignificant challenges: they require large Floating Point (FP) computation,\nraising privacy concerns when handling sensitive data, and are impractical for\nresource-constrained edge devices. While Parameter-Efficient Fine-Tuning (PEFT)\ntechniques reduce trainable parameters, their reliance on floating-point\narithmetic creates fundamental incompatibilities with edge hardware. In this\nwork, we introduce a novel framework for on-device LLM fine-tuning that\neliminates the need for floating-point operations in both inference and\ntraining, named GSQ-Tuning. At its core is the Group-Shared Exponents Integer\nformat, which efficiently represents model parameters in integer format using\nshared exponents among parameter groups. When combined with LoRA-like adapters,\nthis enables fully integer-based fine-tuning that is both memory and compute\nefficient. We demonstrate that our approach achieves accuracy comparable to\nFP16-based fine-tuning while significantly reducing memory usage (50%).\nMoreover, compared to FP8, our method can reduce 5x power consumption and 11x\nchip area with same performance, making large-scale model adaptation feasible\non edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) fine-tuning technologies have achieved\nremarkable results. However, traditional LLM fine-tuning approaches face\nsignificant challenges: they require large Floating Point (FP) computation,\nraising privacy concerns when handling sensitive data, and are impractical for\nresource-constrained edge devices. While Parameter-Efficient Fine-Tuning (PEFT)\ntechniques reduce trainable parameters, their reliance on floating-point\narithmetic creates fundamental incompatibilities with edge hardware. In this\nwork, we introduce a novel framework for on-device LLM fine-tuning that\neliminates the need for floating-point operations in both inference and\ntraining, named GSQ-Tuning. At its core is the Group-Shared Exponents Integer\nformat, which efficiently represents model parameters in integer format using\nshared exponents among parameter groups. When combined with LoRA-like adapters,\nthis enables fully integer-based fine-tuning that is both memory and compute\nefficient. We demonstrate that our approach achieves accuracy comparable to\nFP16-based fine-tuning while significantly reducing memory usage (50%).\nMoreover, compared to FP8, our method can reduce 5x power consumption and 11x\nchip area with same performance, making large-scale model adaptation feasible\non edge devices."
                },
                "authors": [
                    {
                        "name": "Sifan Zhou"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Mingjia Shi"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Dawei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Yang"
                },
                "author": "Dawei Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12913v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12913v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17020v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17020v2",
                "updated": "2025-02-18T14:54:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    54,
                    0,
                    1,
                    49,
                    0
                ],
                "published": "2024-09-25T15:23:46Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    15,
                    23,
                    46,
                    2,
                    269,
                    0
                ],
                "title": "PTQ4RIS: Post-Training Quantization for Referring Image Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PTQ4RIS: Post-Training Quantization for Referring Image Segmentation"
                },
                "summary": "Referring Image Segmentation (RIS), aims to segment the object referred by a\ngiven sentence in an image by understanding both visual and linguistic\ninformation. However, existing RIS methods tend to explore top-performance\nmodels, disregarding considerations for practical applications on\nresources-limited edge devices. This oversight poses a significant challenge\nfor on-device RIS inference. To this end, we propose an effective and efficient\npost-training quantization framework termed PTQ4RIS. Specifically, we first\nconduct an in-depth analysis of the root causes of performance degradation in\nRIS model quantization and propose dual-region quantization (DRQ) and\nreorder-based outlier-retained quantization (RORQ) to address the quantization\ndifficulties in visual and text encoders. Extensive experiments on three\nbenchmarks with different bits settings (from 8 to 4 bits) demonstrates its\nsuperior performance. Importantly, we are the first PTQ method specifically\ndesigned for the RIS task, highlighting the feasibility of PTQ in RIS\napplications. Code and video are available at\n{https://github.com/gugu511yy/PTQ4RIS}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Referring Image Segmentation (RIS), aims to segment the object referred by a\ngiven sentence in an image by understanding both visual and linguistic\ninformation. However, existing RIS methods tend to explore top-performance\nmodels, disregarding considerations for practical applications on\nresources-limited edge devices. This oversight poses a significant challenge\nfor on-device RIS inference. To this end, we propose an effective and efficient\npost-training quantization framework termed PTQ4RIS. Specifically, we first\nconduct an in-depth analysis of the root causes of performance degradation in\nRIS model quantization and propose dual-region quantization (DRQ) and\nreorder-based outlier-retained quantization (RORQ) to address the quantization\ndifficulties in visual and text encoders. Extensive experiments on three\nbenchmarks with different bits settings (from 8 to 4 bits) demonstrates its\nsuperior performance. Importantly, we are the first PTQ method specifically\ndesigned for the RIS task, highlighting the feasibility of PTQ in RIS\napplications. Code and video are available at\n{https://github.com/gugu511yy/PTQ4RIS}."
                },
                "authors": [
                    {
                        "name": "Xiaoyan Jiang"
                    },
                    {
                        "name": "Hang Yang"
                    },
                    {
                        "name": "Kaiying Zhu"
                    },
                    {
                        "name": "Xihe Qiu"
                    },
                    {
                        "name": "Shibo Zhao"
                    },
                    {
                        "name": "Sifan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Sifan Zhou"
                },
                "author": "Sifan Zhou",
                "arxiv_comment": "Accepted by ICRA 2025.(Update the code link.)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17020v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17020v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12911v1",
                "updated": "2025-02-18T14:53:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    53,
                    45,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T14:53:45Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    53,
                    45,
                    1,
                    49,
                    0
                ],
                "title": "Knapsack Optimization-based Schema Linking for LLM-based Text-to-SQL\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knapsack Optimization-based Schema Linking for LLM-based Text-to-SQL\n  Generation"
                },
                "summary": "Generating SQLs from user queries is a long-standing challenge, where the\naccuracy of initial schema linking significantly impacts subsequent SQL\ngeneration performance. However, current schema linking models still struggle\nwith missing relevant schema elements or an excess of redundant ones. A crucial\nreason for this is that commonly used metrics, recall and precision, fail to\ncapture relevant element missing and thus cannot reflect actual schema linking\nperformance. Motivated by this, we propose an enhanced schema linking metric by\nintroducing a restricted missing indicator. Accordingly, we introduce Knapsack\noptimization-based Schema Linking Agent (KaSLA), a plug-in schema linking agent\ndesigned to prevent the missing of relevant schema elements while minimizing\nthe inclusion of redundant ones. KaSLA employs a hierarchical linking strategy\nthat first identifies the optimal table linking and subsequently links columns\nwithin the selected table to reduce linking candidate space. In each linking\nprocess, it utilize a knapsack optimization approach to link potentially\nrelevant elements while accounting for a limited tolerance of potential\nredundant ones.With this optimization, KaSLA-1.6B achieves superior schema\nlinking results compared to large-scale LLMs, including deepseek-v3 with\nstate-of-the-art (SOTA) schema linking method. Extensive experiments on Spider\nand BIRD benchmarks verify that KaSLA can significantly improve the SQL\ngeneration performance of SOTA text-to-SQL models by substituting their schema\nlinking processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating SQLs from user queries is a long-standing challenge, where the\naccuracy of initial schema linking significantly impacts subsequent SQL\ngeneration performance. However, current schema linking models still struggle\nwith missing relevant schema elements or an excess of redundant ones. A crucial\nreason for this is that commonly used metrics, recall and precision, fail to\ncapture relevant element missing and thus cannot reflect actual schema linking\nperformance. Motivated by this, we propose an enhanced schema linking metric by\nintroducing a restricted missing indicator. Accordingly, we introduce Knapsack\noptimization-based Schema Linking Agent (KaSLA), a plug-in schema linking agent\ndesigned to prevent the missing of relevant schema elements while minimizing\nthe inclusion of redundant ones. KaSLA employs a hierarchical linking strategy\nthat first identifies the optimal table linking and subsequently links columns\nwithin the selected table to reduce linking candidate space. In each linking\nprocess, it utilize a knapsack optimization approach to link potentially\nrelevant elements while accounting for a limited tolerance of potential\nredundant ones.With this optimization, KaSLA-1.6B achieves superior schema\nlinking results compared to large-scale LLMs, including deepseek-v3 with\nstate-of-the-art (SOTA) schema linking method. Extensive experiments on Spider\nand BIRD benchmarks verify that KaSLA can significantly improve the SQL\ngeneration performance of SOTA text-to-SQL models by substituting their schema\nlinking processes."
                },
                "authors": [
                    {
                        "name": "Zheng Yuan"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Zijin Hong"
                    },
                    {
                        "name": "Qinggang Zhang"
                    },
                    {
                        "name": "Feiran Huang"
                    },
                    {
                        "name": "Xiao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Huang"
                },
                "author": "Xiao Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.15334v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.15334v3",
                "updated": "2025-02-18T14:49:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    49,
                    52,
                    1,
                    49,
                    0
                ],
                "published": "2023-08-29T14:29:57Z",
                "published_parsed": [
                    2023,
                    8,
                    29,
                    14,
                    29,
                    57,
                    1,
                    241,
                    0
                ],
                "title": "The Responsible Development of Automated Student Feedback with\n  Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Responsible Development of Automated Student Feedback with\n  Generative AI"
                },
                "summary": "Providing rich, constructive feedback to students is essential for supporting\nand enhancing their learning. Recent advancements in Generative Artificial\nIntelligence (AI), particularly with large language models (LLMs), present new\nopportunities to deliver scalable, repeatable, and instant feedback,\neffectively making abundant a resource that has historically been scarce and\ncostly. From a technical perspective, this approach is now feasible due to\nbreakthroughs in AI and Natural Language Processing (NLP). While the potential\neducational benefits are compelling, implementing these technologies also\nintroduces a host of ethical considerations that must be thoughtfully\naddressed. One of the core advantages of AI systems is their ability to\nautomate routine and mundane tasks, potentially freeing up human educators for\nmore nuanced work. However, the ease of automation risks a ``tyranny of the\nmajority'', where the diverse needs of minority or unique learners are\noverlooked, as they may be harder to systematize and less straightforward to\naccommodate. Ensuring inclusivity and equity in AI-generated feedback,\ntherefore, becomes a critical aspect of responsible AI implementation in\neducation. The process of developing machine learning models that produce\nvaluable, personalized, and authentic feedback also requires significant input\nfrom human domain experts. Decisions around whose expertise is incorporated,\nhow it is captured, and when it is applied have profound implications for the\nrelevance and quality of the resulting feedback. Additionally, the maintenance\nand continuous refinement of these models are necessary to adapt feedback to\nevolving contextual, theoretical, and student-related factors. Without ongoing\nadaptation, feedback risks becoming obsolete or mismatched with the current\nneeds of diverse student populations [...]",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Providing rich, constructive feedback to students is essential for supporting\nand enhancing their learning. Recent advancements in Generative Artificial\nIntelligence (AI), particularly with large language models (LLMs), present new\nopportunities to deliver scalable, repeatable, and instant feedback,\neffectively making abundant a resource that has historically been scarce and\ncostly. From a technical perspective, this approach is now feasible due to\nbreakthroughs in AI and Natural Language Processing (NLP). While the potential\neducational benefits are compelling, implementing these technologies also\nintroduces a host of ethical considerations that must be thoughtfully\naddressed. One of the core advantages of AI systems is their ability to\nautomate routine and mundane tasks, potentially freeing up human educators for\nmore nuanced work. However, the ease of automation risks a ``tyranny of the\nmajority'', where the diverse needs of minority or unique learners are\noverlooked, as they may be harder to systematize and less straightforward to\naccommodate. Ensuring inclusivity and equity in AI-generated feedback,\ntherefore, becomes a critical aspect of responsible AI implementation in\neducation. The process of developing machine learning models that produce\nvaluable, personalized, and authentic feedback also requires significant input\nfrom human domain experts. Decisions around whose expertise is incorporated,\nhow it is captured, and when it is applied have profound implications for the\nrelevance and quality of the resulting feedback. Additionally, the maintenance\nand continuous refinement of these models are necessary to adapt feedback to\nevolving contextual, theoretical, and student-related factors. Without ongoing\nadaptation, feedback risks becoming obsolete or mismatched with the current\nneeds of diverse student populations [...]"
                },
                "authors": [
                    {
                        "name": "Euan D Lindsay"
                    },
                    {
                        "name": "Mike Zhang"
                    },
                    {
                        "name": "Aditya Johri"
                    },
                    {
                        "name": "Johannes Bjerva"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Bjerva"
                },
                "author": "Johannes Bjerva",
                "arxiv_comment": "Pre-print of version accepted to EDUCON 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.15334v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.15334v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12904v1",
                "updated": "2025-02-18T14:47:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    47,
                    2,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T14:47:02Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    47,
                    2,
                    1,
                    49,
                    0
                ],
                "title": "Fraud-R1 : A Multi-Round Benchmark for Assessing the Robustness of LLM\n  Against Augmented Fraud and Phishing Inducements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fraud-R1 : A Multi-Round Benchmark for Assessing the Robustness of LLM\n  Against Augmented Fraud and Phishing Inducements"
                },
                "summary": "We introduce Fraud-R1, a benchmark designed to evaluate LLMs' ability to\ndefend against internet fraud and phishing in dynamic, real-world scenarios.\nFraud-R1 comprises 8,564 fraud cases sourced from phishing scams, fake job\npostings, social media, and news, categorized into 5 major fraud types. Unlike\nprevious benchmarks, Fraud-R1 introduces a multi-round evaluation pipeline to\nassess LLMs' resistance to fraud at different stages, including credibility\nbuilding, urgency creation, and emotional manipulation. Furthermore, we\nevaluate 15 LLMs under two settings: 1. Helpful-Assistant, where the LLM\nprovides general decision-making assistance, and 2. Role-play, where the model\nassumes a specific persona, widely used in real-world agent-based interactions.\nOur evaluation reveals the significant challenges in defending against fraud\nand phishing inducement, especially in role-play settings and fake job\npostings. Additionally, we observe a substantial performance gap between\nChinese and English, underscoring the need for improved multilingual fraud\ndetection capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Fraud-R1, a benchmark designed to evaluate LLMs' ability to\ndefend against internet fraud and phishing in dynamic, real-world scenarios.\nFraud-R1 comprises 8,564 fraud cases sourced from phishing scams, fake job\npostings, social media, and news, categorized into 5 major fraud types. Unlike\nprevious benchmarks, Fraud-R1 introduces a multi-round evaluation pipeline to\nassess LLMs' resistance to fraud at different stages, including credibility\nbuilding, urgency creation, and emotional manipulation. Furthermore, we\nevaluate 15 LLMs under two settings: 1. Helpful-Assistant, where the LLM\nprovides general decision-making assistance, and 2. Role-play, where the model\nassumes a specific persona, widely used in real-world agent-based interactions.\nOur evaluation reveals the significant challenges in defending against fraud\nand phishing inducement, especially in role-play settings and fake job\npostings. Additionally, we observe a substantial performance gap between\nChinese and English, underscoring the need for improved multilingual fraud\ndetection capabilities."
                },
                "authors": [
                    {
                        "name": "Shu Yang"
                    },
                    {
                        "name": "Shenzhe Zhu"
                    },
                    {
                        "name": "Zeyu Wu"
                    },
                    {
                        "name": "Keyu Wang"
                    },
                    {
                        "name": "Junchi Yao"
                    },
                    {
                        "name": "Junchao Wu"
                    },
                    {
                        "name": "Lijie Hu"
                    },
                    {
                        "name": "Mengdi Li"
                    },
                    {
                        "name": "Derek F. Wong"
                    },
                    {
                        "name": "Di Wang"
                    }
                ],
                "author_detail": {
                    "name": "Di Wang"
                },
                "author": "Di Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12900v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12900v1",
                "updated": "2025-02-18T14:36:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    36,
                    39,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T14:36:39Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    36,
                    39,
                    1,
                    49,
                    0
                ],
                "title": "Soundwave: Less is More for Speech-Text Alignment in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soundwave: Less is More for Speech-Text Alignment in LLMs"
                },
                "summary": "Existing end-to-end speech large language models (LLMs) usually rely on\nlarge-scale annotated data for training, while data-efficient training has not\nbeen discussed in depth. We focus on two fundamental problems between speech\nand text: the representation space gap and sequence length inconsistency. We\npropose Soundwave, which utilizes an efficient training strategy and a novel\narchitecture to address these issues. Results show that Soundwave outperforms\nthe advanced Qwen2-Audio in speech translation and AIR-Bench speech tasks,\nusing only one-fiftieth of the training data. Further analysis shows that\nSoundwave still retains its intelligence during conversation. The project is\navailable at https://github.com/FreedomIntelligence/Soundwave.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing end-to-end speech large language models (LLMs) usually rely on\nlarge-scale annotated data for training, while data-efficient training has not\nbeen discussed in depth. We focus on two fundamental problems between speech\nand text: the representation space gap and sequence length inconsistency. We\npropose Soundwave, which utilizes an efficient training strategy and a novel\narchitecture to address these issues. Results show that Soundwave outperforms\nthe advanced Qwen2-Audio in speech translation and AIR-Bench speech tasks,\nusing only one-fiftieth of the training data. Further analysis shows that\nSoundwave still retains its intelligence during conversation. The project is\navailable at https://github.com/FreedomIntelligence/Soundwave."
                },
                "authors": [
                    {
                        "name": "Yuhao Zhang"
                    },
                    {
                        "name": "Zhiheng Liu"
                    },
                    {
                        "name": "Fan Bu"
                    },
                    {
                        "name": "Ruiyu Zhang"
                    },
                    {
                        "name": "Benyou Wang"
                    },
                    {
                        "name": "Haizhou Li"
                    }
                ],
                "author_detail": {
                    "name": "Haizhou Li"
                },
                "author": "Haizhou Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12900v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12900v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12896v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12896v1",
                "updated": "2025-02-18T14:32:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    32,
                    44,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T14:32:44Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    32,
                    44,
                    1,
                    49,
                    0
                ],
                "title": "None of the Others: a General Technique to Distinguish Reasoning from\n  Memorization in Multiple-Choice LLM Evaluation Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "None of the Others: a General Technique to Distinguish Reasoning from\n  Memorization in Multiple-Choice LLM Evaluation Benchmarks"
                },
                "summary": "In LLM evaluations, reasoning is often distinguished from recall/memorization\nby performing numerical variations to math-oriented questions. Here we\nintroduce a general variation method for multiple-choice questions that\ncompletely dissociates the correct answer from previously seen tokens or\nconcepts, requiring LLMs to understand and reason (rather than memorizing) in\norder to answer correctly. Using this method, we evaluate state-of-the-art\nproprietary and open-source LLMs on two datasets available in English and\nSpanish: the public MMLU benchmark and the private UNED-Access 2024 dataset.\nResults show that all models experience remarkable accuracy drops under our\nproposed variation, with an average loss of 57% on MMLU and 50% on UNED-Access\n2024, ranging from 10% to 93% across models. Notably, the most accurate model\nin our experimentation (OpenAI-o3-mini) is not the most robust\n(DeepSeek-R1-70B), suggesting that the best models in standard evaluations may\nnot be the ones with better reasoning capabilities. Also, we see larger\naccuracy drops in public (vs private) datasets and questions posed in their\noriginal language (vs a manual translation), which are signs of contamination\nand also point to a relevant role of recall/memorization in current LLMs'\nanswers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In LLM evaluations, reasoning is often distinguished from recall/memorization\nby performing numerical variations to math-oriented questions. Here we\nintroduce a general variation method for multiple-choice questions that\ncompletely dissociates the correct answer from previously seen tokens or\nconcepts, requiring LLMs to understand and reason (rather than memorizing) in\norder to answer correctly. Using this method, we evaluate state-of-the-art\nproprietary and open-source LLMs on two datasets available in English and\nSpanish: the public MMLU benchmark and the private UNED-Access 2024 dataset.\nResults show that all models experience remarkable accuracy drops under our\nproposed variation, with an average loss of 57% on MMLU and 50% on UNED-Access\n2024, ranging from 10% to 93% across models. Notably, the most accurate model\nin our experimentation (OpenAI-o3-mini) is not the most robust\n(DeepSeek-R1-70B), suggesting that the best models in standard evaluations may\nnot be the ones with better reasoning capabilities. Also, we see larger\naccuracy drops in public (vs private) datasets and questions posed in their\noriginal language (vs a manual translation), which are signs of contamination\nand also point to a relevant role of recall/memorization in current LLMs'\nanswers."
                },
                "authors": [
                    {
                        "name": "Eva Snchez Salido"
                    },
                    {
                        "name": "Julio Gonzalo"
                    },
                    {
                        "name": "Guillermo Marco"
                    }
                ],
                "author_detail": {
                    "name": "Guillermo Marco"
                },
                "author": "Guillermo Marco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12896v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12896v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12895v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12895v1",
                "updated": "2025-02-18T14:32:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    32,
                    17,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T14:32:17Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    32,
                    17,
                    1,
                    49,
                    0
                ],
                "title": "Multilingual European Language Models: Benchmarking Approaches and\n  Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual European Language Models: Benchmarking Approaches and\n  Challenges"
                },
                "summary": "The breakthrough of generative large language models (LLMs) that can solve\ndifferent tasks through chat interaction has led to a significant increase in\nthe use of general benchmarks to assess the quality or performance of these\nmodels beyond individual applications. There is also a need for better methods\nto evaluate and also to compare models due to the ever increasing number of new\nmodels published. However, most of the established benchmarks revolve around\nthe English language. This paper analyses the benefits and limitations of\ncurrent evaluation datasets, focusing on multilingual European benchmarks. We\nanalyse seven multilingual benchmarks and identify four major challenges.\nFurthermore, we discuss potential solutions to enhance translation quality and\nmitigate cultural biases, including human-in-the-loop verification and\niterative translation ranking. Our analysis highlights the need for culturally\naware and rigorously validated benchmarks to assess the reasoning and\nquestion-answering capabilities of multilingual LLMs accurately.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The breakthrough of generative large language models (LLMs) that can solve\ndifferent tasks through chat interaction has led to a significant increase in\nthe use of general benchmarks to assess the quality or performance of these\nmodels beyond individual applications. There is also a need for better methods\nto evaluate and also to compare models due to the ever increasing number of new\nmodels published. However, most of the established benchmarks revolve around\nthe English language. This paper analyses the benefits and limitations of\ncurrent evaluation datasets, focusing on multilingual European benchmarks. We\nanalyse seven multilingual benchmarks and identify four major challenges.\nFurthermore, we discuss potential solutions to enhance translation quality and\nmitigate cultural biases, including human-in-the-loop verification and\niterative translation ranking. Our analysis highlights the need for culturally\naware and rigorously validated benchmarks to assess the reasoning and\nquestion-answering capabilities of multilingual LLMs accurately."
                },
                "authors": [
                    {
                        "name": "Fabio Barth"
                    },
                    {
                        "name": "Georg Rehm"
                    }
                ],
                "author_detail": {
                    "name": "Georg Rehm"
                },
                "author": "Georg Rehm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12895v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12895v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12892v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12892v1",
                "updated": "2025-02-18T14:29:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    29,
                    11,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T14:29:11Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    29,
                    11,
                    1,
                    49,
                    0
                ],
                "title": "Archetypal SAE: Adaptive and Stable Dictionary Learning for Concept\n  Extraction in Large Vision Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Archetypal SAE: Adaptive and Stable Dictionary Learning for Concept\n  Extraction in Large Vision Models"
                },
                "summary": "Sparse Autoencoders (SAEs) have emerged as a powerful framework for machine\nlearning interpretability, enabling the unsupervised decomposition of model\nrepresentations into a dictionary of abstract, human-interpretable concepts.\nHowever, we reveal a fundamental limitation: existing SAEs exhibit severe\ninstability, as identical models trained on similar datasets can produce\nsharply different dictionaries, undermining their reliability as an\ninterpretability tool. To address this issue, we draw inspiration from the\nArchetypal Analysis framework introduced by Cutler & Breiman (1994) and present\nArchetypal SAEs (A-SAE), wherein dictionary atoms are constrained to the convex\nhull of data. This geometric anchoring significantly enhances the stability of\ninferred dictionaries, and their mildly relaxed variants RA-SAEs further match\nstate-of-the-art reconstruction abilities. To rigorously assess dictionary\nquality learned by SAEs, we introduce two new benchmarks that test (i)\nplausibility, if dictionaries recover \"true\" classification directions and (ii)\nidentifiability, if dictionaries disentangle synthetic concept mixtures. Across\nall evaluations, RA-SAEs consistently yield more structured representations\nwhile uncovering novel, semantically meaningful concepts in large-scale vision\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Autoencoders (SAEs) have emerged as a powerful framework for machine\nlearning interpretability, enabling the unsupervised decomposition of model\nrepresentations into a dictionary of abstract, human-interpretable concepts.\nHowever, we reveal a fundamental limitation: existing SAEs exhibit severe\ninstability, as identical models trained on similar datasets can produce\nsharply different dictionaries, undermining their reliability as an\ninterpretability tool. To address this issue, we draw inspiration from the\nArchetypal Analysis framework introduced by Cutler & Breiman (1994) and present\nArchetypal SAEs (A-SAE), wherein dictionary atoms are constrained to the convex\nhull of data. This geometric anchoring significantly enhances the stability of\ninferred dictionaries, and their mildly relaxed variants RA-SAEs further match\nstate-of-the-art reconstruction abilities. To rigorously assess dictionary\nquality learned by SAEs, we introduce two new benchmarks that test (i)\nplausibility, if dictionaries recover \"true\" classification directions and (ii)\nidentifiability, if dictionaries disentangle synthetic concept mixtures. Across\nall evaluations, RA-SAEs consistently yield more structured representations\nwhile uncovering novel, semantically meaningful concepts in large-scale vision\nmodels."
                },
                "authors": [
                    {
                        "name": "Thomas Fel"
                    },
                    {
                        "name": "Ekdeep Singh Lubana"
                    },
                    {
                        "name": "Jacob S. Prince"
                    },
                    {
                        "name": "Matthew Kowal"
                    },
                    {
                        "name": "Victor Boutin"
                    },
                    {
                        "name": "Isabel Papadimitriou"
                    },
                    {
                        "name": "Binxu Wang"
                    },
                    {
                        "name": "Martin Wattenberg"
                    },
                    {
                        "name": "Demba Ba"
                    },
                    {
                        "name": "Talia Konkle"
                    }
                ],
                "author_detail": {
                    "name": "Talia Konkle"
                },
                "author": "Talia Konkle",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12892v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12892v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12886v1",
                "updated": "2025-02-18T14:20:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    20,
                    27,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T14:20:27Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    20,
                    27,
                    1,
                    49,
                    0
                ],
                "title": "Are Multilingual Language Models an Off-ramp for Under-resourced\n  Languages? Will we arrive at Digital Language Equality in Europe in 2030?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Multilingual Language Models an Off-ramp for Under-resourced\n  Languages? Will we arrive at Digital Language Equality in Europe in 2030?"
                },
                "summary": "Large language models (LLMs) demonstrate unprecedented capabilities and\ndefine the state of the art for almost all natural language processing (NLP)\ntasks and also for essentially all Language Technology (LT) applications. LLMs\ncan only be trained for languages for which a sufficient amount of pre-training\ndata is available, effectively excluding many languages that are typically\ncharacterised as under-resourced. However, there is both circumstantial and\nempirical evidence that multilingual LLMs, which have been trained using data\nsets that cover multiple languages (including under-resourced ones), do exhibit\nstrong capabilities for some of these under-resourced languages. Eventually,\nthis approach may have the potential to be a technological off-ramp for those\nunder-resourced languages for which \"native\" LLMs, and LLM-based technologies,\ncannot be developed due to a lack of training data. This paper, which\nconcentrates on European languages, examines this idea, analyses the current\nsituation in terms of technology support and summarises related work. The\narticle concludes by focusing on the key open questions that need to be\nanswered for the approach to be put into practice in a systematic way.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate unprecedented capabilities and\ndefine the state of the art for almost all natural language processing (NLP)\ntasks and also for essentially all Language Technology (LT) applications. LLMs\ncan only be trained for languages for which a sufficient amount of pre-training\ndata is available, effectively excluding many languages that are typically\ncharacterised as under-resourced. However, there is both circumstantial and\nempirical evidence that multilingual LLMs, which have been trained using data\nsets that cover multiple languages (including under-resourced ones), do exhibit\nstrong capabilities for some of these under-resourced languages. Eventually,\nthis approach may have the potential to be a technological off-ramp for those\nunder-resourced languages for which \"native\" LLMs, and LLM-based technologies,\ncannot be developed due to a lack of training data. This paper, which\nconcentrates on European languages, examines this idea, analyses the current\nsituation in terms of technology support and summarises related work. The\narticle concludes by focusing on the key open questions that need to be\nanswered for the approach to be put into practice in a systematic way."
                },
                "authors": [
                    {
                        "name": "Georg Rehm"
                    },
                    {
                        "name": "Annika Grtzner-Zahn"
                    },
                    {
                        "name": "Fabio Barth"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Barth"
                },
                "author": "Fabio Barth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12884v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12884v1",
                "updated": "2025-02-18T14:16:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    16,
                    3,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T14:16:03Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    16,
                    3,
                    1,
                    49,
                    0
                ],
                "title": "How desirable is alignment between LLMs and linguistically diverse human\n  users?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How desirable is alignment between LLMs and linguistically diverse human\n  users?"
                },
                "summary": "We discuss how desirable it is that Large Language Models (LLMs) be able to\nadapt or align their language behavior with users who may be diverse in their\nlanguage use. User diversity may come about among others due to i) age\ndifferences; ii) gender characteristics, and/or iii) multilingual experience,\nand associated differences in language processing and use. We consider\npotential consequences for usability, communication, and LLM development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We discuss how desirable it is that Large Language Models (LLMs) be able to\nadapt or align their language behavior with users who may be diverse in their\nlanguage use. User diversity may come about among others due to i) age\ndifferences; ii) gender characteristics, and/or iii) multilingual experience,\nand associated differences in language processing and use. We consider\npotential consequences for usability, communication, and LLM development."
                },
                "authors": [
                    {
                        "name": "Pia Knoeferle"
                    },
                    {
                        "name": "Sebastian Mller"
                    },
                    {
                        "name": "Dorothea Kolossa"
                    },
                    {
                        "name": "Veronika Solopova"
                    },
                    {
                        "name": "Georg Rehm"
                    }
                ],
                "author_detail": {
                    "name": "Georg Rehm"
                },
                "author": "Georg Rehm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12884v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12884v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15091v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15091v2",
                "updated": "2025-02-18T14:12:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    12,
                    31,
                    1,
                    49,
                    0
                ],
                "published": "2024-08-27T14:22:02Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    14,
                    22,
                    2,
                    1,
                    240,
                    0
                ],
                "title": "Relation Also Knows: Rethinking the Recall and Editing of Factual\n  Associations in Auto-Regressive Transformer Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relation Also Knows: Rethinking the Recall and Editing of Factual\n  Associations in Auto-Regressive Transformer Language Models"
                },
                "summary": "The storage and recall of factual associations in auto-regressive transformer\nlanguage models (LMs) have drawn a great deal of attention, inspiring knowledge\nediting by directly modifying the located model weights. Most editing works\nachieve knowledge editing under the guidance of existing interpretations of\nknowledge recall that mainly focus on subject knowledge. However, these\ninterpretations are seriously flawed, neglecting relation information and\nleading to the over-generalizing problem for editing. In this work, we discover\na novel relation-focused perspective to interpret the knowledge recall of\ntransformer LMs during inference and apply it on single knowledge editing to\navoid over-generalizing. Experimental results on the dataset supplemented with\na new R-Specificity criterion demonstrate that our editing approach\nsignificantly alleviates over-generalizing while remaining competitive on other\ncriteria, breaking the domination of subject-focused editing for future\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The storage and recall of factual associations in auto-regressive transformer\nlanguage models (LMs) have drawn a great deal of attention, inspiring knowledge\nediting by directly modifying the located model weights. Most editing works\nachieve knowledge editing under the guidance of existing interpretations of\nknowledge recall that mainly focus on subject knowledge. However, these\ninterpretations are seriously flawed, neglecting relation information and\nleading to the over-generalizing problem for editing. In this work, we discover\na novel relation-focused perspective to interpret the knowledge recall of\ntransformer LMs during inference and apply it on single knowledge editing to\navoid over-generalizing. Experimental results on the dataset supplemented with\na new R-Specificity criterion demonstrate that our editing approach\nsignificantly alleviates over-generalizing while remaining competitive on other\ncriteria, breaking the domination of subject-focused editing for future\nresearch."
                },
                "authors": [
                    {
                        "name": "Xiyu Liu"
                    },
                    {
                        "name": "Zhengxiao Liu"
                    },
                    {
                        "name": "Naibin Gu"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Wanli Ma"
                    },
                    {
                        "name": "Ji Xiang"
                    },
                    {
                        "name": "Weiping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weiping Wang"
                },
                "author": "Weiping Wang",
                "arxiv_comment": "Accepted by AAAI25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15091v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15091v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12876v1",
                "updated": "2025-02-18T14:05:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    5,
                    59,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T14:05:59Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    5,
                    59,
                    1,
                    49,
                    0
                ],
                "title": "Continuous Learning Conversational AI: A Personalized Agent Framework\n  via A2C Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous Learning Conversational AI: A Personalized Agent Framework\n  via A2C Reinforcement Learning"
                },
                "summary": "Creating personalized and adaptable conversational AI remains a key\nchallenge. This paper introduces a Continuous Learning Conversational AI (CLCA)\napproach, implemented using A2C reinforcement learning, to move beyond static\nLarge Language Models (LLMs). We use simulated sales dialogues, generated by\nLLMs, to train an A2C agent. This agent learns to optimize conversation\nstrategies for personalization, focusing on engagement and delivering value.\nOur system architecture integrates reinforcement learning with LLMs for both\ndata creation and response selection. This method offers a practical way to\nbuild personalized AI companions that evolve through continuous learning,\nadvancing beyond traditional static LLM techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creating personalized and adaptable conversational AI remains a key\nchallenge. This paper introduces a Continuous Learning Conversational AI (CLCA)\napproach, implemented using A2C reinforcement learning, to move beyond static\nLarge Language Models (LLMs). We use simulated sales dialogues, generated by\nLLMs, to train an A2C agent. This agent learns to optimize conversation\nstrategies for personalization, focusing on engagement and delivering value.\nOur system architecture integrates reinforcement learning with LLMs for both\ndata creation and response selection. This method offers a practical way to\nbuild personalized AI companions that evolve through continuous learning,\nadvancing beyond traditional static LLM techniques."
                },
                "authors": [
                    {
                        "name": "Nandakishor M"
                    },
                    {
                        "name": "Anjali M"
                    }
                ],
                "author_detail": {
                    "name": "Anjali M"
                },
                "author": "Anjali M",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00152v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00152v2",
                "updated": "2025-02-18T14:02:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    2,
                    12,
                    1,
                    49,
                    0
                ],
                "published": "2024-12-30T21:54:33Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    21,
                    54,
                    33,
                    0,
                    365,
                    0
                ],
                "title": "Temporal reasoning for timeline summarisation in social media",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal reasoning for timeline summarisation in social media"
                },
                "summary": "This paper explores whether enhancing temporal reasoning capabilities in\nLarge Language Models (LLMs) can improve the quality of timeline summarisation,\nthe task of summarising long texts containing sequences of events, such as\nsocial media threads. We first introduce NarrativeReason, a novel dataset\nfocused on temporal relationships among sequential events within narratives,\ndistinguishing it from existing temporal reasoning datasets that primarily\naddress pair-wise event relationships. Our approach then combines temporal\nreasoning with timeline summarisation through a knowledge distillation\nframework, where we first fine-tune a teacher model on temporal reasoning tasks\nand then distill this knowledge into a student model while simultaneously\ntraining it for the task of timeline summarisation. Experimental results\ndemonstrate that our model achieves superior performance on out-of-domain\nmental health-related timeline summarisation tasks, which involve long social\nmedia threads with repetitions of events and a mix of emotions, highlighting\nthe importance and generalisability of leveraging temporal reasoning to improve\ntimeline summarisation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores whether enhancing temporal reasoning capabilities in\nLarge Language Models (LLMs) can improve the quality of timeline summarisation,\nthe task of summarising long texts containing sequences of events, such as\nsocial media threads. We first introduce NarrativeReason, a novel dataset\nfocused on temporal relationships among sequential events within narratives,\ndistinguishing it from existing temporal reasoning datasets that primarily\naddress pair-wise event relationships. Our approach then combines temporal\nreasoning with timeline summarisation through a knowledge distillation\nframework, where we first fine-tune a teacher model on temporal reasoning tasks\nand then distill this knowledge into a student model while simultaneously\ntraining it for the task of timeline summarisation. Experimental results\ndemonstrate that our model achieves superior performance on out-of-domain\nmental health-related timeline summarisation tasks, which involve long social\nmedia threads with repetitions of events and a mix of emotions, highlighting\nthe importance and generalisability of leveraging temporal reasoning to improve\ntimeline summarisation."
                },
                "authors": [
                    {
                        "name": "Jiayu Song"
                    },
                    {
                        "name": "Mahmud Akhter"
                    },
                    {
                        "name": "Dana Atzil Slonim"
                    },
                    {
                        "name": "Maria Liakata"
                    }
                ],
                "author_detail": {
                    "name": "Maria Liakata"
                },
                "author": "Maria Liakata",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00152v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00152v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07460v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07460v2",
                "updated": "2025-02-18T13:55:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    13,
                    55,
                    4,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-11T11:11:05Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    11,
                    11,
                    5,
                    1,
                    42,
                    0
                ],
                "title": "Logarithmic Regret for Online KL-Regularized Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logarithmic Regret for Online KL-Regularized Reinforcement Learning"
                },
                "summary": "Recent advances in Reinforcement Learning from Human Feedback (RLHF) have\nshown that KL-regularization plays a pivotal role in improving the efficiency\nof RL fine-tuning for large language models (LLMs). Despite its empirical\nadvantage, the theoretical difference between KL-regularized RL and standard RL\nremains largely under-explored. While there is a recent line of work on the\ntheoretical analysis of KL-regularized objective in decision making\n\\citep{xiong2024iterative, xie2024exploratory,zhao2024sharp}, these analyses\neither reduce to the traditional RL setting or rely on strong coverage\nassumptions. In this paper, we propose an optimism-based KL-regularized online\ncontextual bandit algorithm, and provide a novel analysis of its regret. By\ncarefully leveraging the benign optimization landscape induced by the\nKL-regularization and the optimistic reward estimation, our algorithm achieves\nan $\\mathcal{O}\\big(\\eta\\log (N_{\\mathcal R} T)\\cdot d_{\\mathcal R}\\big)$\nlogarithmic regret bound, where $\\eta, N_{\\mathcal R},T,d_{\\mathcal R}$ denote\nthe KL-regularization parameter, the cardinality of the reward function class,\nnumber of rounds, and the complexity of the reward function class. Furthermore,\nwe extend our algorithm and analysis to reinforcement learning by developing a\nnovel decomposition over transition steps and also obtain a similar logarithmic\nregret bound.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Reinforcement Learning from Human Feedback (RLHF) have\nshown that KL-regularization plays a pivotal role in improving the efficiency\nof RL fine-tuning for large language models (LLMs). Despite its empirical\nadvantage, the theoretical difference between KL-regularized RL and standard RL\nremains largely under-explored. While there is a recent line of work on the\ntheoretical analysis of KL-regularized objective in decision making\n\\citep{xiong2024iterative, xie2024exploratory,zhao2024sharp}, these analyses\neither reduce to the traditional RL setting or rely on strong coverage\nassumptions. In this paper, we propose an optimism-based KL-regularized online\ncontextual bandit algorithm, and provide a novel analysis of its regret. By\ncarefully leveraging the benign optimization landscape induced by the\nKL-regularization and the optimistic reward estimation, our algorithm achieves\nan $\\mathcal{O}\\big(\\eta\\log (N_{\\mathcal R} T)\\cdot d_{\\mathcal R}\\big)$\nlogarithmic regret bound, where $\\eta, N_{\\mathcal R},T,d_{\\mathcal R}$ denote\nthe KL-regularization parameter, the cardinality of the reward function class,\nnumber of rounds, and the complexity of the reward function class. Furthermore,\nwe extend our algorithm and analysis to reinforcement learning by developing a\nnovel decomposition over transition steps and also obtain a similar logarithmic\nregret bound."
                },
                "authors": [
                    {
                        "name": "Heyang Zhao"
                    },
                    {
                        "name": "Chenlu Ye"
                    },
                    {
                        "name": "Wei Xiong"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07460v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07460v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10774v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10774v3",
                "updated": "2025-02-18T13:53:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    13,
                    53,
                    51,
                    1,
                    49,
                    0
                ],
                "published": "2024-08-20T12:13:04Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    12,
                    13,
                    4,
                    1,
                    233,
                    0
                ],
                "title": "Flexora: Flexible Low Rank Adaptation for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flexora: Flexible Low Rank Adaptation for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are driving advancements in artificial\nintelligence by increasing the scale of model parameters, which has\nsignificantly enhanced generalization ability and unlocked new capabilities in\npractice. However, their performance in specific downstream tasks is usually\nhindered by their knowledge boundaries on these tasks. Thus, fine-tuning\ntechniques, especially the widely used Low-Rank Adaptation (LoRA) method, have\nbeen introduced to expand the boundaries on these tasks, whereas LoRA would\nunderperform on certain tasks owing to its potential overfitting on these\ntasks. To overcome this overfitting and improve the performance of LoRA, we\npropose the flexible low rank adaptation (Flexora) method to automatically and\nflexibly select the most important layers needing to be fine-tuned to achieve\nthe best performance on different downstream tasks. Specifically, Flexora\nfirstly frames this layer selection problem as a well-defined hyperparameter\noptimization (HPO) problem, then addresses it using the unrolled\ndifferentiation (UD) method, and finally selects the most useful layers based\non the optimized hyperparameters. Our extensive experiments on many pretrained\nmodels and natural language tasks show that Flexora is able to consistently\nimprove over the existing baselines, indicating the effectiveness of our\nFlexora in practice. We additionally provide insightful theoretical results and\nmany ablation studies to deliver a comprehensive understanding of our Flexora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are driving advancements in artificial\nintelligence by increasing the scale of model parameters, which has\nsignificantly enhanced generalization ability and unlocked new capabilities in\npractice. However, their performance in specific downstream tasks is usually\nhindered by their knowledge boundaries on these tasks. Thus, fine-tuning\ntechniques, especially the widely used Low-Rank Adaptation (LoRA) method, have\nbeen introduced to expand the boundaries on these tasks, whereas LoRA would\nunderperform on certain tasks owing to its potential overfitting on these\ntasks. To overcome this overfitting and improve the performance of LoRA, we\npropose the flexible low rank adaptation (Flexora) method to automatically and\nflexibly select the most important layers needing to be fine-tuned to achieve\nthe best performance on different downstream tasks. Specifically, Flexora\nfirstly frames this layer selection problem as a well-defined hyperparameter\noptimization (HPO) problem, then addresses it using the unrolled\ndifferentiation (UD) method, and finally selects the most useful layers based\non the optimized hyperparameters. Our extensive experiments on many pretrained\nmodels and natural language tasks show that Flexora is able to consistently\nimprove over the existing baselines, indicating the effectiveness of our\nFlexora in practice. We additionally provide insightful theoretical results and\nmany ablation studies to deliver a comprehensive understanding of our Flexora."
                },
                "authors": [
                    {
                        "name": "Chenxing Wei"
                    },
                    {
                        "name": "Yao Shu"
                    },
                    {
                        "name": "Ying Tiffany He"
                    },
                    {
                        "name": "Fei Richard Yu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Richard Yu"
                },
                "author": "Fei Richard Yu",
                "arxiv_comment": "39 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10774v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10774v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12864v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12864v1",
                "updated": "2025-02-18T13:52:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    13,
                    52,
                    16,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T13:52:16Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    13,
                    52,
                    16,
                    1,
                    49,
                    0
                ],
                "title": "Simpson's Paradox with Any Given Number of Factors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simpson's Paradox with Any Given Number of Factors"
                },
                "summary": "Simpson's Paradox is a well-known phenomenon in statistical science, where\nthe relationship between the response variable $X$ and a certain explanatory\nfactor of interest $A$ reverses when an additional factor $B_1$ is considered.\nThis paper explores the extension of Simpson's Paradox to any given number $n$\nof factors, referred to as the $n$-factor Simpson's Paradox. We first provide a\nrigorous definition of the $n$-factor Simpson's Paradox, then demonstrate the\nexistence of a probability distribution through a geometric construction.\nSpecifically, we show that for any positive integer $n$, it is possible to\nconstruct a probability distribution in which the conclusion about the effect\nof $A$ on $X$ reverses each time an additional factor $B_i$ is introduced for\n$i=1,...,n$. A detailed example for $n = 3$ illustrates the construction. Our\nresults highlight that, contrary to the intuition that more data leads to more\naccurate inferences, the inclusion of additional factors can repeatedly reverse\nconclusions, emphasizing the complexity of statistical inference in the\npresence of multiple confounding variables.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simpson's Paradox is a well-known phenomenon in statistical science, where\nthe relationship between the response variable $X$ and a certain explanatory\nfactor of interest $A$ reverses when an additional factor $B_1$ is considered.\nThis paper explores the extension of Simpson's Paradox to any given number $n$\nof factors, referred to as the $n$-factor Simpson's Paradox. We first provide a\nrigorous definition of the $n$-factor Simpson's Paradox, then demonstrate the\nexistence of a probability distribution through a geometric construction.\nSpecifically, we show that for any positive integer $n$, it is possible to\nconstruct a probability distribution in which the conclusion about the effect\nof $A$ on $X$ reverses each time an additional factor $B_i$ is introduced for\n$i=1,...,n$. A detailed example for $n = 3$ illustrates the construction. Our\nresults highlight that, contrary to the intuition that more data leads to more\naccurate inferences, the inclusion of additional factors can repeatedly reverse\nconclusions, emphasizing the complexity of statistical inference in the\npresence of multiple confounding variables."
                },
                "authors": [
                    {
                        "name": "Guisheng Dai"
                    },
                    {
                        "name": "Weizhen Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weizhen Wang"
                },
                "author": "Weizhen Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12864v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12864v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12860v1",
                "updated": "2025-02-18T13:48:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    13,
                    48,
                    22,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T13:48:22Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    13,
                    48,
                    22,
                    1,
                    49,
                    0
                ],
                "title": "An Experimental Study of SOTA LiDAR Segmentation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Experimental Study of SOTA LiDAR Segmentation Models"
                },
                "summary": "Point cloud segmentation (PCS) is to classify each point in point clouds. The\ntask enables robots to parse their 3D surroundings and run autonomously.\nAccording to different point cloud representations, existing PCS models can be\nroughly divided into point-, voxel-, and range image-based models. However, no\nwork has been found to report comprehensive comparisons among the\nstate-of-the-art point-, voxel-, and range image-based models from an\napplication perspective, bringing difficulty in utilizing these models for\nreal-world scenarios. In this paper, we provide thorough comparisons among the\nmodels by considering the LiDAR data motion compensation and the metrics of\nmodel parameters, max GPU memory allocated during testing, inference latency,\nframes per second, intersection-over-union (IoU) and mean IoU (mIoU) scores.\nThe experimental results benefit engineers when choosing a reasonable PCS model\nfor an application and inspire researchers in the PCS field to design more\npractical models for a real-world scenario.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point cloud segmentation (PCS) is to classify each point in point clouds. The\ntask enables robots to parse their 3D surroundings and run autonomously.\nAccording to different point cloud representations, existing PCS models can be\nroughly divided into point-, voxel-, and range image-based models. However, no\nwork has been found to report comprehensive comparisons among the\nstate-of-the-art point-, voxel-, and range image-based models from an\napplication perspective, bringing difficulty in utilizing these models for\nreal-world scenarios. In this paper, we provide thorough comparisons among the\nmodels by considering the LiDAR data motion compensation and the metrics of\nmodel parameters, max GPU memory allocated during testing, inference latency,\nframes per second, intersection-over-union (IoU) and mean IoU (mIoU) scores.\nThe experimental results benefit engineers when choosing a reasonable PCS model\nfor an application and inspire researchers in the PCS field to design more\npractical models for a real-world scenario."
                },
                "authors": [
                    {
                        "name": "Bike Chen"
                    },
                    {
                        "name": "Antti Tikanmki"
                    },
                    {
                        "name": "Juha Rning"
                    }
                ],
                "author_detail": {
                    "name": "Juha Rning"
                },
                "author": "Juha Rning",
                "arxiv_comment": "No comments",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2502.13145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13145v1",
                "updated": "2025-02-18T18:59:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    59,
                    57,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T18:59:57Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    59,
                    57,
                    1,
                    49,
                    0
                ],
                "title": "Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation"
                },
                "summary": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba"
                },
                "authors": [
                    {
                        "name": "Bencheng Liao"
                    },
                    {
                        "name": "Hongyuan Tao"
                    },
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Tianheng Cheng"
                    },
                    {
                        "name": "Yingyue Li"
                    },
                    {
                        "name": "Haoran Yin"
                    },
                    {
                        "name": "Wenyu Liu"
                    },
                    {
                        "name": "Xinggang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinggang Wang"
                },
                "author": "Xinggang Wang",
                "arxiv_comment": "Code and model are available at https://github.com/hustvl/mmMamba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13146v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13146v1",
                "updated": "2025-02-18T18:59:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    59,
                    57,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T18:59:57Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    59,
                    57,
                    1,
                    49,
                    0
                ],
                "title": "Re-Align: Aligning Vision Language Models via Retrieval-Augmented Direct\n  Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re-Align: Aligning Vision Language Models via Retrieval-Augmented Direct\n  Preference Optimization"
                },
                "summary": "The emergence of large Vision Language Models (VLMs) has broadened the scope\nand capabilities of single-modal Large Language Models (LLMs) by integrating\nvisual modalities, thereby unlocking transformative cross-modal applications in\na variety of real-world scenarios. Despite their impressive performance, VLMs\nare prone to significant hallucinations, particularly in the form of\ncross-modal inconsistencies. Building on the success of Reinforcement Learning\nfrom Human Feedback (RLHF) in aligning LLMs, recent advancements have focused\non applying direct preference optimization (DPO) on carefully curated datasets\nto mitigate these issues. Yet, such approaches typically introduce preference\nsignals in a brute-force manner, neglecting the crucial role of visual\ninformation in the alignment process. In this paper, we introduce Re-Align, a\nnovel alignment framework that leverages image retrieval to construct a\ndual-preference dataset, effectively incorporating both textual and visual\npreference signals. We further introduce rDPO, an extension of the standard\ndirect preference optimization that incorporates an additional visual\npreference objective during fine-tuning. Our experimental results demonstrate\nthat Re-Align not only mitigates hallucinations more effectively than previous\nmethods but also yields significant performance gains in general visual\nquestion-answering (VQA) tasks. Moreover, we show that Re-Align maintains\nrobustness and scalability across a wide range of VLM sizes and architectures.\nThis work represents a significant step forward in aligning multimodal LLMs,\npaving the way for more reliable and effective cross-modal applications. We\nrelease all the code in https://github.com/taco-group/Re-Align.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large Vision Language Models (VLMs) has broadened the scope\nand capabilities of single-modal Large Language Models (LLMs) by integrating\nvisual modalities, thereby unlocking transformative cross-modal applications in\na variety of real-world scenarios. Despite their impressive performance, VLMs\nare prone to significant hallucinations, particularly in the form of\ncross-modal inconsistencies. Building on the success of Reinforcement Learning\nfrom Human Feedback (RLHF) in aligning LLMs, recent advancements have focused\non applying direct preference optimization (DPO) on carefully curated datasets\nto mitigate these issues. Yet, such approaches typically introduce preference\nsignals in a brute-force manner, neglecting the crucial role of visual\ninformation in the alignment process. In this paper, we introduce Re-Align, a\nnovel alignment framework that leverages image retrieval to construct a\ndual-preference dataset, effectively incorporating both textual and visual\npreference signals. We further introduce rDPO, an extension of the standard\ndirect preference optimization that incorporates an additional visual\npreference objective during fine-tuning. Our experimental results demonstrate\nthat Re-Align not only mitigates hallucinations more effectively than previous\nmethods but also yields significant performance gains in general visual\nquestion-answering (VQA) tasks. Moreover, we show that Re-Align maintains\nrobustness and scalability across a wide range of VLM sizes and architectures.\nThis work represents a significant step forward in aligning multimodal LLMs,\npaving the way for more reliable and effective cross-modal applications. We\nrelease all the code in https://github.com/taco-group/Re-Align."
                },
                "authors": [
                    {
                        "name": "Shuo Xing"
                    },
                    {
                        "name": "Yuping Wang"
                    },
                    {
                        "name": "Peiran Li"
                    },
                    {
                        "name": "Ruizheng Bai"
                    },
                    {
                        "name": "Yueqi Wang"
                    },
                    {
                        "name": "Chengxuan Qian"
                    },
                    {
                        "name": "Huaxiu Yao"
                    },
                    {
                        "name": "Zhengzhong Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhengzhong Tu"
                },
                "author": "Zhengzhong Tu",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13146v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13146v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13141v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13141v1",
                "updated": "2025-02-18T18:59:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    59,
                    0,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T18:59:00Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    59,
                    0,
                    1,
                    49,
                    0
                ],
                "title": "UniGuardian: A Unified Defense for Detecting Prompt Injection, Backdoor\n  Attacks and Adversarial Attacks in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniGuardian: A Unified Defense for Detecting Prompt Injection, Backdoor\n  Attacks and Adversarial Attacks in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are vulnerable to attacks like prompt injection,\nbackdoor attacks, and adversarial attacks, which manipulate prompts or models\nto generate harmful outputs. In this paper, departing from traditional deep\nlearning attack paradigms, we explore their intrinsic relationship and\ncollectively term them Prompt Trigger Attacks (PTA). This raises a key\nquestion: Can we determine if a prompt is benign or poisoned? To address this,\nwe propose UniGuardian, the first unified defense mechanism designed to detect\nprompt injection, backdoor attacks, and adversarial attacks in LLMs.\nAdditionally, we introduce a single-forward strategy to optimize the detection\npipeline, enabling simultaneous attack detection and text generation within a\nsingle forward pass. Our experiments confirm that UniGuardian accurately and\nefficiently identifies malicious prompts in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are vulnerable to attacks like prompt injection,\nbackdoor attacks, and adversarial attacks, which manipulate prompts or models\nto generate harmful outputs. In this paper, departing from traditional deep\nlearning attack paradigms, we explore their intrinsic relationship and\ncollectively term them Prompt Trigger Attacks (PTA). This raises a key\nquestion: Can we determine if a prompt is benign or poisoned? To address this,\nwe propose UniGuardian, the first unified defense mechanism designed to detect\nprompt injection, backdoor attacks, and adversarial attacks in LLMs.\nAdditionally, we introduce a single-forward strategy to optimize the detection\npipeline, enabling simultaneous attack detection and text generation within a\nsingle forward pass. Our experiments confirm that UniGuardian accurately and\nefficiently identifies malicious prompts in LLMs."
                },
                "authors": [
                    {
                        "name": "Huawei Lin"
                    },
                    {
                        "name": "Yingjie Lao"
                    },
                    {
                        "name": "Tong Geng"
                    },
                    {
                        "name": "Tan Yu"
                    },
                    {
                        "name": "Weijie Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Weijie Zhao"
                },
                "author": "Weijie Zhao",
                "arxiv_comment": "18 Pages, 8 Figures, 5 Tables, Keywords: Attack Defending, Security,\n  Prompt Injection, Backdoor Attacks, Adversarial Attacks, Prompt Trigger\n  Attacks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13141v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13141v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13138v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13138v1",
                "updated": "2025-02-18T18:57:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    57,
                    21,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T18:57:21Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    57,
                    21,
                    1,
                    49,
                    0
                ],
                "title": "AIDE: AI-Driven Exploration in the Space of Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIDE: AI-Driven Exploration in the Space of Code"
                },
                "summary": "Machine learning, the foundation of modern artificial intelligence, has\ndriven innovations that have fundamentally transformed the world. Yet, behind\nadvancements lies a complex and often tedious process requiring labor and\ncompute intensive iteration and experimentation. Engineers and scientists\ndeveloping machine learning models spend much of their time on trial-and-error\ntasks instead of conceptualizing innovative solutions or research hypotheses.\nTo address this challenge, we introduce AI-Driven Exploration (AIDE), a machine\nlearning engineering agent powered by large language models (LLMs). AIDE frames\nmachine learning engineering as a code optimization problem, and formulates\ntrial-and-error as a tree search in the space of potential solutions. By\nstrategically reusing and refining promising solutions, AIDE effectively trades\ncomputational resources for enhanced performance, achieving state-of-the-art\nresults on multiple machine learning engineering benchmarks, including our\nKaggle evaluations, OpenAI MLE-Bench and METRs RE-Bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning, the foundation of modern artificial intelligence, has\ndriven innovations that have fundamentally transformed the world. Yet, behind\nadvancements lies a complex and often tedious process requiring labor and\ncompute intensive iteration and experimentation. Engineers and scientists\ndeveloping machine learning models spend much of their time on trial-and-error\ntasks instead of conceptualizing innovative solutions or research hypotheses.\nTo address this challenge, we introduce AI-Driven Exploration (AIDE), a machine\nlearning engineering agent powered by large language models (LLMs). AIDE frames\nmachine learning engineering as a code optimization problem, and formulates\ntrial-and-error as a tree search in the space of potential solutions. By\nstrategically reusing and refining promising solutions, AIDE effectively trades\ncomputational resources for enhanced performance, achieving state-of-the-art\nresults on multiple machine learning engineering benchmarks, including our\nKaggle evaluations, OpenAI MLE-Bench and METRs RE-Bench."
                },
                "authors": [
                    {
                        "name": "Zhengyao Jiang"
                    },
                    {
                        "name": "Dominik Schmidt"
                    },
                    {
                        "name": "Dhruv Srikanth"
                    },
                    {
                        "name": "Dixing Xu"
                    },
                    {
                        "name": "Ian Kaplan"
                    },
                    {
                        "name": "Deniss Jacenko"
                    },
                    {
                        "name": "Yuxiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiang Wu"
                },
                "author": "Yuxiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13138v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13138v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13137v1",
                "updated": "2025-02-18T18:57:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    57,
                    9,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T18:57:09Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    57,
                    9,
                    1,
                    49,
                    0
                ],
                "title": "Theorem Prover as a Judge for Synthetic Data Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theorem Prover as a Judge for Synthetic Data Generation"
                },
                "summary": "The demand for synthetic data in mathematical reasoning has increased due to\nits potential to enhance the mathematical capabilities of large language models\n(LLMs). However, ensuring the validity of intermediate reasoning steps remains\na significant challenge, affecting data quality. While formal verification via\ntheorem provers effectively validates LLM reasoning, the autoformalisation of\nmathematical proofs remains error-prone. In response, we introduce iterative\nautoformalisation, an approach that iteratively refines theorem prover\nformalisation to mitigate errors, thereby increasing the execution rate on the\nLean prover from 60% to 87%. Building upon that, we introduce Theorem Prover as\na Judge (TP-as-a-Judge), a method that employs theorem prover formalisation to\nrigorously assess LLM intermediate reasoning, effectively integrating\nautoformalisation with synthetic data generation. Finally, we present\nReinforcement Learning from Theorem Prover Feedback (RLTPF), a framework that\nreplaces human annotation with theorem prover feedback in Reinforcement\nLearning from Human Feedback (RLHF). Across multiple LLMs, applying\nTP-as-a-Judge and RLTPF improves benchmarks with only 3,508 samples, achieving\n5.56% accuracy gain on Mistral-7B for MultiArith, 6.00% on Llama-2-7B for\nSVAMP, and 3.55% on Llama-3.1-8B for AQUA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The demand for synthetic data in mathematical reasoning has increased due to\nits potential to enhance the mathematical capabilities of large language models\n(LLMs). However, ensuring the validity of intermediate reasoning steps remains\na significant challenge, affecting data quality. While formal verification via\ntheorem provers effectively validates LLM reasoning, the autoformalisation of\nmathematical proofs remains error-prone. In response, we introduce iterative\nautoformalisation, an approach that iteratively refines theorem prover\nformalisation to mitigate errors, thereby increasing the execution rate on the\nLean prover from 60% to 87%. Building upon that, we introduce Theorem Prover as\na Judge (TP-as-a-Judge), a method that employs theorem prover formalisation to\nrigorously assess LLM intermediate reasoning, effectively integrating\nautoformalisation with synthetic data generation. Finally, we present\nReinforcement Learning from Theorem Prover Feedback (RLTPF), a framework that\nreplaces human annotation with theorem prover feedback in Reinforcement\nLearning from Human Feedback (RLHF). Across multiple LLMs, applying\nTP-as-a-Judge and RLTPF improves benchmarks with only 3,508 samples, achieving\n5.56% accuracy gain on Mistral-7B for MultiArith, 6.00% on Llama-2-7B for\nSVAMP, and 3.55% on Llama-3.1-8B for AQUA."
                },
                "authors": [
                    {
                        "name": "Joshua Ong Jun Leang"
                    },
                    {
                        "name": "Giwon Hong"
                    },
                    {
                        "name": "Wenda Li"
                    },
                    {
                        "name": "Shay B. Cohen"
                    }
                ],
                "author_detail": {
                    "name": "Shay B. Cohen"
                },
                "author": "Shay B. Cohen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13131v1",
                "updated": "2025-02-18T18:55:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    55,
                    26,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T18:55:26Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    55,
                    26,
                    1,
                    49,
                    0
                ],
                "title": "Rethinking Diverse Human Preference Learning through Principal Component\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Diverse Human Preference Learning through Principal Component\n  Analysis"
                },
                "summary": "Understanding human preferences is crucial for improving foundation models\nand building personalized AI systems. However, preferences are inherently\ndiverse and complex, making it difficult for traditional reward models to\ncapture their full range. While fine-grained preference data can help,\ncollecting it is expensive and hard to scale. In this paper, we introduce\nDecomposed Reward Models (DRMs), a novel approach that extracts diverse human\npreferences from binary comparisons without requiring fine-grained annotations.\nOur key insight is to represent human preferences as vectors and analyze them\nusing Principal Component Analysis (PCA). By constructing a dataset of\nembedding differences between preferred and rejected responses, DRMs identify\northogonal basis vectors that capture distinct aspects of preference. These\ndecomposed rewards can be flexibly combined to align with different user needs,\noffering an interpretable and scalable alternative to traditional reward\nmodels. We demonstrate that DRMs effectively extract meaningful preference\ndimensions (e.g., helpfulness, safety, humor) and adapt to new users without\nadditional training. Our results highlight DRMs as a powerful framework for\npersonalized and interpretable LLM alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding human preferences is crucial for improving foundation models\nand building personalized AI systems. However, preferences are inherently\ndiverse and complex, making it difficult for traditional reward models to\ncapture their full range. While fine-grained preference data can help,\ncollecting it is expensive and hard to scale. In this paper, we introduce\nDecomposed Reward Models (DRMs), a novel approach that extracts diverse human\npreferences from binary comparisons without requiring fine-grained annotations.\nOur key insight is to represent human preferences as vectors and analyze them\nusing Principal Component Analysis (PCA). By constructing a dataset of\nembedding differences between preferred and rejected responses, DRMs identify\northogonal basis vectors that capture distinct aspects of preference. These\ndecomposed rewards can be flexibly combined to align with different user needs,\noffering an interpretable and scalable alternative to traditional reward\nmodels. We demonstrate that DRMs effectively extract meaningful preference\ndimensions (e.g., helpfulness, safety, humor) and adapt to new users without\nadditional training. Our results highlight DRMs as a powerful framework for\npersonalized and interpretable LLM alignment."
                },
                "authors": [
                    {
                        "name": "Feng Luo"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Chunyuan Deng"
                    },
                    {
                        "name": "Jiarui Yao"
                    },
                    {
                        "name": "Jingyan Shen"
                    },
                    {
                        "name": "Huan Zhang"
                    },
                    {
                        "name": "Hanjie Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hanjie Chen"
                },
                "author": "Hanjie Chen",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12118v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12118v2",
                "updated": "2025-02-18T18:54:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    54,
                    12,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-17T18:43:24Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    18,
                    43,
                    24,
                    0,
                    48,
                    0
                ],
                "title": "Scaling Test-Time Compute Without Verification or RL is Suboptimal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Test-Time Compute Without Verification or RL is Suboptimal"
                },
                "summary": "Despite substantial advances in scaling test-time compute, an ongoing debate\nin the community is how it should be scaled up to enable continued and\nefficient improvements with scaling. There are largely two approaches: first,\ndistilling successful search or thinking traces; and second, using verification\n(e.g., 0/1 outcome rewards, reward models, or verifiers) to guide reinforcement\nlearning (RL) and search algorithms. In this paper, we prove that finetuning\nLLMs with verifier-based (VB) methods based on RL or search is far superior to\nverifier-free (VF) approaches based on distilling or cloning search traces,\ngiven a fixed amount of compute/data budget. Further, we show that as we scale\ntest-time compute (measured as the output token length) and training data,\nsuboptimality of VF methods scales poorly compared to VB when the base\npre-trained LLM presents a heterogeneous distribution over correct solution\ntraces (e.g., different lengths, styles, etc.) and admits a non-sharp\ndistribution over rewards on traces sampled from it. We formalize this\ncondition using anti-concentration [Erd\\H{o}s, 1945]. This implies a stronger\nresult that VB methods scale better asymptotically, with the performance gap\nbetween VB and VF methods widening as test-time budget grows. We corroborate\nour theory empirically on both didactic and math reasoning problems with\n3/8/32B-sized pre-trained LLMs, where we find verification is crucial for\nscaling test-time compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite substantial advances in scaling test-time compute, an ongoing debate\nin the community is how it should be scaled up to enable continued and\nefficient improvements with scaling. There are largely two approaches: first,\ndistilling successful search or thinking traces; and second, using verification\n(e.g., 0/1 outcome rewards, reward models, or verifiers) to guide reinforcement\nlearning (RL) and search algorithms. In this paper, we prove that finetuning\nLLMs with verifier-based (VB) methods based on RL or search is far superior to\nverifier-free (VF) approaches based on distilling or cloning search traces,\ngiven a fixed amount of compute/data budget. Further, we show that as we scale\ntest-time compute (measured as the output token length) and training data,\nsuboptimality of VF methods scales poorly compared to VB when the base\npre-trained LLM presents a heterogeneous distribution over correct solution\ntraces (e.g., different lengths, styles, etc.) and admits a non-sharp\ndistribution over rewards on traces sampled from it. We formalize this\ncondition using anti-concentration [Erd\\H{o}s, 1945]. This implies a stronger\nresult that VB methods scale better asymptotically, with the performance gap\nbetween VB and VF methods widening as test-time budget grows. We corroborate\nour theory empirically on both didactic and math reasoning problems with\n3/8/32B-sized pre-trained LLMs, where we find verification is crucial for\nscaling test-time compute."
                },
                "authors": [
                    {
                        "name": "Amrith Setlur"
                    },
                    {
                        "name": "Nived Rajaraman"
                    },
                    {
                        "name": "Sergey Levine"
                    },
                    {
                        "name": "Aviral Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Aviral Kumar"
                },
                "author": "Aviral Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12118v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12118v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13127v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13127v1",
                "updated": "2025-02-18T18:50:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    50,
                    6,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T18:50:06Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    50,
                    6,
                    1,
                    49,
                    0
                ],
                "title": "Facilitating Long Context Understanding via Supervised Chain-of-Thought\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Facilitating Long Context Understanding via Supervised Chain-of-Thought\n  Reasoning"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have enabled them to process\nincreasingly longer sequences, ranging from 2K to 2M tokens and even beyond.\nHowever, simply extending the input sequence length does not necessarily lead\nto effective long-context understanding. In this study, we integrate\nChain-of-Thought (CoT) reasoning into LLMs in a supervised manner to facilitate\neffective long-context understanding. To achieve this, we introduce\nLongFinanceQA, a synthetic dataset in the financial domain designed to improve\nlong-context reasoning. Unlike existing long-context synthetic data,\nLongFinanceQA includes intermediate CoT reasoning before the final conclusion,\nwhich encourages LLMs to perform explicit reasoning, improving accuracy and\ninterpretability in long-context understanding. To generate synthetic CoT\nreasoning, we propose Property-driven Agentic Inference (PAI), an agentic\nframework that simulates human-like reasoning steps, including property\nextraction, retrieval, and summarization. We evaluate PAI's reasoning\ncapabilities by assessing GPT-4o-mini w/ PAI on the Loong benchmark,\noutperforming standard GPT-4o-mini by 20.0%. Furthermore, we fine-tune\nLLaMA-3.1-8B-Instruct on LongFinanceQA, achieving a 24.6% gain on Loong's\nfinancial subset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have enabled them to process\nincreasingly longer sequences, ranging from 2K to 2M tokens and even beyond.\nHowever, simply extending the input sequence length does not necessarily lead\nto effective long-context understanding. In this study, we integrate\nChain-of-Thought (CoT) reasoning into LLMs in a supervised manner to facilitate\neffective long-context understanding. To achieve this, we introduce\nLongFinanceQA, a synthetic dataset in the financial domain designed to improve\nlong-context reasoning. Unlike existing long-context synthetic data,\nLongFinanceQA includes intermediate CoT reasoning before the final conclusion,\nwhich encourages LLMs to perform explicit reasoning, improving accuracy and\ninterpretability in long-context understanding. To generate synthetic CoT\nreasoning, we propose Property-driven Agentic Inference (PAI), an agentic\nframework that simulates human-like reasoning steps, including property\nextraction, retrieval, and summarization. We evaluate PAI's reasoning\ncapabilities by assessing GPT-4o-mini w/ PAI on the Loong benchmark,\noutperforming standard GPT-4o-mini by 20.0%. Furthermore, we fine-tune\nLLaMA-3.1-8B-Instruct on LongFinanceQA, achieving a 24.6% gain on Loong's\nfinancial subset."
                },
                "authors": [
                    {
                        "name": "Jingyang Lin"
                    },
                    {
                        "name": "Andy Wong"
                    },
                    {
                        "name": "Tian Xia"
                    },
                    {
                        "name": "Shenghua He"
                    },
                    {
                        "name": "Hui Wei"
                    },
                    {
                        "name": "Mei Han"
                    },
                    {
                        "name": "Jiebo Luo"
                    }
                ],
                "author_detail": {
                    "name": "Jiebo Luo"
                },
                "author": "Jiebo Luo",
                "arxiv_comment": "15 Pages, 6 Tables, 8 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13127v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13127v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13125v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13125v1",
                "updated": "2025-02-18T18:47:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    47,
                    11,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T18:47:11Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    47,
                    11,
                    1,
                    49,
                    0
                ],
                "title": "RuozhiBench: Evaluating LLMs with Logical Fallacies and Misleading\n  Premises",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RuozhiBench: Evaluating LLMs with Logical Fallacies and Misleading\n  Premises"
                },
                "summary": "Recent advances in large language models (LLMs) have shown that they can\nanswer questions requiring complex reasoning. However, their ability to\nidentify and respond to text containing logical fallacies or deliberately\nmisleading premises remains less studied. To address this gap, we introduce\nRuozhiBench, a bilingual dataset comprising 677 carefully curated questions\nthat contain various forms of deceptive reasoning, meticulously crafted through\nextensive human effort and expert review. In a comprehensive evaluation of 17\nLLMs from 5 Series over RuozhiBench using both open-ended and two-choice\nformats, we conduct extensive analyses on evaluation protocols and result\npatterns. Despite their high scores on conventional benchmarks, these models\nshowed limited ability to detect and reason correctly about logical fallacies,\nwith even the best-performing model, Claude-3-haiku, achieving only 62%\naccuracy compared to the human of more than 90%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have shown that they can\nanswer questions requiring complex reasoning. However, their ability to\nidentify and respond to text containing logical fallacies or deliberately\nmisleading premises remains less studied. To address this gap, we introduce\nRuozhiBench, a bilingual dataset comprising 677 carefully curated questions\nthat contain various forms of deceptive reasoning, meticulously crafted through\nextensive human effort and expert review. In a comprehensive evaluation of 17\nLLMs from 5 Series over RuozhiBench using both open-ended and two-choice\nformats, we conduct extensive analyses on evaluation protocols and result\npatterns. Despite their high scores on conventional benchmarks, these models\nshowed limited ability to detect and reason correctly about logical fallacies,\nwith even the best-performing model, Claude-3-haiku, achieving only 62%\naccuracy compared to the human of more than 90%."
                },
                "authors": [
                    {
                        "name": "Zenan Zhai"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Xudong Han"
                    },
                    {
                        "name": "Zhenxuan Zhang"
                    },
                    {
                        "name": "Yixuan Zhang"
                    },
                    {
                        "name": "Timothy Baldwin"
                    },
                    {
                        "name": "Haonan Li"
                    }
                ],
                "author_detail": {
                    "name": "Haonan Li"
                },
                "author": "Haonan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13125v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13125v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13120v1",
                "updated": "2025-02-18T18:42:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    42,
                    11,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T18:42:11Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    42,
                    11,
                    1,
                    49,
                    0
                ],
                "title": "Adapting Psycholinguistic Research for LLMs: Gender-inclusive Language\n  in a Coreference Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting Psycholinguistic Research for LLMs: Gender-inclusive Language\n  in a Coreference Context"
                },
                "summary": "Gender-inclusive language is often used with the aim of ensuring that all\nindividuals, regardless of gender, can be associated with certain concepts.\nWhile psycholinguistic studies have examined its effects in relation to human\ncognition, it remains unclear how Large Language Models (LLMs) process\ngender-inclusive language. Given that commercial LLMs are gaining an\nincreasingly strong foothold in everyday applications, it is crucial to examine\nwhether LLMs in fact interpret gender-inclusive language neutrally, because the\nlanguage they generate has the potential to influence the language of their\nusers. This study examines whether LLM-generated coreferent terms align with a\ngiven gender expression or reflect model biases. Adapting psycholinguistic\nmethods from French to English and German, we find that in English, LLMs\ngenerally maintain the antecedent's gender but exhibit underlying masculine\nbias. In German, this bias is much stronger, overriding all tested\ngender-neutralization strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gender-inclusive language is often used with the aim of ensuring that all\nindividuals, regardless of gender, can be associated with certain concepts.\nWhile psycholinguistic studies have examined its effects in relation to human\ncognition, it remains unclear how Large Language Models (LLMs) process\ngender-inclusive language. Given that commercial LLMs are gaining an\nincreasingly strong foothold in everyday applications, it is crucial to examine\nwhether LLMs in fact interpret gender-inclusive language neutrally, because the\nlanguage they generate has the potential to influence the language of their\nusers. This study examines whether LLM-generated coreferent terms align with a\ngiven gender expression or reflect model biases. Adapting psycholinguistic\nmethods from French to English and German, we find that in English, LLMs\ngenerally maintain the antecedent's gender but exhibit underlying masculine\nbias. In German, this bias is much stronger, overriding all tested\ngender-neutralization strategies."
                },
                "authors": [
                    {
                        "name": "Marion Bartl"
                    },
                    {
                        "name": "Thomas Brendan Murphy"
                    },
                    {
                        "name": "Susan Leavy"
                    }
                ],
                "author_detail": {
                    "name": "Susan Leavy"
                },
                "author": "Susan Leavy",
                "arxiv_comment": "9 pages, 7 figures, submitted to ACL 2025 (ARR February 2025 cycle)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13119v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13119v1",
                "updated": "2025-02-18T18:42:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    42,
                    9,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T18:42:09Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    42,
                    9,
                    1,
                    49,
                    0
                ],
                "title": "STEER-ME: Assessing the Microeconomic Reasoning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STEER-ME: Assessing the Microeconomic Reasoning of Large Language Models"
                },
                "summary": "How should one judge whether a given large language model (LLM) can reliably\nperform economic reasoning? Most existing LLM benchmarks focus on specific\napplications and fail to present the model with a rich variety of economic\ntasks. A notable exception is Raman et al. [2024], who offer an approach for\ncomprehensively benchmarking strategic decision-making; however, this approach\nfails to address the non-strategic settings prevalent in microeconomics, such\nas supply-and-demand analysis. We address this gap by taxonomizing\nmicroeconomic reasoning into $58$ distinct elements, focusing on the logic of\nsupply and demand, each grounded in up to $10$ distinct domains, $5$\nperspectives, and $3$ types. The generation of benchmark data across this\ncombinatorial space is powered by a novel LLM-assisted data generation protocol\nthat we dub auto-STEER, which generates a set of questions by adapting\nhandwritten templates to target new domains and perspectives. Because it offers\nan automated way of generating fresh questions, auto-STEER mitigates the risk\nthat LLMs will be trained to over-fit evaluation benchmarks; we thus hope that\nit will serve as a useful tool both for evaluating and fine-tuning models for\nyears to come. We demonstrate the usefulness of our benchmark via a case study\non $27$ LLMs, ranging from small open-source models to the current state of the\nart. We examined each model's ability to solve microeconomic problems across\nour whole taxonomy and present the results across a range of prompting\nstrategies and scoring metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How should one judge whether a given large language model (LLM) can reliably\nperform economic reasoning? Most existing LLM benchmarks focus on specific\napplications and fail to present the model with a rich variety of economic\ntasks. A notable exception is Raman et al. [2024], who offer an approach for\ncomprehensively benchmarking strategic decision-making; however, this approach\nfails to address the non-strategic settings prevalent in microeconomics, such\nas supply-and-demand analysis. We address this gap by taxonomizing\nmicroeconomic reasoning into $58$ distinct elements, focusing on the logic of\nsupply and demand, each grounded in up to $10$ distinct domains, $5$\nperspectives, and $3$ types. The generation of benchmark data across this\ncombinatorial space is powered by a novel LLM-assisted data generation protocol\nthat we dub auto-STEER, which generates a set of questions by adapting\nhandwritten templates to target new domains and perspectives. Because it offers\nan automated way of generating fresh questions, auto-STEER mitigates the risk\nthat LLMs will be trained to over-fit evaluation benchmarks; we thus hope that\nit will serve as a useful tool both for evaluating and fine-tuning models for\nyears to come. We demonstrate the usefulness of our benchmark via a case study\non $27$ LLMs, ranging from small open-source models to the current state of the\nart. We examined each model's ability to solve microeconomic problems across\nour whole taxonomy and present the results across a range of prompting\nstrategies and scoring metrics."
                },
                "authors": [
                    {
                        "name": "Narun Raman"
                    },
                    {
                        "name": "Taylor Lundy"
                    },
                    {
                        "name": "Thiago Amin"
                    },
                    {
                        "name": "Jesse Perla"
                    },
                    {
                        "name": "Kevin-Leyton Brown"
                    }
                ],
                "author_detail": {
                    "name": "Kevin-Leyton Brown"
                },
                "author": "Kevin-Leyton Brown",
                "arxiv_comment": "18 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13119v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13119v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13117v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13117v1",
                "updated": "2025-02-18T18:37:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    37,
                    15,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T18:37:15Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    37,
                    15,
                    1,
                    49,
                    0
                ],
                "title": "Performance Evaluation of Large Language Models in Statistical\n  Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Evaluation of Large Language Models in Statistical\n  Programming"
                },
                "summary": "The programming capabilities of large language models (LLMs) have\nrevolutionized automatic code generation and opened new avenues for automatic\nstatistical analysis. However, the validity and quality of these generated\ncodes need to be systematically evaluated before they can be widely adopted.\nDespite their growing prominence, a comprehensive evaluation of statistical\ncode generated by LLMs remains scarce in the literature. In this paper, we\nassess the performance of LLMs, including two versions of ChatGPT and one\nversion of Llama, in the domain of SAS programming for statistical analysis.\nOur study utilizes a set of statistical analysis tasks encompassing diverse\nstatistical topics and datasets. Each task includes a problem description,\ndataset information, and human-verified SAS code. We conduct a comprehensive\nassessment of the quality of SAS code generated by LLMs through human expert\nevaluation based on correctness, effectiveness, readability, executability, and\nthe accuracy of output results. The analysis of rating scores reveals that\nwhile LLMs demonstrate usefulness in generating syntactically correct code,\nthey struggle with tasks requiring deep domain understanding and may produce\nredundant or incorrect results. This study offers valuable insights into the\ncapabilities and limitations of LLMs in statistical programming, providing\nguidance for future advancements in AI-assisted coding systems for statistical\nanalysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The programming capabilities of large language models (LLMs) have\nrevolutionized automatic code generation and opened new avenues for automatic\nstatistical analysis. However, the validity and quality of these generated\ncodes need to be systematically evaluated before they can be widely adopted.\nDespite their growing prominence, a comprehensive evaluation of statistical\ncode generated by LLMs remains scarce in the literature. In this paper, we\nassess the performance of LLMs, including two versions of ChatGPT and one\nversion of Llama, in the domain of SAS programming for statistical analysis.\nOur study utilizes a set of statistical analysis tasks encompassing diverse\nstatistical topics and datasets. Each task includes a problem description,\ndataset information, and human-verified SAS code. We conduct a comprehensive\nassessment of the quality of SAS code generated by LLMs through human expert\nevaluation based on correctness, effectiveness, readability, executability, and\nthe accuracy of output results. The analysis of rating scores reveals that\nwhile LLMs demonstrate usefulness in generating syntactically correct code,\nthey struggle with tasks requiring deep domain understanding and may produce\nredundant or incorrect results. This study offers valuable insights into the\ncapabilities and limitations of LLMs in statistical programming, providing\nguidance for future advancements in AI-assisted coding systems for statistical\nanalysis."
                },
                "authors": [
                    {
                        "name": "Xinyi Song"
                    },
                    {
                        "name": "Kexin Xie"
                    },
                    {
                        "name": "Lina Lee"
                    },
                    {
                        "name": "Ruizhe Chen"
                    },
                    {
                        "name": "Jared M. Clark"
                    },
                    {
                        "name": "Hao He"
                    },
                    {
                        "name": "Haoran He"
                    },
                    {
                        "name": "Jie Min"
                    },
                    {
                        "name": "Xinlei Zhang"
                    },
                    {
                        "name": "Simin Zheng"
                    },
                    {
                        "name": "Zhiyang Zhang"
                    },
                    {
                        "name": "Xinwei Deng"
                    },
                    {
                        "name": "Yili Hong"
                    }
                ],
                "author_detail": {
                    "name": "Yili Hong"
                },
                "author": "Yili Hong",
                "arxiv_comment": "27 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13117v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01171v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01171v2",
                "updated": "2025-02-18T18:32:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    32,
                    25,
                    1,
                    49,
                    0
                ],
                "published": "2024-10-02T01:59:07Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    1,
                    59,
                    7,
                    2,
                    276,
                    0
                ],
                "title": "Multilingual Retrieval Augmented Generation for Culturally-Sensitive\n  Tasks: A Benchmark for Cross-lingual Robustness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Retrieval Augmented Generation for Culturally-Sensitive\n  Tasks: A Benchmark for Cross-lingual Robustness"
                },
                "summary": "The paradigm of retrieval-augmented generated (RAG) helps mitigate\nhallucinations of large language models (LLMs). However, RAG also introduces\nbiases contained within the retrieved documents. These biases can be amplified\nin scenarios which are multilingual and culturally-sensitive, such as\nterritorial disputes. In this paper, we introduce BordIRLines, a benchmark\nconsisting of 720 territorial dispute queries paired with 14k Wikipedia\ndocuments across 49 languages. To evaluate LLMs' cross-lingual robustness for\nthis task, we formalize several modes for multilingual retrieval. Our\nexperiments on several LLMs reveal that retrieving multilingual documents best\nimproves response consistency and decreases geopolitical bias over using purely\nin-language documents, showing how incorporating diverse perspectives improves\nrobustness. Also, querying in low-resource languages displays a much wider\nvariance in the linguistic distribution of response citations. Our further\nexperiments and case studies investigate how cross-lingual RAG is affected by\naspects from IR to document contents. We release our benchmark and code to\nsupport further research towards ensuring equitable information access across\nlanguages at https://huggingface.co/datasets/borderlines/bordirlines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The paradigm of retrieval-augmented generated (RAG) helps mitigate\nhallucinations of large language models (LLMs). However, RAG also introduces\nbiases contained within the retrieved documents. These biases can be amplified\nin scenarios which are multilingual and culturally-sensitive, such as\nterritorial disputes. In this paper, we introduce BordIRLines, a benchmark\nconsisting of 720 territorial dispute queries paired with 14k Wikipedia\ndocuments across 49 languages. To evaluate LLMs' cross-lingual robustness for\nthis task, we formalize several modes for multilingual retrieval. Our\nexperiments on several LLMs reveal that retrieving multilingual documents best\nimproves response consistency and decreases geopolitical bias over using purely\nin-language documents, showing how incorporating diverse perspectives improves\nrobustness. Also, querying in low-resource languages displays a much wider\nvariance in the linguistic distribution of response citations. Our further\nexperiments and case studies investigate how cross-lingual RAG is affected by\naspects from IR to document contents. We release our benchmark and code to\nsupport further research towards ensuring equitable information access across\nlanguages at https://huggingface.co/datasets/borderlines/bordirlines."
                },
                "authors": [
                    {
                        "name": "Bryan Li"
                    },
                    {
                        "name": "Fiona Luo"
                    },
                    {
                        "name": "Samar Haider"
                    },
                    {
                        "name": "Adwait Agashe"
                    },
                    {
                        "name": "Tammy Li"
                    },
                    {
                        "name": "Runqi Liu"
                    },
                    {
                        "name": "Muqing Miao"
                    },
                    {
                        "name": "Shriya Ramakrishnan"
                    },
                    {
                        "name": "Yuan Yuan"
                    },
                    {
                        "name": "Chris Callison-Burch"
                    }
                ],
                "author_detail": {
                    "name": "Chris Callison-Burch"
                },
                "author": "Chris Callison-Burch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01171v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01171v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10923v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10923v2",
                "updated": "2025-02-18T18:25:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    25,
                    56,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-15T22:47:05Z",
                "published_parsed": [
                    2025,
                    2,
                    15,
                    22,
                    47,
                    5,
                    5,
                    46,
                    0
                ],
                "title": "Phoenix -- A Novel Technique for Performance-Aware Orchestration of\n  Thread and Page Table Placement in NUMA Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phoenix -- A Novel Technique for Performance-Aware Orchestration of\n  Thread and Page Table Placement in NUMA Systems"
                },
                "summary": "The emergence of symmetric multi-processing (SMP) systems with non-uniform\nmemory access (NUMA) has prompted extensive research on process and data\nplacement to mitigate the performance impact of NUMA on applications. However,\nexisting solutions often overlook the coordination between the CPU scheduler\nand memory manager, leading to inefficient thread and page table placement.\nMoreover, replication techniques employed to improve locality suffer from\nredundant replicas, scalability barriers, and performance degradation due to\nmemory bandwidth and inter-socket interference. In this paper, we present\nPhoenix, a novel integrated CPU scheduler and memory manager with on-demand\npage table replication mechanism. Phoenix integrates the CPU scheduler and\nmemory management subsystems, allowing for coordinated thread and page table\nplacement. By differentiating between data and page table pages, Phoenix\nenables direct migration or replication of page tables based on application\nbehavior. Additionally, Phoenix employs memory bandwidth management mechanism\nto maintain Quality of Service (QoS) while mitigating coherency maintenance\noverhead. We implemented Phoenix as a loadable kernel module for Linux,\nensuring compatibility with legacy applications and ease of deployment. Our\nevaluation on real hardware demonstrates that Phoenix reduces CPU cycles by\n2.09x and page-walk cycles by 1.58x compared to state-of-the-art solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of symmetric multi-processing (SMP) systems with non-uniform\nmemory access (NUMA) has prompted extensive research on process and data\nplacement to mitigate the performance impact of NUMA on applications. However,\nexisting solutions often overlook the coordination between the CPU scheduler\nand memory manager, leading to inefficient thread and page table placement.\nMoreover, replication techniques employed to improve locality suffer from\nredundant replicas, scalability barriers, and performance degradation due to\nmemory bandwidth and inter-socket interference. In this paper, we present\nPhoenix, a novel integrated CPU scheduler and memory manager with on-demand\npage table replication mechanism. Phoenix integrates the CPU scheduler and\nmemory management subsystems, allowing for coordinated thread and page table\nplacement. By differentiating between data and page table pages, Phoenix\nenables direct migration or replication of page tables based on application\nbehavior. Additionally, Phoenix employs memory bandwidth management mechanism\nto maintain Quality of Service (QoS) while mitigating coherency maintenance\noverhead. We implemented Phoenix as a loadable kernel module for Linux,\nensuring compatibility with legacy applications and ease of deployment. Our\nevaluation on real hardware demonstrates that Phoenix reduces CPU cycles by\n2.09x and page-walk cycles by 1.58x compared to state-of-the-art solutions."
                },
                "authors": [
                    {
                        "name": "Mohammad Siavashi"
                    },
                    {
                        "name": "Alireza Sanaee"
                    },
                    {
                        "name": "Mohsen Sharifi"
                    },
                    {
                        "name": "Gianni Antichi"
                    }
                ],
                "author_detail": {
                    "name": "Gianni Antichi"
                },
                "author": "Gianni Antichi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10923v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10923v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13107v1",
                "updated": "2025-02-18T18:19:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    19,
                    36,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T18:19:36Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    19,
                    36,
                    1,
                    49,
                    0
                ],
                "title": "MatterChat: A Multi-Modal LLM for Material Science",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MatterChat: A Multi-Modal LLM for Material Science"
                },
                "summary": "Understanding and predicting the properties of inorganic materials is crucial\nfor accelerating advancements in materials science and driving applications in\nenergy, electronics, and beyond. Integrating material structure data with\nlanguage-based information through multi-modal large language models (LLMs)\noffers great potential to support these efforts by enhancing human-AI\ninteraction. However, a key challenge lies in integrating atomic structures at\nfull resolution into LLMs. In this work, we introduce MatterChat, a versatile\nstructure-aware multi-modal LLM that unifies material structural data and\ntextual inputs into a single cohesive model. MatterChat employs a bridging\nmodule to effectively align a pretrained machine learning interatomic potential\nwith a pretrained LLM, reducing training costs and enhancing flexibility. Our\nresults demonstrate that MatterChat significantly improves performance in\nmaterial property prediction and human-AI interaction, surpassing\ngeneral-purpose LLMs such as GPT-4. We also demonstrate its usefulness in\napplications such as more advanced scientific reasoning and step-by-step\nmaterial synthesis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and predicting the properties of inorganic materials is crucial\nfor accelerating advancements in materials science and driving applications in\nenergy, electronics, and beyond. Integrating material structure data with\nlanguage-based information through multi-modal large language models (LLMs)\noffers great potential to support these efforts by enhancing human-AI\ninteraction. However, a key challenge lies in integrating atomic structures at\nfull resolution into LLMs. In this work, we introduce MatterChat, a versatile\nstructure-aware multi-modal LLM that unifies material structural data and\ntextual inputs into a single cohesive model. MatterChat employs a bridging\nmodule to effectively align a pretrained machine learning interatomic potential\nwith a pretrained LLM, reducing training costs and enhancing flexibility. Our\nresults demonstrate that MatterChat significantly improves performance in\nmaterial property prediction and human-AI interaction, surpassing\ngeneral-purpose LLMs such as GPT-4. We also demonstrate its usefulness in\napplications such as more advanced scientific reasoning and step-by-step\nmaterial synthesis."
                },
                "authors": [
                    {
                        "name": "Yingheng Tang"
                    },
                    {
                        "name": "Wenbin Xu"
                    },
                    {
                        "name": "Jie Cao"
                    },
                    {
                        "name": "Jianzhu Ma"
                    },
                    {
                        "name": "Weilu Gao"
                    },
                    {
                        "name": "Steve Farrell"
                    },
                    {
                        "name": "Benjamin Erichson"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Andy Nonaka"
                    },
                    {
                        "name": "Zhi Yao"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Yao"
                },
                "author": "Zhi Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13101v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13101v1",
                "updated": "2025-02-18T18:11:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    11,
                    39,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T18:11:39Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    11,
                    39,
                    1,
                    49,
                    0
                ],
                "title": "AI and the Transformation of Accountability and Discretion in Urban\n  Governance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI and the Transformation of Accountability and Discretion in Urban\n  Governance"
                },
                "summary": "The integration of Artificial Intelligence (AI) in urban governance presents\nsignificant opportunities to transform decision-making and enhance\naccountability. The paper highlights AI's potential to reposition human\ndiscretion and reshape specific types of accountability, elevating the\ndecision-making capabilities of both frontline bureaucrats and managers while\nensuring ethical standards and public trust are maintained. While AI can\nenhance bureaucratic flexibility and efficiency, its integration will also\nnecessitate new governance frameworks to mitigate risks associated with uneven\ncapacity distribution, ethical concerns, and public trust. Following the\nliterature review and theoretical discussion, this study introduces a set of\nguiding principles for AI-assisted urban governance, emphasizing equitable AI\ndeployment, adaptive administrative structures, robust data governance,\ntransparent human-AI collaboration, and citizen engagement in oversight\nmechanisms. By critically evaluating AI's dual role in expanding discretion and\nreinforcing accountability, this paper advances a framework for responsible AI\nadoption, ensuring that urban governance remains adaptive, transparent, and\naligned with public values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Artificial Intelligence (AI) in urban governance presents\nsignificant opportunities to transform decision-making and enhance\naccountability. The paper highlights AI's potential to reposition human\ndiscretion and reshape specific types of accountability, elevating the\ndecision-making capabilities of both frontline bureaucrats and managers while\nensuring ethical standards and public trust are maintained. While AI can\nenhance bureaucratic flexibility and efficiency, its integration will also\nnecessitate new governance frameworks to mitigate risks associated with uneven\ncapacity distribution, ethical concerns, and public trust. Following the\nliterature review and theoretical discussion, this study introduces a set of\nguiding principles for AI-assisted urban governance, emphasizing equitable AI\ndeployment, adaptive administrative structures, robust data governance,\ntransparent human-AI collaboration, and citizen engagement in oversight\nmechanisms. By critically evaluating AI's dual role in expanding discretion and\nreinforcing accountability, this paper advances a framework for responsible AI\nadoption, ensuring that urban governance remains adaptive, transparent, and\naligned with public values."
                },
                "authors": [
                    {
                        "name": "Stephen Goldsmith"
                    },
                    {
                        "name": "Juncheng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Juncheng Yang"
                },
                "arxiv_affiliation": "Tony",
                "author": "Juncheng Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13101v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13101v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08820v2",
                "updated": "2025-02-18T18:08:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    8,
                    56,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-12T22:18:34Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    22,
                    18,
                    34,
                    2,
                    43,
                    0
                ],
                "title": "Can a Single Model Master Both Multi-turn Conversations and Tool Use?\n  CoALM: A Unified Conversational Agentic Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can a Single Model Master Both Multi-turn Conversations and Tool Use?\n  CoALM: A Unified Conversational Agentic Language Model"
                },
                "summary": "Large Language Models (LLMs) with API-calling capabilities enabled building\neffective Language Agents (LA), while also revolutionizing the conventional\ntask-oriented dialogue (TOD) paradigm. However, current approaches face a\ncritical dilemma: TOD systems are often trained on a limited set of target\nAPIs, requiring new data to maintain their quality when interfacing with new\nservices, while LAs are not trained to maintain user intent over multi-turn\nconversations. Because both robust multi-turn management and advanced function\ncalling are crucial for effective conversational agents, we evaluate these\nskills on three popular benchmarks: MultiWOZ 2.4 (TOD), BFCL V3 (LA), and\nAPI-Bank (LA), and our analyses reveal that specialized approaches excel in one\ndomain but underperform in the other. To bridge this chasm, we introduce CoALM\n(Conversational Agentic Language Model), a unified approach that integrates\nboth conversational and agentic capabilities. We created CoALM-IT, a carefully\nconstructed multi-task dataset that interleave multi-turn ReAct reasoning with\ncomplex API usage. Using CoALM-IT, we train three models CoALM 8B, CoALM 70B,\nand CoALM 405B, which outperform top domain-specific models, including GPT-4o,\nacross all three benchmarks.This demonstrates the feasibility of a single model\napproach for both TOD and LA, setting a new standard for conversational agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with API-calling capabilities enabled building\neffective Language Agents (LA), while also revolutionizing the conventional\ntask-oriented dialogue (TOD) paradigm. However, current approaches face a\ncritical dilemma: TOD systems are often trained on a limited set of target\nAPIs, requiring new data to maintain their quality when interfacing with new\nservices, while LAs are not trained to maintain user intent over multi-turn\nconversations. Because both robust multi-turn management and advanced function\ncalling are crucial for effective conversational agents, we evaluate these\nskills on three popular benchmarks: MultiWOZ 2.4 (TOD), BFCL V3 (LA), and\nAPI-Bank (LA), and our analyses reveal that specialized approaches excel in one\ndomain but underperform in the other. To bridge this chasm, we introduce CoALM\n(Conversational Agentic Language Model), a unified approach that integrates\nboth conversational and agentic capabilities. We created CoALM-IT, a carefully\nconstructed multi-task dataset that interleave multi-turn ReAct reasoning with\ncomplex API usage. Using CoALM-IT, we train three models CoALM 8B, CoALM 70B,\nand CoALM 405B, which outperform top domain-specific models, including GPT-4o,\nacross all three benchmarks.This demonstrates the feasibility of a single model\napproach for both TOD and LA, setting a new standard for conversational agents."
                },
                "authors": [
                    {
                        "name": "Emre Can Acikgoz"
                    },
                    {
                        "name": "Jeremiah Greer"
                    },
                    {
                        "name": "Akul Datta"
                    },
                    {
                        "name": "Ze Yang"
                    },
                    {
                        "name": "William Zeng"
                    },
                    {
                        "name": "Oussama Elachqar"
                    },
                    {
                        "name": "Emmanouil Koukoumidis"
                    },
                    {
                        "name": "Dilek Hakkani-Tr"
                    },
                    {
                        "name": "Gokhan Tur"
                    }
                ],
                "author_detail": {
                    "name": "Gokhan Tur"
                },
                "author": "Gokhan Tur",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13096v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13096v1",
                "updated": "2025-02-18T18:08:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    8,
                    6,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T18:08:06Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    8,
                    6,
                    1,
                    49,
                    0
                ],
                "title": "Validation and demonstration of the AEFC as a practical safeguards tool\n  for inventory verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Validation and demonstration of the AEFC as a practical safeguards tool\n  for inventory verification"
                },
                "summary": "The Advanced Experimental Fuel Counter (AEFC) is a nondestructive assay (NDA)\ninstrument designed to determine the residual fissile mass in irradiated fuel\nassemblies for safeguards verification purposes. This is done by actively\ninterrogating an assembly with a neutron source and measuring the total\n(Singles) and correlated (Doubles) neutron count rates resulting from induced\nfissions in the irradiated nuclear fuel and relating those rates to the\nresidual fissile mass using calibration curves. Comprehensive NDA measurements\nof the irradiated fuel inventory at Israeli Research Reactor 1 (IRR-1) were\ntaken with the AEFC to validate a set of previously developed calibration\ncurves. During the campaign, measurements were acquired of 32 standard fuel\nassemblies and three control assemblies in just nine days. This is a\nsignificant majority of the research reactor's irradiated fuel inventory and\nthe largest data set gathered by the AEFC to date. Many of the fuel assemblies\nmeasured during the campaign had much shorter cooling times than those\nassemblies previously measured with the instrument. Calibration curves\ndeveloped from previous AEFC deployments were used to determine the residual\nU-235 mass in the measured standard fuel assemblies. The results of the\ncampaign demonstrated that the AEFC can be used to estimate the U-235 mass\nremaining in a large number of irradiated fuel assemblies with 1%-5%\nuncertainty in a reasonable amount of time despite operating in a high dose\nenvironment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Advanced Experimental Fuel Counter (AEFC) is a nondestructive assay (NDA)\ninstrument designed to determine the residual fissile mass in irradiated fuel\nassemblies for safeguards verification purposes. This is done by actively\ninterrogating an assembly with a neutron source and measuring the total\n(Singles) and correlated (Doubles) neutron count rates resulting from induced\nfissions in the irradiated nuclear fuel and relating those rates to the\nresidual fissile mass using calibration curves. Comprehensive NDA measurements\nof the irradiated fuel inventory at Israeli Research Reactor 1 (IRR-1) were\ntaken with the AEFC to validate a set of previously developed calibration\ncurves. During the campaign, measurements were acquired of 32 standard fuel\nassemblies and three control assemblies in just nine days. This is a\nsignificant majority of the research reactor's irradiated fuel inventory and\nthe largest data set gathered by the AEFC to date. Many of the fuel assemblies\nmeasured during the campaign had much shorter cooling times than those\nassemblies previously measured with the instrument. Calibration curves\ndeveloped from previous AEFC deployments were used to determine the residual\nU-235 mass in the measured standard fuel assemblies. The results of the\ncampaign demonstrated that the AEFC can be used to estimate the U-235 mass\nremaining in a large number of irradiated fuel assemblies with 1%-5%\nuncertainty in a reasonable amount of time despite operating in a high dose\nenvironment."
                },
                "authors": [
                    {
                        "name": "G. Long"
                    },
                    {
                        "name": "G. Gabrieli"
                    },
                    {
                        "name": "I. Levy"
                    },
                    {
                        "name": "A. Pesach"
                    },
                    {
                        "name": "I. Zilberman"
                    },
                    {
                        "name": "K. Ben-Meir"
                    },
                    {
                        "name": "O. Ozeri"
                    },
                    {
                        "name": "L. Zilberfarb"
                    },
                    {
                        "name": "O. Rivin"
                    },
                    {
                        "name": "A. Krakovich"
                    },
                    {
                        "name": "I. Martinez"
                    },
                    {
                        "name": "C. Rael"
                    },
                    {
                        "name": "M. Watson"
                    },
                    {
                        "name": "A. Trahan"
                    },
                    {
                        "name": "M. L. Ruch"
                    },
                    {
                        "name": "U. Steinitz"
                    }
                ],
                "author_detail": {
                    "name": "U. Steinitz"
                },
                "author": "U. Steinitz",
                "arxiv_comment": "20 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13096v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13096v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13095v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13095v1",
                "updated": "2025-02-18T18:06:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    6,
                    48,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T18:06:48Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    6,
                    48,
                    1,
                    49,
                    0
                ],
                "title": "Understanding and Rectifying Safety Perception Distortion in VLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and Rectifying Safety Perception Distortion in VLMs"
                },
                "summary": "Recent studies reveal that vision-language models (VLMs) become more\nsusceptible to harmful requests and jailbreak attacks after integrating the\nvision modality, exhibiting greater vulnerability than their text-only LLM\nbackbones. To uncover the root cause of this phenomenon, we conduct an in-depth\nanalysis and identify a key issue: multimodal inputs introduce an\nmodality-induced activation shift toward a \"safer\" direction compared to their\ntext-only counterparts, leading VLMs to systematically overestimate the safety\nof harmful inputs. We refer to this issue as safety perception distortion. To\nmitigate such distortion, we propose Activation Shift Disentanglement and\nCalibration (ShiftDC), a training-free method that decomposes and calibrates\nthe modality-induced activation shift to reduce the impact of modality on\nsafety. By isolating and removing the safety-relevant component, ShiftDC\nrestores the inherent safety alignment of the LLM backbone while preserving the\nvision-language capabilities of VLMs. Empirical results demonstrate that\nShiftDC significantly enhances alignment performance on safety benchmarks\nwithout impairing model utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies reveal that vision-language models (VLMs) become more\nsusceptible to harmful requests and jailbreak attacks after integrating the\nvision modality, exhibiting greater vulnerability than their text-only LLM\nbackbones. To uncover the root cause of this phenomenon, we conduct an in-depth\nanalysis and identify a key issue: multimodal inputs introduce an\nmodality-induced activation shift toward a \"safer\" direction compared to their\ntext-only counterparts, leading VLMs to systematically overestimate the safety\nof harmful inputs. We refer to this issue as safety perception distortion. To\nmitigate such distortion, we propose Activation Shift Disentanglement and\nCalibration (ShiftDC), a training-free method that decomposes and calibrates\nthe modality-induced activation shift to reduce the impact of modality on\nsafety. By isolating and removing the safety-relevant component, ShiftDC\nrestores the inherent safety alignment of the LLM backbone while preserving the\nvision-language capabilities of VLMs. Empirical results demonstrate that\nShiftDC significantly enhances alignment performance on safety benchmarks\nwithout impairing model utility."
                },
                "authors": [
                    {
                        "name": "Xiaohan Zou"
                    },
                    {
                        "name": "Jian Kang"
                    },
                    {
                        "name": "George Kesidis"
                    },
                    {
                        "name": "Lu Lin"
                    }
                ],
                "author_detail": {
                    "name": "Lu Lin"
                },
                "author": "Lu Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13095v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13095v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13092v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13092v1",
                "updated": "2025-02-18T17:59:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    59,
                    48,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T17:59:48Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    59,
                    48,
                    1,
                    49,
                    0
                ],
                "title": "Text2World: Benchmarking Large Language Models for Symbolic World Model\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text2World: Benchmarking Large Language Models for Symbolic World Model\n  Generation"
                },
                "summary": "Recently, there has been growing interest in leveraging large language models\n(LLMs) to generate symbolic world models from textual descriptions. Although\nLLMs have been extensively explored in the context of world modeling, prior\nstudies encountered several challenges, including evaluation randomness,\ndependence on indirect metrics, and a limited domain scope. To address these\nlimitations, we introduce a novel benchmark, Text2World, based on planning\ndomain definition language (PDDL), featuring hundreds of diverse domains and\nemploying multi-criteria, execution-based metrics for a more robust evaluation.\nWe benchmark current LLMs using Text2World and find that reasoning models\ntrained with large-scale reinforcement learning outperform others. However,\neven the best-performing model still demonstrates limited capabilities in world\nmodeling. Building on these insights, we examine several promising strategies\nto enhance the world modeling capabilities of LLMs, including test-time\nscaling, agent training, and more. We hope that Text2World can serve as a\ncrucial resource, laying the groundwork for future research in leveraging LLMs\nas world models. The project page is available at\nhttps://text-to-world.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, there has been growing interest in leveraging large language models\n(LLMs) to generate symbolic world models from textual descriptions. Although\nLLMs have been extensively explored in the context of world modeling, prior\nstudies encountered several challenges, including evaluation randomness,\ndependence on indirect metrics, and a limited domain scope. To address these\nlimitations, we introduce a novel benchmark, Text2World, based on planning\ndomain definition language (PDDL), featuring hundreds of diverse domains and\nemploying multi-criteria, execution-based metrics for a more robust evaluation.\nWe benchmark current LLMs using Text2World and find that reasoning models\ntrained with large-scale reinforcement learning outperform others. However,\neven the best-performing model still demonstrates limited capabilities in world\nmodeling. Building on these insights, we examine several promising strategies\nto enhance the world modeling capabilities of LLMs, including test-time\nscaling, agent training, and more. We hope that Text2World can serve as a\ncrucial resource, laying the groundwork for future research in leveraging LLMs\nas world models. The project page is available at\nhttps://text-to-world.github.io/."
                },
                "authors": [
                    {
                        "name": "Mengkang Hu"
                    },
                    {
                        "name": "Tianxing Chen"
                    },
                    {
                        "name": "Yude Zou"
                    },
                    {
                        "name": "Yuheng Lei"
                    },
                    {
                        "name": "Qiguang Chen"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Hongyuan Zhang"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Ping Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ping Luo"
                },
                "author": "Ping Luo",
                "arxiv_comment": "Project page: https://text-to-world.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13092v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01077v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01077v2",
                "updated": "2025-02-18T17:57:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    57,
                    26,
                    1,
                    49,
                    0
                ],
                "published": "2024-11-01T23:18:32Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    23,
                    18,
                    32,
                    4,
                    306,
                    0
                ],
                "title": "Emoji Attack: Enhancing Jailbreak Attacks Against Judge LLM Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emoji Attack: Enhancing Jailbreak Attacks Against Judge LLM Detection"
                },
                "summary": "Jailbreaking techniques trick Large Language Models (LLMs) into producing\nrestricted outputs, posing a serious threat. One line of defense is to use\nanother LLM as a Judge to evaluate the harmfulness of generated text. However,\nwe reveal that these Judge LLMs are vulnerable to token segmentation bias, an\nissue that arises when delimiters alter the tokenization process, splitting\nwords into smaller sub-tokens. This disrupts the embeddings of the entire\nsequence, reducing detection accuracy and allowing harmful content to be\nmisclassified as safe. In this paper, we introduce Emoji Attack, a novel\nstrategy that amplifies existing jailbreak prompts by exploiting token\nsegmentation bias. Our method leverages in-context learning to systematically\ninsert emojis into text before it is evaluated by a Judge LLM, inducing\nembedding distortions that significantly lower the likelihood of detecting\nunsafe content. Unlike traditional delimiters, emojis also introduce semantic\nambiguity, making them particularly effective in this attack. Through\nexperiments on state-of-the-art Judge LLMs, we demonstrate that Emoji Attack\nsubstantially reduces the \"unsafe\" prediction rate, bypassing existing\nsafeguards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreaking techniques trick Large Language Models (LLMs) into producing\nrestricted outputs, posing a serious threat. One line of defense is to use\nanother LLM as a Judge to evaluate the harmfulness of generated text. However,\nwe reveal that these Judge LLMs are vulnerable to token segmentation bias, an\nissue that arises when delimiters alter the tokenization process, splitting\nwords into smaller sub-tokens. This disrupts the embeddings of the entire\nsequence, reducing detection accuracy and allowing harmful content to be\nmisclassified as safe. In this paper, we introduce Emoji Attack, a novel\nstrategy that amplifies existing jailbreak prompts by exploiting token\nsegmentation bias. Our method leverages in-context learning to systematically\ninsert emojis into text before it is evaluated by a Judge LLM, inducing\nembedding distortions that significantly lower the likelihood of detecting\nunsafe content. Unlike traditional delimiters, emojis also introduce semantic\nambiguity, making them particularly effective in this attack. Through\nexperiments on state-of-the-art Judge LLMs, we demonstrate that Emoji Attack\nsubstantially reduces the \"unsafe\" prediction rate, bypassing existing\nsafeguards."
                },
                "authors": [
                    {
                        "name": "Zhipeng Wei"
                    },
                    {
                        "name": "Yuqi Liu"
                    },
                    {
                        "name": "N. Benjamin Erichson"
                    }
                ],
                "author_detail": {
                    "name": "N. Benjamin Erichson"
                },
                "author": "N. Benjamin Erichson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01077v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01077v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15939v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15939v2",
                "updated": "2025-02-18T17:19:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    19,
                    24,
                    1,
                    49,
                    0
                ],
                "published": "2024-10-21T12:12:21Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    12,
                    12,
                    21,
                    0,
                    295,
                    0
                ],
                "title": "CausalGraph2LLM: Evaluating LLMs for Causal Queries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CausalGraph2LLM: Evaluating LLMs for Causal Queries"
                },
                "summary": "Causality is essential in scientific research, enabling researchers to\ninterpret true relationships between variables. These causal relationships are\noften represented by causal graphs, which are directed acyclic graphs. With the\nrecent advancements in Large Language Models (LLMs), there is an increasing\ninterest in exploring their capabilities in causal reasoning and their\npotential use to hypothesize causal graphs. These tasks necessitate the LLMs to\nencode the causal graph effectively for subsequent downstream tasks. In this\npaper, we introduce CausalGraph2LLM, a comprehensive benchmark comprising over\n700k queries across diverse causal graph settings to evaluate the causal\nreasoning capabilities of LLMs. We categorize the causal queries into two\ntypes: graph-level and node-level queries. We benchmark both open-sourced and\npropriety models for our study. Our findings reveal that while LLMs show\npromise in this domain, they are highly sensitive to the encoding used. Even\ncapable models like GPT-4 and Gemini-1.5 exhibit sensitivity to encoding, with\ndeviations of about $60\\%$. We further demonstrate this sensitivity for\ndownstream causal intervention tasks. Moreover, we observe that LLMs can often\ndisplay biases when presented with contextual information about a causal graph,\npotentially stemming from their parametric memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causality is essential in scientific research, enabling researchers to\ninterpret true relationships between variables. These causal relationships are\noften represented by causal graphs, which are directed acyclic graphs. With the\nrecent advancements in Large Language Models (LLMs), there is an increasing\ninterest in exploring their capabilities in causal reasoning and their\npotential use to hypothesize causal graphs. These tasks necessitate the LLMs to\nencode the causal graph effectively for subsequent downstream tasks. In this\npaper, we introduce CausalGraph2LLM, a comprehensive benchmark comprising over\n700k queries across diverse causal graph settings to evaluate the causal\nreasoning capabilities of LLMs. We categorize the causal queries into two\ntypes: graph-level and node-level queries. We benchmark both open-sourced and\npropriety models for our study. Our findings reveal that while LLMs show\npromise in this domain, they are highly sensitive to the encoding used. Even\ncapable models like GPT-4 and Gemini-1.5 exhibit sensitivity to encoding, with\ndeviations of about $60\\%$. We further demonstrate this sensitivity for\ndownstream causal intervention tasks. Moreover, we observe that LLMs can often\ndisplay biases when presented with contextual information about a causal graph,\npotentially stemming from their parametric memory."
                },
                "authors": [
                    {
                        "name": "Ivaxi Sheth"
                    },
                    {
                        "name": "Bahare Fatemi"
                    },
                    {
                        "name": "Mario Fritz"
                    }
                ],
                "author_detail": {
                    "name": "Mario Fritz"
                },
                "author": "Mario Fritz",
                "arxiv_comment": "NAACL'25 Findings, Code - https://github.com/ivaxi0s/CausalGraph2LLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15939v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15939v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13069v1",
                "updated": "2025-02-18T17:12:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    12,
                    26,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T17:12:26Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    12,
                    26,
                    1,
                    49,
                    0
                ],
                "title": "Interactive Agents to Overcome Ambiguity in Software Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interactive Agents to Overcome Ambiguity in Software Engineering"
                },
                "summary": "AI agents are increasingly being deployed to automate tasks, often based on\nambiguous and underspecified user instructions. Making unwarranted assumptions\nand failing to ask clarifying questions can lead to suboptimal outcomes, safety\nrisks due to tool misuse, and wasted computational resources. In this work, we\nstudy the ability of LLM agents to handle ambiguous instructions in interactive\ncode generation settings by evaluating proprietary and open-weight models on\ntheir performance across three key steps: (a) leveraging interactivity to\nimprove performance in ambiguous scenarios, (b) detecting ambiguity, and (c)\nasking targeted questions. Our findings reveal that models struggle to\ndistinguish between well-specified and underspecified instructions. However,\nwhen models interact for underspecified inputs, they effectively obtain vital\ninformation from the user, leading to significant improvements in performance\nand underscoring the value of effective interaction. Our study highlights\ncritical gaps in how current state-of-the-art models handle ambiguity in\ncomplex software engineering tasks and structures the evaluation into distinct\nsteps to enable targeted improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI agents are increasingly being deployed to automate tasks, often based on\nambiguous and underspecified user instructions. Making unwarranted assumptions\nand failing to ask clarifying questions can lead to suboptimal outcomes, safety\nrisks due to tool misuse, and wasted computational resources. In this work, we\nstudy the ability of LLM agents to handle ambiguous instructions in interactive\ncode generation settings by evaluating proprietary and open-weight models on\ntheir performance across three key steps: (a) leveraging interactivity to\nimprove performance in ambiguous scenarios, (b) detecting ambiguity, and (c)\nasking targeted questions. Our findings reveal that models struggle to\ndistinguish between well-specified and underspecified instructions. However,\nwhen models interact for underspecified inputs, they effectively obtain vital\ninformation from the user, leading to significant improvements in performance\nand underscoring the value of effective interaction. Our study highlights\ncritical gaps in how current state-of-the-art models handle ambiguity in\ncomplex software engineering tasks and structures the evaluation into distinct\nsteps to enable targeted improvements."
                },
                "authors": [
                    {
                        "name": "Sanidhya Vijayvargiya"
                    },
                    {
                        "name": "Xuhui Zhou"
                    },
                    {
                        "name": "Akhila Yerukola"
                    },
                    {
                        "name": "Maarten Sap"
                    },
                    {
                        "name": "Graham Neubig"
                    }
                ],
                "author_detail": {
                    "name": "Graham Neubig"
                },
                "author": "Graham Neubig",
                "arxiv_comment": "15 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06254v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06254v2",
                "updated": "2025-02-18T17:10:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    10,
                    39,
                    1,
                    49,
                    0
                ],
                "published": "2025-01-09T02:54:19Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    2,
                    54,
                    19,
                    3,
                    9,
                    0
                ],
                "title": "Rethinking Evaluation of Sparse Autoencoders through the Representation\n  of Polysemous Words",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Evaluation of Sparse Autoencoders through the Representation\n  of Polysemous Words"
                },
                "summary": "Sparse autoencoders (SAEs) have gained a lot of attention as a promising tool\nto improve the interpretability of large language models (LLMs) by mapping the\ncomplex superposition of polysemantic neurons into monosemantic features and\ncomposing a sparse dictionary of words. However, traditional performance\nmetrics like Mean Squared Error and L0 sparsity ignore the evaluation of the\nsemantic representational power of SAEs -- whether they can acquire\ninterpretable monosemantic features while preserving the semantic relationship\nof words. For instance, it is not obvious whether a learned sparse feature\ncould distinguish different meanings in one word. In this paper, we propose a\nsuite of evaluations for SAEs to analyze the quality of monosemantic features\nby focusing on polysemous words. Our findings reveal that SAEs developed to\nimprove the MSE-L0 Pareto frontier may confuse interpretability, which does not\nnecessarily enhance the extraction of monosemantic features. The analysis of\nSAEs with polysemous words can also figure out the internal mechanism of LLMs;\ndeeper layers and the Attention module contribute to distinguishing polysemy in\na word. Our semantics focused evaluation offers new insights into the polysemy\nand the existing SAE objective and contributes to the development of more\npractical SAEs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse autoencoders (SAEs) have gained a lot of attention as a promising tool\nto improve the interpretability of large language models (LLMs) by mapping the\ncomplex superposition of polysemantic neurons into monosemantic features and\ncomposing a sparse dictionary of words. However, traditional performance\nmetrics like Mean Squared Error and L0 sparsity ignore the evaluation of the\nsemantic representational power of SAEs -- whether they can acquire\ninterpretable monosemantic features while preserving the semantic relationship\nof words. For instance, it is not obvious whether a learned sparse feature\ncould distinguish different meanings in one word. In this paper, we propose a\nsuite of evaluations for SAEs to analyze the quality of monosemantic features\nby focusing on polysemous words. Our findings reveal that SAEs developed to\nimprove the MSE-L0 Pareto frontier may confuse interpretability, which does not\nnecessarily enhance the extraction of monosemantic features. The analysis of\nSAEs with polysemous words can also figure out the internal mechanism of LLMs;\ndeeper layers and the Attention module contribute to distinguishing polysemy in\na word. Our semantics focused evaluation offers new insights into the polysemy\nand the existing SAE objective and contributes to the development of more\npractical SAEs."
                },
                "authors": [
                    {
                        "name": "Gouki Minegishi"
                    },
                    {
                        "name": "Hiroki Furuta"
                    },
                    {
                        "name": "Yusuke Iwasawa"
                    },
                    {
                        "name": "Yutaka Matsuo"
                    }
                ],
                "author_detail": {
                    "name": "Yutaka Matsuo"
                },
                "author": "Yutaka Matsuo",
                "arxiv_comment": "Published at ICLR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06254v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06254v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06656v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06656v2",
                "updated": "2025-02-18T17:05:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    5,
                    33,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-10T16:47:00Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    47,
                    0,
                    0,
                    41,
                    0
                ],
                "title": "A Frontier AI Risk Management Framework: Bridging the Gap Between\n  Current AI Practices and Established Risk Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Frontier AI Risk Management Framework: Bridging the Gap Between\n  Current AI Practices and Established Risk Management"
                },
                "summary": "The recent development of powerful AI systems has highlighted the need for\nrobust risk management frameworks in the AI industry. Although companies have\nbegun to implement safety frameworks, current approaches often lack the\nsystematic rigor found in other high-risk industries. This paper presents a\ncomprehensive risk management framework for the development of frontier AI that\nbridges this gap by integrating established risk management principles with\nemerging AI-specific practices. The framework consists of four key components:\n(1) risk identification (through literature review, open-ended red-teaming, and\nrisk modeling), (2) risk analysis and evaluation using quantitative metrics and\nclearly defined thresholds, (3) risk treatment through mitigation measures such\nas containment, deployment controls, and assurance processes, and (4) risk\ngovernance establishing clear organizational structures and accountability.\nDrawing from best practices in mature industries such as aviation or nuclear\npower, while accounting for AI's unique challenges, this framework provides AI\ndevelopers with actionable guidelines for implementing robust risk management.\nThe paper details how each component should be implemented throughout the\nlife-cycle of the AI system - from planning through deployment - and emphasizes\nthe importance and feasibility of conducting risk management work prior to the\nfinal training run to minimize the burden associated with it.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent development of powerful AI systems has highlighted the need for\nrobust risk management frameworks in the AI industry. Although companies have\nbegun to implement safety frameworks, current approaches often lack the\nsystematic rigor found in other high-risk industries. This paper presents a\ncomprehensive risk management framework for the development of frontier AI that\nbridges this gap by integrating established risk management principles with\nemerging AI-specific practices. The framework consists of four key components:\n(1) risk identification (through literature review, open-ended red-teaming, and\nrisk modeling), (2) risk analysis and evaluation using quantitative metrics and\nclearly defined thresholds, (3) risk treatment through mitigation measures such\nas containment, deployment controls, and assurance processes, and (4) risk\ngovernance establishing clear organizational structures and accountability.\nDrawing from best practices in mature industries such as aviation or nuclear\npower, while accounting for AI's unique challenges, this framework provides AI\ndevelopers with actionable guidelines for implementing robust risk management.\nThe paper details how each component should be implemented throughout the\nlife-cycle of the AI system - from planning through deployment - and emphasizes\nthe importance and feasibility of conducting risk management work prior to the\nfinal training run to minimize the burden associated with it."
                },
                "authors": [
                    {
                        "name": "Simeon Campos"
                    },
                    {
                        "name": "Henry Papadatos"
                    },
                    {
                        "name": "Fabien Roger"
                    },
                    {
                        "name": "Chlo Touzet"
                    },
                    {
                        "name": "Malcolm Murray"
                    },
                    {
                        "name": "Otter Quarks"
                    }
                ],
                "author_detail": {
                    "name": "Otter Quarks"
                },
                "author": "Otter Quarks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06656v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06656v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13059v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13059v1",
                "updated": "2025-02-18T17:04:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    4,
                    26,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T17:04:26Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    4,
                    26,
                    1,
                    49,
                    0
                ],
                "title": "SimpleVQA: Multimodal Factuality Evaluation for Multimodal Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimpleVQA: Multimodal Factuality Evaluation for Multimodal Large\n  Language Models"
                },
                "summary": "The increasing application of multi-modal large language models (MLLMs)\nacross various sectors have spotlighted the essence of their output reliability\nand accuracy, particularly their ability to produce content grounded in factual\ninformation (e.g. common and domain-specific knowledge). In this work, we\nintroduce SimpleVQA, the first comprehensive multi-modal benchmark to evaluate\nthe factuality ability of MLLMs to answer natural language short questions.\nSimpleVQA is characterized by six key features: it covers multiple tasks and\nmultiple scenarios, ensures high quality and challenging queries, maintains\nstatic and timeless reference answers, and is straightforward to evaluate. Our\napproach involves categorizing visual question-answering items into 9 different\ntasks around objective events or common knowledge and situating these within 9\ntopics. Rigorous quality control processes are implemented to guarantee\nhigh-quality, concise, and clear answers, facilitating evaluation with minimal\nvariance via an LLM-as-a-judge scoring system. Using SimpleVQA, we perform a\ncomprehensive assessment of leading 18 MLLMs and 8 text-only LLMs, delving into\ntheir image comprehension and text generation abilities by identifying and\nanalyzing error cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing application of multi-modal large language models (MLLMs)\nacross various sectors have spotlighted the essence of their output reliability\nand accuracy, particularly their ability to produce content grounded in factual\ninformation (e.g. common and domain-specific knowledge). In this work, we\nintroduce SimpleVQA, the first comprehensive multi-modal benchmark to evaluate\nthe factuality ability of MLLMs to answer natural language short questions.\nSimpleVQA is characterized by six key features: it covers multiple tasks and\nmultiple scenarios, ensures high quality and challenging queries, maintains\nstatic and timeless reference answers, and is straightforward to evaluate. Our\napproach involves categorizing visual question-answering items into 9 different\ntasks around objective events or common knowledge and situating these within 9\ntopics. Rigorous quality control processes are implemented to guarantee\nhigh-quality, concise, and clear answers, facilitating evaluation with minimal\nvariance via an LLM-as-a-judge scoring system. Using SimpleVQA, we perform a\ncomprehensive assessment of leading 18 MLLMs and 8 text-only LLMs, delving into\ntheir image comprehension and text generation abilities by identifying and\nanalyzing error cases."
                },
                "authors": [
                    {
                        "name": "Xianfu Cheng"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Shiwei Zhang"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Xiangyuan Guan"
                    },
                    {
                        "name": "Xianjie Wu"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Yuying Mai"
                    },
                    {
                        "name": "Yutao Zeng"
                    },
                    {
                        "name": "Zhoufutu Wen"
                    },
                    {
                        "name": "Ke Jin"
                    },
                    {
                        "name": "Baorui Wang"
                    },
                    {
                        "name": "Weixiao Zhou"
                    },
                    {
                        "name": "Yunhong Lu"
                    },
                    {
                        "name": "Tongliang Li"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Zhoujun Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhoujun Li"
                },
                "author": "Zhoujun Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13059v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13059v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13055v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13055v1",
                "updated": "2025-02-18T17:01:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    1,
                    37,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T17:01:37Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    1,
                    37,
                    1,
                    49,
                    0
                ],
                "title": "LAMD: Context-driven Android Malware Detection and Classification with\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAMD: Context-driven Android Malware Detection and Classification with\n  LLMs"
                },
                "summary": "The rapid growth of mobile applications has escalated Android malware\nthreats. Although there are numerous detection methods, they often struggle\nwith evolving attacks, dataset biases, and limited explainability. Large\nLanguage Models (LLMs) offer a promising alternative with their zero-shot\ninference and reasoning capabilities. However, applying LLMs to Android malware\ndetection presents two key challenges: (1)the extensive support code in Android\napplications, often spanning thousands of classes, exceeds LLMs' context limits\nand obscures malicious behavior within benign functionality; (2)the structural\ncomplexity and interdependencies of Android applications surpass LLMs'\nsequence-based reasoning, fragmenting code analysis and hindering malicious\nintent inference. To address these challenges, we propose LAMD, a practical\ncontext-driven framework to enable LLM-based Android malware detection. LAMD\nintegrates key context extraction to isolate security-critical code regions and\nconstruct program structures, then applies tier-wise code reasoning to analyze\napplication behavior progressively, from low-level instructions to high-level\nsemantics, providing final prediction and explanation. A well-designed factual\nconsistency verification mechanism is equipped to mitigate LLM hallucinations\nfrom the first tier. Evaluation in real-world settings demonstrates LAMD's\neffectiveness over conventional detectors, establishing a feasible basis for\nLLM-driven malware analysis in dynamic threat landscapes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of mobile applications has escalated Android malware\nthreats. Although there are numerous detection methods, they often struggle\nwith evolving attacks, dataset biases, and limited explainability. Large\nLanguage Models (LLMs) offer a promising alternative with their zero-shot\ninference and reasoning capabilities. However, applying LLMs to Android malware\ndetection presents two key challenges: (1)the extensive support code in Android\napplications, often spanning thousands of classes, exceeds LLMs' context limits\nand obscures malicious behavior within benign functionality; (2)the structural\ncomplexity and interdependencies of Android applications surpass LLMs'\nsequence-based reasoning, fragmenting code analysis and hindering malicious\nintent inference. To address these challenges, we propose LAMD, a practical\ncontext-driven framework to enable LLM-based Android malware detection. LAMD\nintegrates key context extraction to isolate security-critical code regions and\nconstruct program structures, then applies tier-wise code reasoning to analyze\napplication behavior progressively, from low-level instructions to high-level\nsemantics, providing final prediction and explanation. A well-designed factual\nconsistency verification mechanism is equipped to mitigate LLM hallucinations\nfrom the first tier. Evaluation in real-world settings demonstrates LAMD's\neffectiveness over conventional detectors, establishing a feasible basis for\nLLM-driven malware analysis in dynamic threat landscapes."
                },
                "authors": [
                    {
                        "name": "Xingzhi Qian"
                    },
                    {
                        "name": "Xinran Zheng"
                    },
                    {
                        "name": "Yiling He"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Lorenzo Cavallaro"
                    }
                ],
                "author_detail": {
                    "name": "Lorenzo Cavallaro"
                },
                "author": "Lorenzo Cavallaro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13055v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13055v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13053v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13053v1",
                "updated": "2025-02-18T17:01:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    1,
                    28,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T17:01:28Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    1,
                    28,
                    1,
                    49,
                    0
                ],
                "title": "AEIA-MN: Evaluating the Robustness of Multimodal LLM-Powered Mobile\n  Agents Against Active Environmental Injection Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AEIA-MN: Evaluating the Robustness of Multimodal LLM-Powered Mobile\n  Agents Against Active Environmental Injection Attacks"
                },
                "summary": "As researchers continuously optimize AI agents to perform tasks more\neffectively within operating systems, they often neglect to address the\ncritical need for enabling these agents to identify \"impostors\" within the\nsystem. Through an analysis of the agents' operating environment, we identified\na potential threat: attackers can disguise their attack methods as\nenvironmental elements, injecting active disturbances into the agents'\nexecution process, thereby disrupting their decision-making. We define this\ntype of attack as Active Environment Injection Attack (AEIA). Based on this, we\npropose AEIA-MN, an active environment injection attack scheme that exploits\ninteraction vulnerabilities in the mobile operating system to evaluate the\nrobustness of MLLM-based agents against such threats. Experimental results show\nthat even advanced MLLMs are highly vulnerable to this attack, achieving a\nmaximum attack success rate of 93% in the AndroidWorld benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As researchers continuously optimize AI agents to perform tasks more\neffectively within operating systems, they often neglect to address the\ncritical need for enabling these agents to identify \"impostors\" within the\nsystem. Through an analysis of the agents' operating environment, we identified\na potential threat: attackers can disguise their attack methods as\nenvironmental elements, injecting active disturbances into the agents'\nexecution process, thereby disrupting their decision-making. We define this\ntype of attack as Active Environment Injection Attack (AEIA). Based on this, we\npropose AEIA-MN, an active environment injection attack scheme that exploits\ninteraction vulnerabilities in the mobile operating system to evaluate the\nrobustness of MLLM-based agents against such threats. Experimental results show\nthat even advanced MLLMs are highly vulnerable to this attack, achieving a\nmaximum attack success rate of 93% in the AndroidWorld benchmark."
                },
                "authors": [
                    {
                        "name": "Yurun Chen"
                    },
                    {
                        "name": "Xueyu Hu"
                    },
                    {
                        "name": "Keting Yin"
                    },
                    {
                        "name": "Juncheng Li"
                    },
                    {
                        "name": "Shengyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shengyu Zhang"
                },
                "author": "Shengyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13053v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13053v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13044v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13044v1",
                "updated": "2025-02-18T16:56:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    56,
                    15,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T16:56:15Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    56,
                    15,
                    1,
                    49,
                    0
                ],
                "title": "Do we still need Human Annotators? Prompting Large Language Models for\n  Aspect Sentiment Quad Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do we still need Human Annotators? Prompting Large Language Models for\n  Aspect Sentiment Quad Prediction"
                },
                "summary": "Aspect sentiment quadruple prediction (ASQP) facilitates a detailed\nunderstanding of opinions expressed in a text by identifying the opinion term,\naspect term, aspect category and sentiment polarity for each opinion. However,\nannotating a full set of training examples to fine-tune models for ASQP is a\nresource-intensive process. In this study, we explore the capabilities of large\nlanguage models (LLMs) for zero- and few-shot learning on the ASQP task across\nfive diverse datasets. We report F1 scores slightly below those obtained with\nstate-of-the-art fine-tuned models but exceeding previously reported zero- and\nfew-shot performance. In the 40-shot setting on the Rest16 restaurant domain\ndataset, LLMs achieved an F1 score of 52.46, compared to 60.39 by the\nbest-performing fine-tuned method MVP. Additionally, we report the performance\nof LLMs in target aspect sentiment detection (TASD), where the F1 scores were\nalso close to fine-tuned models, achieving 66.03 on Rest16 in the 40-shot\nsetting, compared to 72.76 with MVP. While human annotators remain essential\nfor achieving optimal performance, LLMs can reduce the need for extensive\nmanual annotation in ASQP tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aspect sentiment quadruple prediction (ASQP) facilitates a detailed\nunderstanding of opinions expressed in a text by identifying the opinion term,\naspect term, aspect category and sentiment polarity for each opinion. However,\nannotating a full set of training examples to fine-tune models for ASQP is a\nresource-intensive process. In this study, we explore the capabilities of large\nlanguage models (LLMs) for zero- and few-shot learning on the ASQP task across\nfive diverse datasets. We report F1 scores slightly below those obtained with\nstate-of-the-art fine-tuned models but exceeding previously reported zero- and\nfew-shot performance. In the 40-shot setting on the Rest16 restaurant domain\ndataset, LLMs achieved an F1 score of 52.46, compared to 60.39 by the\nbest-performing fine-tuned method MVP. Additionally, we report the performance\nof LLMs in target aspect sentiment detection (TASD), where the F1 scores were\nalso close to fine-tuned models, achieving 66.03 on Rest16 in the 40-shot\nsetting, compared to 72.76 with MVP. While human annotators remain essential\nfor achieving optimal performance, LLMs can reduce the need for extensive\nmanual annotation in ASQP tasks."
                },
                "authors": [
                    {
                        "name": "Nils Constantin Hellwig"
                    },
                    {
                        "name": "Jakob Fehle"
                    },
                    {
                        "name": "Udo Kruschwitz"
                    },
                    {
                        "name": "Christian Wolff"
                    }
                ],
                "author_detail": {
                    "name": "Christian Wolff"
                },
                "author": "Christian Wolff",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13044v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13044v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18623v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18623v2",
                "updated": "2025-02-18T16:53:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    53,
                    58,
                    1,
                    49,
                    0
                ],
                "published": "2025-01-27T00:20:48Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    0,
                    20,
                    48,
                    0,
                    27,
                    0
                ],
                "title": "VLMaterial: Procedural Material Generation with Large Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLMaterial: Procedural Material Generation with Large Vision-Language\n  Models"
                },
                "summary": "Procedural materials, represented as functional node graphs, are ubiquitous\nin computer graphics for photorealistic material appearance design. They allow\nusers to perform intuitive and precise editing to achieve desired visual\nappearances. However, creating a procedural material given an input image\nrequires professional knowledge and significant effort. In this work, we\nleverage the ability to convert procedural materials into standard Python\nprograms and fine-tune a large pre-trained vision-language model (VLM) to\ngenerate such programs from input images. To enable effective fine-tuning, we\nalso contribute an open-source procedural material dataset and propose to\nperform program-level augmentation by prompting another pre-trained large\nlanguage model (LLM). Through extensive evaluation, we show that our method\noutperforms previous methods on both synthetic and real-world examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Procedural materials, represented as functional node graphs, are ubiquitous\nin computer graphics for photorealistic material appearance design. They allow\nusers to perform intuitive and precise editing to achieve desired visual\nappearances. However, creating a procedural material given an input image\nrequires professional knowledge and significant effort. In this work, we\nleverage the ability to convert procedural materials into standard Python\nprograms and fine-tune a large pre-trained vision-language model (VLM) to\ngenerate such programs from input images. To enable effective fine-tuning, we\nalso contribute an open-source procedural material dataset and propose to\nperform program-level augmentation by prompting another pre-trained large\nlanguage model (LLM). Through extensive evaluation, we show that our method\noutperforms previous methods on both synthetic and real-world examples."
                },
                "authors": [
                    {
                        "name": "Beichen Li"
                    },
                    {
                        "name": "Rundi Wu"
                    },
                    {
                        "name": "Armando Solar-Lezama"
                    },
                    {
                        "name": "Changxi Zheng"
                    },
                    {
                        "name": "Liang Shi"
                    },
                    {
                        "name": "Bernd Bickel"
                    },
                    {
                        "name": "Wojciech Matusik"
                    }
                ],
                "author_detail": {
                    "name": "Wojciech Matusik"
                },
                "author": "Wojciech Matusik",
                "arxiv_comment": "ICLR 2025 Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18623v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18623v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18585v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18585v2",
                "updated": "2025-02-18T16:51:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    51,
                    53,
                    1,
                    49,
                    0
                ],
                "published": "2025-01-30T18:58:18Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    58,
                    18,
                    3,
                    30,
                    0
                ],
                "title": "Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs"
                },
                "summary": "Large language models (LLMs) such as OpenAI's o1 have demonstrated remarkable\nabilities in complex reasoning tasks by scaling test-time compute and\nexhibiting human-like deep thinking. However, we identify a phenomenon we term\nunderthinking, where o1-like LLMs frequently switch between different reasoning\nthoughts without sufficiently exploring promising paths to reach a correct\nsolution. This behavior leads to inadequate depth of reasoning and decreased\nperformance, particularly on challenging mathematical problems. To\nsystematically analyze this issue, we conduct experiments on three challenging\ntest sets and two representative open-source o1-like models, revealing that\nfrequent thought switching correlates with incorrect responses. We introduce a\nnovel metric to quantify underthinking by measuring token efficiency in\nincorrect answers. To address underthinking, we propose a decoding strategy\nwith thought switching penalty TIP that discourages premature transitions\nbetween thoughts, encouraging deeper exploration of each reasoning path.\nExperimental results demonstrate that our approach improves accuracy across\nchallenging datasets without requiring model fine-tuning. Our findings\ncontribute to understanding reasoning inefficiencies in o1-like LLMs and offer\na practical solution to enhance their problem-solving capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) such as OpenAI's o1 have demonstrated remarkable\nabilities in complex reasoning tasks by scaling test-time compute and\nexhibiting human-like deep thinking. However, we identify a phenomenon we term\nunderthinking, where o1-like LLMs frequently switch between different reasoning\nthoughts without sufficiently exploring promising paths to reach a correct\nsolution. This behavior leads to inadequate depth of reasoning and decreased\nperformance, particularly on challenging mathematical problems. To\nsystematically analyze this issue, we conduct experiments on three challenging\ntest sets and two representative open-source o1-like models, revealing that\nfrequent thought switching correlates with incorrect responses. We introduce a\nnovel metric to quantify underthinking by measuring token efficiency in\nincorrect answers. To address underthinking, we propose a decoding strategy\nwith thought switching penalty TIP that discourages premature transitions\nbetween thoughts, encouraging deeper exploration of each reasoning path.\nExperimental results demonstrate that our approach improves accuracy across\nchallenging datasets without requiring model fine-tuning. Our findings\ncontribute to understanding reasoning inefficiencies in o1-like LLMs and offer\na practical solution to enhance their problem-solving capabilities."
                },
                "authors": [
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Qiuzhi Liu"
                    },
                    {
                        "name": "Jiahao Xu"
                    },
                    {
                        "name": "Tian Liang"
                    },
                    {
                        "name": "Xingyu Chen"
                    },
                    {
                        "name": "Zhiwei He"
                    },
                    {
                        "name": "Linfeng Song"
                    },
                    {
                        "name": "Dian Yu"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Zhuosheng Zhang"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "arxiv_comment": "1. We have updated the results for DeepSeek-R1, and all of our\n  original conclusions remain valid. 2. Our proposed Tip approach remains\n  effective in Best-of-N scenarios (e.g., self-consistency and Laconic\n  Decoding) when built on DeepSeek-R1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18585v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18585v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13031v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13031v1",
                "updated": "2025-02-18T16:46:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    46,
                    47,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T16:46:47Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    46,
                    47,
                    1,
                    49,
                    0
                ],
                "title": "HPSS: Heuristic Prompting Strategy Search for LLM Evaluators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HPSS: Heuristic Prompting Strategy Search for LLM Evaluators"
                },
                "summary": "Since the adoption of large language models (LLMs) for text evaluation has\nbecome increasingly prevalent in the field of natural language processing\n(NLP), a series of existing works attempt to optimize the prompts for LLM\nevaluators to improve their alignment with human judgment. However, their\nefforts are limited to optimizing individual factors of evaluation prompts,\nsuch as evaluation criteria or output formats, neglecting the combinatorial\nimpact of multiple factors, which leads to insufficient optimization of the\nevaluation pipeline. Nevertheless, identifying well-behaved prompting\nstrategies for adjusting multiple factors requires extensive enumeration. To\nthis end, we comprehensively integrate 8 key factors for evaluation prompts and\npropose a novel automatic prompting strategy optimization method called\nHeuristic Prompting Strategy Search (HPSS). Inspired by the genetic algorithm,\nHPSS conducts an iterative search to find well-behaved prompting strategies for\nLLM evaluators. A heuristic function is employed to guide the search process,\nenhancing the performance of our algorithm. Extensive experiments across four\nevaluation tasks demonstrate the effectiveness of HPSS, consistently\noutperforming both human-designed evaluation prompts and existing automatic\nprompt optimization methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the adoption of large language models (LLMs) for text evaluation has\nbecome increasingly prevalent in the field of natural language processing\n(NLP), a series of existing works attempt to optimize the prompts for LLM\nevaluators to improve their alignment with human judgment. However, their\nefforts are limited to optimizing individual factors of evaluation prompts,\nsuch as evaluation criteria or output formats, neglecting the combinatorial\nimpact of multiple factors, which leads to insufficient optimization of the\nevaluation pipeline. Nevertheless, identifying well-behaved prompting\nstrategies for adjusting multiple factors requires extensive enumeration. To\nthis end, we comprehensively integrate 8 key factors for evaluation prompts and\npropose a novel automatic prompting strategy optimization method called\nHeuristic Prompting Strategy Search (HPSS). Inspired by the genetic algorithm,\nHPSS conducts an iterative search to find well-behaved prompting strategies for\nLLM evaluators. A heuristic function is employed to guide the search process,\nenhancing the performance of our algorithm. Extensive experiments across four\nevaluation tasks demonstrate the effectiveness of HPSS, consistently\noutperforming both human-designed evaluation prompts and existing automatic\nprompt optimization methods."
                },
                "authors": [
                    {
                        "name": "Bosi Wen"
                    },
                    {
                        "name": "Pei Ke"
                    },
                    {
                        "name": "Yufei Sun"
                    },
                    {
                        "name": "Cunxiang Wang"
                    },
                    {
                        "name": "Xiaotao Gu"
                    },
                    {
                        "name": "Jinfeng Zhou"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Hongning Wang"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang",
                "arxiv_comment": "32 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13031v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13031v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13019v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13019v1",
                "updated": "2025-02-18T16:38:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    38,
                    39,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T16:38:39Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    38,
                    39,
                    1,
                    49,
                    0
                ],
                "title": "Oreo: A Plug-in Context Reconstructor to Enhance Retrieval-Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oreo: A Plug-in Context Reconstructor to Enhance Retrieval-Augmented\n  Generation"
                },
                "summary": "Despite the remarkable capabilities of Large Language Models (LLMs) in\nvarious NLP tasks, they remain vulnerable to hallucinations due to their\nlimited parametric knowledge and lack of domain-specific expertise.\nRetrieval-Augmented Generation (RAG) addresses this challenge by incorporating\nexternal document retrieval to augment the knowledge base of LLMs. In this\napproach, RAG retrieves document chunks from an external corpus in response to\na query, which are then used as context for the downstream language model to\ngenerate an answer. However, these retrieved knowledge sources often include\nirrelevant or erroneous information, undermining the effectiveness of RAG in\ndownstream tasks. To overcome this limitation, we introduce a compact,\nefficient, and pluggable module designed to refine external knowledge sources\nbefore feeding them to the generator. The module reconstructs retrieved content\nby extracting the most relevant and supportive information and reorganising it\ninto a concise, query-specific format. Through a three-stage training paradigm\n- comprising supervised fine-tuning, contrastive multi-task learning, and\nreinforcement learning-based alignment - it prioritises critical knowledge and\naligns it with the generator's preferences. This method enables LLMs to produce\noutputs that are more accurate, reliable, and contextually appropriate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable capabilities of Large Language Models (LLMs) in\nvarious NLP tasks, they remain vulnerable to hallucinations due to their\nlimited parametric knowledge and lack of domain-specific expertise.\nRetrieval-Augmented Generation (RAG) addresses this challenge by incorporating\nexternal document retrieval to augment the knowledge base of LLMs. In this\napproach, RAG retrieves document chunks from an external corpus in response to\na query, which are then used as context for the downstream language model to\ngenerate an answer. However, these retrieved knowledge sources often include\nirrelevant or erroneous information, undermining the effectiveness of RAG in\ndownstream tasks. To overcome this limitation, we introduce a compact,\nefficient, and pluggable module designed to refine external knowledge sources\nbefore feeding them to the generator. The module reconstructs retrieved content\nby extracting the most relevant and supportive information and reorganising it\ninto a concise, query-specific format. Through a three-stage training paradigm\n- comprising supervised fine-tuning, contrastive multi-task learning, and\nreinforcement learning-based alignment - it prioritises critical knowledge and\naligns it with the generator's preferences. This method enables LLMs to produce\noutputs that are more accurate, reliable, and contextually appropriate."
                },
                "authors": [
                    {
                        "name": "Sha Li"
                    },
                    {
                        "name": "Naren Ramarkrishnan"
                    }
                ],
                "author_detail": {
                    "name": "Naren Ramarkrishnan"
                },
                "author": "Naren Ramarkrishnan",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13019v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13019v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16813v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16813v2",
                "updated": "2025-02-18T16:36:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    36,
                    25,
                    1,
                    49,
                    0
                ],
                "published": "2024-09-25T11:09:39Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    11,
                    9,
                    39,
                    2,
                    269,
                    0
                ],
                "title": "PeerArg: Argumentative Peer Review with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PeerArg: Argumentative Peer Review with LLMs"
                },
                "summary": "Peer review is an essential process to determine the quality of papers\nsubmitted to scientific conferences or journals. However, it is subjective and\nprone to biases. Several studies have been conducted to apply techniques from\nNLP to support peer review, but they are based on black-box techniques and\ntheir outputs are difficult to interpret and trust. In this paper, we propose a\nnovel pipeline to support and understand the reviewing and decision-making\nprocesses of peer review: the PeerArg system combining LLMs with methods from\nknowledge representation. PeerArg takes in input a set of reviews for a paper\nand outputs the paper acceptance prediction. We evaluate the performance of the\nPeerArg pipeline on three different datasets, in comparison with a novel\nend-2-end LLM that uses few-shot learning to predict paper acceptance given\nreviews. The results indicate that the end-2-end LLM is capable of predicting\npaper acceptance from reviews, but a variant of the PeerArg pipeline\noutperforms this LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Peer review is an essential process to determine the quality of papers\nsubmitted to scientific conferences or journals. However, it is subjective and\nprone to biases. Several studies have been conducted to apply techniques from\nNLP to support peer review, but they are based on black-box techniques and\ntheir outputs are difficult to interpret and trust. In this paper, we propose a\nnovel pipeline to support and understand the reviewing and decision-making\nprocesses of peer review: the PeerArg system combining LLMs with methods from\nknowledge representation. PeerArg takes in input a set of reviews for a paper\nand outputs the paper acceptance prediction. We evaluate the performance of the\nPeerArg pipeline on three different datasets, in comparison with a novel\nend-2-end LLM that uses few-shot learning to predict paper acceptance given\nreviews. The results indicate that the end-2-end LLM is capable of predicting\npaper acceptance from reviews, but a variant of the PeerArg pipeline\noutperforms this LLM."
                },
                "authors": [
                    {
                        "name": "Purin Sukpanichnant"
                    },
                    {
                        "name": "Anna Rapberger"
                    },
                    {
                        "name": "Francesca Toni"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Toni"
                },
                "author": "Francesca Toni",
                "arxiv_comment": "Presented at NeLaMKRR@KR, 2024 (arXiv:2410.05339)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16813v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16813v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13016v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13016v1",
                "updated": "2025-02-18T16:34:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    34,
                    45,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T16:34:45Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    34,
                    45,
                    1,
                    49,
                    0
                ],
                "title": "LLM-Powered Proactive Data Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Powered Proactive Data Systems"
                },
                "summary": "With the power of LLMs, we now have the ability to query data that was\npreviously impossible to query, including text, images, and video. However,\ndespite this enormous potential, most present-day data systems that leverage\nLLMs are reactive, reflecting our community's desire to map LLMs to known\nabstractions. Most data systems treat LLMs as an opaque black box that operates\non user inputs and data as is, optimizing them much like any other approximate,\nexpensive UDFs, in conjunction with other relational operators. Such data\nsystems do as they are told, but fail to understand and leverage what the LLM\nis being asked to do (i.e. the underlying operations, which may be\nerror-prone), the data the LLM is operating on (e.g., long, complex documents),\nor what the user really needs. They don't take advantage of the characteristics\nof the operations and/or the data at hand, or ensure correctness of results\nwhen there are imprecisions and ambiguities. We argue that data systems instead\nneed to be proactive: they need to be given more agency -- armed with the power\nof LLMs -- to understand and rework the user inputs and the data and to make\ndecisions on how the operations and the data should be represented and\nprocessed. By allowing the data system to parse, rewrite, and decompose user\ninputs and data, or to interact with the user in ways that go beyond the\nstandard single-shot query-result paradigm, the data system is able to address\nuser needs more efficiently and effectively. These new capabilities lead to a\nrich design space where the data system takes more initiative: they are\nempowered to perform optimization based on the transformation operations, data\ncharacteristics, and user intent. We discuss various successful examples of how\nthis framework has been and can be applied in real-world tasks, and present\nfuture directions for this ambitious research agenda.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the power of LLMs, we now have the ability to query data that was\npreviously impossible to query, including text, images, and video. However,\ndespite this enormous potential, most present-day data systems that leverage\nLLMs are reactive, reflecting our community's desire to map LLMs to known\nabstractions. Most data systems treat LLMs as an opaque black box that operates\non user inputs and data as is, optimizing them much like any other approximate,\nexpensive UDFs, in conjunction with other relational operators. Such data\nsystems do as they are told, but fail to understand and leverage what the LLM\nis being asked to do (i.e. the underlying operations, which may be\nerror-prone), the data the LLM is operating on (e.g., long, complex documents),\nor what the user really needs. They don't take advantage of the characteristics\nof the operations and/or the data at hand, or ensure correctness of results\nwhen there are imprecisions and ambiguities. We argue that data systems instead\nneed to be proactive: they need to be given more agency -- armed with the power\nof LLMs -- to understand and rework the user inputs and the data and to make\ndecisions on how the operations and the data should be represented and\nprocessed. By allowing the data system to parse, rewrite, and decompose user\ninputs and data, or to interact with the user in ways that go beyond the\nstandard single-shot query-result paradigm, the data system is able to address\nuser needs more efficiently and effectively. These new capabilities lead to a\nrich design space where the data system takes more initiative: they are\nempowered to perform optimization based on the transformation operations, data\ncharacteristics, and user intent. We discuss various successful examples of how\nthis framework has been and can be applied in real-world tasks, and present\nfuture directions for this ambitious research agenda."
                },
                "authors": [
                    {
                        "name": "Sepanta Zeighami"
                    },
                    {
                        "name": "Yiming Lin"
                    },
                    {
                        "name": "Shreya Shankar"
                    },
                    {
                        "name": "Aditya Parameswaran"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Parameswaran"
                },
                "author": "Aditya Parameswaran",
                "arxiv_journal_ref": "IEEE Data Engineering Bulletin March 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13016v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13016v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13012v1",
                "updated": "2025-02-18T16:33:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    33,
                    33,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T16:33:33Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    33,
                    33,
                    1,
                    49,
                    0
                ],
                "title": "Towards a Design Guideline for RPA Evaluation: A Survey of Large\n  Language Model-Based Role-Playing Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a Design Guideline for RPA Evaluation: A Survey of Large\n  Language Model-Based Role-Playing Agents"
                },
                "summary": "Role-Playing Agent (RPA) is an increasingly popular type of LLM Agent that\nsimulates human-like behaviors in a variety of tasks. However, evaluating RPAs\nis challenging due to diverse task requirements and agent designs. This paper\nproposes an evidence-based, actionable, and generalizable evaluation design\nguideline for LLM-based RPA by systematically reviewing 1,676 papers published\nbetween Jan. 2021 and Dec. 2024. Our analysis identifies six agent attributes,\nseven task attributes, and seven evaluation metrics from existing literature.\nBased on these findings, we present an RPA evaluation design guideline to help\nresearchers develop more systematic and consistent evaluation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role-Playing Agent (RPA) is an increasingly popular type of LLM Agent that\nsimulates human-like behaviors in a variety of tasks. However, evaluating RPAs\nis challenging due to diverse task requirements and agent designs. This paper\nproposes an evidence-based, actionable, and generalizable evaluation design\nguideline for LLM-based RPA by systematically reviewing 1,676 papers published\nbetween Jan. 2021 and Dec. 2024. Our analysis identifies six agent attributes,\nseven task attributes, and seven evaluation metrics from existing literature.\nBased on these findings, we present an RPA evaluation design guideline to help\nresearchers develop more systematic and consistent evaluation methods."
                },
                "authors": [
                    {
                        "name": "Chaoran Chen"
                    },
                    {
                        "name": "Bingsheng Yao"
                    },
                    {
                        "name": "Ruishi Zou"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Weimin Lyu"
                    },
                    {
                        "name": "Toby Jia-Jun Li"
                    },
                    {
                        "name": "Dakuo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Dakuo Wang"
                },
                "author": "Dakuo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13010v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13010v1",
                "updated": "2025-02-18T16:29:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    29,
                    45,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T16:29:45Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    29,
                    45,
                    1,
                    49,
                    0
                ],
                "title": "Adaptive Knowledge Graphs Enhance Medical Question Answering: Bridging\n  the Gap Between LLMs and Evolving Medical Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Knowledge Graphs Enhance Medical Question Answering: Bridging\n  the Gap Between LLMs and Evolving Medical Knowledge"
                },
                "summary": "Large Language Models (LLMs) have significantly advanced medical\nquestion-answering by leveraging extensive clinical data and medical\nliterature. However, the rapid evolution of medical knowledge and the\nlabor-intensive process of manually updating domain-specific resources pose\nchallenges to the reliability of these systems. To address this, we introduce\nAdaptive Medical Graph-RAG (AMG-RAG), a comprehensive framework that automates\nthe construction and continuous updating of medical knowledge graphs,\nintegrates reasoning, and retrieves current external evidence, such as PubMed\nand WikiSearch. By dynamically linking new findings and complex medical\nconcepts, AMG-RAG not only improves accuracy but also enhances interpretability\nin medical queries.\n  Evaluations on the MEDQA and MEDMCQA benchmarks demonstrate the effectiveness\nof AMG-RAG, achieving an F1 score of 74.1 percent on MEDQA and an accuracy of\n66.34 percent on MEDMCQA, outperforming both comparable models and those 10 to\n100 times larger. Notably, these improvements are achieved without increasing\ncomputational overhead, highlighting the critical role of automated knowledge\ngraph generation and external evidence retrieval in delivering up-to-date,\ntrustworthy medical insights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have significantly advanced medical\nquestion-answering by leveraging extensive clinical data and medical\nliterature. However, the rapid evolution of medical knowledge and the\nlabor-intensive process of manually updating domain-specific resources pose\nchallenges to the reliability of these systems. To address this, we introduce\nAdaptive Medical Graph-RAG (AMG-RAG), a comprehensive framework that automates\nthe construction and continuous updating of medical knowledge graphs,\nintegrates reasoning, and retrieves current external evidence, such as PubMed\nand WikiSearch. By dynamically linking new findings and complex medical\nconcepts, AMG-RAG not only improves accuracy but also enhances interpretability\nin medical queries.\n  Evaluations on the MEDQA and MEDMCQA benchmarks demonstrate the effectiveness\nof AMG-RAG, achieving an F1 score of 74.1 percent on MEDQA and an accuracy of\n66.34 percent on MEDMCQA, outperforming both comparable models and those 10 to\n100 times larger. Notably, these improvements are achieved without increasing\ncomputational overhead, highlighting the critical role of automated knowledge\ngraph generation and external evidence retrieval in delivering up-to-date,\ntrustworthy medical insights."
                },
                "authors": [
                    {
                        "name": "Mohammad Reza Rezaei"
                    },
                    {
                        "name": "Reza Saadati Fard"
                    },
                    {
                        "name": "Jayson Parker"
                    },
                    {
                        "name": "Rahul G. Krishnan"
                    },
                    {
                        "name": "Milad Lankarany"
                    }
                ],
                "author_detail": {
                    "name": "Milad Lankarany"
                },
                "author": "Milad Lankarany",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13010v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13010v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13931v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13931v3",
                "updated": "2025-02-18T16:27:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    27,
                    26,
                    1,
                    49,
                    0
                ],
                "published": "2024-09-20T22:34:37Z",
                "published_parsed": [
                    2024,
                    9,
                    20,
                    22,
                    34,
                    37,
                    4,
                    264,
                    0
                ],
                "title": "On-Device Collaborative Language Modeling via a Mixture of Generalists\n  and Specialists",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-Device Collaborative Language Modeling via a Mixture of Generalists\n  and Specialists"
                },
                "summary": "On-device LLMs have gained increasing attention for their ability to enhance\nprivacy and provide a personalized user experience. To facilitate private\nlearning with scarce data, Federated Learning has become a standard approach.\nHowever, it faces challenges such as computational resource heterogeneity and\ndata heterogeneity among end users. We propose CoMiGS ($\\textbf{Co}$llaborative\nlearning with a $\\textbf{Mi}$xture of $\\textbf{G}$eneralists and\n$\\textbf{S}$pecialists), the first approach to address both challenges. A key\ninnovation of our method is the bi-level optimization formulation of the\nMixture-of-Experts learning objective, where the router is optimized using a\nseparate validation set to ensure alignment with the target distribution. We\nsolve our objective with alternating minimization, for which we provide a\ntheoretical analysis. Our method shares generalist experts across users while\nlocalizing a varying number of specialist experts, thereby adapting to users'\ncomputational resources and preserving privacy. Through extensive experiments,\nwe show CoMiGS effectively balances general and personalized knowledge for each\ntoken generation. We demonstrate that CoMiGS remains robust against\noverfitting-due to the generalists' regularizing effect-while adapting to local\ndata through specialist expertise. We open source our codebase for\ncollaborative LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-device LLMs have gained increasing attention for their ability to enhance\nprivacy and provide a personalized user experience. To facilitate private\nlearning with scarce data, Federated Learning has become a standard approach.\nHowever, it faces challenges such as computational resource heterogeneity and\ndata heterogeneity among end users. We propose CoMiGS ($\\textbf{Co}$llaborative\nlearning with a $\\textbf{Mi}$xture of $\\textbf{G}$eneralists and\n$\\textbf{S}$pecialists), the first approach to address both challenges. A key\ninnovation of our method is the bi-level optimization formulation of the\nMixture-of-Experts learning objective, where the router is optimized using a\nseparate validation set to ensure alignment with the target distribution. We\nsolve our objective with alternating minimization, for which we provide a\ntheoretical analysis. Our method shares generalist experts across users while\nlocalizing a varying number of specialist experts, thereby adapting to users'\ncomputational resources and preserving privacy. Through extensive experiments,\nwe show CoMiGS effectively balances general and personalized knowledge for each\ntoken generation. We demonstrate that CoMiGS remains robust against\noverfitting-due to the generalists' regularizing effect-while adapting to local\ndata through specialist expertise. We open source our codebase for\ncollaborative LLMs."
                },
                "authors": [
                    {
                        "name": "Dongyang Fan"
                    },
                    {
                        "name": "Bettina Messmer"
                    },
                    {
                        "name": "Nikita Doikov"
                    },
                    {
                        "name": "Martin Jaggi"
                    }
                ],
                "author_detail": {
                    "name": "Martin Jaggi"
                },
                "author": "Martin Jaggi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13931v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13931v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13001v1",
                "updated": "2025-02-18T16:21:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    21,
                    22,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T16:21:22Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    21,
                    22,
                    1,
                    49,
                    0
                ],
                "title": "You need to MIMIC to get FAME: Solving Meeting Transcript Scarcity with\n  a Multi-Agent Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "You need to MIMIC to get FAME: Solving Meeting Transcript Scarcity with\n  a Multi-Agent Conversations"
                },
                "summary": "Meeting summarization suffers from limited high-quality data, mainly due to\nprivacy restrictions and expensive collection processes. We address this gap\nwith FAME, a dataset of 500 meetings in English and 300 in German produced by\nMIMIC, our new multi-agent meeting synthesis framework that generates meeting\ntranscripts on a given knowledge source by defining psychologically grounded\nparticipant profiles, outlining the conversation, and orchestrating a large\nlanguage model (LLM) debate. A modular post-processing step refines these\noutputs, mitigating potential repetitiveness and overly formal tones, ensuring\ncoherent, credible dialogues at scale. We also propose a psychologically\ngrounded evaluation framework assessing naturalness, social behavior\nauthenticity, and transcript difficulties. Human assessments show that FAME\napproximates real-meeting spontaneity (4.5/5 in naturalness), preserves\nspeaker-centric challenges (3/5 in spoken language), and introduces richer\ninformation-oriented difficulty (4/5 in difficulty). These findings highlight\nthat FAME is a good and scalable proxy for real-world meeting conditions. It\nenables new test scenarios for meeting summarization research and other\nconversation-centric applications in tasks requiring conversation data or\nsimulating social scenarios under behavioral constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meeting summarization suffers from limited high-quality data, mainly due to\nprivacy restrictions and expensive collection processes. We address this gap\nwith FAME, a dataset of 500 meetings in English and 300 in German produced by\nMIMIC, our new multi-agent meeting synthesis framework that generates meeting\ntranscripts on a given knowledge source by defining psychologically grounded\nparticipant profiles, outlining the conversation, and orchestrating a large\nlanguage model (LLM) debate. A modular post-processing step refines these\noutputs, mitigating potential repetitiveness and overly formal tones, ensuring\ncoherent, credible dialogues at scale. We also propose a psychologically\ngrounded evaluation framework assessing naturalness, social behavior\nauthenticity, and transcript difficulties. Human assessments show that FAME\napproximates real-meeting spontaneity (4.5/5 in naturalness), preserves\nspeaker-centric challenges (3/5 in spoken language), and introduces richer\ninformation-oriented difficulty (4/5 in difficulty). These findings highlight\nthat FAME is a good and scalable proxy for real-world meeting conditions. It\nenables new test scenarios for meeting summarization research and other\nconversation-centric applications in tasks requiring conversation data or\nsimulating social scenarios under behavioral constraints."
                },
                "authors": [
                    {
                        "name": "Frederic Kirstein"
                    },
                    {
                        "name": "Muneeb Khan"
                    },
                    {
                        "name": "Jan Philip Wahle"
                    },
                    {
                        "name": "Terry Ruas"
                    },
                    {
                        "name": "Bela Gipp"
                    }
                ],
                "author_detail": {
                    "name": "Bela Gipp"
                },
                "author": "Bela Gipp",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12998v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12998v1",
                "updated": "2025-02-18T16:19:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    19,
                    8,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T16:19:08Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    19,
                    8,
                    1,
                    49,
                    0
                ],
                "title": "Personalized Top-k Set Queries Over Predicted Scores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Top-k Set Queries Over Predicted Scores"
                },
                "summary": "This work studies the applicability of expensive external oracles such as\nlarge language models in answering top-k queries over predicted scores. Such\nscores are incurred by user-defined functions to answer personalized queries\nover multi-modal data. We propose a generic computational framework that\nhandles arbitrary set-based scoring functions, as long as the functions could\nbe decomposed into constructs, each of which sent to an oracle (in our case an\nLLM) to predict partial scores. At a given point in time, the framework assumes\na set of responses and their partial predicted scores, and it maintains a\ncollection of possible sets that are likely to be the true top-k. Since calling\noracles is costly, our framework judiciously identifies the next construct,\ni.e., the next best question to ask the oracle so as to maximize the likelihood\nof identifying the true top-k. We present a principled probabilistic model that\nquantifies that likelihood. We study efficiency opportunities in designing\nalgorithms. We run an evaluation with three large scale datasets, scoring\nfunctions, and baselines. Experiments indicate the efficacy of our framework,\nas it achieves an order of magnitude improvement over baselines in requiring\nLLM calls while ensuring result accuracy. Scalability experiments further\nindicate that our framework could be used in large-scale applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work studies the applicability of expensive external oracles such as\nlarge language models in answering top-k queries over predicted scores. Such\nscores are incurred by user-defined functions to answer personalized queries\nover multi-modal data. We propose a generic computational framework that\nhandles arbitrary set-based scoring functions, as long as the functions could\nbe decomposed into constructs, each of which sent to an oracle (in our case an\nLLM) to predict partial scores. At a given point in time, the framework assumes\na set of responses and their partial predicted scores, and it maintains a\ncollection of possible sets that are likely to be the true top-k. Since calling\noracles is costly, our framework judiciously identifies the next construct,\ni.e., the next best question to ask the oracle so as to maximize the likelihood\nof identifying the true top-k. We present a principled probabilistic model that\nquantifies that likelihood. We study efficiency opportunities in designing\nalgorithms. We run an evaluation with three large scale datasets, scoring\nfunctions, and baselines. Experiments indicate the efficacy of our framework,\nas it achieves an order of magnitude improvement over baselines in requiring\nLLM calls while ensuring result accuracy. Scalability experiments further\nindicate that our framework could be used in large-scale applications."
                },
                "authors": [
                    {
                        "name": "Sohrab Namazi Nia"
                    },
                    {
                        "name": "Subhodeep Ghosh"
                    },
                    {
                        "name": "Senjuti Basu Roy"
                    },
                    {
                        "name": "Sihem Amer-Yahia"
                    }
                ],
                "author_detail": {
                    "name": "Sihem Amer-Yahia"
                },
                "author": "Sihem Amer-Yahia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12998v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12998v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07832v9",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07832v9",
                "updated": "2025-02-18T16:14:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    14,
                    2,
                    1,
                    49,
                    0
                ],
                "published": "2024-07-31T14:49:35Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    49,
                    35,
                    2,
                    213,
                    0
                ],
                "title": "LADDER: Language Driven Slice Discovery and Error Rectification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LADDER: Language Driven Slice Discovery and Error Rectification"
                },
                "summary": "Error slice discovery is crucial to diagnose and mitigate model errors.\nCurrent clustering or discrete attribute-based slice discovery methods face key\nlimitations: 1) clustering results in incoherent slices, while assigning\ndiscrete attributes to slices leads to incomplete coverage of error patterns\ndue to missing or insufficient attributes; 2) these methods lack complex\nreasoning, preventing them from fully explaining model biases; 3) they fail to\nintegrate \\textit{domain knowledge}, limiting their usage in specialized fields\n\\eg radiology. We propose\\ladder (\\underline{La}nguage-\\underline{D}riven\n\\underline{D}iscovery and \\underline{E}rror \\underline{R}ectification), to\naddress the limitations by: (1) leveraging the flexibility of natural language\nto address incompleteness, (2) employing LLM's latent \\textit{domain knowledge}\nand advanced reasoning to analyze sentences and derive testable hypotheses\ndirectly, identifying biased attributes, and form coherent error slices without\nclustering. Existing mitigation methods typically address only the\nworst-performing group, often amplifying errors in other subgroups. In\ncontrast,\\ladder generates pseudo attributes from the discovered hypotheses to\nmitigate errors across all biases without explicit attribute annotations or\nprior knowledge of bias. Rigorous evaluations on 6 datasets spanning natural\nand medical images -- comparing 200+ classifiers with diverse architectures,\npretraining strategies, and LLMs -- show that\\ladder consistently outperforms\nexisting baselines in discovering and mitigating biases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Error slice discovery is crucial to diagnose and mitigate model errors.\nCurrent clustering or discrete attribute-based slice discovery methods face key\nlimitations: 1) clustering results in incoherent slices, while assigning\ndiscrete attributes to slices leads to incomplete coverage of error patterns\ndue to missing or insufficient attributes; 2) these methods lack complex\nreasoning, preventing them from fully explaining model biases; 3) they fail to\nintegrate \\textit{domain knowledge}, limiting their usage in specialized fields\n\\eg radiology. We propose\\ladder (\\underline{La}nguage-\\underline{D}riven\n\\underline{D}iscovery and \\underline{E}rror \\underline{R}ectification), to\naddress the limitations by: (1) leveraging the flexibility of natural language\nto address incompleteness, (2) employing LLM's latent \\textit{domain knowledge}\nand advanced reasoning to analyze sentences and derive testable hypotheses\ndirectly, identifying biased attributes, and form coherent error slices without\nclustering. Existing mitigation methods typically address only the\nworst-performing group, often amplifying errors in other subgroups. In\ncontrast,\\ladder generates pseudo attributes from the discovered hypotheses to\nmitigate errors across all biases without explicit attribute annotations or\nprior knowledge of bias. Rigorous evaluations on 6 datasets spanning natural\nand medical images -- comparing 200+ classifiers with diverse architectures,\npretraining strategies, and LLMs -- show that\\ladder consistently outperforms\nexisting baselines in discovering and mitigating biases."
                },
                "authors": [
                    {
                        "name": "Shantanu Ghosh"
                    },
                    {
                        "name": "Rayan Syed"
                    },
                    {
                        "name": "Chenyu Wang"
                    },
                    {
                        "name": "Clare B. Poynton"
                    },
                    {
                        "name": "Shyam Visweswaran"
                    },
                    {
                        "name": "Kayhan Batmanghelich"
                    }
                ],
                "author_detail": {
                    "name": "Kayhan Batmanghelich"
                },
                "author": "Kayhan Batmanghelich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07832v9",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07832v9",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12988v1",
                "updated": "2025-02-18T16:11:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    11,
                    54,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T16:11:54Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    11,
                    54,
                    1,
                    49,
                    0
                ],
                "title": "Beyond Profile: From Surface-Level Facts to Deep Persona Simulation in\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Profile: From Surface-Level Facts to Deep Persona Simulation in\n  LLMs"
                },
                "summary": "Previous approaches to persona simulation large language models (LLMs) have\ntypically relied on learning basic biographical information, or using limited\nrole-play dialogue datasets to capture a character's responses. However, a\nholistic representation of an individual goes beyond surface-level facts or\nconversations to deeper thoughts and thinking. In this work, we introduce\nCharacterBot, a model designed to replicate both the linguistic patterns and\ndistinctive thought processes of a character. Using Lu Xun, a renowned Chinese\nwriter, as a case study, we propose four training tasks derived from his 17\nessay collections. These include a pre-training task focused on mastering\nexternal linguistic structures and knowledge, as well as three fine-tuning\ntasks: multiple-choice question answering, generative question answering, and\nstyle transfer, each aligning the LLM with Lu Xun's internal ideation and\nwriting style. To optimize learning across these tasks, we introduce a CharLoRA\nparameter updating mechanism, where a general linguistic style expert\ncollaborates with other task-specific experts to better study both the language\nstyle and the understanding of deeper thoughts. We evaluate CharacterBot on\nthree tasks for linguistic accuracy and opinion comprehension, demonstrating\nthat it significantly outperforms the baselines on our adapted metrics. We hope\nthat this work inspires future research on deep character persona simulation\nLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous approaches to persona simulation large language models (LLMs) have\ntypically relied on learning basic biographical information, or using limited\nrole-play dialogue datasets to capture a character's responses. However, a\nholistic representation of an individual goes beyond surface-level facts or\nconversations to deeper thoughts and thinking. In this work, we introduce\nCharacterBot, a model designed to replicate both the linguistic patterns and\ndistinctive thought processes of a character. Using Lu Xun, a renowned Chinese\nwriter, as a case study, we propose four training tasks derived from his 17\nessay collections. These include a pre-training task focused on mastering\nexternal linguistic structures and knowledge, as well as three fine-tuning\ntasks: multiple-choice question answering, generative question answering, and\nstyle transfer, each aligning the LLM with Lu Xun's internal ideation and\nwriting style. To optimize learning across these tasks, we introduce a CharLoRA\nparameter updating mechanism, where a general linguistic style expert\ncollaborates with other task-specific experts to better study both the language\nstyle and the understanding of deeper thoughts. We evaluate CharacterBot on\nthree tasks for linguistic accuracy and opinion comprehension, demonstrating\nthat it significantly outperforms the baselines on our adapted metrics. We hope\nthat this work inspires future research on deep character persona simulation\nLLM."
                },
                "authors": [
                    {
                        "name": "Zixiao Wang"
                    },
                    {
                        "name": "Duzhen Zhang"
                    },
                    {
                        "name": "Ishita Agrawal"
                    },
                    {
                        "name": "Shen Gao"
                    },
                    {
                        "name": "Le Song"
                    },
                    {
                        "name": "Xiuying Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiuying Chen"
                },
                "author": "Xiuying Chen",
                "arxiv_comment": "19 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09992v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09992v2",
                "updated": "2025-02-18T16:08:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    8,
                    59,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-14T08:23:51Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    8,
                    23,
                    51,
                    4,
                    45,
                    0
                ],
                "title": "Large Language Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Diffusion Models"
                },
                "summary": "Autoregressive models (ARMs) are widely regarded as the cornerstone of large\nlanguage models (LLMs). We challenge this notion by introducing LLaDA, a\ndiffusion model trained from scratch under the pre-training and supervised\nfine-tuning (SFT) paradigm. LLaDA models distributions through a forward data\nmasking process and a reverse process, parameterized by a vanilla Transformer\nto predict masked tokens. By optimizing a likelihood bound, it provides a\nprincipled generative approach for probabilistic inference. Across extensive\nbenchmarks, LLaDA demonstrates strong scalability, outperforming our\nself-constructed ARM baselines. Remarkably, LLaDA 8B is competitive with strong\nLLMs like LLaMA3 8B in in-context learning and, after SFT, exhibits impressive\ninstruction-following abilities in case studies such as multi-turn dialogue.\nMoreover, LLaDA addresses the reversal curse, surpassing GPT-4o in a reversal\npoem completion task. Our findings establish diffusion models as a viable and\npromising alternative to ARMs, challenging the assumption that key LLM\ncapabilities discussed above are inherently tied to ARMs. Project page and\ncodes: https://ml-gsai.github.io/LLaDA-demo/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive models (ARMs) are widely regarded as the cornerstone of large\nlanguage models (LLMs). We challenge this notion by introducing LLaDA, a\ndiffusion model trained from scratch under the pre-training and supervised\nfine-tuning (SFT) paradigm. LLaDA models distributions through a forward data\nmasking process and a reverse process, parameterized by a vanilla Transformer\nto predict masked tokens. By optimizing a likelihood bound, it provides a\nprincipled generative approach for probabilistic inference. Across extensive\nbenchmarks, LLaDA demonstrates strong scalability, outperforming our\nself-constructed ARM baselines. Remarkably, LLaDA 8B is competitive with strong\nLLMs like LLaMA3 8B in in-context learning and, after SFT, exhibits impressive\ninstruction-following abilities in case studies such as multi-turn dialogue.\nMoreover, LLaDA addresses the reversal curse, surpassing GPT-4o in a reversal\npoem completion task. Our findings establish diffusion models as a viable and\npromising alternative to ARMs, challenging the assumption that key LLM\ncapabilities discussed above are inherently tied to ARMs. Project page and\ncodes: https://ml-gsai.github.io/LLaDA-demo/."
                },
                "authors": [
                    {
                        "name": "Shen Nie"
                    },
                    {
                        "name": "Fengqi Zhu"
                    },
                    {
                        "name": "Zebin You"
                    },
                    {
                        "name": "Xiaolu Zhang"
                    },
                    {
                        "name": "Jingyang Ou"
                    },
                    {
                        "name": "Jun Hu"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Yankai Lin"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    },
                    {
                        "name": "Chongxuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Chongxuan Li"
                },
                "author": "Chongxuan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09992v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09992v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12982v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12982v1",
                "updated": "2025-02-18T16:04:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    4,
                    57,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T16:04:57Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    4,
                    57,
                    1,
                    49,
                    0
                ],
                "title": "Sailor2: Sailing in South-East Asia with Inclusive Multilingual LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sailor2: Sailing in South-East Asia with Inclusive Multilingual LLMs"
                },
                "summary": "Sailor2 is a family of cutting-edge multilingual language models for\nSouth-East Asian (SEA) languages, available in 1B, 8B, and 20B sizes to suit\ndiverse applications. Building on Qwen2.5, Sailor2 undergoes continuous\npre-training on 500B tokens (400B SEA-specific and 100B replay tokens) to\nsupport 13 SEA languages while retaining proficiency in Chinese and English.\nSailor2-20B model achieves a 50-50 win rate against GPT-4o across SEA\nlanguages. We also deliver a comprehensive cookbook on how to develop the\nmultilingual model in an efficient manner, including five key aspects: data\ncuration, pre-training, post-training, model customization and evaluation. We\nhope that Sailor2 model (Apache 2.0 license) will drive language development in\nthe SEA region, and Sailor2 cookbook will inspire researchers to build more\ninclusive LLMs for other under-served languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sailor2 is a family of cutting-edge multilingual language models for\nSouth-East Asian (SEA) languages, available in 1B, 8B, and 20B sizes to suit\ndiverse applications. Building on Qwen2.5, Sailor2 undergoes continuous\npre-training on 500B tokens (400B SEA-specific and 100B replay tokens) to\nsupport 13 SEA languages while retaining proficiency in Chinese and English.\nSailor2-20B model achieves a 50-50 win rate against GPT-4o across SEA\nlanguages. We also deliver a comprehensive cookbook on how to develop the\nmultilingual model in an efficient manner, including five key aspects: data\ncuration, pre-training, post-training, model customization and evaluation. We\nhope that Sailor2 model (Apache 2.0 license) will drive language development in\nthe SEA region, and Sailor2 cookbook will inspire researchers to build more\ninclusive LLMs for other under-served languages."
                },
                "authors": [
                    {
                        "name": "Longxu Dou"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Changyu Chen"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Ziqi Jin"
                    },
                    {
                        "name": "Zichen Liu"
                    },
                    {
                        "name": "Tongyao Zhu"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Penghui Yang"
                    },
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Yongchi Zhao"
                    },
                    {
                        "name": "Xiachong Feng"
                    },
                    {
                        "name": "Xin Mao"
                    },
                    {
                        "name": "Man Tsung Yeung"
                    },
                    {
                        "name": "Kunat Pipatanakul"
                    },
                    {
                        "name": "Fajri Koto"
                    },
                    {
                        "name": "Min Si Thu"
                    },
                    {
                        "name": "Hynek Kydlek"
                    },
                    {
                        "name": "Zeyi Liu"
                    },
                    {
                        "name": "Qunshu Lin"
                    },
                    {
                        "name": "Sittipong Sripaisarnmongkol"
                    },
                    {
                        "name": "Kridtaphad Sae-Khow"
                    },
                    {
                        "name": "Nirattisai Thongchim"
                    },
                    {
                        "name": "Taechawat Konkaew"
                    },
                    {
                        "name": "Narong Borijindargoon"
                    },
                    {
                        "name": "Anh Dao"
                    },
                    {
                        "name": "Matichon Maneegard"
                    },
                    {
                        "name": "Phakphum Artkaew"
                    },
                    {
                        "name": "Zheng-Xin Yong"
                    },
                    {
                        "name": "Quan Nguyen"
                    },
                    {
                        "name": "Wannaphong Phatthiyaphaibun"
                    },
                    {
                        "name": "Hoang H. Tran"
                    },
                    {
                        "name": "Mike Zhang"
                    },
                    {
                        "name": "Shiqi Chen"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Xinyi Wan"
                    },
                    {
                        "name": "Wei Lu"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "arxiv_comment": "49 pages, 16 figures. Technical Report of Sailor2:\n  https://sea-sailor.github.io/blog/sailor2/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12982v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13203v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13203v4",
                "updated": "2025-02-18T15:58:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    58,
                    8,
                    1,
                    49,
                    0
                ],
                "published": "2024-09-20T04:17:13Z",
                "published_parsed": [
                    2024,
                    9,
                    20,
                    4,
                    17,
                    13,
                    4,
                    264,
                    0
                ],
                "title": "Neural-Symbolic Collaborative Distillation: Advancing Small Language\n  Models for Complex Reasoning Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural-Symbolic Collaborative Distillation: Advancing Small Language\n  Models for Complex Reasoning Tasks"
                },
                "summary": "In this paper, we propose $\\textbf{Ne}$ural-$\\textbf{Sy}$mbolic\n$\\textbf{C}$ollaborative $\\textbf{D}$istillation ($\\textbf{NesyCD}$), a novel\nknowledge distillation method for learning the complex reasoning abilities of\nLarge Language Models (LLMs, e.g., \\textgreater 13B). We argue that complex\nreasoning tasks are difficult for Small Language Models (SLMs, e.g., $\\leq$\n7B), as these tasks demand not only general cognitive abilities but also\nspecialized knowledge, which is often sparse and difficult for these\nneural-based SLMs to effectively capture. Therefore, NesyCD distills the\ngeneral capabilities and specialized knowledge in LLMs using different manners.\nOn the one hand, we distill only general abilities from teacher LLMs into the\nstudent SLMs of parameterized neural networks. On the other hand, for the\nspecialized abilities and uncommon knowledge of a complex reasoning task, we\nemploy a symbolic knowledge distillation approach to obtain and store the\nspecialized knowledge within a symbolic knowledge base (KB). By decoupling\ngeneral and specialized capabilities, the proposed NesyCD can achieve superior\nperformance cost-effectively, utilizing smaller models and blending\nparameterized neural networks with symbolic KB. Moreover, the specialized KB\ngeneralizes well and is comprehended and manipulated by humans. Our experiments\nshow that NesyCD significantly boosts SLMs' complex reasoning performance on\nin-domain (BBH, GSM8K) and out-of-domain (AGIEval, ARC) datasets. Notably, our\napproach enabled the LLaMA3-8B and Qwen2-7B to surpass GPT-3.5-turbo in\nperformance and come close to matching LLaMA3-70B, despite the latter having\nnine times more parameters. Our code will be available at\nhttps://github.com/Xnhyacinth/NesyCD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose $\\textbf{Ne}$ural-$\\textbf{Sy}$mbolic\n$\\textbf{C}$ollaborative $\\textbf{D}$istillation ($\\textbf{NesyCD}$), a novel\nknowledge distillation method for learning the complex reasoning abilities of\nLarge Language Models (LLMs, e.g., \\textgreater 13B). We argue that complex\nreasoning tasks are difficult for Small Language Models (SLMs, e.g., $\\leq$\n7B), as these tasks demand not only general cognitive abilities but also\nspecialized knowledge, which is often sparse and difficult for these\nneural-based SLMs to effectively capture. Therefore, NesyCD distills the\ngeneral capabilities and specialized knowledge in LLMs using different manners.\nOn the one hand, we distill only general abilities from teacher LLMs into the\nstudent SLMs of parameterized neural networks. On the other hand, for the\nspecialized abilities and uncommon knowledge of a complex reasoning task, we\nemploy a symbolic knowledge distillation approach to obtain and store the\nspecialized knowledge within a symbolic knowledge base (KB). By decoupling\ngeneral and specialized capabilities, the proposed NesyCD can achieve superior\nperformance cost-effectively, utilizing smaller models and blending\nparameterized neural networks with symbolic KB. Moreover, the specialized KB\ngeneralizes well and is comprehended and manipulated by humans. Our experiments\nshow that NesyCD significantly boosts SLMs' complex reasoning performance on\nin-domain (BBH, GSM8K) and out-of-domain (AGIEval, ARC) datasets. Notably, our\napproach enabled the LLaMA3-8B and Qwen2-7B to surpass GPT-3.5-turbo in\nperformance and come close to matching LLaMA3-70B, despite the latter having\nnine times more parameters. Our code will be available at\nhttps://github.com/Xnhyacinth/NesyCD."
                },
                "authors": [
                    {
                        "name": "Huanxuan Liao"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Yao Xu"
                    },
                    {
                        "name": "Yuanzhe Zhang"
                    },
                    {
                        "name": "Kang Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhao"
                },
                "author": "Jun Zhao",
                "arxiv_comment": "Accepted to AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13203v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13203v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12975v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12975v1",
                "updated": "2025-02-18T15:56:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    56,
                    46,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T15:56:46Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    56,
                    46,
                    1,
                    49,
                    0
                ],
                "title": "Instance-Level Moving Object Segmentation from a Single Image with\n  Events",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instance-Level Moving Object Segmentation from a Single Image with\n  Events"
                },
                "summary": "Moving object segmentation plays a crucial role in understanding dynamic\nscenes involving multiple moving objects, while the difficulties lie in taking\ninto account both spatial texture structures and temporal motion cues. Existing\nmethods based on video frames encounter difficulties in distinguishing whether\npixel displacements of an object are caused by camera motion or object motion\ndue to the complexities of accurate image-based motion modeling. Recent\nadvances exploit the motion sensitivity of novel event cameras to counter\nconventional images' inadequate motion modeling capabilities, but instead lead\nto challenges in segmenting pixel-level object masks due to the lack of dense\ntexture structures in events. To address these two limitations imposed by\nunimodal settings, we propose the first instance-level moving object\nsegmentation framework that integrates complementary texture and motion cues.\nOur model incorporates implicit cross-modal masked attention augmentation,\nexplicit contrastive feature learning, and flow-guided motion enhancement to\nexploit dense texture information from a single image and rich motion\ninformation from events, respectively. By leveraging the augmented texture and\nmotion features, we separate mask segmentation from motion classification to\nhandle varying numbers of independently moving objects. Through extensive\nevaluations on multiple datasets, as well as ablation experiments with\ndifferent input settings and real-time efficiency analysis of the proposed\nframework, we believe that our first attempt to incorporate image and event\ndata for practical deployment can provide new insights for future work in\nevent-based motion related works. The source code with model training and\npre-trained weights is released at https://npucvr.github.io/EvInsMOS",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Moving object segmentation plays a crucial role in understanding dynamic\nscenes involving multiple moving objects, while the difficulties lie in taking\ninto account both spatial texture structures and temporal motion cues. Existing\nmethods based on video frames encounter difficulties in distinguishing whether\npixel displacements of an object are caused by camera motion or object motion\ndue to the complexities of accurate image-based motion modeling. Recent\nadvances exploit the motion sensitivity of novel event cameras to counter\nconventional images' inadequate motion modeling capabilities, but instead lead\nto challenges in segmenting pixel-level object masks due to the lack of dense\ntexture structures in events. To address these two limitations imposed by\nunimodal settings, we propose the first instance-level moving object\nsegmentation framework that integrates complementary texture and motion cues.\nOur model incorporates implicit cross-modal masked attention augmentation,\nexplicit contrastive feature learning, and flow-guided motion enhancement to\nexploit dense texture information from a single image and rich motion\ninformation from events, respectively. By leveraging the augmented texture and\nmotion features, we separate mask segmentation from motion classification to\nhandle varying numbers of independently moving objects. Through extensive\nevaluations on multiple datasets, as well as ablation experiments with\ndifferent input settings and real-time efficiency analysis of the proposed\nframework, we believe that our first attempt to incorporate image and event\ndata for practical deployment can provide new insights for future work in\nevent-based motion related works. The source code with model training and\npre-trained weights is released at https://npucvr.github.io/EvInsMOS"
                },
                "authors": [
                    {
                        "name": "Zhexiong Wan"
                    },
                    {
                        "name": "Bin Fan"
                    },
                    {
                        "name": "Le Hui"
                    },
                    {
                        "name": "Yuchao Dai"
                    },
                    {
                        "name": "Gim Hee Lee"
                    }
                ],
                "author_detail": {
                    "name": "Gim Hee Lee"
                },
                "author": "Gim Hee Lee",
                "arxiv_comment": "accepted by IJCV",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12975v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12975v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12974v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12974v1",
                "updated": "2025-02-18T15:56:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    56,
                    34,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T15:56:34Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    56,
                    34,
                    1,
                    49,
                    0
                ],
                "title": "Learning More Effective Representations for Dense Retrieval through\n  Deliberate Thinking Before Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning More Effective Representations for Dense Retrieval through\n  Deliberate Thinking Before Search"
                },
                "summary": "Recent dense retrievers usually thrive on the emergency capabilities of Large\nLanguage Models (LLMs), using them to encode queries and documents into an\nembedding space for retrieval. These LLM-based dense retrievers have shown\npromising performance across various retrieval scenarios. However, relying on a\nsingle embedding to represent documents proves less effective in capturing\ndifferent perspectives of documents for matching. In this paper, we propose\nDeliberate Thinking based Dense Retriever (DEBATER), which enhances these\nLLM-based retrievers by enabling them to learn more effective document\nrepresentations through a step-by-step thinking process. DEBATER introduces the\nChain-of-Deliberation mechanism to iteratively optimize document\nrepresentations using a continuous chain of thought. To consolidate information\nfrom various thinking steps, DEBATER also incorporates the Self Distillation\nmechanism, which identifies the most informative thinking steps and integrates\nthem into a unified text embedding. Experimental results show that DEBATER\nsignificantly outperforms existing methods across several retrieval benchmarks,\ndemonstrating superior accuracy and robustness. All codes are available at\nhttps://github.com/OpenBMB/DEBATER.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent dense retrievers usually thrive on the emergency capabilities of Large\nLanguage Models (LLMs), using them to encode queries and documents into an\nembedding space for retrieval. These LLM-based dense retrievers have shown\npromising performance across various retrieval scenarios. However, relying on a\nsingle embedding to represent documents proves less effective in capturing\ndifferent perspectives of documents for matching. In this paper, we propose\nDeliberate Thinking based Dense Retriever (DEBATER), which enhances these\nLLM-based retrievers by enabling them to learn more effective document\nrepresentations through a step-by-step thinking process. DEBATER introduces the\nChain-of-Deliberation mechanism to iteratively optimize document\nrepresentations using a continuous chain of thought. To consolidate information\nfrom various thinking steps, DEBATER also incorporates the Self Distillation\nmechanism, which identifies the most informative thinking steps and integrates\nthem into a unified text embedding. Experimental results show that DEBATER\nsignificantly outperforms existing methods across several retrieval benchmarks,\ndemonstrating superior accuracy and robustness. All codes are available at\nhttps://github.com/OpenBMB/DEBATER."
                },
                "authors": [
                    {
                        "name": "Yifan Ji"
                    },
                    {
                        "name": "Zhipeng Xu"
                    },
                    {
                        "name": "Zhenghao Liu"
                    },
                    {
                        "name": "Yukun Yan"
                    },
                    {
                        "name": "Shi Yu"
                    },
                    {
                        "name": "Yishan Li"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Yu Gu"
                    },
                    {
                        "name": "Ge Yu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12974v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12974v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12382v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12382v5",
                "updated": "2025-02-18T15:54:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    54,
                    28,
                    1,
                    49,
                    0
                ],
                "published": "2024-06-18T08:14:28Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    8,
                    14,
                    28,
                    1,
                    170,
                    0
                ],
                "title": "From Instance Training to Instruction Learning: Task Adapters Generation\n  from Instructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Instance Training to Instruction Learning: Task Adapters Generation\n  from Instructions"
                },
                "summary": "Large language models (LLMs) have acquired the ability to solve general tasks\nby utilizing instruction finetuning (IFT). However, IFT still relies heavily on\ninstance training of extensive task data, which greatly limits the adaptability\nof LLMs to real-world scenarios where labeled task instances are scarce and\nbroader task generalization becomes paramount. Contrary to LLMs, humans acquire\nskills and complete tasks not merely through repeated practice but also by\nunderstanding and following instructional guidelines. This paper is dedicated\nto simulating human learning to address the shortcomings of instance training,\nfocusing on instruction learning to enhance cross-task generalization. Within\nthis context, we introduce Task Adapters Generation from Instructions (TAGI),\nwhich automatically constructs the task-specific model in a parameter\ngeneration manner based on the given task instructions without retraining for\nunseen tasks. Specifically, we utilize knowledge distillation to enhance the\nconsistency between TAGI developed through Learning with Instruction and\ntask-specific models developed through Training with Instance, by aligning the\nlabels, output logits, and adapter parameters between them. TAGI is endowed\nwith cross-task generalization capabilities through a two-stage training\nprocess that includes hypernetwork pretraining and finetuning. We evaluate TAGI\non the Super-Natural Instructions and P3 datasets. The experimental results\ndemonstrate that TAGI can match or even outperform traditional meta-trained\nmodels and other hypernetwork models, while significantly reducing\ncomputational requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have acquired the ability to solve general tasks\nby utilizing instruction finetuning (IFT). However, IFT still relies heavily on\ninstance training of extensive task data, which greatly limits the adaptability\nof LLMs to real-world scenarios where labeled task instances are scarce and\nbroader task generalization becomes paramount. Contrary to LLMs, humans acquire\nskills and complete tasks not merely through repeated practice but also by\nunderstanding and following instructional guidelines. This paper is dedicated\nto simulating human learning to address the shortcomings of instance training,\nfocusing on instruction learning to enhance cross-task generalization. Within\nthis context, we introduce Task Adapters Generation from Instructions (TAGI),\nwhich automatically constructs the task-specific model in a parameter\ngeneration manner based on the given task instructions without retraining for\nunseen tasks. Specifically, we utilize knowledge distillation to enhance the\nconsistency between TAGI developed through Learning with Instruction and\ntask-specific models developed through Training with Instance, by aligning the\nlabels, output logits, and adapter parameters between them. TAGI is endowed\nwith cross-task generalization capabilities through a two-stage training\nprocess that includes hypernetwork pretraining and finetuning. We evaluate TAGI\non the Super-Natural Instructions and P3 datasets. The experimental results\ndemonstrate that TAGI can match or even outperform traditional meta-trained\nmodels and other hypernetwork models, while significantly reducing\ncomputational requirements."
                },
                "authors": [
                    {
                        "name": "Huanxuan Liao"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Yao Xu"
                    },
                    {
                        "name": "Yuanzhe Zhang"
                    },
                    {
                        "name": "Yanchao Hao"
                    },
                    {
                        "name": "Shengping Liu"
                    },
                    {
                        "name": "Kang Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhao"
                },
                "author": "Jun Zhao",
                "arxiv_comment": "accepted to NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12382v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12382v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06253v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06253v3",
                "updated": "2025-02-18T15:53:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    53,
                    11,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-10T08:37:21Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    8,
                    37,
                    21,
                    0,
                    41,
                    0
                ],
                "title": "Find Central Dogma Again: Leveraging Multilingual Transfer in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Find Central Dogma Again: Leveraging Multilingual Transfer in Large\n  Language Models"
                },
                "summary": "In recent years, large language models (LLMs) have achieved state-of-the-art\nresults in various biological sequence analysis tasks, such as sequence\nclassification, structure prediction, and function prediction. Similar to\nadvancements in AI for other scientific fields, deeper research into biological\nLLMs has begun to focus on using these models to rediscover important existing\nbiological laws or uncover entirely new patterns in biological sequences. This\nstudy leverages GPT-like LLMs to utilize language transfer capabilities to\nrediscover the genetic code rules of the central dogma. In our experimental\ndesign, we transformed the central dogma into a binary classification problem\nof aligning DNA sequences with protein sequences, where positive examples are\nmatching DNA and protein sequences, and negative examples are non-matching\npairs. We first trained a GPT-2 model from scratch using a dataset comprising\nprotein sequences, DNA sequences, and sequences from languages such as English\nand Chinese. Subsequently, we fine-tuned the model using the natural language\nsentences similarity judgment dataset from PAWS-X. When tested on a dataset for\nDNA and protein sequence alignment judgment, the fine-tuned model achieved a\nclassification accuracy of 81%. The study also analyzed factors contributing to\nthis zero-shot capability, including model training stability and types of\ntraining data. This research demonstrates that LLMs can, through the transfer\nof natural language capabilities and solely relying on the analysis of\nsequences themselves, rediscover the central dogma without prior knowledge of\nit. This study bridges natural language and genetic language, opening a new\ndoor for AI-driven biological research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, large language models (LLMs) have achieved state-of-the-art\nresults in various biological sequence analysis tasks, such as sequence\nclassification, structure prediction, and function prediction. Similar to\nadvancements in AI for other scientific fields, deeper research into biological\nLLMs has begun to focus on using these models to rediscover important existing\nbiological laws or uncover entirely new patterns in biological sequences. This\nstudy leverages GPT-like LLMs to utilize language transfer capabilities to\nrediscover the genetic code rules of the central dogma. In our experimental\ndesign, we transformed the central dogma into a binary classification problem\nof aligning DNA sequences with protein sequences, where positive examples are\nmatching DNA and protein sequences, and negative examples are non-matching\npairs. We first trained a GPT-2 model from scratch using a dataset comprising\nprotein sequences, DNA sequences, and sequences from languages such as English\nand Chinese. Subsequently, we fine-tuned the model using the natural language\nsentences similarity judgment dataset from PAWS-X. When tested on a dataset for\nDNA and protein sequence alignment judgment, the fine-tuned model achieved a\nclassification accuracy of 81%. The study also analyzed factors contributing to\nthis zero-shot capability, including model training stability and types of\ntraining data. This research demonstrates that LLMs can, through the transfer\nof natural language capabilities and solely relying on the analysis of\nsequences themselves, rediscover the central dogma without prior knowledge of\nit. This study bridges natural language and genetic language, opening a new\ndoor for AI-driven biological research."
                },
                "authors": [
                    {
                        "name": "Wang Liang"
                    }
                ],
                "author_detail": {
                    "name": "Wang Liang"
                },
                "author": "Wang Liang",
                "arxiv_comment": "31 pages,8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06253v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06253v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "92-10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22071v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22071v2",
                "updated": "2025-02-18T15:52:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    52,
                    52,
                    1,
                    49,
                    0
                ],
                "published": "2024-10-29T14:31:33Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    14,
                    31,
                    33,
                    1,
                    303,
                    0
                ],
                "title": "Distinguishing Ignorance from Error in LLM Hallucinations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distinguishing Ignorance from Error in LLM Hallucinations"
                },
                "summary": "Large language models (LLMs) are susceptible to hallucinations -- factually\nincorrect outputs -- leading to a large body of work on detecting and\nmitigating such cases. We argue that it is important to distinguish between two\ntypes of hallucinations: ones where the model does not hold the correct answer\nin its parameters, which we term HK-, and ones where the model answers\nincorrectly despite having the required knowledge, termed HK+. We first find\nthat HK+ hallucinations are prevalent and occur across models and datasets.\nThen, we demonstrate that distinguishing between these two cases is beneficial\nfor mitigating hallucinations. Importantly, we show that different models\nhallucinate on different examples, which motivates constructing model-specific\nhallucination datasets for training detectors. Overall, our findings draw\nattention to classifying types of hallucinations and provide means to handle\nthem more effectively. The code is available at\nhttps://github.com/technion-cs-nlp/hallucination-mitigation .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are susceptible to hallucinations -- factually\nincorrect outputs -- leading to a large body of work on detecting and\nmitigating such cases. We argue that it is important to distinguish between two\ntypes of hallucinations: ones where the model does not hold the correct answer\nin its parameters, which we term HK-, and ones where the model answers\nincorrectly despite having the required knowledge, termed HK+. We first find\nthat HK+ hallucinations are prevalent and occur across models and datasets.\nThen, we demonstrate that distinguishing between these two cases is beneficial\nfor mitigating hallucinations. Importantly, we show that different models\nhallucinate on different examples, which motivates constructing model-specific\nhallucination datasets for training detectors. Overall, our findings draw\nattention to classifying types of hallucinations and provide means to handle\nthem more effectively. The code is available at\nhttps://github.com/technion-cs-nlp/hallucination-mitigation ."
                },
                "authors": [
                    {
                        "name": "Adi Simhi"
                    },
                    {
                        "name": "Jonathan Herzig"
                    },
                    {
                        "name": "Idan Szpektor"
                    },
                    {
                        "name": "Yonatan Belinkov"
                    }
                ],
                "author_detail": {
                    "name": "Yonatan Belinkov"
                },
                "author": "Yonatan Belinkov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22071v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22071v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12970v1",
                "updated": "2025-02-18T15:48:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    48,
                    46,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T15:48:46Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    48,
                    46,
                    1,
                    49,
                    0
                ],
                "title": "Reasoning-to-Defend: Safety-Aware Reasoning Can Defend Large Language\n  Models from Jailbreaking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning-to-Defend: Safety-Aware Reasoning Can Defend Large Language\n  Models from Jailbreaking"
                },
                "summary": "The reasoning abilities of Large Language Models (LLMs) have demonstrated\nremarkable advancement and exceptional performance across diverse domains.\nHowever, leveraging these reasoning capabilities to enhance LLM safety against\nadversarial attacks and jailbreak queries remains largely unexplored. To bridge\nthis gap, we propose Reasoning-to-Defend (R2D), a novel training paradigm that\nintegrates safety reflections of queries and responses into LLMs' generation\nprocess, unlocking a safety-aware reasoning mechanism. This approach enables\nself-evaluation at each reasoning step to create safety pivot tokens as\nindicators of the response's safety status. Furthermore, in order to improve\nthe learning efficiency of pivot token prediction, we propose Contrastive Pivot\nOptimization(CPO), which enhances the model's ability to perceive the safety\nstatus of dialogues. Through this mechanism, LLMs dynamically adjust their\nresponse strategies during reasoning, significantly enhancing their defense\ncapabilities against jailbreak attacks. Extensive experimental results\ndemonstrate that R2D effectively mitigates various attacks and improves overall\nsafety, highlighting the substantial potential of safety-aware reasoning in\nstrengthening LLMs' robustness against jailbreaks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reasoning abilities of Large Language Models (LLMs) have demonstrated\nremarkable advancement and exceptional performance across diverse domains.\nHowever, leveraging these reasoning capabilities to enhance LLM safety against\nadversarial attacks and jailbreak queries remains largely unexplored. To bridge\nthis gap, we propose Reasoning-to-Defend (R2D), a novel training paradigm that\nintegrates safety reflections of queries and responses into LLMs' generation\nprocess, unlocking a safety-aware reasoning mechanism. This approach enables\nself-evaluation at each reasoning step to create safety pivot tokens as\nindicators of the response's safety status. Furthermore, in order to improve\nthe learning efficiency of pivot token prediction, we propose Contrastive Pivot\nOptimization(CPO), which enhances the model's ability to perceive the safety\nstatus of dialogues. Through this mechanism, LLMs dynamically adjust their\nresponse strategies during reasoning, significantly enhancing their defense\ncapabilities against jailbreak attacks. Extensive experimental results\ndemonstrate that R2D effectively mitigates various attacks and improves overall\nsafety, highlighting the substantial potential of safety-aware reasoning in\nstrengthening LLMs' robustness against jailbreaks."
                },
                "authors": [
                    {
                        "name": "Junda Zhu"
                    },
                    {
                        "name": "Lingyong Yan"
                    },
                    {
                        "name": "Shuaiqiang Wang"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Lei Sha"
                    }
                ],
                "author_detail": {
                    "name": "Lei Sha"
                },
                "author": "Lei Sha",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12964v1",
                "updated": "2025-02-18T15:46:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    46,
                    31,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T15:46:31Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    46,
                    31,
                    1,
                    49,
                    0
                ],
                "title": "Trust Me, I'm Wrong: High-Certainty Hallucinations in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trust Me, I'm Wrong: High-Certainty Hallucinations in LLMs"
                },
                "summary": "Large Language Models (LLMs) often generate outputs that lack grounding in\nreal-world facts, a phenomenon known as hallucinations. Prior research has\nassociated hallucinations with model uncertainty, leveraging this relationship\nfor hallucination detection and mitigation. In this paper, we challenge the\nunderlying assumption that all hallucinations are associated with uncertainty.\nUsing knowledge detection and uncertainty measurement methods, we demonstrate\nthat models can hallucinate with high certainty even when they have the correct\nknowledge. We further show that high-certainty hallucinations are consistent\nacross models and datasets, distinctive enough to be singled out, and challenge\nexisting mitigation methods. Our findings reveal an overlooked aspect of\nhallucinations, emphasizing the need to understand their origins and improve\nmitigation strategies to enhance LLM safety. The code is available at\nhttps://github.com/technion-cs-nlp/Trust_me_Im_wrong .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often generate outputs that lack grounding in\nreal-world facts, a phenomenon known as hallucinations. Prior research has\nassociated hallucinations with model uncertainty, leveraging this relationship\nfor hallucination detection and mitigation. In this paper, we challenge the\nunderlying assumption that all hallucinations are associated with uncertainty.\nUsing knowledge detection and uncertainty measurement methods, we demonstrate\nthat models can hallucinate with high certainty even when they have the correct\nknowledge. We further show that high-certainty hallucinations are consistent\nacross models and datasets, distinctive enough to be singled out, and challenge\nexisting mitigation methods. Our findings reveal an overlooked aspect of\nhallucinations, emphasizing the need to understand their origins and improve\nmitigation strategies to enhance LLM safety. The code is available at\nhttps://github.com/technion-cs-nlp/Trust_me_Im_wrong ."
                },
                "authors": [
                    {
                        "name": "Adi Simhi"
                    },
                    {
                        "name": "Itay Itzhak"
                    },
                    {
                        "name": "Fazl Barez"
                    },
                    {
                        "name": "Gabriel Stanovsky"
                    },
                    {
                        "name": "Yonatan Belinkov"
                    }
                ],
                "author_detail": {
                    "name": "Yonatan Belinkov"
                },
                "author": "Yonatan Belinkov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12962v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12962v1",
                "updated": "2025-02-18T15:45:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    45,
                    36,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T15:45:36Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    45,
                    36,
                    1,
                    49,
                    0
                ],
                "title": "Infinite Retrieval: Attention Enhanced LLMs in Long-Context Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Infinite Retrieval: Attention Enhanced LLMs in Long-Context Processing"
                },
                "summary": "Limited by the context window size of Large Language Models(LLMs), handling\nvarious tasks with input tokens exceeding the upper limit has been challenging,\nwhether it is a simple direct retrieval task or a complex multi-hop reasoning\ntask. Although various methods have been proposed to enhance the long-context\nprocessing capabilities of LLMs, they either incur substantial post-training\ncosts, or require additional tool modules(e.g.,RAG), or have not shown\nsignificant improvement in realistic tasks. Our work observes the correlation\nbetween the attention distribution and generated answers across each layer, and\nestablishes the attention allocation aligns with retrieval-augmented\ncapabilities through experiments. Drawing on the above insights, we propose a\nnovel method InfiniRetri that leverages the LLMs's own attention information to\nenable accurate retrieval across inputs of infinitely length. Our evaluations\nindicate that InfiniRetri achieves 100% accuracy in the\nNeedle-In-a-Haystack(NIH) test over 1M tokens using a 0.5B parameter model,\nsurpassing other method or larger models and setting a new\nstate-of-the-art(SOTA). Moreover, our method achieves significant performance\nimprovements on real-world benchmarks, with a maximum 288% improvement. In\naddition, InfiniRetri can be applied to any Transformer-based LLMs without\nadditional training and substantially reduces inference latency and compute\noverhead in long texts. In summary, our comprehensive studies show\nInfiniRetri's potential for practical applications and creates a paradigm for\nretrievaling information using LLMs own capabilities under infinite-length\ntokens. Code will be released in link.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Limited by the context window size of Large Language Models(LLMs), handling\nvarious tasks with input tokens exceeding the upper limit has been challenging,\nwhether it is a simple direct retrieval task or a complex multi-hop reasoning\ntask. Although various methods have been proposed to enhance the long-context\nprocessing capabilities of LLMs, they either incur substantial post-training\ncosts, or require additional tool modules(e.g.,RAG), or have not shown\nsignificant improvement in realistic tasks. Our work observes the correlation\nbetween the attention distribution and generated answers across each layer, and\nestablishes the attention allocation aligns with retrieval-augmented\ncapabilities through experiments. Drawing on the above insights, we propose a\nnovel method InfiniRetri that leverages the LLMs's own attention information to\nenable accurate retrieval across inputs of infinitely length. Our evaluations\nindicate that InfiniRetri achieves 100% accuracy in the\nNeedle-In-a-Haystack(NIH) test over 1M tokens using a 0.5B parameter model,\nsurpassing other method or larger models and setting a new\nstate-of-the-art(SOTA). Moreover, our method achieves significant performance\nimprovements on real-world benchmarks, with a maximum 288% improvement. In\naddition, InfiniRetri can be applied to any Transformer-based LLMs without\nadditional training and substantially reduces inference latency and compute\noverhead in long texts. In summary, our comprehensive studies show\nInfiniRetri's potential for practical applications and creates a paradigm for\nretrievaling information using LLMs own capabilities under infinite-length\ntokens. Code will be released in link."
                },
                "authors": [
                    {
                        "name": "Xiaoju Ye"
                    },
                    {
                        "name": "Zhichun Wang"
                    },
                    {
                        "name": "Jingyuan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jingyuan Wang"
                },
                "author": "Jingyuan Wang",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12962v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12962v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12961v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12961v1",
                "updated": "2025-02-18T15:45:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    45,
                    1,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T15:45:01Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    45,
                    1,
                    1,
                    49,
                    0
                ],
                "title": "Adaptive Tool Use in Large Language Models with Meta-Cognition Trigger",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Tool Use in Large Language Models with Meta-Cognition Trigger"
                },
                "summary": "Large language models (LLMs) have shown remarkable emergent capabilities,\ntransforming the execution of functional tasks by leveraging external tools for\ncomplex problems that require specialized processing or real-time data. While\nexisting research expands LLMs access to diverse tools (e.g., program\ninterpreters, search engines, weather/map apps), the necessity of using these\ntools is often overlooked, leading to indiscriminate tool invocation. This\nnaive approach raises two key issues:(1) increased delays due to unnecessary\ntool calls, and (2) potential errors resulting from faulty interactions with\nexternal tools. In this paper, we introduce meta-cognition as a proxy for LLMs\nself-assessment of their capabilities, representing the model's awareness of\nits own limitations. Based on this, we propose MeCo, an adaptive\ndecision-making strategy for external tool use. MeCo quantifies metacognitive\nscores by capturing high-level cognitive signals in the representation space,\nguiding when to invoke tools. Notably, MeCo is fine-tuning-free and incurs\nminimal cost. Our experiments show that MeCo accurately detects LLMs' internal\ncognitive signals and significantly improves tool-use decision-making across\nmultiple base models and benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable emergent capabilities,\ntransforming the execution of functional tasks by leveraging external tools for\ncomplex problems that require specialized processing or real-time data. While\nexisting research expands LLMs access to diverse tools (e.g., program\ninterpreters, search engines, weather/map apps), the necessity of using these\ntools is often overlooked, leading to indiscriminate tool invocation. This\nnaive approach raises two key issues:(1) increased delays due to unnecessary\ntool calls, and (2) potential errors resulting from faulty interactions with\nexternal tools. In this paper, we introduce meta-cognition as a proxy for LLMs\nself-assessment of their capabilities, representing the model's awareness of\nits own limitations. Based on this, we propose MeCo, an adaptive\ndecision-making strategy for external tool use. MeCo quantifies metacognitive\nscores by capturing high-level cognitive signals in the representation space,\nguiding when to invoke tools. Notably, MeCo is fine-tuning-free and incurs\nminimal cost. Our experiments show that MeCo accurately detects LLMs' internal\ncognitive signals and significantly improves tool-use decision-making across\nmultiple base models and benchmarks."
                },
                "authors": [
                    {
                        "name": "Wenjun Li"
                    },
                    {
                        "name": "Dexun Li"
                    },
                    {
                        "name": "Kuicai Dong"
                    },
                    {
                        "name": "Cong Zhang"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12961v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12961v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05193v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05193v2",
                "updated": "2025-02-18T15:38:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    38,
                    18,
                    1,
                    49,
                    0
                ],
                "published": "2024-10-07T16:50:47Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    16,
                    50,
                    47,
                    0,
                    281,
                    0
                ],
                "title": "RevisEval: Improving LLM-as-a-Judge via Response-Adapted References",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RevisEval: Improving LLM-as-a-Judge via Response-Adapted References"
                },
                "summary": "With significant efforts in recent studies, LLM-as-a-Judge has become a\ncost-effective alternative to human evaluation for assessing text generation\nquality in a wide range of tasks. However, there still remains a reliability\ngap between LLM-as-a-Judge and human evaluation. One important reason is the\nlack of guided oracles in the evaluation process. Motivated by the role of\nreference pervasively used in classic text evaluation, we introduce RevisEval,\na novel text generation evaluation paradigm via the response-adapted\nreferences. RevisEval is driven by the key observation that an ideal reference\nshould maintain the necessary relevance to the response to be evaluated.\nSpecifically, RevisEval leverages the text revision capabilities of large\nlanguage models (LLMs) to adaptively revise the response, then treat the\nrevised text as the reference (response-adapted reference) for the subsequent\nevaluation. Extensive experiments demonstrate that RevisEval outperforms\ntraditional reference-free and reference-based evaluation paradigms that use\nLLM-as-a-Judge across NLG tasks and open-ended instruction-following tasks.\nMore importantly, our response-adapted references can further boost the\nclassical text metrics, e.g., BLEU and BERTScore, compared to traditional\nreferences and even rival the LLM-as-a-Judge. A detailed analysis is also\nconducted to confirm RevisEval's effectiveness in bias reduction, the impact of\ninference cost, and reference relevance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With significant efforts in recent studies, LLM-as-a-Judge has become a\ncost-effective alternative to human evaluation for assessing text generation\nquality in a wide range of tasks. However, there still remains a reliability\ngap between LLM-as-a-Judge and human evaluation. One important reason is the\nlack of guided oracles in the evaluation process. Motivated by the role of\nreference pervasively used in classic text evaluation, we introduce RevisEval,\na novel text generation evaluation paradigm via the response-adapted\nreferences. RevisEval is driven by the key observation that an ideal reference\nshould maintain the necessary relevance to the response to be evaluated.\nSpecifically, RevisEval leverages the text revision capabilities of large\nlanguage models (LLMs) to adaptively revise the response, then treat the\nrevised text as the reference (response-adapted reference) for the subsequent\nevaluation. Extensive experiments demonstrate that RevisEval outperforms\ntraditional reference-free and reference-based evaluation paradigms that use\nLLM-as-a-Judge across NLG tasks and open-ended instruction-following tasks.\nMore importantly, our response-adapted references can further boost the\nclassical text metrics, e.g., BLEU and BERTScore, compared to traditional\nreferences and even rival the LLM-as-a-Judge. A detailed analysis is also\nconducted to confirm RevisEval's effectiveness in bias reduction, the impact of\ninference cost, and reference relevance."
                },
                "authors": [
                    {
                        "name": "Qiyuan Zhang"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Tiezheng YU"
                    },
                    {
                        "name": "Yuxin Jiang"
                    },
                    {
                        "name": "Chuhan Wu"
                    },
                    {
                        "name": "Liangyou Li"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Fuyuan Lyu"
                    },
                    {
                        "name": "Chen Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ma"
                },
                "author": "Chen Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05193v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05193v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12945v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12945v1",
                "updated": "2025-02-18T15:29:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    29,
                    5,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T15:29:05Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    29,
                    5,
                    1,
                    49,
                    0
                ],
                "title": "LLMPopcorn: An Empirical Study of LLMs as Assistants for Popular\n  Micro-video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMPopcorn: An Empirical Study of LLMs as Assistants for Popular\n  Micro-video Generation"
                },
                "summary": "Popular Micro-videos, dominant on platforms like TikTok and YouTube, hold\nsignificant commercial value. The rise of high-quality AI-generated content has\nspurred interest in AI-driven micro-video creation. However, despite the\nadvanced capabilities of large language models (LLMs) like ChatGPT and DeepSeek\nin text generation and reasoning, their potential to assist the creation of\npopular micro-videos remains largely unexplored.\n  In this paper, we conduct an empirical study on LLM-assisted popular\nmicro-video generation (LLMPopcorn). Specifically, we investigate the following\nresearch questions: (i) How can LLMs be effectively utilized to assist popular\nmicro-video generation? (ii) To what extent can prompt-based enhancements\noptimize the LLM-generated content for higher popularity? (iii) How well do\nvarious LLMs and video generators perform in the popular micro-video generation\ntask? By exploring these questions, we show that advanced LLMs like DeepSeek-V3\nenable micro-video generation to achieve popularity comparable to human-created\ncontent. Prompt enhancements further boost popularity, and benchmarking\nhighlights DeepSeek-V3 and DeepSeek-R1 among LLMs, while LTX-Video and\nHunyuanVideo lead in video generation. This pioneering work advances\nAI-assisted micro-video creation, uncovering new research opportunities. We\nwill release the code and datasets to support future studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Popular Micro-videos, dominant on platforms like TikTok and YouTube, hold\nsignificant commercial value. The rise of high-quality AI-generated content has\nspurred interest in AI-driven micro-video creation. However, despite the\nadvanced capabilities of large language models (LLMs) like ChatGPT and DeepSeek\nin text generation and reasoning, their potential to assist the creation of\npopular micro-videos remains largely unexplored.\n  In this paper, we conduct an empirical study on LLM-assisted popular\nmicro-video generation (LLMPopcorn). Specifically, we investigate the following\nresearch questions: (i) How can LLMs be effectively utilized to assist popular\nmicro-video generation? (ii) To what extent can prompt-based enhancements\noptimize the LLM-generated content for higher popularity? (iii) How well do\nvarious LLMs and video generators perform in the popular micro-video generation\ntask? By exploring these questions, we show that advanced LLMs like DeepSeek-V3\nenable micro-video generation to achieve popularity comparable to human-created\ncontent. Prompt enhancements further boost popularity, and benchmarking\nhighlights DeepSeek-V3 and DeepSeek-R1 among LLMs, while LTX-Video and\nHunyuanVideo lead in video generation. This pioneering work advances\nAI-assisted micro-video creation, uncovering new research opportunities. We\nwill release the code and datasets to support future studies."
                },
                "authors": [
                    {
                        "name": "Junchen Fu"
                    },
                    {
                        "name": "Xuri Ge"
                    },
                    {
                        "name": "Kaiwen Zheng"
                    },
                    {
                        "name": "Ioannis Arapakis"
                    },
                    {
                        "name": "Xin Xin"
                    },
                    {
                        "name": "Joemon M. Jose"
                    }
                ],
                "author_detail": {
                    "name": "Joemon M. Jose"
                },
                "author": "Joemon M. Jose",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12945v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12945v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12509v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12509v2",
                "updated": "2025-02-18T15:24:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    24,
                    25,
                    1,
                    49,
                    0
                ],
                "published": "2024-12-17T03:37:31Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    3,
                    37,
                    31,
                    1,
                    352,
                    0
                ],
                "title": "Can You Trust LLM Judgments? Reliability of LLM-as-a-Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can You Trust LLM Judgments? Reliability of LLM-as-a-Judge"
                },
                "summary": "Large Language Models (LLMs) have become increasingly powerful and\nubiquitous, but their stochastic nature poses challenges to the reliability of\ntheir outputs. While deterministic settings can improve consistency, they do\nnot guarantee reliability, as a single sample from the model's probability\ndistribution can still be misleading. Building upon the concept of\nLLM-as-a-judge, we introduce a novel framework for rigorously evaluating the\nreliability of LLM judgments, leveraging McDonald's omega. We evaluate the\nreliability of LLMs when judging the outputs of other LLMs on standard\nsingle-turn and multi-turn benchmarks, simultaneously investigating the impact\nof temperature on reliability. By analyzing these results, we demonstrate the\nlimitations of fixed randomness and the importance of considering multiple\nsamples, which we show has significant implications for downstream\napplications. Our findings highlight the need for a nuanced understanding of\nLLM reliability and the potential risks associated with over-reliance on\nsingle-shot evaluations. This work provides a crucial step towards building\nmore trustworthy and reliable LLM-based systems and applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become increasingly powerful and\nubiquitous, but their stochastic nature poses challenges to the reliability of\ntheir outputs. While deterministic settings can improve consistency, they do\nnot guarantee reliability, as a single sample from the model's probability\ndistribution can still be misleading. Building upon the concept of\nLLM-as-a-judge, we introduce a novel framework for rigorously evaluating the\nreliability of LLM judgments, leveraging McDonald's omega. We evaluate the\nreliability of LLMs when judging the outputs of other LLMs on standard\nsingle-turn and multi-turn benchmarks, simultaneously investigating the impact\nof temperature on reliability. By analyzing these results, we demonstrate the\nlimitations of fixed randomness and the importance of considering multiple\nsamples, which we show has significant implications for downstream\napplications. Our findings highlight the need for a nuanced understanding of\nLLM reliability and the potential risks associated with over-reliance on\nsingle-shot evaluations. This work provides a crucial step towards building\nmore trustworthy and reliable LLM-based systems and applications."
                },
                "authors": [
                    {
                        "name": "Kayla Schroeder"
                    },
                    {
                        "name": "Zach Wood-Doughty"
                    }
                ],
                "author_detail": {
                    "name": "Zach Wood-Doughty"
                },
                "author": "Zach Wood-Doughty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12509v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12509v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15175v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15175v3",
                "updated": "2025-02-18T15:22:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    22,
                    49,
                    1,
                    49,
                    0
                ],
                "published": "2024-11-18T00:21:14Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    0,
                    21,
                    14,
                    0,
                    323,
                    0
                ],
                "title": "ToxiLab: How Well Do Open-Source LLMs Generate Synthetic Toxicity Data?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToxiLab: How Well Do Open-Source LLMs Generate Synthetic Toxicity Data?"
                },
                "summary": "Effective toxic content detection relies heavily on high-quality and diverse\ndata, which serve as the foundation for robust content moderation models.\nSynthetic data has become a common approach for training models across various\nNLP tasks. However, its effectiveness remains uncertain for highly subjective\ntasks like hate speech detection, with previous research yielding mixed\nresults. This study explores the potential of open-source LLMs for harmful data\nsynthesis, utilizing controlled prompting and supervised fine-tuning techniques\nto enhance data quality and diversity. We systematically evaluated 6 open\nsource LLMs on 5 datasets, assessing their ability to generate diverse,\nhigh-quality harmful data while minimizing hallucination and duplication. Our\nresults show that Mistral consistently outperforms other open models, and\nsupervised fine-tuning significantly enhances data reliability and diversity.\nWe further analyze the trade-offs between prompt-based vs. fine-tuned toxic\ndata synthesis, discuss real-world deployment challenges, and highlight ethical\nconsiderations. Our findings demonstrate that fine-tuned open source LLMs\nprovide scalable and cost-effective solutions to augment toxic content\ndetection datasets, paving the way for more accessible and transparent content\nmoderation tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective toxic content detection relies heavily on high-quality and diverse\ndata, which serve as the foundation for robust content moderation models.\nSynthetic data has become a common approach for training models across various\nNLP tasks. However, its effectiveness remains uncertain for highly subjective\ntasks like hate speech detection, with previous research yielding mixed\nresults. This study explores the potential of open-source LLMs for harmful data\nsynthesis, utilizing controlled prompting and supervised fine-tuning techniques\nto enhance data quality and diversity. We systematically evaluated 6 open\nsource LLMs on 5 datasets, assessing their ability to generate diverse,\nhigh-quality harmful data while minimizing hallucination and duplication. Our\nresults show that Mistral consistently outperforms other open models, and\nsupervised fine-tuning significantly enhances data reliability and diversity.\nWe further analyze the trade-offs between prompt-based vs. fine-tuned toxic\ndata synthesis, discuss real-world deployment challenges, and highlight ethical\nconsiderations. Our findings demonstrate that fine-tuned open source LLMs\nprovide scalable and cost-effective solutions to augment toxic content\ndetection datasets, paving the way for more accessible and transparent content\nmoderation tools."
                },
                "authors": [
                    {
                        "name": "Zheng Hui"
                    },
                    {
                        "name": "Zhaoxiao Guo"
                    },
                    {
                        "name": "Hang Zhao"
                    },
                    {
                        "name": "Juanyong Duan"
                    },
                    {
                        "name": "Lin Ai"
                    },
                    {
                        "name": "Yinheng Li"
                    },
                    {
                        "name": "Julia Hirschberg"
                    },
                    {
                        "name": "Congrui Huang"
                    }
                ],
                "author_detail": {
                    "name": "Congrui Huang"
                },
                "author": "Congrui Huang",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15175v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15175v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12932v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12932v1",
                "updated": "2025-02-18T15:14:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    14,
                    58,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T15:14:58Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    14,
                    58,
                    1,
                    49,
                    0
                ],
                "title": "Synthetic Data Generation for Culturally Nuanced Commonsense Reasoning\n  in Low-Resource Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic Data Generation for Culturally Nuanced Commonsense Reasoning\n  in Low-Resource Languages"
                },
                "summary": "Quantifying reasoning capability in low-resource languages remains a\nchallenge in NLP due to data scarcity and limited access to annotators. While\nLLM-assisted dataset construction has proven useful for medium- and\nhigh-resource languages, its effectiveness in low-resource languages,\nparticularly for commonsense reasoning, is still unclear. In this paper, we\ncompare three dataset creation strategies: (1) LLM-assisted dataset generation,\n(2) machine translation, and (3) human-written data by native speakers, to\nbuild a culturally nuanced story comprehension dataset. We focus on Javanese\nand Sundanese, two major local languages in Indonesia, and evaluate the\neffectiveness of open-weight and closed-weight LLMs in assisting dataset\ncreation through extensive manual validation. To assess the utility of\nsynthetic data, we fine-tune language models on classification and generation\ntasks using this data and evaluate performance on a human-written test set. Our\nfindings indicate that LLM-assisted data creation outperforms machine\ntranslation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying reasoning capability in low-resource languages remains a\nchallenge in NLP due to data scarcity and limited access to annotators. While\nLLM-assisted dataset construction has proven useful for medium- and\nhigh-resource languages, its effectiveness in low-resource languages,\nparticularly for commonsense reasoning, is still unclear. In this paper, we\ncompare three dataset creation strategies: (1) LLM-assisted dataset generation,\n(2) machine translation, and (3) human-written data by native speakers, to\nbuild a culturally nuanced story comprehension dataset. We focus on Javanese\nand Sundanese, two major local languages in Indonesia, and evaluate the\neffectiveness of open-weight and closed-weight LLMs in assisting dataset\ncreation through extensive manual validation. To assess the utility of\nsynthetic data, we fine-tune language models on classification and generation\ntasks using this data and evaluate performance on a human-written test set. Our\nfindings indicate that LLM-assisted data creation outperforms machine\ntranslation."
                },
                "authors": [
                    {
                        "name": "Salsabila Zahirah Pranida"
                    },
                    {
                        "name": "Rifo Ahmad Genadi"
                    },
                    {
                        "name": "Fajri Koto"
                    }
                ],
                "author_detail": {
                    "name": "Fajri Koto"
                },
                "author": "Fajri Koto",
                "arxiv_comment": "18 pages total: 8 pages of main body, 6 pages of appendix. 4 figures\n  in main body, 6 figures in appendix. Submitted to ARR on February 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12932v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12932v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12929v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12929v1",
                "updated": "2025-02-18T15:11:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    11,
                    46,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T15:11:46Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    11,
                    46,
                    1,
                    49,
                    0
                ],
                "title": "Flow-of-Options: Diversified and Improved LLM Reasoning by Thinking\n  Through Options",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow-of-Options: Diversified and Improved LLM Reasoning by Thinking\n  Through Options"
                },
                "summary": "We present a novel reasoning approach called Flow-of-Options (FoO), designed\nto address intrinsic biases in Large Language Models (LLMs). FoO enables LLMs\nto systematically explore a diverse range of possibilities in their reasoning,\nas demonstrated by an FoO-based agentic system for autonomously solving Machine\nLearning tasks (AutoML). Our framework outperforms state-of-the-art baselines,\nachieving improvements of 38.2% - 69.2% on standard data science tasks, and\n37.4% - 47.9% on therapeutic chemistry tasks. With an overall operation cost\nunder $1 per task, our framework is well-suited for cost-sensitive\napplications. Beyond classification and regression, we illustrate the broader\napplicability of our FoO-based agentic system to tasks such as reinforcement\nlearning and image generation. Our framework presents significant advancements\ncompared to current state-of-the-art agentic systems for AutoML, due to the\nbenefits of FoO in enforcing diversity in LLM solutions through compressed,\nexplainable representations that also support long-term memory when combined\nwith case-based reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel reasoning approach called Flow-of-Options (FoO), designed\nto address intrinsic biases in Large Language Models (LLMs). FoO enables LLMs\nto systematically explore a diverse range of possibilities in their reasoning,\nas demonstrated by an FoO-based agentic system for autonomously solving Machine\nLearning tasks (AutoML). Our framework outperforms state-of-the-art baselines,\nachieving improvements of 38.2% - 69.2% on standard data science tasks, and\n37.4% - 47.9% on therapeutic chemistry tasks. With an overall operation cost\nunder $1 per task, our framework is well-suited for cost-sensitive\napplications. Beyond classification and regression, we illustrate the broader\napplicability of our FoO-based agentic system to tasks such as reinforcement\nlearning and image generation. Our framework presents significant advancements\ncompared to current state-of-the-art agentic systems for AutoML, due to the\nbenefits of FoO in enforcing diversity in LLM solutions through compressed,\nexplainable representations that also support long-term memory when combined\nwith case-based reasoning."
                },
                "authors": [
                    {
                        "name": "Lakshmi Nair"
                    },
                    {
                        "name": "Ian Trase"
                    },
                    {
                        "name": "Mark Kim"
                    }
                ],
                "author_detail": {
                    "name": "Mark Kim"
                },
                "author": "Mark Kim",
                "arxiv_comment": "Github code: https://github.com/flagshippioneering/Flow-of-Options",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12929v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12929v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12928v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12928v1",
                "updated": "2025-02-18T15:09:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    9,
                    58,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T15:09:58Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    9,
                    58,
                    1,
                    49,
                    0
                ],
                "title": "Finedeep: Mitigating Sparse Activation in Dense LLMs via Multi-Layer\n  Fine-Grained Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finedeep: Mitigating Sparse Activation in Dense LLMs via Multi-Layer\n  Fine-Grained Experts"
                },
                "summary": "Large language models have demonstrated exceptional performance across a wide\nrange of tasks. However, dense models usually suffer from sparse activation,\nwhere many activation values tend towards zero (i.e., being inactivated). We\nargue that this could restrict the efficient exploration of model\nrepresentation space. To mitigate this issue, we propose Finedeep, a\ndeep-layered fine-grained expert architecture for dense models. Our framework\npartitions the feed-forward neural network layers of traditional dense models\ninto small experts, arranges them across multiple sub-layers. A novel routing\nmechanism is proposed to determine each expert's contribution. We conduct\nextensive experiments across various model sizes, demonstrating that our\napproach significantly outperforms traditional dense architectures in terms of\nperplexity and benchmark performance while maintaining a comparable number of\nparameters and floating-point operations. Moreover, we find that Finedeep\nachieves optimal results when balancing depth and width, specifically by\nadjusting the number of expert sub-layers and the number of experts per\nsub-layer. Empirical results confirm that Finedeep effectively alleviates\nsparse activation and efficiently utilizes representation capacity in dense\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have demonstrated exceptional performance across a wide\nrange of tasks. However, dense models usually suffer from sparse activation,\nwhere many activation values tend towards zero (i.e., being inactivated). We\nargue that this could restrict the efficient exploration of model\nrepresentation space. To mitigate this issue, we propose Finedeep, a\ndeep-layered fine-grained expert architecture for dense models. Our framework\npartitions the feed-forward neural network layers of traditional dense models\ninto small experts, arranges them across multiple sub-layers. A novel routing\nmechanism is proposed to determine each expert's contribution. We conduct\nextensive experiments across various model sizes, demonstrating that our\napproach significantly outperforms traditional dense architectures in terms of\nperplexity and benchmark performance while maintaining a comparable number of\nparameters and floating-point operations. Moreover, we find that Finedeep\nachieves optimal results when balancing depth and width, specifically by\nadjusting the number of expert sub-layers and the number of experts per\nsub-layer. Empirical results confirm that Finedeep effectively alleviates\nsparse activation and efficiently utilizes representation capacity in dense\nmodels."
                },
                "authors": [
                    {
                        "name": "Leiyu Pan"
                    },
                    {
                        "name": "Zhenpeng Su"
                    },
                    {
                        "name": "Minxuan Lv"
                    },
                    {
                        "name": "Yizhe Xiong"
                    },
                    {
                        "name": "Xiangwen Zhang"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    },
                    {
                        "name": "Cheng Luo"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Deyi Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Deyi Xiong"
                },
                "author": "Deyi Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12928v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12928v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12927v1",
                "updated": "2025-02-18T15:09:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    9,
                    29,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T15:09:29Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    9,
                    29,
                    1,
                    49,
                    0
                ],
                "title": "SEFL: Harnessing Large Language Model Agents to Improve Educational\n  Feedback Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEFL: Harnessing Large Language Model Agents to Improve Educational\n  Feedback Systems"
                },
                "summary": "Providing high-quality feedback is crucial for student success but is\nconstrained by time, cost, and limited data availability. We introduce\nSynthetic Educational Feedback Loops (SEFL), a novel framework designed to\ndeliver immediate, on-demand feedback at scale without relying on extensive,\nreal-world student data. In SEFL, two large language models (LLMs) operate in\nteacher--student roles to simulate assignment completion and formative\nfeedback, generating abundant synthetic pairs of student work and corresponding\ncritiques. We then fine-tune smaller, more computationally efficient LLMs on\nthese synthetic pairs, enabling them to replicate key features of high-quality,\ngoal-oriented feedback. Unlike personalized tutoring approaches that offer\nmulti-turn, individualized instruction, SEFL specifically focuses on\nreplicating the teacher-->student feedback loop for diverse assignments.\nThrough both LLM-as-a-judge and human evaluations, we demonstrate that\nSEFL-tuned models outperform their non-tuned counterparts in feedback quality,\nclarity, and timeliness. These findings reveal SEFL's potential to transform\nfeedback processes for higher education and beyond, offering an ethical and\nscalable alternative to conventional manual feedback cycles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Providing high-quality feedback is crucial for student success but is\nconstrained by time, cost, and limited data availability. We introduce\nSynthetic Educational Feedback Loops (SEFL), a novel framework designed to\ndeliver immediate, on-demand feedback at scale without relying on extensive,\nreal-world student data. In SEFL, two large language models (LLMs) operate in\nteacher--student roles to simulate assignment completion and formative\nfeedback, generating abundant synthetic pairs of student work and corresponding\ncritiques. We then fine-tune smaller, more computationally efficient LLMs on\nthese synthetic pairs, enabling them to replicate key features of high-quality,\ngoal-oriented feedback. Unlike personalized tutoring approaches that offer\nmulti-turn, individualized instruction, SEFL specifically focuses on\nreplicating the teacher-->student feedback loop for diverse assignments.\nThrough both LLM-as-a-judge and human evaluations, we demonstrate that\nSEFL-tuned models outperform their non-tuned counterparts in feedback quality,\nclarity, and timeliness. These findings reveal SEFL's potential to transform\nfeedback processes for higher education and beyond, offering an ethical and\nscalable alternative to conventional manual feedback cycles."
                },
                "authors": [
                    {
                        "name": "Mike Zhang"
                    },
                    {
                        "name": "Amalie Pernille Dilling"
                    },
                    {
                        "name": "Lon Gondelman"
                    },
                    {
                        "name": "Niels Erik Ruan Lyngdorf"
                    },
                    {
                        "name": "Euan D. Lindsay"
                    },
                    {
                        "name": "Johannes Bjerva"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Bjerva"
                },
                "author": "Johannes Bjerva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05759v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05759v2",
                "updated": "2025-02-18T15:07:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    7,
                    53,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-09T03:37:06Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    3,
                    37,
                    6,
                    6,
                    40,
                    0
                ],
                "title": "Reinforced Lifelong Editing for Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforced Lifelong Editing for Language Models"
                },
                "summary": "Large language models (LLMs) acquire information from pre-training corpora,\nbut their stored knowledge can become inaccurate or outdated over time. Model\nediting addresses this challenge by modifying model parameters without\nretraining, and prevalent approaches leverage hypernetworks to generate these\nparameter updates. However, they face significant challenges in lifelong\nediting due to their incompatibility with LLM parameters that dynamically\nchange during the editing process. To address this, we observed that\nhypernetwork-based lifelong editing aligns with reinforcement learning modeling\nand proposed RLEdit, an RL-based editing method. By treating editing losses as\nrewards and optimizing hypernetwork parameters at the full knowledge sequence\nlevel, we enable it to precisely capture LLM changes and generate appropriate\nparameter updates. Our extensive empirical evaluation across several LLMs\ndemonstrates that RLEdit outperforms existing methods in lifelong editing with\nsuperior effectiveness and efficiency, achieving a 59.24% improvement while\nrequiring only 2.11% of the time compared to most approaches. Our code is\navailable at: https://github.com/zhrli324/RLEdit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) acquire information from pre-training corpora,\nbut their stored knowledge can become inaccurate or outdated over time. Model\nediting addresses this challenge by modifying model parameters without\nretraining, and prevalent approaches leverage hypernetworks to generate these\nparameter updates. However, they face significant challenges in lifelong\nediting due to their incompatibility with LLM parameters that dynamically\nchange during the editing process. To address this, we observed that\nhypernetwork-based lifelong editing aligns with reinforcement learning modeling\nand proposed RLEdit, an RL-based editing method. By treating editing losses as\nrewards and optimizing hypernetwork parameters at the full knowledge sequence\nlevel, we enable it to precisely capture LLM changes and generate appropriate\nparameter updates. Our extensive empirical evaluation across several LLMs\ndemonstrates that RLEdit outperforms existing methods in lifelong editing with\nsuperior effectiveness and efficiency, achieving a 59.24% improvement while\nrequiring only 2.11% of the time compared to most approaches. Our code is\navailable at: https://github.com/zhrli324/RLEdit."
                },
                "authors": [
                    {
                        "name": "Zherui Li"
                    },
                    {
                        "name": "Houcheng Jiang"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Baolong Bi"
                    },
                    {
                        "name": "Zhenhong Zhou"
                    },
                    {
                        "name": "Fei Sun"
                    },
                    {
                        "name": "Junfeng Fang"
                    },
                    {
                        "name": "Xiang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Wang"
                },
                "author": "Xiang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05759v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05759v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12926v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12926v1",
                "updated": "2025-02-18T15:07:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    7,
                    6,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T15:07:06Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    7,
                    6,
                    1,
                    49,
                    0
                ],
                "title": "Towards more Contextual Agents: An extractor-Generator Optimization\n  Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards more Contextual Agents: An extractor-Generator Optimization\n  Framework"
                },
                "summary": "Large Language Model (LLM)-based agents have demonstrated remarkable success\nin solving complex tasks across a wide range of general-purpose applications.\nHowever, their performance often degrades in context-specific scenarios, such\nas specialized industries or research domains, where the absence of\ndomain-relevant knowledge leads to imprecise or suboptimal outcomes. To address\nthis challenge, our work introduces a systematic approach to enhance the\ncontextual adaptability of LLM-based agents by optimizing their underlying\nprompts-critical components that govern agent behavior, roles, and\ninteractions. Manually crafting optimized prompts for context-specific tasks is\nlabor-intensive, error-prone, and lacks scalability. In this work, we introduce\nan Extractor-Generator framework designed to automate the optimization of\ncontextual LLM-based agents. Our method operates through two key stages: (i)\nfeature extraction from a dataset of gold-standard input-output examples, and\n(ii) prompt generation via a high-level optimization strategy that iteratively\nidentifies underperforming cases and applies self-improvement techniques. This\nframework substantially improves prompt adaptability by enabling more precise\ngeneralization across diverse inputs, particularly in context-specific tasks\nwhere maintaining semantic consistency and minimizing error propagation are\ncritical for reliable performance. Although developed with single-stage\nworkflows in mind, the approach naturally extends to multi-stage workflows,\noffering broad applicability across various agent-based systems. Empirical\nevaluations demonstrate that our framework significantly enhances the\nperformance of prompt-optimized agents, providing a structured and efficient\napproach to contextual LLM-based agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based agents have demonstrated remarkable success\nin solving complex tasks across a wide range of general-purpose applications.\nHowever, their performance often degrades in context-specific scenarios, such\nas specialized industries or research domains, where the absence of\ndomain-relevant knowledge leads to imprecise or suboptimal outcomes. To address\nthis challenge, our work introduces a systematic approach to enhance the\ncontextual adaptability of LLM-based agents by optimizing their underlying\nprompts-critical components that govern agent behavior, roles, and\ninteractions. Manually crafting optimized prompts for context-specific tasks is\nlabor-intensive, error-prone, and lacks scalability. In this work, we introduce\nan Extractor-Generator framework designed to automate the optimization of\ncontextual LLM-based agents. Our method operates through two key stages: (i)\nfeature extraction from a dataset of gold-standard input-output examples, and\n(ii) prompt generation via a high-level optimization strategy that iteratively\nidentifies underperforming cases and applies self-improvement techniques. This\nframework substantially improves prompt adaptability by enabling more precise\ngeneralization across diverse inputs, particularly in context-specific tasks\nwhere maintaining semantic consistency and minimizing error propagation are\ncritical for reliable performance. Although developed with single-stage\nworkflows in mind, the approach naturally extends to multi-stage workflows,\noffering broad applicability across various agent-based systems. Empirical\nevaluations demonstrate that our framework significantly enhances the\nperformance of prompt-optimized agents, providing a structured and efficient\napproach to contextual LLM-based agents."
                },
                "authors": [
                    {
                        "name": "Mourad Aouini"
                    },
                    {
                        "name": "Jinan Loubani"
                    }
                ],
                "author_detail": {
                    "name": "Jinan Loubani"
                },
                "author": "Jinan Loubani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12926v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12926v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12925v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12925v1",
                "updated": "2025-02-18T15:04:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    4,
                    33,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T15:04:33Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    4,
                    33,
                    1,
                    49,
                    0
                ],
                "title": "Keep what you need : extracting efficient subnetworks from large audio\n  representation models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep what you need : extracting efficient subnetworks from large audio\n  representation models"
                },
                "summary": "Recently, research on audio foundation models has witnessed notable advances,\nas illustrated by the ever improving results on complex downstream tasks.\nSubsequently, those pretrained networks have quickly been used for various\naudio applications. These improvements have however resulted in a considerable\nincrease both in size and complexity of these models. Along the environmental\nconcerns this issue raises, this prevents the deployment of such networks on\nconsumer-level devices, and precludes their use for real-time applications.\nMoreover, this appears contradictory with the specificity of the tasks for\nwhich these models are used, which are often simpler compared to extracting a\nrich, multi-purpose representation from any type of audio data. In this paper,\nwe address this issue with a simple, yet effective method to extract\nlightweight specialist subnetworks from large foundation models. Specifically,\nwe introduce learnable binary masks in-between the layers of a pretrained\nrepresentation model. When training the end-to-end model on a downstream task,\nwe add a sparsity-inducing loss to the overall objective, hence learning a\ncompact subnetwork specialized on a single task. Importantly, the weights of\nthe foundation model are kept frozen, resulting into low additional training\ncosts. Once trained, the masked computational units can then be removed from\nthe network, implying significant performance gains. We assess our method on\nthree widespread audio foundation models, each based on a different backbone\narchitecture, and illustrate its effectiveness on common audio representation\nevaluation tasks, as well as its versatility on both speech, music, and general\naudio. Code for reproducing the results and supporting webpage are available at\nhttps://github.com/gnvIRCAM/Audio-representation-trimming",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, research on audio foundation models has witnessed notable advances,\nas illustrated by the ever improving results on complex downstream tasks.\nSubsequently, those pretrained networks have quickly been used for various\naudio applications. These improvements have however resulted in a considerable\nincrease both in size and complexity of these models. Along the environmental\nconcerns this issue raises, this prevents the deployment of such networks on\nconsumer-level devices, and precludes their use for real-time applications.\nMoreover, this appears contradictory with the specificity of the tasks for\nwhich these models are used, which are often simpler compared to extracting a\nrich, multi-purpose representation from any type of audio data. In this paper,\nwe address this issue with a simple, yet effective method to extract\nlightweight specialist subnetworks from large foundation models. Specifically,\nwe introduce learnable binary masks in-between the layers of a pretrained\nrepresentation model. When training the end-to-end model on a downstream task,\nwe add a sparsity-inducing loss to the overall objective, hence learning a\ncompact subnetwork specialized on a single task. Importantly, the weights of\nthe foundation model are kept frozen, resulting into low additional training\ncosts. Once trained, the masked computational units can then be removed from\nthe network, implying significant performance gains. We assess our method on\nthree widespread audio foundation models, each based on a different backbone\narchitecture, and illustrate its effectiveness on common audio representation\nevaluation tasks, as well as its versatility on both speech, music, and general\naudio. Code for reproducing the results and supporting webpage are available at\nhttps://github.com/gnvIRCAM/Audio-representation-trimming"
                },
                "authors": [
                    {
                        "name": "David Genova"
                    },
                    {
                        "name": "Philippe Esling"
                    },
                    {
                        "name": "Tom Hurlin"
                    }
                ],
                "author_detail": {
                    "name": "Tom Hurlin"
                },
                "author": "Tom Hurlin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12925v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12925v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12924v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12924v1",
                "updated": "2025-02-18T15:04:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    4,
                    13,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T15:04:13Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    4,
                    13,
                    1,
                    49,
                    0
                ],
                "title": "Conditioning LLMs to Generate Code-Switched Text: A Methodology Grounded\n  in Naturally Occurring Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conditioning LLMs to Generate Code-Switched Text: A Methodology Grounded\n  in Naturally Occurring Data"
                },
                "summary": "Code-switching (CS) is still a critical challenge in Natural Language\nProcessing (NLP). Current Large Language Models (LLMs) struggle to interpret\nand generate code-switched text, primarily due to the scarcity of large-scale\nCS datasets for training. This paper presents a novel methodology to generate\nCS data using LLMs, and test it on the English-Spanish language pair. We\npropose back-translating natural CS sentences into monolingual English, and\nusing the resulting parallel corpus to fine-tune LLMs to turn monolingual\nsentences into CS. Unlike previous approaches to CS generation, our methodology\nuses natural CS data as a starting point, allowing models to learn its natural\ndistribution beyond grammatical patterns. We thoroughly analyse the models'\nperformance through a study on human preferences, a qualitative error analysis\nand an evaluation with popular automatic metrics. Results show that our\nmethodology generates fluent code-switched text, expanding research\nopportunities in CS communication, and that traditional metrics do not\ncorrelate with human judgement when assessing the quality of the generated CS\ndata. We release our code and generated dataset under a CC-BY-NC-SA license.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code-switching (CS) is still a critical challenge in Natural Language\nProcessing (NLP). Current Large Language Models (LLMs) struggle to interpret\nand generate code-switched text, primarily due to the scarcity of large-scale\nCS datasets for training. This paper presents a novel methodology to generate\nCS data using LLMs, and test it on the English-Spanish language pair. We\npropose back-translating natural CS sentences into monolingual English, and\nusing the resulting parallel corpus to fine-tune LLMs to turn monolingual\nsentences into CS. Unlike previous approaches to CS generation, our methodology\nuses natural CS data as a starting point, allowing models to learn its natural\ndistribution beyond grammatical patterns. We thoroughly analyse the models'\nperformance through a study on human preferences, a qualitative error analysis\nand an evaluation with popular automatic metrics. Results show that our\nmethodology generates fluent code-switched text, expanding research\nopportunities in CS communication, and that traditional metrics do not\ncorrelate with human judgement when assessing the quality of the generated CS\ndata. We release our code and generated dataset under a CC-BY-NC-SA license."
                },
                "authors": [
                    {
                        "name": "Maite Heredia"
                    },
                    {
                        "name": "Gorka Labaka"
                    },
                    {
                        "name": "Jeremy Barnes"
                    },
                    {
                        "name": "Aitor Soroa"
                    }
                ],
                "author_detail": {
                    "name": "Aitor Soroa"
                },
                "author": "Aitor Soroa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12924v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12924v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12923v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12923v1",
                "updated": "2025-02-18T15:03:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    3,
                    17,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T15:03:17Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    3,
                    17,
                    1,
                    49,
                    0
                ],
                "title": "On-Device LLMs for Home Assistant: Dual Role in Intent Detection and\n  Response Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-Device LLMs for Home Assistant: Dual Role in Intent Detection and\n  Response Generation"
                },
                "summary": "This paper investigates whether Large Language Models (LLMs), fine-tuned on\nsynthetic but domain-representative data, can perform the twofold task of (i)\nslot and intent detection and (ii) natural language response generation for a\nsmart home assistant, while running solely on resource-limited, CPU-only edge\nhardware. We fine-tune LLMs to produce both JSON action calls and text\nresponses. Our experiments show that 16-bit and 8-bit quantized variants\npreserve high accuracy on slot and intent detection and maintain strong\nsemantic coherence in generated text, while the 4-bit model, while retaining\ngenerative fluency, suffers a noticeable drop in device-service classification\naccuracy. Further evaluations on noisy human (non-synthetic) prompts and\nout-of-domain intents confirm the models' generalization ability, obtaining\naround 80--86\\% accuracy. While the average inference time is 5--6 seconds per\nquery -- acceptable for one-shot commands but suboptimal for multi-turn\ndialogue -- our results affirm that an on-device LLM can effectively unify\ncommand interpretation and flexible response generation for home automation\nwithout relying on specialized hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates whether Large Language Models (LLMs), fine-tuned on\nsynthetic but domain-representative data, can perform the twofold task of (i)\nslot and intent detection and (ii) natural language response generation for a\nsmart home assistant, while running solely on resource-limited, CPU-only edge\nhardware. We fine-tune LLMs to produce both JSON action calls and text\nresponses. Our experiments show that 16-bit and 8-bit quantized variants\npreserve high accuracy on slot and intent detection and maintain strong\nsemantic coherence in generated text, while the 4-bit model, while retaining\ngenerative fluency, suffers a noticeable drop in device-service classification\naccuracy. Further evaluations on noisy human (non-synthetic) prompts and\nout-of-domain intents confirm the models' generalization ability, obtaining\naround 80--86\\% accuracy. While the average inference time is 5--6 seconds per\nquery -- acceptable for one-shot commands but suboptimal for multi-turn\ndialogue -- our results affirm that an on-device LLM can effectively unify\ncommand interpretation and flexible response generation for home automation\nwithout relying on specialized hardware."
                },
                "authors": [
                    {
                        "name": "Rune Birkmose"
                    },
                    {
                        "name": "Nathan Mrkeberg Reece"
                    },
                    {
                        "name": "Esben Hofstedt Norvin"
                    },
                    {
                        "name": "Johannes Bjerva"
                    },
                    {
                        "name": "Mike Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zhang"
                },
                "author": "Mike Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12923v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12921v1",
                "updated": "2025-02-18T15:01:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    1,
                    30,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T15:01:30Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    1,
                    30,
                    1,
                    49,
                    0
                ],
                "title": "Q-STRUM Debate: Query-Driven Contrastive Summarization for\n  Recommendation Comparison",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q-STRUM Debate: Query-Driven Contrastive Summarization for\n  Recommendation Comparison"
                },
                "summary": "Query-driven recommendation with unknown items poses a challenge for users to\nunderstand why certain items are appropriate for their needs. Query-driven\nContrastive Summarization (QCS) is a methodology designed to address this issue\nby leveraging language-based item descriptions to clarify contrasts between\nthem. However, existing state-of-the-art contrastive summarization methods such\nas STRUM-LLM fall short of this goal. To overcome these limitations, we\nintroduce Q-STRUM Debate, a novel extension of STRUM-LLM that employs\ndebate-style prompting to generate focused and contrastive summarizations of\nitem aspects relevant to a query. Leveraging modern large language models\n(LLMs) as powerful tools for generating debates, Q-STRUM Debate provides\nenhanced contrastive summaries. Experiments across three datasets demonstrate\nthat Q-STRUM Debate yields significant performance improvements over existing\nmethods on key contrastive summarization criteria, thus introducing a novel and\nperformant debate prompting methodology for QCS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query-driven recommendation with unknown items poses a challenge for users to\nunderstand why certain items are appropriate for their needs. Query-driven\nContrastive Summarization (QCS) is a methodology designed to address this issue\nby leveraging language-based item descriptions to clarify contrasts between\nthem. However, existing state-of-the-art contrastive summarization methods such\nas STRUM-LLM fall short of this goal. To overcome these limitations, we\nintroduce Q-STRUM Debate, a novel extension of STRUM-LLM that employs\ndebate-style prompting to generate focused and contrastive summarizations of\nitem aspects relevant to a query. Leveraging modern large language models\n(LLMs) as powerful tools for generating debates, Q-STRUM Debate provides\nenhanced contrastive summaries. Experiments across three datasets demonstrate\nthat Q-STRUM Debate yields significant performance improvements over existing\nmethods on key contrastive summarization criteria, thus introducing a novel and\nperformant debate prompting methodology for QCS."
                },
                "authors": [
                    {
                        "name": "George-Kirollos Saad"
                    },
                    {
                        "name": "Scott Sanner"
                    }
                ],
                "author_detail": {
                    "name": "Scott Sanner"
                },
                "author": "Scott Sanner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12920v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12920v1",
                "updated": "2025-02-18T15:01:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    1,
                    2,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T15:01:02Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    1,
                    2,
                    1,
                    49,
                    0
                ],
                "title": "Lightweight Online Adaption for Time Series Foundation Model Forecasts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight Online Adaption for Time Series Foundation Model Forecasts"
                },
                "summary": "Foundation models (FMs) have emerged as a promising approach for time series\nforecasting. While effective, FMs typically remain fixed during deployment due\nto the high computational costs of learning them online. Consequently, deployed\nFMs fail to adapt their forecasts to current data characteristics, despite the\navailability of online feedback from newly arriving data. This raises the\nquestion of whether FM performance can be enhanced by the efficient usage of\nthis feedback. We propose AdapTS to answer this question.\n  AdapTS is a lightweight mechanism for the online adaption of FM forecasts in\nresponse to online feedback. AdapTS consists of two parts: a) the\nAdapTS-Forecaster which is used to learn the current data distribution; and b)\nthe AdapTS-Weighter which is used to combine the forecasts of the FM and the\nAdapTS-Forecaster. We evaluate the performance of AdapTS in conjunction with\nseveral recent FMs across a suite of standard time series datasets. In all of\nour experiments we find that using AdapTS improves performance. This work\ndemonstrates how efficient usage of online feedback can be used to improve FM\nforecasts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models (FMs) have emerged as a promising approach for time series\nforecasting. While effective, FMs typically remain fixed during deployment due\nto the high computational costs of learning them online. Consequently, deployed\nFMs fail to adapt their forecasts to current data characteristics, despite the\navailability of online feedback from newly arriving data. This raises the\nquestion of whether FM performance can be enhanced by the efficient usage of\nthis feedback. We propose AdapTS to answer this question.\n  AdapTS is a lightweight mechanism for the online adaption of FM forecasts in\nresponse to online feedback. AdapTS consists of two parts: a) the\nAdapTS-Forecaster which is used to learn the current data distribution; and b)\nthe AdapTS-Weighter which is used to combine the forecasts of the FM and the\nAdapTS-Forecaster. We evaluate the performance of AdapTS in conjunction with\nseveral recent FMs across a suite of standard time series datasets. In all of\nour experiments we find that using AdapTS improves performance. This work\ndemonstrates how efficient usage of online feedback can be used to improve FM\nforecasts."
                },
                "authors": [
                    {
                        "name": "Thomas L. Lee"
                    },
                    {
                        "name": "William Toner"
                    },
                    {
                        "name": "Rajkarn Singh"
                    },
                    {
                        "name": "Artjom Joosem"
                    },
                    {
                        "name": "Martin Asenov"
                    }
                ],
                "author_detail": {
                    "name": "Martin Asenov"
                },
                "author": "Martin Asenov",
                "arxiv_comment": "8 pages, Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12920v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12920v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12918v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12918v1",
                "updated": "2025-02-18T14:59:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    59,
                    37,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T14:59:37Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    59,
                    37,
                    1,
                    49,
                    0
                ],
                "title": "Query Rewriting via LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query Rewriting via LLMs"
                },
                "summary": "Query rewriting is a classical technique for transforming complex declarative\nSQL queries into ``lean'' equivalents that are conducive to (a) faster\nexecution from a performance perspective, and (b) better understanding from a\ndeveloper perspective. The rewriting is typically achieved via transformation\nrules, but these rules are limited in scope and difficult to update in a\nproduction system. In recent times, LLM-based techniques have also been mooted,\nbut they are prone to both semantic and syntactic errors.\n  We investigate here, how the remarkable cognitive capabilities of LLMs can be\nleveraged for performant query rewriting while incorporating safeguards and\noptimizations to ensure correctness and efficiency. Our study shows that these\ngoals can be progressively achieved through incorporation of (a) an ensemble\nsuite of basic prompts, (b) database-sensitive prompts via redundancy removal\nand selectivity-based rewriting rules, and (c) LLM token probability-guided\nrewrite paths. Further, a suite of statistical and logic-based tools can be\nused to guard against errors produced by the model.\n  We have implemented the above LLM-infused techniques in the LITHE system, and\nevaluated complex analytic queries from multiple benchmarks on contemporary\ndatabase platforms. The results show significant improvements over SOTA\nrewriting techniques -- for instance, on TPC-DS, LITHE constructed productive\n(>1.5x speedup) rewrites for \\emph{two-thirds} of the query suite, delivering\nfour times more coverage than SOTA. Further, the geometric mean of its\nestimated execution speedups was an \\emph{order-of-magnitude} jump over SOTA\nperformance. In essence, LITHE offers a potent and robust LLM-based\nintermediary between enterprise applications and database engines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query rewriting is a classical technique for transforming complex declarative\nSQL queries into ``lean'' equivalents that are conducive to (a) faster\nexecution from a performance perspective, and (b) better understanding from a\ndeveloper perspective. The rewriting is typically achieved via transformation\nrules, but these rules are limited in scope and difficult to update in a\nproduction system. In recent times, LLM-based techniques have also been mooted,\nbut they are prone to both semantic and syntactic errors.\n  We investigate here, how the remarkable cognitive capabilities of LLMs can be\nleveraged for performant query rewriting while incorporating safeguards and\noptimizations to ensure correctness and efficiency. Our study shows that these\ngoals can be progressively achieved through incorporation of (a) an ensemble\nsuite of basic prompts, (b) database-sensitive prompts via redundancy removal\nand selectivity-based rewriting rules, and (c) LLM token probability-guided\nrewrite paths. Further, a suite of statistical and logic-based tools can be\nused to guard against errors produced by the model.\n  We have implemented the above LLM-infused techniques in the LITHE system, and\nevaluated complex analytic queries from multiple benchmarks on contemporary\ndatabase platforms. The results show significant improvements over SOTA\nrewriting techniques -- for instance, on TPC-DS, LITHE constructed productive\n(>1.5x speedup) rewrites for \\emph{two-thirds} of the query suite, delivering\nfour times more coverage than SOTA. Further, the geometric mean of its\nestimated execution speedups was an \\emph{order-of-magnitude} jump over SOTA\nperformance. In essence, LITHE offers a potent and robust LLM-based\nintermediary between enterprise applications and database engines."
                },
                "authors": [
                    {
                        "name": "Sriram Dharwada"
                    },
                    {
                        "name": "Himanshu Devrani"
                    },
                    {
                        "name": "Jayant Haritsa"
                    },
                    {
                        "name": "Harish Doraiswamy"
                    }
                ],
                "author_detail": {
                    "name": "Harish Doraiswamy"
                },
                "author": "Harish Doraiswamy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12918v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12918v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12913v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12913v1",
                "updated": "2025-02-18T14:54:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    54,
                    55,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T14:54:55Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    54,
                    55,
                    1,
                    49,
                    0
                ],
                "title": "GSQ-Tuning: Group-Shared Exponents Integer in Fully Quantized Training\n  for LLMs On-Device Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GSQ-Tuning: Group-Shared Exponents Integer in Fully Quantized Training\n  for LLMs On-Device Fine-tuning"
                },
                "summary": "Large Language Models (LLMs) fine-tuning technologies have achieved\nremarkable results. However, traditional LLM fine-tuning approaches face\nsignificant challenges: they require large Floating Point (FP) computation,\nraising privacy concerns when handling sensitive data, and are impractical for\nresource-constrained edge devices. While Parameter-Efficient Fine-Tuning (PEFT)\ntechniques reduce trainable parameters, their reliance on floating-point\narithmetic creates fundamental incompatibilities with edge hardware. In this\nwork, we introduce a novel framework for on-device LLM fine-tuning that\neliminates the need for floating-point operations in both inference and\ntraining, named GSQ-Tuning. At its core is the Group-Shared Exponents Integer\nformat, which efficiently represents model parameters in integer format using\nshared exponents among parameter groups. When combined with LoRA-like adapters,\nthis enables fully integer-based fine-tuning that is both memory and compute\nefficient. We demonstrate that our approach achieves accuracy comparable to\nFP16-based fine-tuning while significantly reducing memory usage (50%).\nMoreover, compared to FP8, our method can reduce 5x power consumption and 11x\nchip area with same performance, making large-scale model adaptation feasible\non edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) fine-tuning technologies have achieved\nremarkable results. However, traditional LLM fine-tuning approaches face\nsignificant challenges: they require large Floating Point (FP) computation,\nraising privacy concerns when handling sensitive data, and are impractical for\nresource-constrained edge devices. While Parameter-Efficient Fine-Tuning (PEFT)\ntechniques reduce trainable parameters, their reliance on floating-point\narithmetic creates fundamental incompatibilities with edge hardware. In this\nwork, we introduce a novel framework for on-device LLM fine-tuning that\neliminates the need for floating-point operations in both inference and\ntraining, named GSQ-Tuning. At its core is the Group-Shared Exponents Integer\nformat, which efficiently represents model parameters in integer format using\nshared exponents among parameter groups. When combined with LoRA-like adapters,\nthis enables fully integer-based fine-tuning that is both memory and compute\nefficient. We demonstrate that our approach achieves accuracy comparable to\nFP16-based fine-tuning while significantly reducing memory usage (50%).\nMoreover, compared to FP8, our method can reduce 5x power consumption and 11x\nchip area with same performance, making large-scale model adaptation feasible\non edge devices."
                },
                "authors": [
                    {
                        "name": "Sifan Zhou"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Mingjia Shi"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Dawei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Yang"
                },
                "author": "Dawei Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12913v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12913v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12911v1",
                "updated": "2025-02-18T14:53:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    53,
                    45,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T14:53:45Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    53,
                    45,
                    1,
                    49,
                    0
                ],
                "title": "Knapsack Optimization-based Schema Linking for LLM-based Text-to-SQL\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knapsack Optimization-based Schema Linking for LLM-based Text-to-SQL\n  Generation"
                },
                "summary": "Generating SQLs from user queries is a long-standing challenge, where the\naccuracy of initial schema linking significantly impacts subsequent SQL\ngeneration performance. However, current schema linking models still struggle\nwith missing relevant schema elements or an excess of redundant ones. A crucial\nreason for this is that commonly used metrics, recall and precision, fail to\ncapture relevant element missing and thus cannot reflect actual schema linking\nperformance. Motivated by this, we propose an enhanced schema linking metric by\nintroducing a restricted missing indicator. Accordingly, we introduce Knapsack\noptimization-based Schema Linking Agent (KaSLA), a plug-in schema linking agent\ndesigned to prevent the missing of relevant schema elements while minimizing\nthe inclusion of redundant ones. KaSLA employs a hierarchical linking strategy\nthat first identifies the optimal table linking and subsequently links columns\nwithin the selected table to reduce linking candidate space. In each linking\nprocess, it utilize a knapsack optimization approach to link potentially\nrelevant elements while accounting for a limited tolerance of potential\nredundant ones.With this optimization, KaSLA-1.6B achieves superior schema\nlinking results compared to large-scale LLMs, including deepseek-v3 with\nstate-of-the-art (SOTA) schema linking method. Extensive experiments on Spider\nand BIRD benchmarks verify that KaSLA can significantly improve the SQL\ngeneration performance of SOTA text-to-SQL models by substituting their schema\nlinking processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating SQLs from user queries is a long-standing challenge, where the\naccuracy of initial schema linking significantly impacts subsequent SQL\ngeneration performance. However, current schema linking models still struggle\nwith missing relevant schema elements or an excess of redundant ones. A crucial\nreason for this is that commonly used metrics, recall and precision, fail to\ncapture relevant element missing and thus cannot reflect actual schema linking\nperformance. Motivated by this, we propose an enhanced schema linking metric by\nintroducing a restricted missing indicator. Accordingly, we introduce Knapsack\noptimization-based Schema Linking Agent (KaSLA), a plug-in schema linking agent\ndesigned to prevent the missing of relevant schema elements while minimizing\nthe inclusion of redundant ones. KaSLA employs a hierarchical linking strategy\nthat first identifies the optimal table linking and subsequently links columns\nwithin the selected table to reduce linking candidate space. In each linking\nprocess, it utilize a knapsack optimization approach to link potentially\nrelevant elements while accounting for a limited tolerance of potential\nredundant ones.With this optimization, KaSLA-1.6B achieves superior schema\nlinking results compared to large-scale LLMs, including deepseek-v3 with\nstate-of-the-art (SOTA) schema linking method. Extensive experiments on Spider\nand BIRD benchmarks verify that KaSLA can significantly improve the SQL\ngeneration performance of SOTA text-to-SQL models by substituting their schema\nlinking processes."
                },
                "authors": [
                    {
                        "name": "Zheng Yuan"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Zijin Hong"
                    },
                    {
                        "name": "Qinggang Zhang"
                    },
                    {
                        "name": "Feiran Huang"
                    },
                    {
                        "name": "Xiao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Huang"
                },
                "author": "Xiao Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.15334v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.15334v3",
                "updated": "2025-02-18T14:49:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    49,
                    52,
                    1,
                    49,
                    0
                ],
                "published": "2023-08-29T14:29:57Z",
                "published_parsed": [
                    2023,
                    8,
                    29,
                    14,
                    29,
                    57,
                    1,
                    241,
                    0
                ],
                "title": "The Responsible Development of Automated Student Feedback with\n  Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Responsible Development of Automated Student Feedback with\n  Generative AI"
                },
                "summary": "Providing rich, constructive feedback to students is essential for supporting\nand enhancing their learning. Recent advancements in Generative Artificial\nIntelligence (AI), particularly with large language models (LLMs), present new\nopportunities to deliver scalable, repeatable, and instant feedback,\neffectively making abundant a resource that has historically been scarce and\ncostly. From a technical perspective, this approach is now feasible due to\nbreakthroughs in AI and Natural Language Processing (NLP). While the potential\neducational benefits are compelling, implementing these technologies also\nintroduces a host of ethical considerations that must be thoughtfully\naddressed. One of the core advantages of AI systems is their ability to\nautomate routine and mundane tasks, potentially freeing up human educators for\nmore nuanced work. However, the ease of automation risks a ``tyranny of the\nmajority'', where the diverse needs of minority or unique learners are\noverlooked, as they may be harder to systematize and less straightforward to\naccommodate. Ensuring inclusivity and equity in AI-generated feedback,\ntherefore, becomes a critical aspect of responsible AI implementation in\neducation. The process of developing machine learning models that produce\nvaluable, personalized, and authentic feedback also requires significant input\nfrom human domain experts. Decisions around whose expertise is incorporated,\nhow it is captured, and when it is applied have profound implications for the\nrelevance and quality of the resulting feedback. Additionally, the maintenance\nand continuous refinement of these models are necessary to adapt feedback to\nevolving contextual, theoretical, and student-related factors. Without ongoing\nadaptation, feedback risks becoming obsolete or mismatched with the current\nneeds of diverse student populations [...]",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Providing rich, constructive feedback to students is essential for supporting\nand enhancing their learning. Recent advancements in Generative Artificial\nIntelligence (AI), particularly with large language models (LLMs), present new\nopportunities to deliver scalable, repeatable, and instant feedback,\neffectively making abundant a resource that has historically been scarce and\ncostly. From a technical perspective, this approach is now feasible due to\nbreakthroughs in AI and Natural Language Processing (NLP). While the potential\neducational benefits are compelling, implementing these technologies also\nintroduces a host of ethical considerations that must be thoughtfully\naddressed. One of the core advantages of AI systems is their ability to\nautomate routine and mundane tasks, potentially freeing up human educators for\nmore nuanced work. However, the ease of automation risks a ``tyranny of the\nmajority'', where the diverse needs of minority or unique learners are\noverlooked, as they may be harder to systematize and less straightforward to\naccommodate. Ensuring inclusivity and equity in AI-generated feedback,\ntherefore, becomes a critical aspect of responsible AI implementation in\neducation. The process of developing machine learning models that produce\nvaluable, personalized, and authentic feedback also requires significant input\nfrom human domain experts. Decisions around whose expertise is incorporated,\nhow it is captured, and when it is applied have profound implications for the\nrelevance and quality of the resulting feedback. Additionally, the maintenance\nand continuous refinement of these models are necessary to adapt feedback to\nevolving contextual, theoretical, and student-related factors. Without ongoing\nadaptation, feedback risks becoming obsolete or mismatched with the current\nneeds of diverse student populations [...]"
                },
                "authors": [
                    {
                        "name": "Euan D Lindsay"
                    },
                    {
                        "name": "Mike Zhang"
                    },
                    {
                        "name": "Aditya Johri"
                    },
                    {
                        "name": "Johannes Bjerva"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Bjerva"
                },
                "author": "Johannes Bjerva",
                "arxiv_comment": "Pre-print of version accepted to EDUCON 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.15334v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.15334v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12904v1",
                "updated": "2025-02-18T14:47:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    47,
                    2,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T14:47:02Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    47,
                    2,
                    1,
                    49,
                    0
                ],
                "title": "Fraud-R1 : A Multi-Round Benchmark for Assessing the Robustness of LLM\n  Against Augmented Fraud and Phishing Inducements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fraud-R1 : A Multi-Round Benchmark for Assessing the Robustness of LLM\n  Against Augmented Fraud and Phishing Inducements"
                },
                "summary": "We introduce Fraud-R1, a benchmark designed to evaluate LLMs' ability to\ndefend against internet fraud and phishing in dynamic, real-world scenarios.\nFraud-R1 comprises 8,564 fraud cases sourced from phishing scams, fake job\npostings, social media, and news, categorized into 5 major fraud types. Unlike\nprevious benchmarks, Fraud-R1 introduces a multi-round evaluation pipeline to\nassess LLMs' resistance to fraud at different stages, including credibility\nbuilding, urgency creation, and emotional manipulation. Furthermore, we\nevaluate 15 LLMs under two settings: 1. Helpful-Assistant, where the LLM\nprovides general decision-making assistance, and 2. Role-play, where the model\nassumes a specific persona, widely used in real-world agent-based interactions.\nOur evaluation reveals the significant challenges in defending against fraud\nand phishing inducement, especially in role-play settings and fake job\npostings. Additionally, we observe a substantial performance gap between\nChinese and English, underscoring the need for improved multilingual fraud\ndetection capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Fraud-R1, a benchmark designed to evaluate LLMs' ability to\ndefend against internet fraud and phishing in dynamic, real-world scenarios.\nFraud-R1 comprises 8,564 fraud cases sourced from phishing scams, fake job\npostings, social media, and news, categorized into 5 major fraud types. Unlike\nprevious benchmarks, Fraud-R1 introduces a multi-round evaluation pipeline to\nassess LLMs' resistance to fraud at different stages, including credibility\nbuilding, urgency creation, and emotional manipulation. Furthermore, we\nevaluate 15 LLMs under two settings: 1. Helpful-Assistant, where the LLM\nprovides general decision-making assistance, and 2. Role-play, where the model\nassumes a specific persona, widely used in real-world agent-based interactions.\nOur evaluation reveals the significant challenges in defending against fraud\nand phishing inducement, especially in role-play settings and fake job\npostings. Additionally, we observe a substantial performance gap between\nChinese and English, underscoring the need for improved multilingual fraud\ndetection capabilities."
                },
                "authors": [
                    {
                        "name": "Shu Yang"
                    },
                    {
                        "name": "Shenzhe Zhu"
                    },
                    {
                        "name": "Zeyu Wu"
                    },
                    {
                        "name": "Keyu Wang"
                    },
                    {
                        "name": "Junchi Yao"
                    },
                    {
                        "name": "Junchao Wu"
                    },
                    {
                        "name": "Lijie Hu"
                    },
                    {
                        "name": "Mengdi Li"
                    },
                    {
                        "name": "Derek F. Wong"
                    },
                    {
                        "name": "Di Wang"
                    }
                ],
                "author_detail": {
                    "name": "Di Wang"
                },
                "author": "Di Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12900v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12900v1",
                "updated": "2025-02-18T14:36:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    36,
                    39,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T14:36:39Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    36,
                    39,
                    1,
                    49,
                    0
                ],
                "title": "Soundwave: Less is More for Speech-Text Alignment in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soundwave: Less is More for Speech-Text Alignment in LLMs"
                },
                "summary": "Existing end-to-end speech large language models (LLMs) usually rely on\nlarge-scale annotated data for training, while data-efficient training has not\nbeen discussed in depth. We focus on two fundamental problems between speech\nand text: the representation space gap and sequence length inconsistency. We\npropose Soundwave, which utilizes an efficient training strategy and a novel\narchitecture to address these issues. Results show that Soundwave outperforms\nthe advanced Qwen2-Audio in speech translation and AIR-Bench speech tasks,\nusing only one-fiftieth of the training data. Further analysis shows that\nSoundwave still retains its intelligence during conversation. The project is\navailable at https://github.com/FreedomIntelligence/Soundwave.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing end-to-end speech large language models (LLMs) usually rely on\nlarge-scale annotated data for training, while data-efficient training has not\nbeen discussed in depth. We focus on two fundamental problems between speech\nand text: the representation space gap and sequence length inconsistency. We\npropose Soundwave, which utilizes an efficient training strategy and a novel\narchitecture to address these issues. Results show that Soundwave outperforms\nthe advanced Qwen2-Audio in speech translation and AIR-Bench speech tasks,\nusing only one-fiftieth of the training data. Further analysis shows that\nSoundwave still retains its intelligence during conversation. The project is\navailable at https://github.com/FreedomIntelligence/Soundwave."
                },
                "authors": [
                    {
                        "name": "Yuhao Zhang"
                    },
                    {
                        "name": "Zhiheng Liu"
                    },
                    {
                        "name": "Fan Bu"
                    },
                    {
                        "name": "Ruiyu Zhang"
                    },
                    {
                        "name": "Benyou Wang"
                    },
                    {
                        "name": "Haizhou Li"
                    }
                ],
                "author_detail": {
                    "name": "Haizhou Li"
                },
                "author": "Haizhou Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12900v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12900v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12896v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12896v1",
                "updated": "2025-02-18T14:32:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    32,
                    44,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T14:32:44Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    32,
                    44,
                    1,
                    49,
                    0
                ],
                "title": "None of the Others: a General Technique to Distinguish Reasoning from\n  Memorization in Multiple-Choice LLM Evaluation Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "None of the Others: a General Technique to Distinguish Reasoning from\n  Memorization in Multiple-Choice LLM Evaluation Benchmarks"
                },
                "summary": "In LLM evaluations, reasoning is often distinguished from recall/memorization\nby performing numerical variations to math-oriented questions. Here we\nintroduce a general variation method for multiple-choice questions that\ncompletely dissociates the correct answer from previously seen tokens or\nconcepts, requiring LLMs to understand and reason (rather than memorizing) in\norder to answer correctly. Using this method, we evaluate state-of-the-art\nproprietary and open-source LLMs on two datasets available in English and\nSpanish: the public MMLU benchmark and the private UNED-Access 2024 dataset.\nResults show that all models experience remarkable accuracy drops under our\nproposed variation, with an average loss of 57% on MMLU and 50% on UNED-Access\n2024, ranging from 10% to 93% across models. Notably, the most accurate model\nin our experimentation (OpenAI-o3-mini) is not the most robust\n(DeepSeek-R1-70B), suggesting that the best models in standard evaluations may\nnot be the ones with better reasoning capabilities. Also, we see larger\naccuracy drops in public (vs private) datasets and questions posed in their\noriginal language (vs a manual translation), which are signs of contamination\nand also point to a relevant role of recall/memorization in current LLMs'\nanswers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In LLM evaluations, reasoning is often distinguished from recall/memorization\nby performing numerical variations to math-oriented questions. Here we\nintroduce a general variation method for multiple-choice questions that\ncompletely dissociates the correct answer from previously seen tokens or\nconcepts, requiring LLMs to understand and reason (rather than memorizing) in\norder to answer correctly. Using this method, we evaluate state-of-the-art\nproprietary and open-source LLMs on two datasets available in English and\nSpanish: the public MMLU benchmark and the private UNED-Access 2024 dataset.\nResults show that all models experience remarkable accuracy drops under our\nproposed variation, with an average loss of 57% on MMLU and 50% on UNED-Access\n2024, ranging from 10% to 93% across models. Notably, the most accurate model\nin our experimentation (OpenAI-o3-mini) is not the most robust\n(DeepSeek-R1-70B), suggesting that the best models in standard evaluations may\nnot be the ones with better reasoning capabilities. Also, we see larger\naccuracy drops in public (vs private) datasets and questions posed in their\noriginal language (vs a manual translation), which are signs of contamination\nand also point to a relevant role of recall/memorization in current LLMs'\nanswers."
                },
                "authors": [
                    {
                        "name": "Eva Snchez Salido"
                    },
                    {
                        "name": "Julio Gonzalo"
                    },
                    {
                        "name": "Guillermo Marco"
                    }
                ],
                "author_detail": {
                    "name": "Guillermo Marco"
                },
                "author": "Guillermo Marco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12896v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12896v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12895v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12895v1",
                "updated": "2025-02-18T14:32:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    32,
                    17,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T14:32:17Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    32,
                    17,
                    1,
                    49,
                    0
                ],
                "title": "Multilingual European Language Models: Benchmarking Approaches and\n  Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual European Language Models: Benchmarking Approaches and\n  Challenges"
                },
                "summary": "The breakthrough of generative large language models (LLMs) that can solve\ndifferent tasks through chat interaction has led to a significant increase in\nthe use of general benchmarks to assess the quality or performance of these\nmodels beyond individual applications. There is also a need for better methods\nto evaluate and also to compare models due to the ever increasing number of new\nmodels published. However, most of the established benchmarks revolve around\nthe English language. This paper analyses the benefits and limitations of\ncurrent evaluation datasets, focusing on multilingual European benchmarks. We\nanalyse seven multilingual benchmarks and identify four major challenges.\nFurthermore, we discuss potential solutions to enhance translation quality and\nmitigate cultural biases, including human-in-the-loop verification and\niterative translation ranking. Our analysis highlights the need for culturally\naware and rigorously validated benchmarks to assess the reasoning and\nquestion-answering capabilities of multilingual LLMs accurately.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The breakthrough of generative large language models (LLMs) that can solve\ndifferent tasks through chat interaction has led to a significant increase in\nthe use of general benchmarks to assess the quality or performance of these\nmodels beyond individual applications. There is also a need for better methods\nto evaluate and also to compare models due to the ever increasing number of new\nmodels published. However, most of the established benchmarks revolve around\nthe English language. This paper analyses the benefits and limitations of\ncurrent evaluation datasets, focusing on multilingual European benchmarks. We\nanalyse seven multilingual benchmarks and identify four major challenges.\nFurthermore, we discuss potential solutions to enhance translation quality and\nmitigate cultural biases, including human-in-the-loop verification and\niterative translation ranking. Our analysis highlights the need for culturally\naware and rigorously validated benchmarks to assess the reasoning and\nquestion-answering capabilities of multilingual LLMs accurately."
                },
                "authors": [
                    {
                        "name": "Fabio Barth"
                    },
                    {
                        "name": "Georg Rehm"
                    }
                ],
                "author_detail": {
                    "name": "Georg Rehm"
                },
                "author": "Georg Rehm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12895v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12895v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12886v1",
                "updated": "2025-02-18T14:20:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    20,
                    27,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T14:20:27Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    20,
                    27,
                    1,
                    49,
                    0
                ],
                "title": "Are Multilingual Language Models an Off-ramp for Under-resourced\n  Languages? Will we arrive at Digital Language Equality in Europe in 2030?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Multilingual Language Models an Off-ramp for Under-resourced\n  Languages? Will we arrive at Digital Language Equality in Europe in 2030?"
                },
                "summary": "Large language models (LLMs) demonstrate unprecedented capabilities and\ndefine the state of the art for almost all natural language processing (NLP)\ntasks and also for essentially all Language Technology (LT) applications. LLMs\ncan only be trained for languages for which a sufficient amount of pre-training\ndata is available, effectively excluding many languages that are typically\ncharacterised as under-resourced. However, there is both circumstantial and\nempirical evidence that multilingual LLMs, which have been trained using data\nsets that cover multiple languages (including under-resourced ones), do exhibit\nstrong capabilities for some of these under-resourced languages. Eventually,\nthis approach may have the potential to be a technological off-ramp for those\nunder-resourced languages for which \"native\" LLMs, and LLM-based technologies,\ncannot be developed due to a lack of training data. This paper, which\nconcentrates on European languages, examines this idea, analyses the current\nsituation in terms of technology support and summarises related work. The\narticle concludes by focusing on the key open questions that need to be\nanswered for the approach to be put into practice in a systematic way.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate unprecedented capabilities and\ndefine the state of the art for almost all natural language processing (NLP)\ntasks and also for essentially all Language Technology (LT) applications. LLMs\ncan only be trained for languages for which a sufficient amount of pre-training\ndata is available, effectively excluding many languages that are typically\ncharacterised as under-resourced. However, there is both circumstantial and\nempirical evidence that multilingual LLMs, which have been trained using data\nsets that cover multiple languages (including under-resourced ones), do exhibit\nstrong capabilities for some of these under-resourced languages. Eventually,\nthis approach may have the potential to be a technological off-ramp for those\nunder-resourced languages for which \"native\" LLMs, and LLM-based technologies,\ncannot be developed due to a lack of training data. This paper, which\nconcentrates on European languages, examines this idea, analyses the current\nsituation in terms of technology support and summarises related work. The\narticle concludes by focusing on the key open questions that need to be\nanswered for the approach to be put into practice in a systematic way."
                },
                "authors": [
                    {
                        "name": "Georg Rehm"
                    },
                    {
                        "name": "Annika Grtzner-Zahn"
                    },
                    {
                        "name": "Fabio Barth"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Barth"
                },
                "author": "Fabio Barth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12884v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12884v1",
                "updated": "2025-02-18T14:16:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    16,
                    3,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T14:16:03Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    16,
                    3,
                    1,
                    49,
                    0
                ],
                "title": "How desirable is alignment between LLMs and linguistically diverse human\n  users?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How desirable is alignment between LLMs and linguistically diverse human\n  users?"
                },
                "summary": "We discuss how desirable it is that Large Language Models (LLMs) be able to\nadapt or align their language behavior with users who may be diverse in their\nlanguage use. User diversity may come about among others due to i) age\ndifferences; ii) gender characteristics, and/or iii) multilingual experience,\nand associated differences in language processing and use. We consider\npotential consequences for usability, communication, and LLM development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We discuss how desirable it is that Large Language Models (LLMs) be able to\nadapt or align their language behavior with users who may be diverse in their\nlanguage use. User diversity may come about among others due to i) age\ndifferences; ii) gender characteristics, and/or iii) multilingual experience,\nand associated differences in language processing and use. We consider\npotential consequences for usability, communication, and LLM development."
                },
                "authors": [
                    {
                        "name": "Pia Knoeferle"
                    },
                    {
                        "name": "Sebastian Mller"
                    },
                    {
                        "name": "Dorothea Kolossa"
                    },
                    {
                        "name": "Veronika Solopova"
                    },
                    {
                        "name": "Georg Rehm"
                    }
                ],
                "author_detail": {
                    "name": "Georg Rehm"
                },
                "author": "Georg Rehm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12884v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12884v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12876v1",
                "updated": "2025-02-18T14:05:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    5,
                    59,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T14:05:59Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    5,
                    59,
                    1,
                    49,
                    0
                ],
                "title": "Continuous Learning Conversational AI: A Personalized Agent Framework\n  via A2C Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous Learning Conversational AI: A Personalized Agent Framework\n  via A2C Reinforcement Learning"
                },
                "summary": "Creating personalized and adaptable conversational AI remains a key\nchallenge. This paper introduces a Continuous Learning Conversational AI (CLCA)\napproach, implemented using A2C reinforcement learning, to move beyond static\nLarge Language Models (LLMs). We use simulated sales dialogues, generated by\nLLMs, to train an A2C agent. This agent learns to optimize conversation\nstrategies for personalization, focusing on engagement and delivering value.\nOur system architecture integrates reinforcement learning with LLMs for both\ndata creation and response selection. This method offers a practical way to\nbuild personalized AI companions that evolve through continuous learning,\nadvancing beyond traditional static LLM techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creating personalized and adaptable conversational AI remains a key\nchallenge. This paper introduces a Continuous Learning Conversational AI (CLCA)\napproach, implemented using A2C reinforcement learning, to move beyond static\nLarge Language Models (LLMs). We use simulated sales dialogues, generated by\nLLMs, to train an A2C agent. This agent learns to optimize conversation\nstrategies for personalization, focusing on engagement and delivering value.\nOur system architecture integrates reinforcement learning with LLMs for both\ndata creation and response selection. This method offers a practical way to\nbuild personalized AI companions that evolve through continuous learning,\nadvancing beyond traditional static LLM techniques."
                },
                "authors": [
                    {
                        "name": "Nandakishor M"
                    },
                    {
                        "name": "Anjali M"
                    }
                ],
                "author_detail": {
                    "name": "Anjali M"
                },
                "author": "Anjali M",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00152v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00152v2",
                "updated": "2025-02-18T14:02:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    2,
                    12,
                    1,
                    49,
                    0
                ],
                "published": "2024-12-30T21:54:33Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    21,
                    54,
                    33,
                    0,
                    365,
                    0
                ],
                "title": "Temporal reasoning for timeline summarisation in social media",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal reasoning for timeline summarisation in social media"
                },
                "summary": "This paper explores whether enhancing temporal reasoning capabilities in\nLarge Language Models (LLMs) can improve the quality of timeline summarisation,\nthe task of summarising long texts containing sequences of events, such as\nsocial media threads. We first introduce NarrativeReason, a novel dataset\nfocused on temporal relationships among sequential events within narratives,\ndistinguishing it from existing temporal reasoning datasets that primarily\naddress pair-wise event relationships. Our approach then combines temporal\nreasoning with timeline summarisation through a knowledge distillation\nframework, where we first fine-tune a teacher model on temporal reasoning tasks\nand then distill this knowledge into a student model while simultaneously\ntraining it for the task of timeline summarisation. Experimental results\ndemonstrate that our model achieves superior performance on out-of-domain\nmental health-related timeline summarisation tasks, which involve long social\nmedia threads with repetitions of events and a mix of emotions, highlighting\nthe importance and generalisability of leveraging temporal reasoning to improve\ntimeline summarisation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores whether enhancing temporal reasoning capabilities in\nLarge Language Models (LLMs) can improve the quality of timeline summarisation,\nthe task of summarising long texts containing sequences of events, such as\nsocial media threads. We first introduce NarrativeReason, a novel dataset\nfocused on temporal relationships among sequential events within narratives,\ndistinguishing it from existing temporal reasoning datasets that primarily\naddress pair-wise event relationships. Our approach then combines temporal\nreasoning with timeline summarisation through a knowledge distillation\nframework, where we first fine-tune a teacher model on temporal reasoning tasks\nand then distill this knowledge into a student model while simultaneously\ntraining it for the task of timeline summarisation. Experimental results\ndemonstrate that our model achieves superior performance on out-of-domain\nmental health-related timeline summarisation tasks, which involve long social\nmedia threads with repetitions of events and a mix of emotions, highlighting\nthe importance and generalisability of leveraging temporal reasoning to improve\ntimeline summarisation."
                },
                "authors": [
                    {
                        "name": "Jiayu Song"
                    },
                    {
                        "name": "Mahmud Akhter"
                    },
                    {
                        "name": "Dana Atzil Slonim"
                    },
                    {
                        "name": "Maria Liakata"
                    }
                ],
                "author_detail": {
                    "name": "Maria Liakata"
                },
                "author": "Maria Liakata",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00152v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00152v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07460v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07460v2",
                "updated": "2025-02-18T13:55:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    13,
                    55,
                    4,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-11T11:11:05Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    11,
                    11,
                    5,
                    1,
                    42,
                    0
                ],
                "title": "Logarithmic Regret for Online KL-Regularized Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logarithmic Regret for Online KL-Regularized Reinforcement Learning"
                },
                "summary": "Recent advances in Reinforcement Learning from Human Feedback (RLHF) have\nshown that KL-regularization plays a pivotal role in improving the efficiency\nof RL fine-tuning for large language models (LLMs). Despite its empirical\nadvantage, the theoretical difference between KL-regularized RL and standard RL\nremains largely under-explored. While there is a recent line of work on the\ntheoretical analysis of KL-regularized objective in decision making\n\\citep{xiong2024iterative, xie2024exploratory,zhao2024sharp}, these analyses\neither reduce to the traditional RL setting or rely on strong coverage\nassumptions. In this paper, we propose an optimism-based KL-regularized online\ncontextual bandit algorithm, and provide a novel analysis of its regret. By\ncarefully leveraging the benign optimization landscape induced by the\nKL-regularization and the optimistic reward estimation, our algorithm achieves\nan $\\mathcal{O}\\big(\\eta\\log (N_{\\mathcal R} T)\\cdot d_{\\mathcal R}\\big)$\nlogarithmic regret bound, where $\\eta, N_{\\mathcal R},T,d_{\\mathcal R}$ denote\nthe KL-regularization parameter, the cardinality of the reward function class,\nnumber of rounds, and the complexity of the reward function class. Furthermore,\nwe extend our algorithm and analysis to reinforcement learning by developing a\nnovel decomposition over transition steps and also obtain a similar logarithmic\nregret bound.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Reinforcement Learning from Human Feedback (RLHF) have\nshown that KL-regularization plays a pivotal role in improving the efficiency\nof RL fine-tuning for large language models (LLMs). Despite its empirical\nadvantage, the theoretical difference between KL-regularized RL and standard RL\nremains largely under-explored. While there is a recent line of work on the\ntheoretical analysis of KL-regularized objective in decision making\n\\citep{xiong2024iterative, xie2024exploratory,zhao2024sharp}, these analyses\neither reduce to the traditional RL setting or rely on strong coverage\nassumptions. In this paper, we propose an optimism-based KL-regularized online\ncontextual bandit algorithm, and provide a novel analysis of its regret. By\ncarefully leveraging the benign optimization landscape induced by the\nKL-regularization and the optimistic reward estimation, our algorithm achieves\nan $\\mathcal{O}\\big(\\eta\\log (N_{\\mathcal R} T)\\cdot d_{\\mathcal R}\\big)$\nlogarithmic regret bound, where $\\eta, N_{\\mathcal R},T,d_{\\mathcal R}$ denote\nthe KL-regularization parameter, the cardinality of the reward function class,\nnumber of rounds, and the complexity of the reward function class. Furthermore,\nwe extend our algorithm and analysis to reinforcement learning by developing a\nnovel decomposition over transition steps and also obtain a similar logarithmic\nregret bound."
                },
                "authors": [
                    {
                        "name": "Heyang Zhao"
                    },
                    {
                        "name": "Chenlu Ye"
                    },
                    {
                        "name": "Wei Xiong"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07460v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07460v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10774v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10774v3",
                "updated": "2025-02-18T13:53:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    13,
                    53,
                    51,
                    1,
                    49,
                    0
                ],
                "published": "2024-08-20T12:13:04Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    12,
                    13,
                    4,
                    1,
                    233,
                    0
                ],
                "title": "Flexora: Flexible Low Rank Adaptation for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flexora: Flexible Low Rank Adaptation for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are driving advancements in artificial\nintelligence by increasing the scale of model parameters, which has\nsignificantly enhanced generalization ability and unlocked new capabilities in\npractice. However, their performance in specific downstream tasks is usually\nhindered by their knowledge boundaries on these tasks. Thus, fine-tuning\ntechniques, especially the widely used Low-Rank Adaptation (LoRA) method, have\nbeen introduced to expand the boundaries on these tasks, whereas LoRA would\nunderperform on certain tasks owing to its potential overfitting on these\ntasks. To overcome this overfitting and improve the performance of LoRA, we\npropose the flexible low rank adaptation (Flexora) method to automatically and\nflexibly select the most important layers needing to be fine-tuned to achieve\nthe best performance on different downstream tasks. Specifically, Flexora\nfirstly frames this layer selection problem as a well-defined hyperparameter\noptimization (HPO) problem, then addresses it using the unrolled\ndifferentiation (UD) method, and finally selects the most useful layers based\non the optimized hyperparameters. Our extensive experiments on many pretrained\nmodels and natural language tasks show that Flexora is able to consistently\nimprove over the existing baselines, indicating the effectiveness of our\nFlexora in practice. We additionally provide insightful theoretical results and\nmany ablation studies to deliver a comprehensive understanding of our Flexora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are driving advancements in artificial\nintelligence by increasing the scale of model parameters, which has\nsignificantly enhanced generalization ability and unlocked new capabilities in\npractice. However, their performance in specific downstream tasks is usually\nhindered by their knowledge boundaries on these tasks. Thus, fine-tuning\ntechniques, especially the widely used Low-Rank Adaptation (LoRA) method, have\nbeen introduced to expand the boundaries on these tasks, whereas LoRA would\nunderperform on certain tasks owing to its potential overfitting on these\ntasks. To overcome this overfitting and improve the performance of LoRA, we\npropose the flexible low rank adaptation (Flexora) method to automatically and\nflexibly select the most important layers needing to be fine-tuned to achieve\nthe best performance on different downstream tasks. Specifically, Flexora\nfirstly frames this layer selection problem as a well-defined hyperparameter\noptimization (HPO) problem, then addresses it using the unrolled\ndifferentiation (UD) method, and finally selects the most useful layers based\non the optimized hyperparameters. Our extensive experiments on many pretrained\nmodels and natural language tasks show that Flexora is able to consistently\nimprove over the existing baselines, indicating the effectiveness of our\nFlexora in practice. We additionally provide insightful theoretical results and\nmany ablation studies to deliver a comprehensive understanding of our Flexora."
                },
                "authors": [
                    {
                        "name": "Chenxing Wei"
                    },
                    {
                        "name": "Yao Shu"
                    },
                    {
                        "name": "Ying Tiffany He"
                    },
                    {
                        "name": "Fei Richard Yu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Richard Yu"
                },
                "author": "Fei Richard Yu",
                "arxiv_comment": "39 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10774v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10774v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12859v1",
                "updated": "2025-02-18T13:46:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    13,
                    46,
                    47,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T13:46:47Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    13,
                    46,
                    47,
                    1,
                    49,
                    0
                ],
                "title": "PAFT: Prompt-Agnostic Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAFT: Prompt-Agnostic Fine-Tuning"
                },
                "summary": "While Large Language Models (LLMs) adapt well to downstream tasks after\nfine-tuning, this adaptability often compromises prompt robustness, as even\nminor prompt variations can significantly degrade performance. To address this,\nwe propose Prompt-Agnostic Fine-Tuning(PAFT), a simple yet effective approach\nthat dynamically adjusts prompts during fine-tuning. This encourages the model\nto learn underlying task principles rather than overfitting to specific prompt\nformulations. PAFT operates in two stages: First, a diverse set of meaningful,\nsynthetic candidate prompts is constructed. Second, during fine-tuning, prompts\nare randomly sampled from this set to create dynamic training inputs. Extensive\nexperiments across diverse datasets and LLMs demonstrate that models trained\nwith PAFT exhibit strong robustness and generalization across a wide range of\nprompts, including unseen ones. This enhanced robustness improves both model\nperformance and inference speed while maintaining training efficiency. Ablation\nstudies further confirm the effectiveness of PAFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) adapt well to downstream tasks after\nfine-tuning, this adaptability often compromises prompt robustness, as even\nminor prompt variations can significantly degrade performance. To address this,\nwe propose Prompt-Agnostic Fine-Tuning(PAFT), a simple yet effective approach\nthat dynamically adjusts prompts during fine-tuning. This encourages the model\nto learn underlying task principles rather than overfitting to specific prompt\nformulations. PAFT operates in two stages: First, a diverse set of meaningful,\nsynthetic candidate prompts is constructed. Second, during fine-tuning, prompts\nare randomly sampled from this set to create dynamic training inputs. Extensive\nexperiments across diverse datasets and LLMs demonstrate that models trained\nwith PAFT exhibit strong robustness and generalization across a wide range of\nprompts, including unseen ones. This enhanced robustness improves both model\nperformance and inference speed while maintaining training efficiency. Ablation\nstudies further confirm the effectiveness of PAFT."
                },
                "authors": [
                    {
                        "name": "Chenxing Wei"
                    },
                    {
                        "name": "Yao Shu"
                    },
                    {
                        "name": "Mingwen Ou"
                    },
                    {
                        "name": "Ying Tiffany He"
                    },
                    {
                        "name": "Fei Richard Yu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Richard Yu"
                },
                "author": "Fei Richard Yu",
                "arxiv_comment": "20 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15173v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15173v4",
                "updated": "2025-02-18T13:45:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    13,
                    45,
                    50,
                    1,
                    49,
                    0
                ],
                "published": "2024-02-23T08:11:55Z",
                "published_parsed": [
                    2024,
                    2,
                    23,
                    8,
                    11,
                    55,
                    4,
                    54,
                    0
                ],
                "title": "Second-Order Fine-Tuning without Pain for LLMs:A Hessian Informed\n  Zeroth-Order Optimizer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Second-Order Fine-Tuning without Pain for LLMs:A Hessian Informed\n  Zeroth-Order Optimizer"
                },
                "summary": "Fine-tuning large language models (LLMs) with classic first-order optimizers\nentails prohibitive GPU memory due to the backpropagation process. Recent works\nhave turned to zeroth-order optimizers for fine-tuning, which save substantial\nmemory by using two forward passes. However, these optimizers are plagued by\nthe heterogeneity of parameter curvatures across different dimensions. In this\nwork, we propose HiZOO, a diagonal Hessian informed zeroth-order optimizer\nwhich is the first work to leverage the diagonal Hessian to enhance\nzeroth-order optimizer for fine-tuning LLMs. What's more, HiZOO avoids the\nexpensive memory cost and only increases one forward pass per step. Extensive\nexperiments on various models (350M~66B parameters) indicate that HiZOO\nimproves model convergence, significantly reducing training steps and\neffectively enhancing model accuracy. Moreover, we visualize the optimization\ntrajectories of HiZOO on test functions, illustrating its effectiveness in\nhandling heterogeneous curvatures. Lastly, we provide theoretical proofs of\nconvergence for HiZOO. Code is publicly available at\nhttps://anonymous.4open.science/r/HiZOO27F8.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) with classic first-order optimizers\nentails prohibitive GPU memory due to the backpropagation process. Recent works\nhave turned to zeroth-order optimizers for fine-tuning, which save substantial\nmemory by using two forward passes. However, these optimizers are plagued by\nthe heterogeneity of parameter curvatures across different dimensions. In this\nwork, we propose HiZOO, a diagonal Hessian informed zeroth-order optimizer\nwhich is the first work to leverage the diagonal Hessian to enhance\nzeroth-order optimizer for fine-tuning LLMs. What's more, HiZOO avoids the\nexpensive memory cost and only increases one forward pass per step. Extensive\nexperiments on various models (350M~66B parameters) indicate that HiZOO\nimproves model convergence, significantly reducing training steps and\neffectively enhancing model accuracy. Moreover, we visualize the optimization\ntrajectories of HiZOO on test functions, illustrating its effectiveness in\nhandling heterogeneous curvatures. Lastly, we provide theoretical proofs of\nconvergence for HiZOO. Code is publicly available at\nhttps://anonymous.4open.science/r/HiZOO27F8."
                },
                "authors": [
                    {
                        "name": "Yanjun Zhao"
                    },
                    {
                        "name": "Sizhe Dang"
                    },
                    {
                        "name": "Haishan Ye"
                    },
                    {
                        "name": "Guang Dai"
                    },
                    {
                        "name": "Yi Qian"
                    },
                    {
                        "name": "Ivor W. Tsang"
                    }
                ],
                "author_detail": {
                    "name": "Ivor W. Tsang"
                },
                "author": "Ivor W. Tsang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.15173v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15173v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12858v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12858v1",
                "updated": "2025-02-18T13:45:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    13,
                    45,
                    42,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T13:45:42Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    13,
                    45,
                    42,
                    1,
                    49,
                    0
                ],
                "title": "Rejected Dialects: Biases Against African American Language in Reward\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rejected Dialects: Biases Against African American Language in Reward\n  Models"
                },
                "summary": "Preference alignment via reward models helps build safe, helpful, and\nreliable large language models (LLMs). However, subjectivity in preference\njudgments and the lack of representative sampling in preference data collection\ncan introduce new biases, hindering reward models' fairness and equity. In this\nwork, we introduce a framework for evaluating dialect biases in reward models\nand conduct a case study on biases against African American Language (AAL)\nthrough several experiments comparing reward model preferences and behavior on\npaired White Mainstream English (WME) and both machine-translated and\nhuman-written AAL corpora. We show that reward models are less aligned with\nhuman preferences when processing AAL texts vs. WME ones (-4\\% accuracy on\naverage), frequently disprefer AAL-aligned texts vs. WME-aligned ones, and\nsteer conversations toward WME, even when prompted with AAL texts. Our findings\nprovide a targeted analysis of anti-AAL biases at a relatively understudied\nstage in LLM development, highlighting representational harms and ethical\nquestions about the desired behavior of LLMs concerning AAL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference alignment via reward models helps build safe, helpful, and\nreliable large language models (LLMs). However, subjectivity in preference\njudgments and the lack of representative sampling in preference data collection\ncan introduce new biases, hindering reward models' fairness and equity. In this\nwork, we introduce a framework for evaluating dialect biases in reward models\nand conduct a case study on biases against African American Language (AAL)\nthrough several experiments comparing reward model preferences and behavior on\npaired White Mainstream English (WME) and both machine-translated and\nhuman-written AAL corpora. We show that reward models are less aligned with\nhuman preferences when processing AAL texts vs. WME ones (-4\\% accuracy on\naverage), frequently disprefer AAL-aligned texts vs. WME-aligned ones, and\nsteer conversations toward WME, even when prompted with AAL texts. Our findings\nprovide a targeted analysis of anti-AAL biases at a relatively understudied\nstage in LLM development, highlighting representational harms and ethical\nquestions about the desired behavior of LLMs concerning AAL."
                },
                "authors": [
                    {
                        "name": "Joel Mire"
                    },
                    {
                        "name": "Zubin Trivadi Aysola"
                    },
                    {
                        "name": "Daniel Chechelnitsky"
                    },
                    {
                        "name": "Nicholas Deas"
                    },
                    {
                        "name": "Chrysoula Zerva"
                    },
                    {
                        "name": "Maarten Sap"
                    }
                ],
                "author_detail": {
                    "name": "Maarten Sap"
                },
                "author": "Maarten Sap",
                "arxiv_comment": "Accepted to NAACL Findings 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12858v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12858v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; K.4.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12853v1",
                "updated": "2025-02-18T13:40:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    13,
                    40,
                    22,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T13:40:22Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    13,
                    40,
                    22,
                    1,
                    49,
                    0
                ],
                "title": "S$^2$R: Teaching LLMs to Self-verify and Self-correct via Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "S$^2$R: Teaching LLMs to Self-verify and Self-correct via Reinforcement\n  Learning"
                },
                "summary": "Recent studies have demonstrated the effectiveness of LLM test-time scaling.\nHowever, existing approaches to incentivize LLMs' deep thinking abilities\ngenerally require large-scale data or significant training efforts. Meanwhile,\nit remains unclear how to improve the thinking abilities of less powerful base\nmodels. In this work, we introduce S$^2$R, an efficient framework that enhances\nLLM reasoning by teaching models to self-verify and self-correct during\ninference. Specifically, we first initialize LLMs with iterative\nself-verification and self-correction behaviors through supervised fine-tuning\non carefully curated data. The self-verification and self-correction skills are\nthen further strengthened by both outcome-level and process-level reinforcement\nlearning, with minimized resource requirements, enabling the model to\nadaptively refine its reasoning process during inference. Our results\ndemonstrate that, with only 3.1k self-verifying and self-correcting behavior\ninitialization samples, Qwen2.5-math-7B achieves an accuracy improvement from\n51.0\\% to 81.6\\%, outperforming models trained on an equivalent amount of\nlong-CoT distilled data. Extensive experiments and analysis based on three base\nmodels across both in-domain and out-of-domain benchmarks validate the\neffectiveness of S$^2$R. Our code and data are available at\nhttps://github.com/NineAbyss/S2R.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have demonstrated the effectiveness of LLM test-time scaling.\nHowever, existing approaches to incentivize LLMs' deep thinking abilities\ngenerally require large-scale data or significant training efforts. Meanwhile,\nit remains unclear how to improve the thinking abilities of less powerful base\nmodels. In this work, we introduce S$^2$R, an efficient framework that enhances\nLLM reasoning by teaching models to self-verify and self-correct during\ninference. Specifically, we first initialize LLMs with iterative\nself-verification and self-correction behaviors through supervised fine-tuning\non carefully curated data. The self-verification and self-correction skills are\nthen further strengthened by both outcome-level and process-level reinforcement\nlearning, with minimized resource requirements, enabling the model to\nadaptively refine its reasoning process during inference. Our results\ndemonstrate that, with only 3.1k self-verifying and self-correcting behavior\ninitialization samples, Qwen2.5-math-7B achieves an accuracy improvement from\n51.0\\% to 81.6\\%, outperforming models trained on an equivalent amount of\nlong-CoT distilled data. Extensive experiments and analysis based on three base\nmodels across both in-domain and out-of-domain benchmarks validate the\neffectiveness of S$^2$R. Our code and data are available at\nhttps://github.com/NineAbyss/S2R."
                },
                "authors": [
                    {
                        "name": "Ruotian Ma"
                    },
                    {
                        "name": "Peisong Wang"
                    },
                    {
                        "name": "Cheng Liu"
                    },
                    {
                        "name": "Xingyan Liu"
                    },
                    {
                        "name": "Jiaqi Chen"
                    },
                    {
                        "name": "Bang Zhang"
                    },
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Nan Du"
                    },
                    {
                        "name": "Jia Li"
                    }
                ],
                "author_detail": {
                    "name": "Jia Li"
                },
                "author": "Jia Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12842v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12842v1",
                "updated": "2025-02-18T13:22:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    13,
                    22,
                    14,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T13:22:14Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    13,
                    22,
                    14,
                    1,
                    49,
                    0
                ],
                "title": "Towards Adaptive Feedback with AI: Comparing the Feedback Quality of\n  LLMs and Teachers on Experimentation Protocols",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Adaptive Feedback with AI: Comparing the Feedback Quality of\n  LLMs and Teachers on Experimentation Protocols"
                },
                "summary": "Effective feedback is essential for fostering students' success in scientific\ninquiry. With advancements in artificial intelligence, large language models\n(LLMs) offer new possibilities for delivering instant and adaptive feedback.\nHowever, this feedback often lacks the pedagogical validation provided by\nreal-world practitioners. To address this limitation, our study evaluates and\ncompares the feedback quality of LLM agents with that of human teachers and\nscience education experts on student-written experimentation protocols. Four\nblinded raters, all professionals in scientific inquiry and science education,\nevaluated the feedback texts generated by 1) the LLM agent, 2) the teachers and\n3) the science education experts using a five-point Likert scale based on six\ncriteria of effective feedback: Feed Up, Feed Back, Feed Forward, Constructive\nTone, Linguistic Clarity, and Technical Terminology. Our results indicate that\nLLM-generated feedback shows no significant difference to that of teachers and\nexperts in overall quality. However, the LLM agent's performance lags in the\nFeed Back dimension, which involves identifying and explaining errors within\nthe student's work context. Qualitative analysis highlighted the LLM agent's\nlimitations in contextual understanding and in the clear communication of\nspecific errors. Our findings suggest that combining LLM-generated feedback\nwith human expertise can enhance educational practices by leveraging the\nefficiency of LLMs and the nuanced understanding of educators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective feedback is essential for fostering students' success in scientific\ninquiry. With advancements in artificial intelligence, large language models\n(LLMs) offer new possibilities for delivering instant and adaptive feedback.\nHowever, this feedback often lacks the pedagogical validation provided by\nreal-world practitioners. To address this limitation, our study evaluates and\ncompares the feedback quality of LLM agents with that of human teachers and\nscience education experts on student-written experimentation protocols. Four\nblinded raters, all professionals in scientific inquiry and science education,\nevaluated the feedback texts generated by 1) the LLM agent, 2) the teachers and\n3) the science education experts using a five-point Likert scale based on six\ncriteria of effective feedback: Feed Up, Feed Back, Feed Forward, Constructive\nTone, Linguistic Clarity, and Technical Terminology. Our results indicate that\nLLM-generated feedback shows no significant difference to that of teachers and\nexperts in overall quality. However, the LLM agent's performance lags in the\nFeed Back dimension, which involves identifying and explaining errors within\nthe student's work context. Qualitative analysis highlighted the LLM agent's\nlimitations in contextual understanding and in the clear communication of\nspecific errors. Our findings suggest that combining LLM-generated feedback\nwith human expertise can enhance educational practices by leveraging the\nefficiency of LLMs and the nuanced understanding of educators."
                },
                "authors": [
                    {
                        "name": "Kathrin Seler"
                    },
                    {
                        "name": "Arne Bewersdorff"
                    },
                    {
                        "name": "Claudia Nerdel"
                    },
                    {
                        "name": "Enkelejda Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Enkelejda Kasneci"
                },
                "author": "Enkelejda Kasneci",
                "arxiv_comment": "This work has been submitted to the IJAIED for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12842v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12842v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12838v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12838v1",
                "updated": "2025-02-18T13:11:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    13,
                    11,
                    16,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T13:11:16Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    13,
                    11,
                    16,
                    1,
                    49,
                    0
                ],
                "title": "Towards Equitable AI: Detecting Bias in Using Large Language Models for\n  Marketing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Equitable AI: Detecting Bias in Using Large Language Models for\n  Marketing"
                },
                "summary": "The recent advances in large language models (LLMs) have revolutionized\nindustries such as finance, marketing, and customer service by enabling\nsophisticated natural language processing tasks. However, the broad adoption of\nLLMs brings significant challenges, particularly in the form of social biases\nthat can be embedded within their outputs. Biases related to gender, age, and\nother sensitive attributes can lead to unfair treatment, raising ethical\nconcerns and risking both company reputation and customer trust. This study\nexamined bias in finance-related marketing slogans generated by LLMs (i.e.,\nChatGPT) by prompting tailored ads targeting five demographic categories:\ngender, marital status, age, income level, and education level. A total of\n1,700 slogans were generated for 17 unique demographic groups, and key terms\nwere categorized into four thematic groups: empowerment, financial, benefits\nand features, and personalization. Bias was systematically assessed using\nrelative bias calculations and statistically tested with the Kolmogorov-Smirnov\n(KS) test against general slogans generated for any individual. Results\nrevealed that marketing slogans are not neutral; rather, they emphasize\ndifferent themes based on demographic factors. Women, younger individuals,\nlow-income earners, and those with lower education levels receive more distinct\nmessaging compared to older, higher-income, and highly educated individuals.\nThis underscores the need to consider demographic-based biases in AI-generated\nmarketing strategies and their broader societal implications. The findings of\nthis study provide a roadmap for developing more equitable AI systems,\nhighlighting the need for ongoing bias detection and mitigation efforts in\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advances in large language models (LLMs) have revolutionized\nindustries such as finance, marketing, and customer service by enabling\nsophisticated natural language processing tasks. However, the broad adoption of\nLLMs brings significant challenges, particularly in the form of social biases\nthat can be embedded within their outputs. Biases related to gender, age, and\nother sensitive attributes can lead to unfair treatment, raising ethical\nconcerns and risking both company reputation and customer trust. This study\nexamined bias in finance-related marketing slogans generated by LLMs (i.e.,\nChatGPT) by prompting tailored ads targeting five demographic categories:\ngender, marital status, age, income level, and education level. A total of\n1,700 slogans were generated for 17 unique demographic groups, and key terms\nwere categorized into four thematic groups: empowerment, financial, benefits\nand features, and personalization. Bias was systematically assessed using\nrelative bias calculations and statistically tested with the Kolmogorov-Smirnov\n(KS) test against general slogans generated for any individual. Results\nrevealed that marketing slogans are not neutral; rather, they emphasize\ndifferent themes based on demographic factors. Women, younger individuals,\nlow-income earners, and those with lower education levels receive more distinct\nmessaging compared to older, higher-income, and highly educated individuals.\nThis underscores the need to consider demographic-based biases in AI-generated\nmarketing strategies and their broader societal implications. The findings of\nthis study provide a roadmap for developing more equitable AI systems,\nhighlighting the need for ongoing bias detection and mitigation efforts in\nLLMs."
                },
                "authors": [
                    {
                        "name": "Berk Yilmaz"
                    },
                    {
                        "name": "Huthaifa I. Ashqar"
                    }
                ],
                "author_detail": {
                    "name": "Huthaifa I. Ashqar"
                },
                "author": "Huthaifa I. Ashqar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12838v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12838v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12836v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12836v1",
                "updated": "2025-02-18T13:09:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    13,
                    9,
                    59,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T13:09:59Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    13,
                    9,
                    59,
                    1,
                    49,
                    0
                ],
                "title": "An LLM-Powered Agent for Physiological Data Analysis: A Case Study on\n  PPG-based Heart Rate Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An LLM-Powered Agent for Physiological Data Analysis: A Case Study on\n  PPG-based Heart Rate Estimation"
                },
                "summary": "Large language models (LLMs) are revolutionizing healthcare by improving\ndiagnosis, patient care, and decision support through interactive\ncommunication. More recently, they have been applied to analyzing physiological\ntime-series like wearable data for health insight extraction. Existing methods\nembed raw numerical sequences directly into prompts, which exceeds token limits\nand increases computational costs. Additionally, some studies integrated\nfeatures extracted from time-series in textual prompts or applied multimodal\napproaches. However, these methods often produce generic and unreliable outputs\ndue to LLMs' limited analytical rigor and inefficiency in interpreting\ncontinuous waveforms. In this paper, we develop an LLM-powered agent for\nphysiological time-series analysis aimed to bridge the gap in integrating LLMs\nwith well-established analytical tools. Built on the OpenCHA, an open-source\nLLM-powered framework, our agent features an orchestrator that integrates user\ninteraction, data sources, and analytical tools to generate accurate health\ninsights. To evaluate its effectiveness, we implement a case study on heart\nrate (HR) estimation from Photoplethysmogram (PPG) signals using a dataset of\nPPG and Electrocardiogram (ECG) recordings in a remote health monitoring study.\nThe agent's performance is benchmarked against OpenAI GPT-4o-mini and GPT-4o,\nwith ECG serving as the gold standard for HR estimation. Results demonstrate\nthat our agent significantly outperforms benchmark models by achieving lower\nerror rates and more reliable HR estimations. The agent implementation is\npublicly available on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are revolutionizing healthcare by improving\ndiagnosis, patient care, and decision support through interactive\ncommunication. More recently, they have been applied to analyzing physiological\ntime-series like wearable data for health insight extraction. Existing methods\nembed raw numerical sequences directly into prompts, which exceeds token limits\nand increases computational costs. Additionally, some studies integrated\nfeatures extracted from time-series in textual prompts or applied multimodal\napproaches. However, these methods often produce generic and unreliable outputs\ndue to LLMs' limited analytical rigor and inefficiency in interpreting\ncontinuous waveforms. In this paper, we develop an LLM-powered agent for\nphysiological time-series analysis aimed to bridge the gap in integrating LLMs\nwith well-established analytical tools. Built on the OpenCHA, an open-source\nLLM-powered framework, our agent features an orchestrator that integrates user\ninteraction, data sources, and analytical tools to generate accurate health\ninsights. To evaluate its effectiveness, we implement a case study on heart\nrate (HR) estimation from Photoplethysmogram (PPG) signals using a dataset of\nPPG and Electrocardiogram (ECG) recordings in a remote health monitoring study.\nThe agent's performance is benchmarked against OpenAI GPT-4o-mini and GPT-4o,\nwith ECG serving as the gold standard for HR estimation. Results demonstrate\nthat our agent significantly outperforms benchmark models by achieving lower\nerror rates and more reliable HR estimations. The agent implementation is\npublicly available on GitHub."
                },
                "authors": [
                    {
                        "name": "Mohammad Feli"
                    },
                    {
                        "name": "Iman Azimi"
                    },
                    {
                        "name": "Pasi Liljeberg"
                    },
                    {
                        "name": "Amir M. Rahmani"
                    }
                ],
                "author_detail": {
                    "name": "Amir M. Rahmani"
                },
                "author": "Amir M. Rahmani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12836v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12836v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08524v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08524v2",
                "updated": "2025-02-18T13:08:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    13,
                    8,
                    33,
                    1,
                    49,
                    0
                ],
                "published": "2024-10-11T04:55:08Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    4,
                    55,
                    8,
                    4,
                    285,
                    0
                ],
                "title": "IGNN-Solver: A Graph Neural Solver for Implicit Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IGNN-Solver: A Graph Neural Solver for Implicit Graph Neural Networks"
                },
                "summary": "Implicit graph neural networks (IGNNs), which exhibit strong expressive power\nwith a single layer, have recently demonstrated remarkable performance in\ncapturing long-range dependencies (LRD) in underlying graphs while effectively\nmitigating the over-smoothing problem. However, IGNNs rely on computationally\nexpensive fixed-point iterations, which lead to significant speed and\nscalability limitations, hindering their application to large-scale graphs. To\nachieve fast fixed-point solving for IGNNs, we propose a novel graph neural\nsolver, IGNN-Solver, which leverages the generalized Anderson Acceleration\nmethod, parameterized by a tiny GNN, and learns iterative updates as a\ngraph-dependent temporal process. To improve effectiveness on large-scale graph\ntasks, we further integrate sparsification and storage compression methods,\nspecifically tailored for the IGNN-Solver, into its design. Extensive\nexperiments demonstrate that the IGNN-Solver significantly accelerates\ninference on both small- and large-scale tasks, achieving a $1.5\\times$ to\n$8\\times$ speedup without sacrificing accuracy. This advantage becomes more\npronounced as the graph scale grows, facilitating its large-scale deployment in\nreal-world applications. The code to reproduce our results is available at\nhttps://github.com/landrarwolf/IGNN-Solver.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit graph neural networks (IGNNs), which exhibit strong expressive power\nwith a single layer, have recently demonstrated remarkable performance in\ncapturing long-range dependencies (LRD) in underlying graphs while effectively\nmitigating the over-smoothing problem. However, IGNNs rely on computationally\nexpensive fixed-point iterations, which lead to significant speed and\nscalability limitations, hindering their application to large-scale graphs. To\nachieve fast fixed-point solving for IGNNs, we propose a novel graph neural\nsolver, IGNN-Solver, which leverages the generalized Anderson Acceleration\nmethod, parameterized by a tiny GNN, and learns iterative updates as a\ngraph-dependent temporal process. To improve effectiveness on large-scale graph\ntasks, we further integrate sparsification and storage compression methods,\nspecifically tailored for the IGNN-Solver, into its design. Extensive\nexperiments demonstrate that the IGNN-Solver significantly accelerates\ninference on both small- and large-scale tasks, achieving a $1.5\\times$ to\n$8\\times$ speedup without sacrificing accuracy. This advantage becomes more\npronounced as the graph scale grows, facilitating its large-scale deployment in\nreal-world applications. The code to reproduce our results is available at\nhttps://github.com/landrarwolf/IGNN-Solver."
                },
                "authors": [
                    {
                        "name": "Junchao Lin"
                    },
                    {
                        "name": "Zenan Ling"
                    },
                    {
                        "name": "Zhanbo Feng"
                    },
                    {
                        "name": "Jingwen Xu"
                    },
                    {
                        "name": "Minxuan Liao"
                    },
                    {
                        "name": "Feng Zhou"
                    },
                    {
                        "name": "Tianqi Hou"
                    },
                    {
                        "name": "Zhenyu Liao"
                    },
                    {
                        "name": "Robert C. Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Robert C. Qiu"
                },
                "author": "Robert C. Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08524v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08524v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13622v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13622v3",
                "updated": "2025-02-18T13:05:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    13,
                    5,
                    36,
                    1,
                    49,
                    0
                ],
                "published": "2025-01-23T12:44:45Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    44,
                    45,
                    3,
                    23,
                    0
                ],
                "title": "Coarse-to-Fine Process Reward Modeling for Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coarse-to-Fine Process Reward Modeling for Mathematical Reasoning"
                },
                "summary": "The Process Reward Model (PRM) plays a crucial role in mathematical reasoning\ntasks, requiring high-quality supervised process data. However, we observe that\nreasoning steps generated by Large Language Models (LLMs) often fail to exhibit\nstrictly incremental information, leading to redundancy that can hinder\neffective reasoning. To address this issue, we propose CFPRM, a simple yet\neffective coarse-to-fine strategy. Instead of focusing on the detection of\nredundant steps, our approach first establishes a coarse-grained window to\nmerge adjacent reasoning steps into unified, holistic steps. The window size is\nthen progressively reduced to extract fine-grained reasoning steps, enabling\ndata collection at multiple granularities for training. By leveraging this\nhierarchical refinement process, CFPRM mitigates redundancy while preserving\nessential fine-grained knowledge. Extensive experiments on two reasoning\ndatasets across three loss criteria validate the CFPRM's effectiveness and\nversatility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Process Reward Model (PRM) plays a crucial role in mathematical reasoning\ntasks, requiring high-quality supervised process data. However, we observe that\nreasoning steps generated by Large Language Models (LLMs) often fail to exhibit\nstrictly incremental information, leading to redundancy that can hinder\neffective reasoning. To address this issue, we propose CFPRM, a simple yet\neffective coarse-to-fine strategy. Instead of focusing on the detection of\nredundant steps, our approach first establishes a coarse-grained window to\nmerge adjacent reasoning steps into unified, holistic steps. The window size is\nthen progressively reduced to extract fine-grained reasoning steps, enabling\ndata collection at multiple granularities for training. By leveraging this\nhierarchical refinement process, CFPRM mitigates redundancy while preserving\nessential fine-grained knowledge. Extensive experiments on two reasoning\ndatasets across three loss criteria validate the CFPRM's effectiveness and\nversatility."
                },
                "authors": [
                    {
                        "name": "Yulan Hu"
                    },
                    {
                        "name": "Ge Chen"
                    },
                    {
                        "name": "Jinman Zhao"
                    },
                    {
                        "name": "Sheng Ouyang"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13622v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13622v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.11409v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.11409v5",
                "updated": "2025-02-18T12:53:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    12,
                    53,
                    47,
                    1,
                    49,
                    0
                ],
                "published": "2023-10-17T17:15:41Z",
                "published_parsed": [
                    2023,
                    10,
                    17,
                    17,
                    15,
                    41,
                    1,
                    290,
                    0
                ],
                "title": "LLMs as Hackers: Autonomous Linux Privilege Escalation Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Hackers: Autonomous Linux Privilege Escalation Attacks"
                },
                "summary": "Penetration testing, an essential component of software security testing,\nallows organizations to identify and remediate vulnerabilities in their\nsystems, thus bolstering their defense mechanisms against cyberattacks. One\nrecent advancement in the realm of penetration testing is the utilization of\nLanguage Models (LLMs). We explore the intersection of LLMs and penetration\ntesting to gain insight into their capabilities and challenges in the context\nof privilege escalation. We introduce a fully automated privilege-escalation\ntool designed for evaluating the efficacy of LLMs for (ethical) hacking,\nexecuting benchmarks using multiple LLMs, and investigating their respective\nresults.\n  Our results show that GPT-4-turbo is well suited to exploit vulnerabilities\n(33-83% of vulnerabilities). GPT-3.5-turbo can abuse 16-50% of vulnerabilities,\nwhile local models, such as Llama3, can only exploit between 0 and 33% of the\nvulnerabilities.\n  We analyze the impact of different context sizes, in-context learning,\noptional high-level guidance mechanisms, and memory management techniques. We\ndiscuss challenging areas for LLMs, including maintaining focus during testing,\ncoping with errors, and finally comparing LLMs with human hackers.\n  The current version of the LLM-guided privilege-escalation prototype can be\nfound at https://github.com/ipa-labs/hackingBuddyGPT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Penetration testing, an essential component of software security testing,\nallows organizations to identify and remediate vulnerabilities in their\nsystems, thus bolstering their defense mechanisms against cyberattacks. One\nrecent advancement in the realm of penetration testing is the utilization of\nLanguage Models (LLMs). We explore the intersection of LLMs and penetration\ntesting to gain insight into their capabilities and challenges in the context\nof privilege escalation. We introduce a fully automated privilege-escalation\ntool designed for evaluating the efficacy of LLMs for (ethical) hacking,\nexecuting benchmarks using multiple LLMs, and investigating their respective\nresults.\n  Our results show that GPT-4-turbo is well suited to exploit vulnerabilities\n(33-83% of vulnerabilities). GPT-3.5-turbo can abuse 16-50% of vulnerabilities,\nwhile local models, such as Llama3, can only exploit between 0 and 33% of the\nvulnerabilities.\n  We analyze the impact of different context sizes, in-context learning,\noptional high-level guidance mechanisms, and memory management techniques. We\ndiscuss challenging areas for LLMs, including maintaining focus during testing,\ncoping with errors, and finally comparing LLMs with human hackers.\n  The current version of the LLM-guided privilege-escalation prototype can be\nfound at https://github.com/ipa-labs/hackingBuddyGPT."
                },
                "authors": [
                    {
                        "name": "Andreas Happe"
                    },
                    {
                        "name": "Aaron Kaplan"
                    },
                    {
                        "name": "Juergen Cito"
                    }
                ],
                "author_detail": {
                    "name": "Juergen Cito"
                },
                "author": "Juergen Cito",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.11409v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.11409v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08115v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08115v2",
                "updated": "2025-02-18T12:50:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    12,
                    50,
                    0,
                    1,
                    49,
                    0
                ],
                "published": "2024-10-10T17:00:06Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    0,
                    6,
                    3,
                    284,
                    0
                ],
                "title": "Optima: Optimizing Effectiveness and Efficiency for LLM-Based\n  Multi-Agent System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optima: Optimizing Effectiveness and Efficiency for LLM-Based\n  Multi-Agent System"
                },
                "summary": "Large Language Model (LLM) based multi-agent systems (MAS) show remarkable\npotential in collaborative problem-solving, yet they still face critical\nchallenges: low communication efficiency, poor scalability, and a lack of\neffective parameter-updating optimization methods. We present Optima, a novel\nframework that addresses these issues by significantly enhancing both\ncommunication efficiency and task effectiveness in LLM-based MAS through LLM\ntraining. Optima employs an iterative generate, rank, select, and train\nparadigm with a reward function balancing task performance, token efficiency,\nand communication readability. We explore various RL algorithms, including\nSupervised Fine-Tuning, Direct Preference Optimization, and their hybrid\napproaches, providing insights into their effectiveness-efficiency trade-offs.\nWe integrate Monte Carlo Tree Search-inspired techniques for DPO data\ngeneration, treating conversation turns as tree nodes to explore diverse\ninteraction paths. Evaluated on common multi-agent tasks, including\ninformation-asymmetric question answering and complex reasoning, Optima shows\nconsistent and substantial improvements over single-agent baselines and vanilla\nMAS based on Llama 3 8B, achieving up to 2.8x performance gain with less than\n10\\% tokens on tasks requiring heavy information exchange. Moreover, Optima's\nefficiency gains open new possibilities for leveraging inference-compute more\neffectively, leading to improved inference-time scaling laws. By addressing\nfundamental challenges in LLM-based MAS, Optima shows the potential towards\nscalable, efficient, and effective MAS\n(https://chenweize1998.github.io/optima-project-page).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) based multi-agent systems (MAS) show remarkable\npotential in collaborative problem-solving, yet they still face critical\nchallenges: low communication efficiency, poor scalability, and a lack of\neffective parameter-updating optimization methods. We present Optima, a novel\nframework that addresses these issues by significantly enhancing both\ncommunication efficiency and task effectiveness in LLM-based MAS through LLM\ntraining. Optima employs an iterative generate, rank, select, and train\nparadigm with a reward function balancing task performance, token efficiency,\nand communication readability. We explore various RL algorithms, including\nSupervised Fine-Tuning, Direct Preference Optimization, and their hybrid\napproaches, providing insights into their effectiveness-efficiency trade-offs.\nWe integrate Monte Carlo Tree Search-inspired techniques for DPO data\ngeneration, treating conversation turns as tree nodes to explore diverse\ninteraction paths. Evaluated on common multi-agent tasks, including\ninformation-asymmetric question answering and complex reasoning, Optima shows\nconsistent and substantial improvements over single-agent baselines and vanilla\nMAS based on Llama 3 8B, achieving up to 2.8x performance gain with less than\n10\\% tokens on tasks requiring heavy information exchange. Moreover, Optima's\nefficiency gains open new possibilities for leveraging inference-compute more\neffectively, leading to improved inference-time scaling laws. By addressing\nfundamental challenges in LLM-based MAS, Optima shows the potential towards\nscalable, efficient, and effective MAS\n(https://chenweize1998.github.io/optima-project-page)."
                },
                "authors": [
                    {
                        "name": "Weize Chen"
                    },
                    {
                        "name": "Jiarui Yuan"
                    },
                    {
                        "name": "Chen Qian"
                    },
                    {
                        "name": "Cheng Yang"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08115v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08115v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12829v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12829v1",
                "updated": "2025-02-18T12:48:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    12,
                    48,
                    37,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T12:48:37Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    12,
                    48,
                    37,
                    1,
                    49,
                    0
                ],
                "title": "KazMMLU: Evaluating Language Models on Kazakh, Russian, and Regional\n  Knowledge of Kazakhstan",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KazMMLU: Evaluating Language Models on Kazakh, Russian, and Regional\n  Knowledge of Kazakhstan"
                },
                "summary": "Despite having a population of twenty million, Kazakhstan's culture and\nlanguage remain underrepresented in the field of natural language processing.\nAlthough large language models (LLMs) continue to advance worldwide, progress\nin Kazakh language has been limited, as seen in the scarcity of dedicated\nmodels and benchmark evaluations. To address this gap, we introduce KazMMLU,\nthe first MMLU-style dataset specifically designed for Kazakh language. KazMMLU\ncomprises 23,000 questions that cover various educational levels, including\nSTEM, humanities, and social sciences, sourced from authentic educational\nmaterials and manually validated by native speakers and educators. The dataset\nincludes 10,969 Kazakh questions and 12,031 Russian questions, reflecting\nKazakhstan's bilingual education system and rich local context. Our evaluation\nof several state-of-the-art multilingual models (Llama-3.1, Qwen-2.5, GPT-4,\nand DeepSeek V3) demonstrates substantial room for improvement, as even the\nbest-performing models struggle to achieve competitive performance in Kazakh\nand Russian. These findings underscore significant performance gaps compared to\nhigh-resource languages. We hope that our dataset will enable further research\nand development of Kazakh-centric LLMs. Data and code will be made available\nupon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite having a population of twenty million, Kazakhstan's culture and\nlanguage remain underrepresented in the field of natural language processing.\nAlthough large language models (LLMs) continue to advance worldwide, progress\nin Kazakh language has been limited, as seen in the scarcity of dedicated\nmodels and benchmark evaluations. To address this gap, we introduce KazMMLU,\nthe first MMLU-style dataset specifically designed for Kazakh language. KazMMLU\ncomprises 23,000 questions that cover various educational levels, including\nSTEM, humanities, and social sciences, sourced from authentic educational\nmaterials and manually validated by native speakers and educators. The dataset\nincludes 10,969 Kazakh questions and 12,031 Russian questions, reflecting\nKazakhstan's bilingual education system and rich local context. Our evaluation\nof several state-of-the-art multilingual models (Llama-3.1, Qwen-2.5, GPT-4,\nand DeepSeek V3) demonstrates substantial room for improvement, as even the\nbest-performing models struggle to achieve competitive performance in Kazakh\nand Russian. These findings underscore significant performance gaps compared to\nhigh-resource languages. We hope that our dataset will enable further research\nand development of Kazakh-centric LLMs. Data and code will be made available\nupon acceptance."
                },
                "authors": [
                    {
                        "name": "Mukhammed Togmanov"
                    },
                    {
                        "name": "Nurdaulet Mukhituly"
                    },
                    {
                        "name": "Diana Turmakhan"
                    },
                    {
                        "name": "Jonibek Mansurov"
                    },
                    {
                        "name": "Maiya Goloburda"
                    },
                    {
                        "name": "Akhmed Sakip"
                    },
                    {
                        "name": "Zhuohan Xie"
                    },
                    {
                        "name": "Yuxia Wang"
                    },
                    {
                        "name": "Bekassyl Syzdykov"
                    },
                    {
                        "name": "Nurkhan Laiyk"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Fajri Koto"
                    }
                ],
                "author_detail": {
                    "name": "Fajri Koto"
                },
                "author": "Fajri Koto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12829v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12829v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07982v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07982v2",
                "updated": "2025-02-18T12:48:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    12,
                    48,
                    27,
                    1,
                    49,
                    0
                ],
                "published": "2024-08-15T07:03:00Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    7,
                    3,
                    0,
                    3,
                    228,
                    0
                ],
                "title": "Toward a Dialogue System Using a Large Language Model to Recognize User\n  Emotions with a Camera",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward a Dialogue System Using a Large Language Model to Recognize User\n  Emotions with a Camera"
                },
                "summary": "The performance of ChatGPT\\copyright{} and other LLMs has improved\ntremendously, and in online environments, they are increasingly likely to be\nused in a wide variety of situations, such as ChatBot on web pages, call center\noperations using voice interaction, and dialogue functions using agents. In the\noffline environment, multimodal dialogue functions are also being realized,\nsuch as guidance by Artificial Intelligence agents (AI agents) using tablet\nterminals and dialogue systems in the form of LLMs mounted on robots. In this\nmultimodal dialogue, mutual emotion recognition between the AI and the user\nwill become important. So far, there have been methods for expressing emotions\non the part of the AI agent or for recognizing them using textual or voice\ninformation of the user's utterances, but methods for AI agents to recognize\nemotions from the user's facial expressions have not been studied. In this\nstudy, we examined whether or not LLM-based AI agents can interact with users\naccording to their emotional states by capturing the user in dialogue with a\ncamera, recognizing emotions from facial expressions, and adding such emotion\ninformation to prompts. The results confirmed that AI agents can have\nconversations according to the emotional state for emotional states with\nrelatively high scores, such as Happy and Angry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of ChatGPT\\copyright{} and other LLMs has improved\ntremendously, and in online environments, they are increasingly likely to be\nused in a wide variety of situations, such as ChatBot on web pages, call center\noperations using voice interaction, and dialogue functions using agents. In the\noffline environment, multimodal dialogue functions are also being realized,\nsuch as guidance by Artificial Intelligence agents (AI agents) using tablet\nterminals and dialogue systems in the form of LLMs mounted on robots. In this\nmultimodal dialogue, mutual emotion recognition between the AI and the user\nwill become important. So far, there have been methods for expressing emotions\non the part of the AI agent or for recognizing them using textual or voice\ninformation of the user's utterances, but methods for AI agents to recognize\nemotions from the user's facial expressions have not been studied. In this\nstudy, we examined whether or not LLM-based AI agents can interact with users\naccording to their emotional states by capturing the user in dialogue with a\ncamera, recognizing emotions from facial expressions, and adding such emotion\ninformation to prompts. The results confirmed that AI agents can have\nconversations according to the emotional state for emotional states with\nrelatively high scores, such as Happy and Angry."
                },
                "authors": [
                    {
                        "name": "Hiroki Tanioka"
                    },
                    {
                        "name": "Tetsushi Ueta"
                    },
                    {
                        "name": "Masahiko Sano"
                    }
                ],
                "author_detail": {
                    "name": "Masahiko Sano"
                },
                "author": "Masahiko Sano",
                "arxiv_comment": "4 pages, 5 figures, 1 table, The 1st InterAI: Interactive AI for\n  Human-Centered Robotics workshop in conjunction with IEEE Ro-MAN 2024,\n  Pasadona, LA, USA, Aug. 2024",
                "arxiv_journal_ref": "The 1st InterAI Workshop: Interactive AI for Human-centered\n  Robotics, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07982v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07982v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T40",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04780v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04780v2",
                "updated": "2025-02-18T12:47:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    12,
                    47,
                    58,
                    1,
                    49,
                    0
                ],
                "published": "2024-10-07T06:45:22Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    6,
                    45,
                    22,
                    0,
                    281,
                    0
                ],
                "title": "Mitigating Modality Prior-Induced Hallucinations in Multimodal Large\n  Language Models via Deciphering Attention Causality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Modality Prior-Induced Hallucinations in Multimodal Large\n  Language Models via Deciphering Attention Causality"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have emerged as a central focus in\nboth industry and academia, but often suffer from biases introduced by visual\nand language priors, which can lead to multimodal hallucination. These biases\narise from the visual encoder and the Large Language Model (LLM) backbone,\naffecting the attention mechanism responsible for aligning multimodal inputs.\nExisting decoding-based mitigation methods focus on statistical correlations\nand overlook the causal relationships between attention mechanisms and model\noutput, limiting their effectiveness in addressing these biases. To tackle this\nissue, we propose a causal inference framework termed CausalMM that applies\nstructural causal modeling to MLLMs, treating modality priors as a confounder\nbetween attention mechanisms and output. Specifically, by employing backdoor\nadjustment and counterfactual reasoning at both the visual and language\nattention levels, our method mitigates the negative effects of modality priors\nand enhances the alignment of MLLM's inputs and outputs, with a maximum score\nimprovement of 65.3% on 6 VLind-Bench indicators and 164 points on MME\nBenchmark compared to conventional methods. Extensive experiments validate the\neffectiveness of our approach while being a plug-and-play solution. Our code is\navailable at: https://github.com/The-Martyr/CausalMM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have emerged as a central focus in\nboth industry and academia, but often suffer from biases introduced by visual\nand language priors, which can lead to multimodal hallucination. These biases\narise from the visual encoder and the Large Language Model (LLM) backbone,\naffecting the attention mechanism responsible for aligning multimodal inputs.\nExisting decoding-based mitigation methods focus on statistical correlations\nand overlook the causal relationships between attention mechanisms and model\noutput, limiting their effectiveness in addressing these biases. To tackle this\nissue, we propose a causal inference framework termed CausalMM that applies\nstructural causal modeling to MLLMs, treating modality priors as a confounder\nbetween attention mechanisms and output. Specifically, by employing backdoor\nadjustment and counterfactual reasoning at both the visual and language\nattention levels, our method mitigates the negative effects of modality priors\nand enhances the alignment of MLLM's inputs and outputs, with a maximum score\nimprovement of 65.3% on 6 VLind-Bench indicators and 164 points on MME\nBenchmark compared to conventional methods. Extensive experiments validate the\neffectiveness of our approach while being a plug-and-play solution. Our code is\navailable at: https://github.com/The-Martyr/CausalMM"
                },
                "authors": [
                    {
                        "name": "Guanyu Zhou"
                    },
                    {
                        "name": "Yibo Yan"
                    },
                    {
                        "name": "Xin Zou"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Aiwei Liu"
                    },
                    {
                        "name": "Xuming Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xuming Hu"
                },
                "author": "Xuming Hu",
                "arxiv_comment": "Accepted by The Thirteenth International Conference on Learning\n  Representations (ICLR 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04780v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04780v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12825v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12825v1",
                "updated": "2025-02-18T12:46:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    12,
                    46,
                    18,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T12:46:18Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    12,
                    46,
                    18,
                    1,
                    49,
                    0
                ],
                "title": "Reasoning and the Trusting Behavior of DeepSeek and GPT: An Experiment\n  Revealing Hidden Fault Lines in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning and the Trusting Behavior of DeepSeek and GPT: An Experiment\n  Revealing Hidden Fault Lines in Large Language Models"
                },
                "summary": "When encountering increasingly frequent performance improvements or cost\nreductions from a new large language model (LLM), developers of applications\nleveraging LLMs must decide whether to take advantage of these improvements or\nstay with older tried-and-tested models. Low perceived switching frictions can\nlead to choices that do not consider more subtle behavior changes that the\ntransition may induce. Our experiments use a popular game-theoretic behavioral\neconomics model of trust to show stark differences in the trusting behavior of\nOpenAI's and DeepSeek's models. We highlight a collapse in the economic trust\nbehavior of the o1-mini and o3-mini models as they reconcile profit-maximizing\nand risk-seeking with future returns from trust, and contrast it with\nDeepSeek's more sophisticated and profitable trusting behavior that stems from\nan ability to incorporate deeper concepts like forward planning and\ntheory-of-mind. As LLMs form the basis for high-stakes commercial systems, our\nresults highlight the perils of relying on LLM performance benchmarks that are\ntoo narrowly defined and suggest that careful analysis of their hidden fault\nlines should be part of any organization's AI strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When encountering increasingly frequent performance improvements or cost\nreductions from a new large language model (LLM), developers of applications\nleveraging LLMs must decide whether to take advantage of these improvements or\nstay with older tried-and-tested models. Low perceived switching frictions can\nlead to choices that do not consider more subtle behavior changes that the\ntransition may induce. Our experiments use a popular game-theoretic behavioral\neconomics model of trust to show stark differences in the trusting behavior of\nOpenAI's and DeepSeek's models. We highlight a collapse in the economic trust\nbehavior of the o1-mini and o3-mini models as they reconcile profit-maximizing\nand risk-seeking with future returns from trust, and contrast it with\nDeepSeek's more sophisticated and profitable trusting behavior that stems from\nan ability to incorporate deeper concepts like forward planning and\ntheory-of-mind. As LLMs form the basis for high-stakes commercial systems, our\nresults highlight the perils of relying on LLM performance benchmarks that are\ntoo narrowly defined and suggest that careful analysis of their hidden fault\nlines should be part of any organization's AI strategy."
                },
                "authors": [
                    {
                        "name": "Rubing Lu"
                    },
                    {
                        "name": "Joo Sedoc"
                    },
                    {
                        "name": "Arun Sundararajan"
                    }
                ],
                "author_detail": {
                    "name": "Arun Sundararajan"
                },
                "author": "Arun Sundararajan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12825v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12825v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12821v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12821v1",
                "updated": "2025-02-18T12:32:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    12,
                    32,
                    11,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T12:32:11Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    12,
                    32,
                    11,
                    1,
                    49,
                    0
                ],
                "title": "Pitfalls of Scale: Investigating the Inverse Task of Redefinition in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pitfalls of Scale: Investigating the Inverse Task of Redefinition in\n  Large Language Models"
                },
                "summary": "Inverse tasks can uncover potential reasoning gaps as Large Language Models\n(LLMs) scale up. In this work, we explore the redefinition task, in which we\nassign alternative values to well-known physical constants and units of\nmeasure, prompting LLMs to respond accordingly. Our findings show that not only\ndoes model performance degrade with scale, but its false confidence also rises.\nMoreover, while factors such as prompting strategies or response formatting are\ninfluential, they do not preclude LLMs from anchoring to memorized values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inverse tasks can uncover potential reasoning gaps as Large Language Models\n(LLMs) scale up. In this work, we explore the redefinition task, in which we\nassign alternative values to well-known physical constants and units of\nmeasure, prompting LLMs to respond accordingly. Our findings show that not only\ndoes model performance degrade with scale, but its false confidence also rises.\nMoreover, while factors such as prompting strategies or response formatting are\ninfluential, they do not preclude LLMs from anchoring to memorized values."
                },
                "authors": [
                    {
                        "name": "Elena Stringli"
                    },
                    {
                        "name": "Maria Lymperaiou"
                    },
                    {
                        "name": "Giorgos Filandrianos"
                    },
                    {
                        "name": "Giorgos Stamou"
                    }
                ],
                "author_detail": {
                    "name": "Giorgos Stamou"
                },
                "author": "Giorgos Stamou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12821v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12821v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11177v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11177v2",
                "updated": "2025-02-18T12:31:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    12,
                    31,
                    49,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-16T15:57:55Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    15,
                    57,
                    55,
                    6,
                    47,
                    0
                ],
                "title": "The Mirage of Model Editing: Revisiting Evaluation in the Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mirage of Model Editing: Revisiting Evaluation in the Wild"
                },
                "summary": "Despite near-perfect results in artificial evaluations, the effectiveness of\nmodel editing in real-world applications remains unexplored. To bridge this\ngap, we propose to study model editing in question answering (QA) by\nestablishing a rigorous evaluation practice to assess the effectiveness of\nediting methods in correcting LLMs' errors. It consists of QAEdit, a new\nbenchmark derived from popular QA datasets, and a standardized evaluation\nframework. Our single editing experiments indicate that current editing methods\nperform substantially worse than previously reported (38.5% vs. ~96%). Through\nmodule analysis and controlled experiments, we demonstrate that this\nperformance decline stems from issues in evaluation practices of prior editing\nresearch. One key issue is the inappropriate use of teacher forcing in testing\nprevents error propagation by feeding ground truth tokens (inaccessible in\nreal-world scenarios) as input. Furthermore, we simulate real-world deployment\nby sequential editing, revealing that current approaches fail drastically with\nonly 1000 edits. Our analysis provides a fundamental reexamination of both the\nreal-world applicability of existing model editing methods and their evaluation\npractices, and establishes a rigorous evaluation framework with key insights to\nadvance reliable and practical model editing research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite near-perfect results in artificial evaluations, the effectiveness of\nmodel editing in real-world applications remains unexplored. To bridge this\ngap, we propose to study model editing in question answering (QA) by\nestablishing a rigorous evaluation practice to assess the effectiveness of\nediting methods in correcting LLMs' errors. It consists of QAEdit, a new\nbenchmark derived from popular QA datasets, and a standardized evaluation\nframework. Our single editing experiments indicate that current editing methods\nperform substantially worse than previously reported (38.5% vs. ~96%). Through\nmodule analysis and controlled experiments, we demonstrate that this\nperformance decline stems from issues in evaluation practices of prior editing\nresearch. One key issue is the inappropriate use of teacher forcing in testing\nprevents error propagation by feeding ground truth tokens (inaccessible in\nreal-world scenarios) as input. Furthermore, we simulate real-world deployment\nby sequential editing, revealing that current approaches fail drastically with\nonly 1000 edits. Our analysis provides a fundamental reexamination of both the\nreal-world applicability of existing model editing methods and their evaluation\npractices, and establishes a rigorous evaluation framework with key insights to\nadvance reliable and practical model editing research."
                },
                "authors": [
                    {
                        "name": "Wanli Yang"
                    },
                    {
                        "name": "Fei Sun"
                    },
                    {
                        "name": "Jiajun Tan"
                    },
                    {
                        "name": "Xinyu Ma"
                    },
                    {
                        "name": "Qi Cao"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Huawei Shen"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11177v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11177v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]