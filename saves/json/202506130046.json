[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2506.09758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09758v1",
                "updated": "2025-06-11T14:03:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    3,
                    13,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T14:03:13Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    3,
                    13,
                    2,
                    162,
                    0
                ],
                "title": "Mainframe-style channel controllers for modern disaggregated memory\n  systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mainframe-style channel controllers for modern disaggregated memory\n  systems"
                },
                "summary": "Despite the promise of alleviating the main memory bottleneck, and the\nexistence of commercial hardware implementations, techniques for Near-Data\nProcessing have seen relatively little real-world deployment. The idea has\nreceived renewed interest with the appearance of disaggregated or \"far\" memory,\nfor example in the use of CXL memory pools.\n  However, we argue that the lack of a clear OS-centric abstraction of\nNear-Data Processing is a major barrier to adoption of the technology. Inspired\nby the channel controllers which interface the CPU to disk drives in mainframe\nsystems, we propose memory channel controllers as a convenient, portable, and\nvirtualizable abstraction of Near-Data Processing for modern disaggregated\nmemory systems.\n  In addition to providing a clean abstraction that enables OS integration\nwhile requiring no changes to CPU architecture, memory channel controllers\nincorporate another key innovation: they exploit the cache coherence provided\nby emerging interconnects to provide a much richer programming model, with more\nfine-grained interaction, than has been possible with existing designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the promise of alleviating the main memory bottleneck, and the\nexistence of commercial hardware implementations, techniques for Near-Data\nProcessing have seen relatively little real-world deployment. The idea has\nreceived renewed interest with the appearance of disaggregated or \"far\" memory,\nfor example in the use of CXL memory pools.\n  However, we argue that the lack of a clear OS-centric abstraction of\nNear-Data Processing is a major barrier to adoption of the technology. Inspired\nby the channel controllers which interface the CPU to disk drives in mainframe\nsystems, we propose memory channel controllers as a convenient, portable, and\nvirtualizable abstraction of Near-Data Processing for modern disaggregated\nmemory systems.\n  In addition to providing a clean abstraction that enables OS integration\nwhile requiring no changes to CPU architecture, memory channel controllers\nincorporate another key innovation: they exploit the cache coherence provided\nby emerging interconnects to provide a much richer programming model, with more\nfine-grained interaction, than has been possible with existing designs."
                },
                "authors": [
                    {
                        "name": "Zikai Liu"
                    },
                    {
                        "name": "Jasmin Schult"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09536v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09536v1",
                "updated": "2025-06-11T09:08:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    9,
                    8,
                    59,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T09:08:59Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    9,
                    8,
                    59,
                    2,
                    162,
                    0
                ],
                "title": "Commissioning, characterization and first high dose rate irradiations at\n  a compact X-ray tube for microbeam and minibeam radiation therapy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Commissioning, characterization and first high dose rate irradiations at\n  a compact X-ray tube for microbeam and minibeam radiation therapy"
                },
                "summary": "Minibeam and microbeam radiation therapy promise improved treatment outcomes\nthrough reduced normal tissue toxicity at better tumor control rates. The lack\nof suitable compact radiation sources limits the clinical application of\nminibeams to superficial tumors and renders it impossible for microbeams. We\ndeveloped and constructed the first prototype of a compact line-focus X-ray\ntube (LFXT) with technology potentially suitable for clinical translation of\nminibeams and microbeams. We give an overview of the commissioning process\npreceding the first operation, present optical and radiological focal spot\ncharacterization methods, and dosimetric measurements. Additionally, we report\non first preclinical in vitro cell and in vivo mouse brain irradiations\nconducted with the LFXT prototype. The focal spot characterization resulted in\na strongly eccentric electron distribution with a width of 72.3 $\\mu$m.\nDosimetry showed sharp microbeam dose profiles with steep lateral penumbras and\na peak-to-valley dose ratio above 10 throughout a 70 mm thick PMMA phantom. An\nopen-field dose rate of 4.3 Gy/s was measured at an acceleration voltage of 150\nkV and a beam current of 17.4 mA at 150 mm distance from the focal spot. In\nvitro and in vivo experiments demonstrated the feasibility of the LFXT for\nminibeam and microbeam applications with field sizes of 1.5-2 cm. The mice\ndisplayed no observable side effects throughout the follow-up period after\nwhole-brain 260 $\\mu$m-minibeam irradiation. We successfully constructed and\ncommissioned the first proof-of-concept LFXT prototype. Dosimetric\ncharacterizations of the achieved microbeam field showed the superiority of the\nLFXT compared to conventional X-ray tubes in terms of beam quality. In future\ndevelopments, the remaining limitations of the prototype will be addressed for\nimproved minibeam and first ever microbeam radiation therapy in a clinical\nsetting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minibeam and microbeam radiation therapy promise improved treatment outcomes\nthrough reduced normal tissue toxicity at better tumor control rates. The lack\nof suitable compact radiation sources limits the clinical application of\nminibeams to superficial tumors and renders it impossible for microbeams. We\ndeveloped and constructed the first prototype of a compact line-focus X-ray\ntube (LFXT) with technology potentially suitable for clinical translation of\nminibeams and microbeams. We give an overview of the commissioning process\npreceding the first operation, present optical and radiological focal spot\ncharacterization methods, and dosimetric measurements. Additionally, we report\non first preclinical in vitro cell and in vivo mouse brain irradiations\nconducted with the LFXT prototype. The focal spot characterization resulted in\na strongly eccentric electron distribution with a width of 72.3 $\\mu$m.\nDosimetry showed sharp microbeam dose profiles with steep lateral penumbras and\na peak-to-valley dose ratio above 10 throughout a 70 mm thick PMMA phantom. An\nopen-field dose rate of 4.3 Gy/s was measured at an acceleration voltage of 150\nkV and a beam current of 17.4 mA at 150 mm distance from the focal spot. In\nvitro and in vivo experiments demonstrated the feasibility of the LFXT for\nminibeam and microbeam applications with field sizes of 1.5-2 cm. The mice\ndisplayed no observable side effects throughout the follow-up period after\nwhole-brain 260 $\\mu$m-minibeam irradiation. We successfully constructed and\ncommissioned the first proof-of-concept LFXT prototype. Dosimetric\ncharacterizations of the achieved microbeam field showed the superiority of the\nLFXT compared to conventional X-ray tubes in terms of beam quality. In future\ndevelopments, the remaining limitations of the prototype will be addressed for\nimproved minibeam and first ever microbeam radiation therapy in a clinical\nsetting."
                },
                "authors": [
                    {
                        "name": "Christian Petrich"
                    },
                    {
                        "name": "Johanna Winter"
                    },
                    {
                        "name": "Anton Dimroth"
                    },
                    {
                        "name": "Thomas Beiser"
                    },
                    {
                        "name": "Monika Dehn"
                    },
                    {
                        "name": "Jessica Stolz"
                    },
                    {
                        "name": "Jacopo Frignani"
                    },
                    {
                        "name": "Stephanie E. Combs"
                    },
                    {
                        "name": "Franz Schilling"
                    },
                    {
                        "name": "Ghaleb Natour"
                    },
                    {
                        "name": "Kurt Aulenbacher"
                    },
                    {
                        "name": "Thomas E. Schmid"
                    },
                    {
                        "name": "Jan J. Wilkens"
                    },
                    {
                        "name": "Stefan Bartzsch"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Bartzsch"
                },
                "author": "Stefan Bartzsch",
                "arxiv_comment": "CP, JW, and AD share first authorship",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09536v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09536v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09720v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09720v2",
                "updated": "2025-06-11T06:01:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    6,
                    1,
                    15,
                    2,
                    162,
                    0
                ],
                "published": "2025-02-13T19:11:40Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    11,
                    40,
                    3,
                    44,
                    0
                ],
                "title": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs"
                },
                "summary": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent works have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Metas SpinQuant (perplexity 7.3), OstQuant (7.3) and QuaRot\n(8.2). Comparisons on bigger models (up to 70B) and on various LLM evaluation\nbenchmarks confirm uniform superiority of NestQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent works have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Metas SpinQuant (perplexity 7.3), OstQuant (7.3) and QuaRot\n(8.2). Comparisons on bigger models (up to 70B) and on various LLM evaluation\nbenchmarks confirm uniform superiority of NestQuant."
                },
                "authors": [
                    {
                        "name": "Semyon Savkin"
                    },
                    {
                        "name": "Eitan Porat"
                    },
                    {
                        "name": "Or Ordentlich"
                    },
                    {
                        "name": "Yury Polyanskiy"
                    }
                ],
                "author_detail": {
                    "name": "Yury Polyanskiy"
                },
                "author": "Yury Polyanskiy",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09720v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09720v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07564v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07564v3",
                "updated": "2025-06-11T03:14:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    3,
                    14,
                    10,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-09T09:04:37Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    9,
                    4,
                    37,
                    0,
                    160,
                    0
                ],
                "title": "SAFEFLOW: A Principled Protocol for Trustworthy and Transactional\n  Autonomous Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAFEFLOW: A Principled Protocol for Trustworthy and Transactional\n  Autonomous Agent Systems"
                },
                "summary": "Recent advances in large language models (LLMs) and vision-language models\n(VLMs) have enabled powerful autonomous agents capable of complex reasoning and\nmulti-modal tool use. Despite their growing capabilities, today's agent\nframeworks remain fragile, lacking principled mechanisms for secure information\nflow, reliability, and multi-agent coordination. In this work, we introduce\nSAFEFLOW, a new protocol-level framework for building trustworthy LLM/VLM-based\nagents. SAFEFLOW enforces fine-grained information flow control (IFC),\nprecisely tracking provenance, integrity, and confidentiality of all the data\nexchanged between agents, tools, users, and environments. By constraining LLM\nreasoning to respect these security labels, SAFEFLOW prevents untrusted or\nadversarial inputs from contaminating high-integrity decisions. To ensure\nrobustness in concurrent multi-agent settings, SAFEFLOW introduces\ntransactional execution, conflict resolution, and secure scheduling over shared\nstate, preserving global consistency across agents. We further introduce\nmechanisms, including write-ahead logging, rollback, and secure caches, that\nfurther enhance resilience against runtime errors and policy violations. To\nvalidate the performances, we built SAFEFLOWBENCH, a comprehensive benchmark\nsuite designed to evaluate agent reliability under adversarial, noisy, and\nconcurrent operational conditions. Extensive experiments demonstrate that\nagents built with SAFEFLOW maintain impressive task performance and security\nguarantees even in hostile environments, substantially outperforming\nstate-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for\nprincipled, robust, and secure agent ecosystems, advancing the frontier of\nreliable autonomy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) and vision-language models\n(VLMs) have enabled powerful autonomous agents capable of complex reasoning and\nmulti-modal tool use. Despite their growing capabilities, today's agent\nframeworks remain fragile, lacking principled mechanisms for secure information\nflow, reliability, and multi-agent coordination. In this work, we introduce\nSAFEFLOW, a new protocol-level framework for building trustworthy LLM/VLM-based\nagents. SAFEFLOW enforces fine-grained information flow control (IFC),\nprecisely tracking provenance, integrity, and confidentiality of all the data\nexchanged between agents, tools, users, and environments. By constraining LLM\nreasoning to respect these security labels, SAFEFLOW prevents untrusted or\nadversarial inputs from contaminating high-integrity decisions. To ensure\nrobustness in concurrent multi-agent settings, SAFEFLOW introduces\ntransactional execution, conflict resolution, and secure scheduling over shared\nstate, preserving global consistency across agents. We further introduce\nmechanisms, including write-ahead logging, rollback, and secure caches, that\nfurther enhance resilience against runtime errors and policy violations. To\nvalidate the performances, we built SAFEFLOWBENCH, a comprehensive benchmark\nsuite designed to evaluate agent reliability under adversarial, noisy, and\nconcurrent operational conditions. Extensive experiments demonstrate that\nagents built with SAFEFLOW maintain impressive task performance and security\nguarantees even in hostile environments, substantially outperforming\nstate-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for\nprincipled, robust, and secure agent ecosystems, advancing the frontier of\nreliable autonomy."
                },
                "authors": [
                    {
                        "name": "Peiran Li"
                    },
                    {
                        "name": "Xinkai Zou"
                    },
                    {
                        "name": "Zhuohang Wu"
                    },
                    {
                        "name": "Ruifeng Li"
                    },
                    {
                        "name": "Shuo Xing"
                    },
                    {
                        "name": "Hanwen Zheng"
                    },
                    {
                        "name": "Zhikai Hu"
                    },
                    {
                        "name": "Yuping Wang"
                    },
                    {
                        "name": "Haoxi Li"
                    },
                    {
                        "name": "Qin Yuan"
                    },
                    {
                        "name": "Yingmo Zhang"
                    },
                    {
                        "name": "Zhengzhong Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhengzhong Tu"
                },
                "author": "Zhengzhong Tu",
                "arxiv_comment": "Former versions either contain unrelated content or cannot be\n  properly converted to PDF",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07564v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07564v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09350v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09350v1",
                "updated": "2025-06-11T03:04:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    3,
                    4,
                    23,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T03:04:23Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    3,
                    4,
                    23,
                    2,
                    162,
                    0
                ],
                "title": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video\n  Generation"
                },
                "summary": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2"
                },
                "authors": [
                    {
                        "name": "Shanchuan Lin"
                    },
                    {
                        "name": "Ceyuan Yang"
                    },
                    {
                        "name": "Hao He"
                    },
                    {
                        "name": "Jianwen Jiang"
                    },
                    {
                        "name": "Yuxi Ren"
                    },
                    {
                        "name": "Xin Xia"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Xuefeng Xiao"
                    },
                    {
                        "name": "Lu Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Jiang"
                },
                "author": "Lu Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09350v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09350v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09342v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09342v1",
                "updated": "2025-06-11T02:48:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    2,
                    48,
                    16,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T02:48:16Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    2,
                    48,
                    16,
                    2,
                    162,
                    0
                ],
                "title": "Latent Multi-Head Attention for Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent Multi-Head Attention for Small Language Models"
                },
                "summary": "We present the first comprehensive study of latent multi-head attention (MLA)\nfor small language models, revealing interesting efficiency-quality trade-offs.\nTraining 30M-parameter GPT models on 100,000 synthetic stories, we benchmark\nthree architectural variants: standard multi-head attention (MHA), MLA, and MLA\nwith rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE\nwith half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory\nreduction while incurring only a 0.3% increase in validation loss (essentially\nmatching MHA quality)- a Pareto improvement for memory constrained deployment.\nWe further show that RoPE is crucial for MLA in small models: without it, MLA\nunderperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by\n2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2\nachieves a 1.4 times speedup over full-rank MLA while maintaining the memory\nsavings. GPT-4 evaluations corroborate perplexity results, with ours achieving\nthe highest quality scores (7.4/10) across grammar, creativity, and consistency\nmetrics. Code and models will be released upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the first comprehensive study of latent multi-head attention (MLA)\nfor small language models, revealing interesting efficiency-quality trade-offs.\nTraining 30M-parameter GPT models on 100,000 synthetic stories, we benchmark\nthree architectural variants: standard multi-head attention (MHA), MLA, and MLA\nwith rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE\nwith half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory\nreduction while incurring only a 0.3% increase in validation loss (essentially\nmatching MHA quality)- a Pareto improvement for memory constrained deployment.\nWe further show that RoPE is crucial for MLA in small models: without it, MLA\nunderperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by\n2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2\nachieves a 1.4 times speedup over full-rank MLA while maintaining the memory\nsavings. GPT-4 evaluations corroborate perplexity results, with ours achieving\nthe highest quality scores (7.4/10) across grammar, creativity, and consistency\nmetrics. Code and models will be released upon acceptance."
                },
                "authors": [
                    {
                        "name": "Sushant Mehta"
                    },
                    {
                        "name": "Raj Dandekar"
                    },
                    {
                        "name": "Rajat Dandekar"
                    },
                    {
                        "name": "Sreedath Panat"
                    }
                ],
                "author_detail": {
                    "name": "Sreedath Panat"
                },
                "author": "Sreedath Panat",
                "arxiv_comment": "6 pages, 1 figure. 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09342v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09342v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04593v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04593v2",
                "updated": "2025-06-11T02:24:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    2,
                    24,
                    7,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-05T03:16:51Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    3,
                    16,
                    51,
                    3,
                    156,
                    0
                ],
                "title": "Federated Learning Assisted Edge Caching Scheme Based on Lightweight\n  Architecture DDPM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning Assisted Edge Caching Scheme Based on Lightweight\n  Architecture DDPM"
                },
                "summary": "Edge caching is an emerging technology that empowers caching units at edge\nnodes, allowing users to fetch contents of interest that have been pre-cached\nat the edge nodes. The key to pre-caching is to maximize the cache hit\npercentage for cached content without compromising users' privacy. In this\nletter, we propose a federated learning (FL) assisted edge caching scheme based\non lightweight architecture denoising diffusion probabilistic model (LDPM). Our\nsimulation results verify that our proposed scheme achieves a higher cache hit\npercentage compared to existing FL-based methods and baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge caching is an emerging technology that empowers caching units at edge\nnodes, allowing users to fetch contents of interest that have been pre-cached\nat the edge nodes. The key to pre-caching is to maximize the cache hit\npercentage for cached content without compromising users' privacy. In this\nletter, we propose a federated learning (FL) assisted edge caching scheme based\non lightweight architecture denoising diffusion probabilistic model (LDPM). Our\nsimulation results verify that our proposed scheme achieves a higher cache hit\npercentage compared to existing FL-based methods and baseline methods."
                },
                "authors": [
                    {
                        "name": "Xun Li"
                    },
                    {
                        "name": "Qiong Wu"
                    },
                    {
                        "name": "Pingyi Fan"
                    },
                    {
                        "name": "Kezhi Wang"
                    },
                    {
                        "name": "Nan Cheng"
                    },
                    {
                        "name": "Khaled B. Letaief"
                    }
                ],
                "author_detail": {
                    "name": "Khaled B. Letaief"
                },
                "author": "Khaled B. Letaief",
                "arxiv_comment": "This paper has been submitted to IEEE letters. The source code has\n  been released at:\n  https://github.com/qiongwu86/Federated-Learning-Assisted-Edge-Caching-Scheme-Based-on-Lightweight-Architecture-DDPM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04593v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04593v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09282v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09282v1",
                "updated": "2025-06-10T22:46:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    10,
                    22,
                    46,
                    12,
                    1,
                    161,
                    0
                ],
                "published": "2025-06-10T22:46:12Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    22,
                    46,
                    12,
                    1,
                    161,
                    0
                ],
                "title": "ScalableHD: Scalable and High-Throughput Hyperdimensional Computing\n  Inference on Multi-Core CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ScalableHD: Scalable and High-Throughput Hyperdimensional Computing\n  Inference on Multi-Core CPUs"
                },
                "summary": "Hyperdimensional Computing (HDC) is a brain-inspired computing paradigm that\nrepresents and manipulates information using high-dimensional vectors, called\nhypervectors (HV). Traditional HDC methods, while robust to noise and\ninherently parallel, rely on single-pass, non-parametric training and often\nsuffer from low accuracy. To address this, recent approaches adopt iterative\ntraining of base and class HVs, typically accelerated on GPUs. Inference,\nhowever, remains lightweight and well-suited for real-time execution. Yet,\nefficient HDC inference has been studied almost exclusively on specialized\nhardware such as FPGAs and GPUs, with limited attention to general-purpose\nmulti-core CPUs. To address this gap, we propose ScalableHD for scalable and\nhigh-throughput HDC inference on multi-core CPUs. ScalableHD employs a\ntwo-stage pipelined execution model, where each stage is parallelized across\ncores and processes chunks of base and class HVs. Intermediate results are\nstreamed between stages using a producer-consumer mechanism, enabling\non-the-fly consumption and improving cache locality. To maximize performance,\nScalableHD integrates memory tiling and NUMA-aware worker-to-core binding.\nFurther, it features two execution variants tailored for small and large batch\nsizes, each designed to exploit compute parallelism based on workload\ncharacteristics while mitigating the memory-bound compute pattern that limits\nHDC inference performance on modern multi-core CPUs. ScalableHD achieves up to\n10x speedup in throughput (samples per second) over state-of-the-art baselines\nsuch as TorchHD, across a diverse set of tasks ranging from human activity\nrecognition to image classification, while preserving task accuracy.\nFurthermore, ScalableHD exhibits robust scalability: increasing the number of\ncores yields near-proportional throughput improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hyperdimensional Computing (HDC) is a brain-inspired computing paradigm that\nrepresents and manipulates information using high-dimensional vectors, called\nhypervectors (HV). Traditional HDC methods, while robust to noise and\ninherently parallel, rely on single-pass, non-parametric training and often\nsuffer from low accuracy. To address this, recent approaches adopt iterative\ntraining of base and class HVs, typically accelerated on GPUs. Inference,\nhowever, remains lightweight and well-suited for real-time execution. Yet,\nefficient HDC inference has been studied almost exclusively on specialized\nhardware such as FPGAs and GPUs, with limited attention to general-purpose\nmulti-core CPUs. To address this gap, we propose ScalableHD for scalable and\nhigh-throughput HDC inference on multi-core CPUs. ScalableHD employs a\ntwo-stage pipelined execution model, where each stage is parallelized across\ncores and processes chunks of base and class HVs. Intermediate results are\nstreamed between stages using a producer-consumer mechanism, enabling\non-the-fly consumption and improving cache locality. To maximize performance,\nScalableHD integrates memory tiling and NUMA-aware worker-to-core binding.\nFurther, it features two execution variants tailored for small and large batch\nsizes, each designed to exploit compute parallelism based on workload\ncharacteristics while mitigating the memory-bound compute pattern that limits\nHDC inference performance on modern multi-core CPUs. ScalableHD achieves up to\n10x speedup in throughput (samples per second) over state-of-the-art baselines\nsuch as TorchHD, across a diverse set of tasks ranging from human activity\nrecognition to image classification, while preserving task accuracy.\nFurthermore, ScalableHD exhibits robust scalability: increasing the number of\ncores yields near-proportional throughput improvements."
                },
                "authors": [
                    {
                        "name": "Dhruv Parikh"
                    },
                    {
                        "name": "Viktor Prasanna"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Prasanna"
                },
                "author": "Viktor Prasanna",
                "arxiv_comment": "IC3",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09282v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09282v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07254v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07254v2",
                "updated": "2025-06-10T22:01:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    10,
                    22,
                    1,
                    14,
                    1,
                    161,
                    0
                ],
                "published": "2025-06-08T18:43:31Z",
                "published_parsed": [
                    2025,
                    6,
                    8,
                    18,
                    43,
                    31,
                    6,
                    159,
                    0
                ],
                "title": "A Stable Whitening Optimizer for Efficient Neural Network Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Stable Whitening Optimizer for Efficient Neural Network Training"
                },
                "summary": "In this work, we take an experimentally grounded look at neural network\noptimization. Building on the Shampoo family of algorithms, we identify and\nalleviate three key issues, resulting in the proposed SPlus method. First, we\nfind that naive Shampoo is prone to divergence when matrix-inverses are cached\nfor long periods. We introduce an alternate bounded update combining a\nhistorical eigenbasis with instantaneous normalization, resulting in\nacross-the-board stability and significantly lower computational requirements.\nSecond, we adapt a shape-aware scaling to enable learning rate transfer across\nnetwork width. Third, we find that high learning rates result in large\nparameter noise, and propose a simple iterate-averaging scheme which unblocks\nfaster learning. To properly confirm these findings, we introduce a pointed\nTransformer training benchmark, considering three objectives (language\nmodelling, image classification, and diffusion modelling) across different\nstages of training. On average, SPlus is able to reach the validation\nperformance of Adam within 44% of the gradient steps and 62% of the wallclock\ntime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we take an experimentally grounded look at neural network\noptimization. Building on the Shampoo family of algorithms, we identify and\nalleviate three key issues, resulting in the proposed SPlus method. First, we\nfind that naive Shampoo is prone to divergence when matrix-inverses are cached\nfor long periods. We introduce an alternate bounded update combining a\nhistorical eigenbasis with instantaneous normalization, resulting in\nacross-the-board stability and significantly lower computational requirements.\nSecond, we adapt a shape-aware scaling to enable learning rate transfer across\nnetwork width. Third, we find that high learning rates result in large\nparameter noise, and propose a simple iterate-averaging scheme which unblocks\nfaster learning. To properly confirm these findings, we introduce a pointed\nTransformer training benchmark, considering three objectives (language\nmodelling, image classification, and diffusion modelling) across different\nstages of training. On average, SPlus is able to reach the validation\nperformance of Adam within 44% of the gradient steps and 62% of the wallclock\ntime."
                },
                "authors": [
                    {
                        "name": "Kevin Frans"
                    },
                    {
                        "name": "Sergey Levine"
                    },
                    {
                        "name": "Pieter Abbeel"
                    }
                ],
                "author_detail": {
                    "name": "Pieter Abbeel"
                },
                "author": "Pieter Abbeel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07254v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07254v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09045v1",
                "updated": "2025-06-10T17:59:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    10,
                    17,
                    59,
                    2,
                    1,
                    161,
                    0
                ],
                "published": "2025-06-10T17:59:02Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    17,
                    59,
                    2,
                    1,
                    161,
                    0
                ],
                "title": "MagCache: Fast Video Generation with Magnitude-Aware Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagCache: Fast Video Generation with Magnitude-Aware Cache"
                },
                "summary": "Existing acceleration techniques for video diffusion models often rely on\nuniform heuristics or time-embedding variants to skip timesteps and reuse\ncached features. These approaches typically require extensive calibration with\ncurated prompts and risk inconsistent outputs due to prompt-specific\noverfitting. In this paper, we introduce a novel and robust discovery: a\nunified magnitude law observed across different models and prompts.\nSpecifically, the magnitude ratio of successive residual outputs decreases\nmonotonically and steadily in most timesteps while rapidly in the last several\nsteps. Leveraging this insight, we introduce a Magnitude-aware Cache (MagCache)\nthat adaptively skips unimportant timesteps using an error modeling mechanism\nand adaptive caching strategy. Unlike existing methods requiring dozens of\ncurated samples for calibration, MagCache only requires a single sample for\ncalibration. Experimental results show that MagCache achieves 2.1x and 2.68x\nspeedups on Open-Sora and Wan 2.1, respectively, while preserving superior\nvisual fidelity. It significantly outperforms existing methods in LPIPS, SSIM,\nand PSNR, under comparable computational budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing acceleration techniques for video diffusion models often rely on\nuniform heuristics or time-embedding variants to skip timesteps and reuse\ncached features. These approaches typically require extensive calibration with\ncurated prompts and risk inconsistent outputs due to prompt-specific\noverfitting. In this paper, we introduce a novel and robust discovery: a\nunified magnitude law observed across different models and prompts.\nSpecifically, the magnitude ratio of successive residual outputs decreases\nmonotonically and steadily in most timesteps while rapidly in the last several\nsteps. Leveraging this insight, we introduce a Magnitude-aware Cache (MagCache)\nthat adaptively skips unimportant timesteps using an error modeling mechanism\nand adaptive caching strategy. Unlike existing methods requiring dozens of\ncurated samples for calibration, MagCache only requires a single sample for\ncalibration. Experimental results show that MagCache achieves 2.1x and 2.68x\nspeedups on Open-Sora and Wan 2.1, respectively, while preserving superior\nvisual fidelity. It significantly outperforms existing methods in LPIPS, SSIM,\nand PSNR, under comparable computational budgets."
                },
                "authors": [
                    {
                        "name": "Zehong Ma"
                    },
                    {
                        "name": "Longhui Wei"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Shiliang Zhang"
                    },
                    {
                        "name": "Qi Tian"
                    }
                ],
                "author_detail": {
                    "name": "Qi Tian"
                },
                "author": "Qi Tian",
                "arxiv_comment": "Project Page: https://zehong-ma.github.io/MagCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08842v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08842v1",
                "updated": "2025-06-10T14:29:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    10,
                    14,
                    29,
                    2,
                    1,
                    161,
                    0
                ],
                "published": "2025-06-10T14:29:02Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    14,
                    29,
                    2,
                    1,
                    161,
                    0
                ],
                "title": "STI-SNN: A 0.14 GOPS/W/PE Single-Timestep Inference FPGA-based SNN\n  Accelerator with Algorithm and Hardware Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STI-SNN: A 0.14 GOPS/W/PE Single-Timestep Inference FPGA-based SNN\n  Accelerator with Algorithm and Hardware Co-Design"
                },
                "summary": "Brain-inspired Spiking Neural Networks (SNNs) have attracted attention for\ntheir event-driven characteristics and high energy efficiency. However, the\ntemporal dependency and irregularity of spikes present significant challenges\nfor hardware parallel processing and data reuse, leading to some existing\naccelerators falling short in processing latency and energy efficiency. To\novercome these challenges, we introduce the STI-SNN accelerator, designed for\nresource-constrained applications with high energy efficiency, flexibility, and\nlow latency. The accelerator is designed through algorithm and hardware\nco-design. Firstly, STI-SNN can perform inference in a single timestep. At the\nalgorithm level, we introduce a temporal pruning approach based on the temporal\nefficient training (TET) loss function. This approach alleviates spike\ndisappearance during timestep reduction, maintains inference accuracy, and\nexpands TET's application. In hardware design, we analyze data access patterns\nand adopt the output stationary (OS) dataflow, eliminating the need to store\nmembrane potentials and access memory operations. Furthermore, based on the OS\ndataflow, we propose a compressed and sorted representation of spikes, then\ncached in the line buffer to reduce the memory access cost and improve reuse\nefficiency. Secondly, STI-SNN supports different convolution methods. By\nadjusting the computation mode of processing elements (PEs) and parameterizing\nthe computation array, STI-SNN can accommodate lightweight models based on\ndepthwise separable convolutions (DSCs), further enhancing hardware\nflexibility. Lastly, STI-SNN also supports both inter-layer and intra-layer\nparallel processing. For inter-layer parallelism, we ...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Brain-inspired Spiking Neural Networks (SNNs) have attracted attention for\ntheir event-driven characteristics and high energy efficiency. However, the\ntemporal dependency and irregularity of spikes present significant challenges\nfor hardware parallel processing and data reuse, leading to some existing\naccelerators falling short in processing latency and energy efficiency. To\novercome these challenges, we introduce the STI-SNN accelerator, designed for\nresource-constrained applications with high energy efficiency, flexibility, and\nlow latency. The accelerator is designed through algorithm and hardware\nco-design. Firstly, STI-SNN can perform inference in a single timestep. At the\nalgorithm level, we introduce a temporal pruning approach based on the temporal\nefficient training (TET) loss function. This approach alleviates spike\ndisappearance during timestep reduction, maintains inference accuracy, and\nexpands TET's application. In hardware design, we analyze data access patterns\nand adopt the output stationary (OS) dataflow, eliminating the need to store\nmembrane potentials and access memory operations. Furthermore, based on the OS\ndataflow, we propose a compressed and sorted representation of spikes, then\ncached in the line buffer to reduce the memory access cost and improve reuse\nefficiency. Secondly, STI-SNN supports different convolution methods. By\nadjusting the computation mode of processing elements (PEs) and parameterizing\nthe computation array, STI-SNN can accommodate lightweight models based on\ndepthwise separable convolutions (DSCs), further enhancing hardware\nflexibility. Lastly, STI-SNN also supports both inter-layer and intra-layer\nparallel processing. For inter-layer parallelism, we ..."
                },
                "authors": [
                    {
                        "name": "Kainan Wang"
                    },
                    {
                        "name": "Chengyi Yang"
                    },
                    {
                        "name": "Chengting Yu"
                    },
                    {
                        "name": "Yee Sin Ang"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Aili Wang"
                    }
                ],
                "author_detail": {
                    "name": "Aili Wang"
                },
                "author": "Aili Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08842v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08842v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12397v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12397v4",
                "updated": "2025-06-10T13:50:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    10,
                    13,
                    50,
                    34,
                    1,
                    161,
                    0
                ],
                "published": "2025-04-16T18:03:21Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    18,
                    3,
                    21,
                    2,
                    106,
                    0
                ],
                "title": "Activated LoRA: Fine-tuned LLMs for Intrinsics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activated LoRA: Fine-tuned LLMs for Intrinsics"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is inefficient, as the key-value (KV) cache of the entire\nturn history must be recomputed with the LoRA weights before generation can\nbegin. To address this problem, we propose Activated LoRA (aLoRA), an adapter\narchitecture which modifies the LoRA framework to only adapt weights for the\ntokens in the sequence \\emph{after} the aLoRA is invoked. This change crucially\nallows aLoRA to accept the base model's KV cache of the input string, meaning\nthat aLoRA can be instantly activated whenever needed in a chain without\nrecomputing the cache. This enables building what we call \\emph{intrinsics},\ni.e. specialized models invoked to perform well-defined operations on portions\nof an input chain or conversation that otherwise uses the base model by\ndefault. We train a set of aLoRA-based intrinsics models, demonstrating\ncompetitive accuracy with standard LoRA while achieving significant inference\nbenefits. The codebase is at https://github.com/IBM/activated-lora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is inefficient, as the key-value (KV) cache of the entire\nturn history must be recomputed with the LoRA weights before generation can\nbegin. To address this problem, we propose Activated LoRA (aLoRA), an adapter\narchitecture which modifies the LoRA framework to only adapt weights for the\ntokens in the sequence \\emph{after} the aLoRA is invoked. This change crucially\nallows aLoRA to accept the base model's KV cache of the input string, meaning\nthat aLoRA can be instantly activated whenever needed in a chain without\nrecomputing the cache. This enables building what we call \\emph{intrinsics},\ni.e. specialized models invoked to perform well-defined operations on portions\nof an input chain or conversation that otherwise uses the base model by\ndefault. We train a set of aLoRA-based intrinsics models, demonstrating\ncompetitive accuracy with standard LoRA while achieving significant inference\nbenefits. The codebase is at https://github.com/IBM/activated-lora."
                },
                "authors": [
                    {
                        "name": "Kristjan Greenewald"
                    },
                    {
                        "name": "Luis Lastras"
                    },
                    {
                        "name": "Thomas Parnell"
                    },
                    {
                        "name": "Vraj Shah"
                    },
                    {
                        "name": "Lucian Popa"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Chulaka Gunasekara"
                    },
                    {
                        "name": "Ambrish Rawat"
                    },
                    {
                        "name": "David Cox"
                    }
                ],
                "author_detail": {
                    "name": "David Cox"
                },
                "author": "David Cox",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12397v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12397v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08529v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08529v1",
                "updated": "2025-06-10T07:49:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    10,
                    7,
                    49,
                    33,
                    1,
                    161,
                    0
                ],
                "published": "2025-06-10T07:49:33Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    7,
                    49,
                    33,
                    1,
                    161,
                    0
                ],
                "title": "LiftVSR: Lifting Image Diffusion to Video Super-Resolution via Hybrid\n  Temporal Modeling with Only 4$\\times$RTX 4090s",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiftVSR: Lifting Image Diffusion to Video Super-Resolution via Hybrid\n  Temporal Modeling with Only 4$\\times$RTX 4090s"
                },
                "summary": "Diffusion models have significantly advanced video super-resolution (VSR) by\nenhancing perceptual quality, largely through elaborately designed temporal\nmodeling to ensure inter-frame consistency. However, existing methods usually\nsuffer from limited temporal coherence and prohibitively high computational\ncosts (e.g., typically requiring over 8 NVIDIA A100-80G GPUs), especially for\nlong videos. In this work, we propose LiftVSR, an efficient VSR framework that\nleverages and elevates the image-wise diffusion prior from PixArt-$\\alpha$,\nachieving state-of-the-art results using only 4$\\times$RTX 4090 GPUs. To\nbalance long-term consistency and efficiency, we introduce a hybrid temporal\nmodeling mechanism that decomposes temporal learning into two complementary\ncomponents: (i) Dynamic Temporal Attention (DTA) for fine-grained temporal\nmodeling within short frame segment ($\\textit{i.e.}$, low complexity), and (ii)\nAttention Memory Cache (AMC) for long-term temporal modeling across segments\n($\\textit{i.e.}$, consistency). Specifically, DTA identifies multiple token\nflows across frames within multi-head query and key tokens to warp inter-frame\ncontexts in the value tokens. AMC adaptively aggregates historical segment\ninformation via a cache unit, ensuring long-term coherence with minimal\noverhead. To further stabilize the cache interaction during inference, we\nintroduce an asymmetric sampling strategy that mitigates feature mismatches\narising from different diffusion sampling steps. Extensive experiments on\nseveral typical VSR benchmarks have demonstrated that LiftVSR achieves\nimpressive performance with significantly lower computational costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have significantly advanced video super-resolution (VSR) by\nenhancing perceptual quality, largely through elaborately designed temporal\nmodeling to ensure inter-frame consistency. However, existing methods usually\nsuffer from limited temporal coherence and prohibitively high computational\ncosts (e.g., typically requiring over 8 NVIDIA A100-80G GPUs), especially for\nlong videos. In this work, we propose LiftVSR, an efficient VSR framework that\nleverages and elevates the image-wise diffusion prior from PixArt-$\\alpha$,\nachieving state-of-the-art results using only 4$\\times$RTX 4090 GPUs. To\nbalance long-term consistency and efficiency, we introduce a hybrid temporal\nmodeling mechanism that decomposes temporal learning into two complementary\ncomponents: (i) Dynamic Temporal Attention (DTA) for fine-grained temporal\nmodeling within short frame segment ($\\textit{i.e.}$, low complexity), and (ii)\nAttention Memory Cache (AMC) for long-term temporal modeling across segments\n($\\textit{i.e.}$, consistency). Specifically, DTA identifies multiple token\nflows across frames within multi-head query and key tokens to warp inter-frame\ncontexts in the value tokens. AMC adaptively aggregates historical segment\ninformation via a cache unit, ensuring long-term coherence with minimal\noverhead. To further stabilize the cache interaction during inference, we\nintroduce an asymmetric sampling strategy that mitigates feature mismatches\narising from different diffusion sampling steps. Extensive experiments on\nseveral typical VSR benchmarks have demonstrated that LiftVSR achieves\nimpressive performance with significantly lower computational costs."
                },
                "authors": [
                    {
                        "name": "Xijun Wang"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Bingchen Li"
                    },
                    {
                        "name": "Zhibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhibo Chen"
                },
                "author": "Zhibo Chen",
                "arxiv_comment": "Project page: https://kopperx.github.io/projects/liftvsr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08529v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08529v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08373v1",
                "updated": "2025-06-10T02:37:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    10,
                    2,
                    37,
                    46,
                    1,
                    161,
                    0
                ],
                "published": "2025-06-10T02:37:46Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    2,
                    37,
                    46,
                    1,
                    161,
                    0
                ],
                "title": "Draft-based Approximate Inference for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Draft-based Approximate Inference for LLMs"
                },
                "summary": "Optimizing inference for long-context Large Language Models (LLMs) is\nincreasingly important due to the quadratic compute and linear memory\ncomplexity of Transformers. Existing approximation methods, such as key-value\n(KV) cache dropping, sparse attention, and prompt compression, typically rely\non rough predictions of token or KV pair importance. We propose a novel\nframework for approximate LLM inference that leverages small draft models to\nmore accurately predict the importance of tokens and KV pairs. Specifically, we\nintroduce two instantiations of our proposed framework: (i) SpecKV, which\nleverages a draft output to accurately assess the importance of each KV pair\nfor more effective KV cache dropping, and (ii) SpecPC, which uses the draft\nmodel's attention activations to identify and discard unimportant prompt\ntokens. To the best of our knowledge, this is the first work to use draft\nmodels for approximate LLM inference acceleration, extending their utility\nbeyond traditional lossless speculative decoding. We motivate our methods with\ntheoretical and empirical analyses, and show a strong correlation between the\nattention patterns of draft and target models. Extensive experiments on\nlong-context benchmarks show that our methods consistently achieve higher\naccuracy than existing baselines, while preserving the same improvements in\nmemory usage, latency, and throughput. Our code is available at\nhttps://github.com/furiosa-ai/draft-based-approx-llm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing inference for long-context Large Language Models (LLMs) is\nincreasingly important due to the quadratic compute and linear memory\ncomplexity of Transformers. Existing approximation methods, such as key-value\n(KV) cache dropping, sparse attention, and prompt compression, typically rely\non rough predictions of token or KV pair importance. We propose a novel\nframework for approximate LLM inference that leverages small draft models to\nmore accurately predict the importance of tokens and KV pairs. Specifically, we\nintroduce two instantiations of our proposed framework: (i) SpecKV, which\nleverages a draft output to accurately assess the importance of each KV pair\nfor more effective KV cache dropping, and (ii) SpecPC, which uses the draft\nmodel's attention activations to identify and discard unimportant prompt\ntokens. To the best of our knowledge, this is the first work to use draft\nmodels for approximate LLM inference acceleration, extending their utility\nbeyond traditional lossless speculative decoding. We motivate our methods with\ntheoretical and empirical analyses, and show a strong correlation between the\nattention patterns of draft and target models. Extensive experiments on\nlong-context benchmarks show that our methods consistently achieve higher\naccuracy than existing baselines, while preserving the same improvements in\nmemory usage, latency, and throughput. Our code is available at\nhttps://github.com/furiosa-ai/draft-based-approx-llm."
                },
                "authors": [
                    {
                        "name": "Kevin Galim"
                    },
                    {
                        "name": "Ethan Ewer"
                    },
                    {
                        "name": "Wonjun Kang"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Hyung Il Koo"
                    },
                    {
                        "name": "Kangwook Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kangwook Lee"
                },
                "author": "Kangwook Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08161v1",
                "updated": "2025-06-09T19:13:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    19,
                    13,
                    16,
                    0,
                    160,
                    0
                ],
                "published": "2025-06-09T19:13:16Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    19,
                    13,
                    16,
                    0,
                    160,
                    0
                ],
                "title": "GATE: Geometry-Aware Trained Encoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GATE: Geometry-Aware Trained Encoding"
                },
                "summary": "The encoding of input parameters is one of the fundamental building blocks of\nneural network algorithms. Its goal is to map the input data to a\nhigher-dimensional space, typically supported by trained feature vectors. The\nmapping is crucial for the efficiency and approximation quality of neural\nnetworks. We propose a novel geometry-aware encoding called GATE that stores\nfeature vectors on the surface of triangular meshes. Our encoding is suitable\nfor neural rendering-related algorithms, for example, neural radiance caching.\nIt also avoids limitations of previous hash-based encoding schemes, such as\nhash collisions, selection of resolution versus scene size, and divergent\nmemory access. Our approach decouples feature vector density from geometry\ndensity using mesh colors, while allowing for finer control over neural network\ntraining and adaptive level-of-detail.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The encoding of input parameters is one of the fundamental building blocks of\nneural network algorithms. Its goal is to map the input data to a\nhigher-dimensional space, typically supported by trained feature vectors. The\nmapping is crucial for the efficiency and approximation quality of neural\nnetworks. We propose a novel geometry-aware encoding called GATE that stores\nfeature vectors on the surface of triangular meshes. Our encoding is suitable\nfor neural rendering-related algorithms, for example, neural radiance caching.\nIt also avoids limitations of previous hash-based encoding schemes, such as\nhash collisions, selection of resolution versus scene size, and divergent\nmemory access. Our approach decouples feature vector density from geometry\ndensity using mesh colors, while allowing for finer control over neural network\ntraining and adaptive level-of-detail."
                },
                "authors": [
                    {
                        "name": "Jakub Bokansk"
                    },
                    {
                        "name": "Daniel Meister"
                    },
                    {
                        "name": "Carsten Benthin"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Benthin"
                },
                "author": "Carsten Benthin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08009v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08009v1",
                "updated": "2025-06-09T17:59:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    17,
                    59,
                    55,
                    0,
                    160,
                    0
                ],
                "published": "2025-06-09T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    17,
                    59,
                    55,
                    0,
                    160,
                    0
                ],
                "title": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video\n  Diffusion"
                },
                "summary": "We introduce Self Forcing, a novel training paradigm for autoregressive video\ndiffusion models. It addresses the longstanding issue of exposure bias, where\nmodels trained on ground-truth context must generate sequences conditioned on\ntheir own imperfect outputs during inference. Unlike prior methods that denoise\nfuture frames based on ground-truth context frames, Self Forcing conditions\neach frame's generation on previously self-generated outputs by performing\nautoregressive rollout with key-value (KV) caching during training. This\nstrategy enables supervision through a holistic loss at the video level that\ndirectly evaluates the quality of the entire generated sequence, rather than\nrelying solely on traditional frame-wise objectives. To ensure training\nefficiency, we employ a few-step diffusion model along with a stochastic\ngradient truncation strategy, effectively balancing computational cost and\nperformance. We further introduce a rolling KV cache mechanism that enables\nefficient autoregressive video extrapolation. Extensive experiments demonstrate\nthat our approach achieves real-time streaming video generation with sub-second\nlatency on a single GPU, while matching or even surpassing the generation\nquality of significantly slower and non-causal diffusion models. Project\nwebsite: http://self-forcing.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Self Forcing, a novel training paradigm for autoregressive video\ndiffusion models. It addresses the longstanding issue of exposure bias, where\nmodels trained on ground-truth context must generate sequences conditioned on\ntheir own imperfect outputs during inference. Unlike prior methods that denoise\nfuture frames based on ground-truth context frames, Self Forcing conditions\neach frame's generation on previously self-generated outputs by performing\nautoregressive rollout with key-value (KV) caching during training. This\nstrategy enables supervision through a holistic loss at the video level that\ndirectly evaluates the quality of the entire generated sequence, rather than\nrelying solely on traditional frame-wise objectives. To ensure training\nefficiency, we employ a few-step diffusion model along with a stochastic\ngradient truncation strategy, effectively balancing computational cost and\nperformance. We further introduce a rolling KV cache mechanism that enables\nefficient autoregressive video extrapolation. Extensive experiments demonstrate\nthat our approach achieves real-time streaming video generation with sub-second\nlatency on a single GPU, while matching or even surpassing the generation\nquality of significantly slower and non-causal diffusion models. Project\nwebsite: http://self-forcing.github.io/"
                },
                "authors": [
                    {
                        "name": "Xun Huang"
                    },
                    {
                        "name": "Zhengqi Li"
                    },
                    {
                        "name": "Guande He"
                    },
                    {
                        "name": "Mingyuan Zhou"
                    },
                    {
                        "name": "Eli Shechtman"
                    }
                ],
                "author_detail": {
                    "name": "Eli Shechtman"
                },
                "author": "Eli Shechtman",
                "arxiv_comment": "Project website: http://self-forcing.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08009v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08009v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16886v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16886v2",
                "updated": "2025-06-09T15:31:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    15,
                    31,
                    53,
                    0,
                    160,
                    0
                ],
                "published": "2025-02-24T06:33:39Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    6,
                    33,
                    39,
                    0,
                    55,
                    0
                ],
                "title": "DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal\n  Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal\n  Performance"
                },
                "summary": "To alleviate memory burden during inference of large language models (LLMs),\nnumerous studies have focused on compressing the KV cache by exploring aspects\nsuch as attention sparsity. These techniques are often designed with a\npre-defined KV budget; however, as the optimal budget varies by different input\nlengths and task types, the existence of a fixed budget could result in\ninconsistent performance accepting inputs of diverse domains. To address this\nlimitation, we propose a new KV cache compression objective: to always ensure\nthe full-cache performance regardless of specific inputs, while maximizing KV\ncache pruning as much as possible. To achieve this goal, we introduce a novel\nKV cache compression method dubbed DBudgetKV, which features an attention-based\nmetric to signal when the remaining KV cache is unlikely to match the\nfull-cache performance, then halting the pruning process. Empirical evaluation\nspanning diverse context lengths, task types, and model sizes suggests that our\nmethod achieves lossless KV pruning effectively and robustly, exceeding 25%\ncompression ratio on average. Furthermore, our method is easy to integrate\nwithin LLM inference, not only optimizing memory space, but also showing\nreduced inference time compared to existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To alleviate memory burden during inference of large language models (LLMs),\nnumerous studies have focused on compressing the KV cache by exploring aspects\nsuch as attention sparsity. These techniques are often designed with a\npre-defined KV budget; however, as the optimal budget varies by different input\nlengths and task types, the existence of a fixed budget could result in\ninconsistent performance accepting inputs of diverse domains. To address this\nlimitation, we propose a new KV cache compression objective: to always ensure\nthe full-cache performance regardless of specific inputs, while maximizing KV\ncache pruning as much as possible. To achieve this goal, we introduce a novel\nKV cache compression method dubbed DBudgetKV, which features an attention-based\nmetric to signal when the remaining KV cache is unlikely to match the\nfull-cache performance, then halting the pruning process. Empirical evaluation\nspanning diverse context lengths, task types, and model sizes suggests that our\nmethod achieves lossless KV pruning effectively and robustly, exceeding 25%\ncompression ratio on average. Furthermore, our method is easy to integrate\nwithin LLM inference, not only optimizing memory space, but also showing\nreduced inference time compared to existing methods."
                },
                "authors": [
                    {
                        "name": "Xuanfan Ni"
                    },
                    {
                        "name": "Liyan Xu"
                    },
                    {
                        "name": "Chenyang Lyu"
                    },
                    {
                        "name": "Longyue Wang"
                    },
                    {
                        "name": "Mo Yu"
                    },
                    {
                        "name": "Lemao Liu"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Piji Li"
                    }
                ],
                "author_detail": {
                    "name": "Piji Li"
                },
                "author": "Piji Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16886v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16886v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07703v1",
                "updated": "2025-06-09T12:41:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    12,
                    41,
                    31,
                    0,
                    160,
                    0
                ],
                "published": "2025-06-09T12:41:31Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    12,
                    41,
                    31,
                    0,
                    160,
                    0
                ],
                "title": "$d$-Wave Flat Fermi Surface in Altermagnets Enables Maximum\n  Charge-to-Spin Conversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$d$-Wave Flat Fermi Surface in Altermagnets Enables Maximum\n  Charge-to-Spin Conversion"
                },
                "summary": "Altermagnets combine antiferromagnetic order with ferromagnet-like spin\nsplitting, a duality that unlocks ultrafast spin-dependent responses. This\nunique property creates unprecedented opportunities for spin-current\ngeneration, overcoming the intrinsic limitations of conventional spin-transfer\nand spin-orbit torque approaches in magnetic memory technologies. Here, we\nestablish a fundamental relationship between Fermi surface geometry and\ntime-reversal-odd ($\\mathcal{T}$-odd) spin currents in altermagnets through\ncombined model analysis and first-principles calculations. We demonstrate that\na $d$-wave altermagnet with a flat Fermi surface can achieve a theoretical\nupper limit of charge-to-spin conversion efficiency (CSE) of 100%. This\nmechanism is realized in the newly discovered room-temperature altermagnetic\nmetal KV$_2$O$_2$Se, which exhibits a CSE of $\\sim$78% at the charge neutrality\npoint, nearly double that of RuO$_2$, setting a new record for\n$\\mathcal{T}$-odd CSE. Under electron doping, this efficiency further increases\nto $\\sim$98%, approaching the theoretical limit. Our work advances the\nfundamental understanding of $\\mathcal{T}$-odd spin currents via Fermi surface\ngeometry engineering and provides key insights for developing next-generation\naltermagnet-based memory devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Altermagnets combine antiferromagnetic order with ferromagnet-like spin\nsplitting, a duality that unlocks ultrafast spin-dependent responses. This\nunique property creates unprecedented opportunities for spin-current\ngeneration, overcoming the intrinsic limitations of conventional spin-transfer\nand spin-orbit torque approaches in magnetic memory technologies. Here, we\nestablish a fundamental relationship between Fermi surface geometry and\ntime-reversal-odd ($\\mathcal{T}$-odd) spin currents in altermagnets through\ncombined model analysis and first-principles calculations. We demonstrate that\na $d$-wave altermagnet with a flat Fermi surface can achieve a theoretical\nupper limit of charge-to-spin conversion efficiency (CSE) of 100%. This\nmechanism is realized in the newly discovered room-temperature altermagnetic\nmetal KV$_2$O$_2$Se, which exhibits a CSE of $\\sim$78% at the charge neutrality\npoint, nearly double that of RuO$_2$, setting a new record for\n$\\mathcal{T}$-odd CSE. Under electron doping, this efficiency further increases\nto $\\sim$98%, approaching the theoretical limit. Our work advances the\nfundamental understanding of $\\mathcal{T}$-odd spin currents via Fermi surface\ngeometry engineering and provides key insights for developing next-generation\naltermagnet-based memory devices."
                },
                "authors": [
                    {
                        "name": "Junwen Lai"
                    },
                    {
                        "name": "Tianye Yu"
                    },
                    {
                        "name": "Peitao Liu"
                    },
                    {
                        "name": "Long Liu"
                    },
                    {
                        "name": "Guozhong Xing"
                    },
                    {
                        "name": "Xing-Qiu Chen"
                    },
                    {
                        "name": "Yan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yan Sun"
                },
                "author": "Yan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07639v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07639v1",
                "updated": "2025-06-09T11:04:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    11,
                    4,
                    13,
                    0,
                    160,
                    0
                ],
                "published": "2025-06-09T11:04:13Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    11,
                    4,
                    13,
                    0,
                    160,
                    0
                ],
                "title": "Fast ECoT: Efficient Embodied Chain-of-Thought via Thoughts Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast ECoT: Efficient Embodied Chain-of-Thought via Thoughts Reuse"
                },
                "summary": "Embodied Chain-of-Thought (ECoT) reasoning enhances vision-language-action\n(VLA) models by improving performance and interpretability through intermediate\nreasoning steps. However, its sequential autoregressive token generation\nintroduces significant inference latency, limiting real-time deployment. We\npropose Fast ECoT, an inference-time acceleration method that exploits the\nstructured and repetitive nature of ECoT to (1) cache and reuse high-level\nreasoning across timesteps and (2) parallelise the generation of modular\nreasoning steps. Additionally, we introduce an asynchronous scheduler that\ndecouples reasoning from action decoding, further boosting responsiveness. Fast\nECoT requires no model changes or additional training and integrates easily\ninto existing VLA pipelines. Experiments in both simulation (LIBERO) and\nreal-world robot tasks show up to a 7.5% reduction in latency with comparable\nor improved task success rate and reasoning faithfulness, bringing ECoT\npolicies closer to practical real-time deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied Chain-of-Thought (ECoT) reasoning enhances vision-language-action\n(VLA) models by improving performance and interpretability through intermediate\nreasoning steps. However, its sequential autoregressive token generation\nintroduces significant inference latency, limiting real-time deployment. We\npropose Fast ECoT, an inference-time acceleration method that exploits the\nstructured and repetitive nature of ECoT to (1) cache and reuse high-level\nreasoning across timesteps and (2) parallelise the generation of modular\nreasoning steps. Additionally, we introduce an asynchronous scheduler that\ndecouples reasoning from action decoding, further boosting responsiveness. Fast\nECoT requires no model changes or additional training and integrates easily\ninto existing VLA pipelines. Experiments in both simulation (LIBERO) and\nreal-world robot tasks show up to a 7.5% reduction in latency with comparable\nor improved task success rate and reasoning faithfulness, bringing ECoT\npolicies closer to practical real-time deployment."
                },
                "authors": [
                    {
                        "name": "Zhekai Duan"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Shikai Geng"
                    },
                    {
                        "name": "Gaowen Liu"
                    },
                    {
                        "name": "Joschka Boedecker"
                    },
                    {
                        "name": "Chris Xiaoxuan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Chris Xiaoxuan Lu"
                },
                "author": "Chris Xiaoxuan Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07639v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07639v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14317v2",
                "updated": "2025-06-09T09:48:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    9,
                    48,
                    43,
                    0,
                    160,
                    0
                ],
                "published": "2025-02-20T07:10:43Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    7,
                    10,
                    43,
                    3,
                    51,
                    0
                ],
                "title": "ParallelComp: Parallel Long-Context Compressor for Length Extrapolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParallelComp: Parallel Long-Context Compressor for Length Extrapolation"
                },
                "summary": "Extrapolating ultra-long contexts (text length >128K) remains a major\nchallenge for large language models (LLMs), as most training-free extrapolation\nmethods are not only severely limited by memory bottlenecks, but also suffer\nfrom the attention sink, which restricts their scalability and effectiveness in\npractice. In this work, we propose ParallelComp, a parallel long-context\ncompression method that effectively overcomes the memory bottleneck, enabling\n8B-parameter LLMs to extrapolate from 8K to 128K tokens on a single A100 80GB\nGPU in a training-free setting. ParallelComp splits the input into chunks,\ndynamically evicting redundant chunks and irrelevant tokens, supported by a\nparallel KV cache eviction mechanism. Importantly, we present a systematic\ntheoretical and empirical analysis of attention biases in parallel\nattention-including the attention sink, recency bias, and middle bias-and\nreveal that these biases exhibit distinctive patterns under ultra-long context\nsettings. We further design a KV cache eviction technique to mitigate this\nphenomenon. Experimental results show that ParallelComp enables an 8B model\n(trained on 8K context) to achieve 91.17% of GPT-4's performance under\nultra-long contexts, outperforming closed-source models such as Claude-2 and\nKimi-Chat. We achieve a 1.76x improvement in chunk throughput, thereby\nachieving a 23.50x acceleration in the prefill stage with negligible\nperformance loss and pave the way for scalable and robust ultra-long contexts\nextrapolation in LLMs. We release the code at\nhttps://github.com/menik1126/ParallelComp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extrapolating ultra-long contexts (text length >128K) remains a major\nchallenge for large language models (LLMs), as most training-free extrapolation\nmethods are not only severely limited by memory bottlenecks, but also suffer\nfrom the attention sink, which restricts their scalability and effectiveness in\npractice. In this work, we propose ParallelComp, a parallel long-context\ncompression method that effectively overcomes the memory bottleneck, enabling\n8B-parameter LLMs to extrapolate from 8K to 128K tokens on a single A100 80GB\nGPU in a training-free setting. ParallelComp splits the input into chunks,\ndynamically evicting redundant chunks and irrelevant tokens, supported by a\nparallel KV cache eviction mechanism. Importantly, we present a systematic\ntheoretical and empirical analysis of attention biases in parallel\nattention-including the attention sink, recency bias, and middle bias-and\nreveal that these biases exhibit distinctive patterns under ultra-long context\nsettings. We further design a KV cache eviction technique to mitigate this\nphenomenon. Experimental results show that ParallelComp enables an 8B model\n(trained on 8K context) to achieve 91.17% of GPT-4's performance under\nultra-long contexts, outperforming closed-source models such as Claude-2 and\nKimi-Chat. We achieve a 1.76x improvement in chunk throughput, thereby\nachieving a 23.50x acceleration in the prefill stage with negligible\nperformance loss and pave the way for scalable and robust ultra-long contexts\nextrapolation in LLMs. We release the code at\nhttps://github.com/menik1126/ParallelComp."
                },
                "authors": [
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Jianghan Shen"
                    },
                    {
                        "name": "Chuanyang Zheng"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Chenyang Zhao"
                    },
                    {
                        "name": "Chiwun Yang"
                    },
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Hongxia Yang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong",
                "arxiv_comment": "This paper has been accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07533v1",
                "updated": "2025-06-09T08:16:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    8,
                    16,
                    24,
                    0,
                    160,
                    0
                ],
                "published": "2025-06-09T08:16:24Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    8,
                    16,
                    24,
                    0,
                    160,
                    0
                ],
                "title": "MoQAE: Mixed-Precision Quantization for Long-Context LLM Inference via\n  Mixture of Quantization-Aware Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoQAE: Mixed-Precision Quantization for Long-Context LLM Inference via\n  Mixture of Quantization-Aware Experts"
                },
                "summary": "One of the primary challenges in optimizing large language models (LLMs) for\nlong-context inference lies in the high memory consumption of the Key-Value\n(KV) cache. Existing approaches, such as quantization, have demonstrated\npromising results in reducing memory usage. However, current quantization\nmethods cannot take both effectiveness and efficiency into account. In this\npaper, we propose MoQAE, a novel mixed-precision quantization method via\nmixture of quantization-aware experts. First, we view different quantization\nbit-width configurations as experts and use the traditional mixture of experts\n(MoE) method to select the optimal configuration. To avoid the inefficiency\ncaused by inputting tokens one by one into the router in the traditional MoE\nmethod, we input the tokens into the router chunk by chunk. Second, we design a\nlightweight router-only fine-tuning process to train MoQAE with a comprehensive\nloss to learn the trade-off between model accuracy and memory usage. Finally,\nwe introduce a routing freezing (RF) and a routing sharing (RS) mechanism to\nfurther reduce the inference overhead. Extensive experiments on multiple\nbenchmark datasets demonstrate that our method outperforms state-of-the-art KV\ncache quantization approaches in both efficiency and effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the primary challenges in optimizing large language models (LLMs) for\nlong-context inference lies in the high memory consumption of the Key-Value\n(KV) cache. Existing approaches, such as quantization, have demonstrated\npromising results in reducing memory usage. However, current quantization\nmethods cannot take both effectiveness and efficiency into account. In this\npaper, we propose MoQAE, a novel mixed-precision quantization method via\nmixture of quantization-aware experts. First, we view different quantization\nbit-width configurations as experts and use the traditional mixture of experts\n(MoE) method to select the optimal configuration. To avoid the inefficiency\ncaused by inputting tokens one by one into the router in the traditional MoE\nmethod, we input the tokens into the router chunk by chunk. Second, we design a\nlightweight router-only fine-tuning process to train MoQAE with a comprehensive\nloss to learn the trade-off between model accuracy and memory usage. Finally,\nwe introduce a routing freezing (RF) and a routing sharing (RS) mechanism to\nfurther reduce the inference overhead. Extensive experiments on multiple\nbenchmark datasets demonstrate that our method outperforms state-of-the-art KV\ncache quantization approaches in both efficiency and effectiveness."
                },
                "authors": [
                    {
                        "name": "Wei Tao"
                    },
                    {
                        "name": "Haocheng Lu"
                    },
                    {
                        "name": "Xiaoyang Qu"
                    },
                    {
                        "name": "Bin Zhang"
                    },
                    {
                        "name": "Kai Lu"
                    },
                    {
                        "name": "Jiguang Wan"
                    },
                    {
                        "name": "Jianzong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianzong Wang"
                },
                "author": "Jianzong Wang",
                "arxiv_comment": "Accepted by the 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02469v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02469v2",
                "updated": "2025-06-09T07:58:19Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    7,
                    58,
                    19,
                    0,
                    160,
                    0
                ],
                "published": "2025-01-05T07:41:53Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    7,
                    41,
                    53,
                    6,
                    5,
                    0
                ],
                "title": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks"
                },
                "summary": "The minimal infrastructure requirements of LoRa make it suitable for\ndeployments in remote and disaster-stricken areas. Concomitantly, the modern\nera is witnessing the proliferation of web applications in all aspects of human\nlife, including IoT and other network services. Contemporary IoT and network\nsolutions heavily rely on web applications to render services. However, despite\nthe recent research and development pivoted around LoRa, there is still a lack\nof studies focusing on web application access over LoRa networks. Specifically,\ntechnical challenges like payload size limitation, low data rate, and\ncontentions in multi-user setups limit the applicability of LoRa for web\napplications. Hence, we propose LoRaWeb, which enables web access over LoRa\nnetworks. The LoRaWeb hardware tethers a WiFi hotspot to which the client\ndevices connect and access the web pages using a web browser. LoRa backbone of\nthe network handles the web page transmission from the requester and receiver\ndevices. LoRaWeb implements a synchronization procedure to address the\naforementioned challenges for effective message exchange between requesters and\nresponders. The system implements a caching mechanism to reduce latency and\ncontention. Additionally, it implements a message-slicing mechanism in the\napplication layer to overcome the hardware limitations on the message length.\nThe actual hardware-based implementation results indicate seamless deployment,\nand the results indicate an average access time of ~$0.95 S$ for a $1.5 KB$ and\n~$6 S$ for a $10 KB$ size web page.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The minimal infrastructure requirements of LoRa make it suitable for\ndeployments in remote and disaster-stricken areas. Concomitantly, the modern\nera is witnessing the proliferation of web applications in all aspects of human\nlife, including IoT and other network services. Contemporary IoT and network\nsolutions heavily rely on web applications to render services. However, despite\nthe recent research and development pivoted around LoRa, there is still a lack\nof studies focusing on web application access over LoRa networks. Specifically,\ntechnical challenges like payload size limitation, low data rate, and\ncontentions in multi-user setups limit the applicability of LoRa for web\napplications. Hence, we propose LoRaWeb, which enables web access over LoRa\nnetworks. The LoRaWeb hardware tethers a WiFi hotspot to which the client\ndevices connect and access the web pages using a web browser. LoRa backbone of\nthe network handles the web page transmission from the requester and receiver\ndevices. LoRaWeb implements a synchronization procedure to address the\naforementioned challenges for effective message exchange between requesters and\nresponders. The system implements a caching mechanism to reduce latency and\ncontention. Additionally, it implements a message-slicing mechanism in the\napplication layer to overcome the hardware limitations on the message length.\nThe actual hardware-based implementation results indicate seamless deployment,\nand the results indicate an average access time of ~$0.95 S$ for a $1.5 KB$ and\n~$6 S$ for a $10 KB$ size web page."
                },
                "authors": [
                    {
                        "name": "Atonu Ghosh"
                    },
                    {
                        "name": "Sudip Misra"
                    }
                ],
                "author_detail": {
                    "name": "Sudip Misra"
                },
                "author": "Sudip Misra",
                "arxiv_comment": "The paper needs major modifications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02469v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02469v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06266v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06266v2",
                "updated": "2025-06-09T05:21:52Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    5,
                    21,
                    52,
                    0,
                    160,
                    0
                ],
                "published": "2025-06-06T17:48:23Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    17,
                    48,
                    23,
                    4,
                    157,
                    0
                ],
                "title": "Cartridges: Lightweight and general-purpose long context representations\n  via self-study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cartridges: Lightweight and general-purpose long context representations\n  via self-study"
                },
                "summary": "Large language models are often used to answer queries grounded in large text\ncorpora (e.g. codebases, legal documents, or chat histories) by placing the\nentire corpus in the context window and leveraging in-context learning (ICL).\nAlthough current models support contexts of 100K-1M tokens, this setup is\ncostly to serve because the memory consumption of the KV cache scales with\ninput length. We explore an alternative: training a smaller KV cache offline on\neach corpus. At inference time, we load this trained KV cache, which we call a\nCartridge, and decode a response. Critically, the cost of training a Cartridge\ncan be amortized across all the queries referencing the same corpus. However,\nwe find that the naive approach of training the Cartridge with next-token\nprediction on the corpus is not competitive with ICL. Instead, we propose\nself-study, a training recipe in which we generate synthetic conversations\nabout the corpus and train the Cartridge with a context-distillation objective.\nWe find that Cartridges trained with self-study replicate the functionality of\nICL, while being significantly cheaper to serve. On challenging long-context\nbenchmarks, Cartridges trained with self-study match ICL performance while\nusing 38.6x less memory and enabling 26.4x higher throughput. Self-study also\nextends the model's effective context length (e.g. from 128k to 484k tokens on\nMTOB) and surprisingly, leads to Cartridges that can be composed at inference\ntime without retraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are often used to answer queries grounded in large text\ncorpora (e.g. codebases, legal documents, or chat histories) by placing the\nentire corpus in the context window and leveraging in-context learning (ICL).\nAlthough current models support contexts of 100K-1M tokens, this setup is\ncostly to serve because the memory consumption of the KV cache scales with\ninput length. We explore an alternative: training a smaller KV cache offline on\neach corpus. At inference time, we load this trained KV cache, which we call a\nCartridge, and decode a response. Critically, the cost of training a Cartridge\ncan be amortized across all the queries referencing the same corpus. However,\nwe find that the naive approach of training the Cartridge with next-token\nprediction on the corpus is not competitive with ICL. Instead, we propose\nself-study, a training recipe in which we generate synthetic conversations\nabout the corpus and train the Cartridge with a context-distillation objective.\nWe find that Cartridges trained with self-study replicate the functionality of\nICL, while being significantly cheaper to serve. On challenging long-context\nbenchmarks, Cartridges trained with self-study match ICL performance while\nusing 38.6x less memory and enabling 26.4x higher throughput. Self-study also\nextends the model's effective context length (e.g. from 128k to 484k tokens on\nMTOB) and surprisingly, leads to Cartridges that can be composed at inference\ntime without retraining."
                },
                "authors": [
                    {
                        "name": "Sabri Eyuboglu"
                    },
                    {
                        "name": "Ryan Ehrlich"
                    },
                    {
                        "name": "Simran Arora"
                    },
                    {
                        "name": "Neel Guha"
                    },
                    {
                        "name": "Dylan Zinsley"
                    },
                    {
                        "name": "Emily Liu"
                    },
                    {
                        "name": "Will Tennien"
                    },
                    {
                        "name": "Atri Rudra"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Azalia Mirhoseini"
                    },
                    {
                        "name": "Christopher Re"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Re"
                },
                "author": "Christopher Re",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06266v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06266v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07334v1",
                "updated": "2025-06-09T00:30:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    0,
                    30,
                    8,
                    0,
                    160,
                    0
                ],
                "published": "2025-06-09T00:30:08Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    0,
                    30,
                    8,
                    0,
                    160,
                    0
                ],
                "title": "Graph-KV: Breaking Sequence via Injecting Structural Biases into Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-KV: Breaking Sequence via Injecting Structural Biases into Large\n  Language Models"
                },
                "summary": "Modern large language models (LLMs) are inherently auto-regressive, requiring\ninput to be serialized into flat sequences regardless of their structural\ndependencies. This serialization hinders the model's ability to leverage\nstructural inductive biases, especially in tasks such as retrieval-augmented\ngeneration (RAG) and reasoning on data with native graph structures, where\ninter-segment dependencies are crucial. We introduce Graph-KV with the\npotential to overcome this limitation. Graph-KV leverages the KV-cache of text\nsegments as condensed representations and governs their interaction through\nstructural inductive biases. In this framework, 'target' segments selectively\nattend only to the KV-caches of their designated 'source' segments, rather than\nall preceding segments in a serialized sequence. This approach induces a\ngraph-structured block mask, sparsifying attention and enabling a\nmessage-passing-like step within the LLM. Furthermore, strategically allocated\npositional encodings for source and target segments reduce positional bias and\ncontext window consumption. We evaluate Graph-KV across three scenarios: (1)\nseven RAG benchmarks spanning direct inference, multi-hop reasoning, and\nlong-document understanding; (2) Arxiv-QA, a novel academic paper QA task with\nfull-text scientific papers structured as citation ego-graphs; and (3) paper\ntopic classification within a citation network. By effectively reducing\npositional bias and harnessing structural inductive biases, Graph-KV\nsubstantially outperforms baselines, including standard costly sequential\nencoding, across various settings. Code and the Graph-KV data are publicly\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language models (LLMs) are inherently auto-regressive, requiring\ninput to be serialized into flat sequences regardless of their structural\ndependencies. This serialization hinders the model's ability to leverage\nstructural inductive biases, especially in tasks such as retrieval-augmented\ngeneration (RAG) and reasoning on data with native graph structures, where\ninter-segment dependencies are crucial. We introduce Graph-KV with the\npotential to overcome this limitation. Graph-KV leverages the KV-cache of text\nsegments as condensed representations and governs their interaction through\nstructural inductive biases. In this framework, 'target' segments selectively\nattend only to the KV-caches of their designated 'source' segments, rather than\nall preceding segments in a serialized sequence. This approach induces a\ngraph-structured block mask, sparsifying attention and enabling a\nmessage-passing-like step within the LLM. Furthermore, strategically allocated\npositional encodings for source and target segments reduce positional bias and\ncontext window consumption. We evaluate Graph-KV across three scenarios: (1)\nseven RAG benchmarks spanning direct inference, multi-hop reasoning, and\nlong-document understanding; (2) Arxiv-QA, a novel academic paper QA task with\nfull-text scientific papers structured as citation ego-graphs; and (3) paper\ntopic classification within a citation network. By effectively reducing\npositional bias and harnessing structural inductive biases, Graph-KV\nsubstantially outperforms baselines, including standard costly sequential\nencoding, across various settings. Code and the Graph-KV data are publicly\navailable."
                },
                "authors": [
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Peihao Wang"
                    },
                    {
                        "name": "Mufei Li"
                    },
                    {
                        "name": "Shikun Liu"
                    },
                    {
                        "name": "Siqi Miao"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Pan Li"
                    }
                ],
                "author_detail": {
                    "name": "Pan Li"
                },
                "author": "Pan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07311v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07311v1",
                "updated": "2025-06-08T22:59:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    8,
                    22,
                    59,
                    20,
                    6,
                    159,
                    0
                ],
                "published": "2025-06-08T22:59:20Z",
                "published_parsed": [
                    2025,
                    6,
                    8,
                    22,
                    59,
                    20,
                    6,
                    159,
                    0
                ],
                "title": "Paged Attention Meets FlexAttention: Unlocking Long-Context Efficiency\n  in Deployed Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Paged Attention Meets FlexAttention: Unlocking Long-Context Efficiency\n  in Deployed Inference"
                },
                "summary": "Large Language Models (LLMs) encounter severe memory inefficiencies during\nlong-context inference due to conventional handling of key-value (KV) caches.\nIn this work, we introduce a novel integration of PagedAttention with PyTorch's\nFlexAttention, addressing internal fragmentation and inefficiencies associated\nwith monolithic KV cache allocations. Implemented within IBM's Foundation Model\nStack (FMS), our fused attention kernel efficiently gathers scattered KV data.\nOur benchmarks on an NVIDIA L4 GPU (24GB) demonstrate significantly reduced\ninference latency, growing only linearly (~2x) with sequence length from 128 to\n2048 tokens when utilizing a global KV cache, compared to exponential latency\nincreases without caching. While peak memory usage remains largely unchanged\nfor single-step evaluations (dominated by model weights and activations), paged\nattention causes minimal incremental memory usage, observable only at sequence\nlengths exceeding 2048 tokens due to its power-of-two cache allocations. We\nopen-source the full implementation and discuss its implications for future\nlong-context model deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) encounter severe memory inefficiencies during\nlong-context inference due to conventional handling of key-value (KV) caches.\nIn this work, we introduce a novel integration of PagedAttention with PyTorch's\nFlexAttention, addressing internal fragmentation and inefficiencies associated\nwith monolithic KV cache allocations. Implemented within IBM's Foundation Model\nStack (FMS), our fused attention kernel efficiently gathers scattered KV data.\nOur benchmarks on an NVIDIA L4 GPU (24GB) demonstrate significantly reduced\ninference latency, growing only linearly (~2x) with sequence length from 128 to\n2048 tokens when utilizing a global KV cache, compared to exponential latency\nincreases without caching. While peak memory usage remains largely unchanged\nfor single-step evaluations (dominated by model weights and activations), paged\nattention causes minimal incremental memory usage, observable only at sequence\nlengths exceeding 2048 tokens due to its power-of-two cache allocations. We\nopen-source the full implementation and discuss its implications for future\nlong-context model deployment."
                },
                "authors": [
                    {
                        "name": "Thomas Joshi"
                    },
                    {
                        "name": "Herman Saini"
                    },
                    {
                        "name": "Neil Dhillon"
                    },
                    {
                        "name": "Antoni Viros i Martin"
                    },
                    {
                        "name": "Kaoutar El Maghraoui"
                    }
                ],
                "author_detail": {
                    "name": "Kaoutar El Maghraoui"
                },
                "author": "Kaoutar El Maghraoui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07311v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07311v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18077v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18077v3",
                "updated": "2025-06-08T21:23:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    8,
                    21,
                    23,
                    22,
                    6,
                    159,
                    0
                ],
                "published": "2024-11-27T06:10:49Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    6,
                    10,
                    49,
                    2,
                    332,
                    0
                ],
                "title": "MiniKV: Pushing the Limits of LLM Inference via 2-Bit\n  Layer-Discriminative KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniKV: Pushing the Limits of LLM Inference via 2-Bit\n  Layer-Discriminative KV Cache"
                },
                "summary": "How to efficiently serve LLMs in practice has become exceptionally\nchallenging due to their prohibitive memory and computation requirements. In\nthis study, we investigate optimizing the KV cache, whose memory footprint\nposes a critical bottleneck in LLM inference, especially when dealing with long\ncontext tasks. To tackle the challenge, we introduce MiniKV, a KV cache\noptimization method that simultaneously preserves long context task accuracy\nwhile significantly reducing KV cache size via a novel 2-bit\nlayer-discriminative KV cache. More importantly, we develop specialized CUDA\nkernels to make MiniKV compatible with FlashAttention. Experiments on a wide\nrange of long context tasks show that MiniKV effectively achieves 86% KV cache\ncompression ratio while recovering over 98.5% of accuracy, outperforming\nstate-of-the-art methods while achieving excellent measured system performance\nimprovements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to efficiently serve LLMs in practice has become exceptionally\nchallenging due to their prohibitive memory and computation requirements. In\nthis study, we investigate optimizing the KV cache, whose memory footprint\nposes a critical bottleneck in LLM inference, especially when dealing with long\ncontext tasks. To tackle the challenge, we introduce MiniKV, a KV cache\noptimization method that simultaneously preserves long context task accuracy\nwhile significantly reducing KV cache size via a novel 2-bit\nlayer-discriminative KV cache. More importantly, we develop specialized CUDA\nkernels to make MiniKV compatible with FlashAttention. Experiments on a wide\nrange of long context tasks show that MiniKV effectively achieves 86% KV cache\ncompression ratio while recovering over 98.5% of accuracy, outperforming\nstate-of-the-art methods while achieving excellent measured system performance\nimprovements."
                },
                "authors": [
                    {
                        "name": "Akshat Sharma"
                    },
                    {
                        "name": "Hangliang Ding"
                    },
                    {
                        "name": "Jianping Li"
                    },
                    {
                        "name": "Neel Dani"
                    },
                    {
                        "name": "Minjia Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Minjia Zhang"
                },
                "author": "Minjia Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18077v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18077v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04107v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04107v3",
                "updated": "2025-06-08T20:04:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    8,
                    20,
                    4,
                    17,
                    6,
                    159,
                    0
                ],
                "published": "2024-08-07T22:10:26Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "title": "FDC: Fast KV Dimensionality Compression for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FDC: Fast KV Dimensionality Compression for Efficient LLM Inference"
                },
                "summary": "In large-language models, memory constraints in the Key-Value Cache (KVC)\npose a challenge during inference. In this work, we propose FDC, a fast KV\ndimensionality compression system that eliminates the decompression overhead\nincurred in the existing KV dimensionality compression system, Palu, and\nreduces attention time. Moreover, FDC employs adaptive compression, tailoring\nKV compression rates across heads and layers based on their contributions to\ninference to maximize overall compression while maintaining an accuracy loss\nconstraint. Additionally, FDC enhances the attention kernel to balance the\nuneven workloads caused by the adaptive compression approach to further reduce\nattention computation latency. Comprehensive experiments demonstrate that\ncompared to Palu, FDC can reduce Job Completion Time (JCT) by up to 64%, and\ndelivers up to 1.97X throughput under the same latency, while maintaining 99%\nof the accuracy without compression. When state-of-the-art eviction and\nquantization methods are combined with FDC, they exhibit similar improvements\ncompared to those combined with Palu. We open-sourced the code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-language models, memory constraints in the Key-Value Cache (KVC)\npose a challenge during inference. In this work, we propose FDC, a fast KV\ndimensionality compression system that eliminates the decompression overhead\nincurred in the existing KV dimensionality compression system, Palu, and\nreduces attention time. Moreover, FDC employs adaptive compression, tailoring\nKV compression rates across heads and layers based on their contributions to\ninference to maximize overall compression while maintaining an accuracy loss\nconstraint. Additionally, FDC enhances the attention kernel to balance the\nuneven workloads caused by the adaptive compression approach to further reduce\nattention computation latency. Comprehensive experiments demonstrate that\ncompared to Palu, FDC can reduce Job Completion Time (JCT) by up to 64%, and\ndelivers up to 1.97X throughput under the same latency, while maintaining 99%\nof the accuracy without compression. When state-of-the-art eviction and\nquantization methods are combined with FDC, they exhibit similar improvements\ncompared to those combined with Palu. We open-sourced the code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04107v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04107v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2210.08508v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2210.08508v2",
                "updated": "2025-06-08T16:07:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    8,
                    16,
                    7,
                    44,
                    6,
                    159,
                    0
                ],
                "published": "2022-10-16T11:21:26Z",
                "published_parsed": [
                    2022,
                    10,
                    16,
                    11,
                    21,
                    26,
                    6,
                    289,
                    0
                ],
                "title": "RevaMp3D: Architecting the Processor Core and Cache Hierarchy for\n  Systems with Monolithically-Integrated Logic and Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RevaMp3D: Architecting the Processor Core and Cache Hierarchy for\n  Systems with Monolithically-Integrated Logic and Memory"
                },
                "summary": "Recent nano-technological advances enable the Monolithic 3D (M3D) integration\nof multiple memory and logic layers in a single chip, allowing for fine-grained\nconnections between layers and significantly alleviating main memory\nbottlenecks. We show for a variety of workloads, on a state-of-the-art\nM3D-based system, that the performance and energy bottlenecks shift from main\nmemory to the processor core and cache hierarchy. Therefore, there is a need to\nrevisit current designs that have been conventionally tailored to tackle the\nmemory bottleneck. Based on the insights from our design space exploration, we\npropose RevaMp3D, introducing five key changes. First, we propose removing the\nshared last-level cache, as this delivers speedups comparable to or exceeding\nthose from increasing its size or reducing its latency across all workloads.\nSecond, since improving L1 cache latency has a large impact on performance, we\nreduce L1 latency by leveraging an M3D layout to shorten its wires. Third, we\nrepurpose the area from the removed cache to widen and scale up pipeline\nstructures, accommodating more in-flight requests that are efficiently served\nby M3D memory. To avoid latency penalties from these larger structures, we\nleverage M3D layouts. Fourth, to facilitate high thread-level parallelism, we\npropose a new fine-grained synchronization technique, using M3D's dense\ninter-layer connectivity. Fifth, we leverage the M3D main memory to mitigate\nthe core bottlenecks. We propose a processor frontend design that memoizes the\nrepetitive fetched, decoded, and reordered instructions, stores them in main\nmemory, and turns off the relevant parts of the core when possible. RevaMp3D\nprovides 1.2x-2.9x speedup and 1.2x-1.4x energy reduction compared to a\nstate-of-the-art M3D system. We also analyze RevaMp3D's design decisions across\nvarious memory latencies to facilitate latency-aware design decisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent nano-technological advances enable the Monolithic 3D (M3D) integration\nof multiple memory and logic layers in a single chip, allowing for fine-grained\nconnections between layers and significantly alleviating main memory\nbottlenecks. We show for a variety of workloads, on a state-of-the-art\nM3D-based system, that the performance and energy bottlenecks shift from main\nmemory to the processor core and cache hierarchy. Therefore, there is a need to\nrevisit current designs that have been conventionally tailored to tackle the\nmemory bottleneck. Based on the insights from our design space exploration, we\npropose RevaMp3D, introducing five key changes. First, we propose removing the\nshared last-level cache, as this delivers speedups comparable to or exceeding\nthose from increasing its size or reducing its latency across all workloads.\nSecond, since improving L1 cache latency has a large impact on performance, we\nreduce L1 latency by leveraging an M3D layout to shorten its wires. Third, we\nrepurpose the area from the removed cache to widen and scale up pipeline\nstructures, accommodating more in-flight requests that are efficiently served\nby M3D memory. To avoid latency penalties from these larger structures, we\nleverage M3D layouts. Fourth, to facilitate high thread-level parallelism, we\npropose a new fine-grained synchronization technique, using M3D's dense\ninter-layer connectivity. Fifth, we leverage the M3D main memory to mitigate\nthe core bottlenecks. We propose a processor frontend design that memoizes the\nrepetitive fetched, decoded, and reordered instructions, stores them in main\nmemory, and turns off the relevant parts of the core when possible. RevaMp3D\nprovides 1.2x-2.9x speedup and 1.2x-1.4x energy reduction compared to a\nstate-of-the-art M3D system. We also analyze RevaMp3D's design decisions across\nvarious memory latencies to facilitate latency-aware design decisions."
                },
                "authors": [
                    {
                        "name": "Nika Mansouri Ghiasi"
                    },
                    {
                        "name": "Mohammad Sadrosadati"
                    },
                    {
                        "name": "Geraldo F. Oliveira"
                    },
                    {
                        "name": "Konstantinos Kanellopoulos"
                    },
                    {
                        "name": "Rachata Ausavarungnirun"
                    },
                    {
                        "name": "Juan Gmez Luna"
                    },
                    {
                        "name": "Joo Ferreira"
                    },
                    {
                        "name": "Jeremie S. Kim"
                    },
                    {
                        "name": "Christina Giannoula"
                    },
                    {
                        "name": "Nandita Vijaykumar"
                    },
                    {
                        "name": "Jisung Park"
                    },
                    {
                        "name": "Onur Mutlu"
                    }
                ],
                "author_detail": {
                    "name": "Onur Mutlu"
                },
                "author": "Onur Mutlu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2210.08508v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2210.08508v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v5",
                "updated": "2025-06-08T16:04:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    8,
                    16,
                    4,
                    59,
                    6,
                    159,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning"
                },
                "summary": "While Transformer models have achieved remarkable success in various domains,\nthe effectiveness of information propagation through deep networks remains a\ncritical challenge. Standard hidden state residuals often fail to adequately\npreserve initial token-level information in deeper layers. This paper\nintroduces ResFormer, a novel architecture that enhances information flow by\nincorporating value residual connections in addition to hidden state residuals.\nAnd a variant is SVFormer, where all layers share the first layer's value\nembedding. Comprehensive empirical evidence demonstrates ResFormer achieves\nequivalent validation loss with 16.11\\% fewer model parameters and 20.3\\% less\ntraining data compared to Transformer, while maintaining similar memory usage\nand computational cost. Besides, SVFormer reduces KV cache size by nearly half\nwith only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformer models have achieved remarkable success in various domains,\nthe effectiveness of information propagation through deep networks remains a\ncritical challenge. Standard hidden state residuals often fail to adequately\npreserve initial token-level information in deeper layers. This paper\nintroduces ResFormer, a novel architecture that enhances information flow by\nincorporating value residual connections in addition to hidden state residuals.\nAnd a variant is SVFormer, where all layers share the first layer's value\nembedding. Comprehensive empirical evidence demonstrates ResFormer achieves\nequivalent validation loss with 16.11\\% fewer model parameters and 20.3\\% less\ntraining data compared to Transformer, while maintaining similar memory usage\nand computational cost. Besides, SVFormer reduces KV cache size by nearly half\nwith only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Fares Obeid"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07200v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07200v1",
                "updated": "2025-06-08T15:48:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    8,
                    15,
                    48,
                    16,
                    6,
                    159,
                    0
                ],
                "published": "2025-06-08T15:48:16Z",
                "published_parsed": [
                    2025,
                    6,
                    8,
                    15,
                    48,
                    16,
                    6,
                    159,
                    0
                ],
                "title": "Efficient RL-based Cache Vulnerability Exploration by Penalizing Useless\n  Agent Actions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient RL-based Cache Vulnerability Exploration by Penalizing Useless\n  Agent Actions"
                },
                "summary": "Cache-timing attacks exploit microarchitectural characteristics to leak\nsensitive data, posing a severe threat to modern systems. Despite its severity,\nanalyzing the vulnerability of a given cache structure against cache-timing\nattacks is challenging. To this end, a method based on Reinforcement Learning\n(RL) has been proposed to automatically explore vulnerabilities for a given\ncache structure. However, a naive RL-based approach suffers from inefficiencies\ndue to the agent performing actions that do not contribute to the exploration.\nIn this paper, we propose a method to identify these useless actions during\ntraining and penalize them so that the agent avoids them and the exploration\nefficiency is improved. Experiments on 17 cache structures show that our\ntraining mechanism reduces the number of useless actions by up to 43.08%. This\nresulted in the reduction of training time by 28\\% in the base case and 4.84\\%\nin the geomean compared to a naive RL-based approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-timing attacks exploit microarchitectural characteristics to leak\nsensitive data, posing a severe threat to modern systems. Despite its severity,\nanalyzing the vulnerability of a given cache structure against cache-timing\nattacks is challenging. To this end, a method based on Reinforcement Learning\n(RL) has been proposed to automatically explore vulnerabilities for a given\ncache structure. However, a naive RL-based approach suffers from inefficiencies\ndue to the agent performing actions that do not contribute to the exploration.\nIn this paper, we propose a method to identify these useless actions during\ntraining and penalize them so that the agent avoids them and the exploration\nefficiency is improved. Experiments on 17 cache structures show that our\ntraining mechanism reduces the number of useless actions by up to 43.08%. This\nresulted in the reduction of training time by 28\\% in the base case and 4.84\\%\nin the geomean compared to a naive RL-based approach."
                },
                "authors": [
                    {
                        "name": "Kanato Nakanishi"
                    },
                    {
                        "name": "Soramichi Akiyama"
                    }
                ],
                "author_detail": {
                    "name": "Soramichi Akiyama"
                },
                "author": "Soramichi Akiyama",
                "arxiv_comment": "Presented in Machine Learning for Computer Architecture and Systems\n  (MLArchSys), June 21, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07200v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07200v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16220v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16220v2",
                "updated": "2025-06-08T09:30:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    8,
                    9,
                    30,
                    12,
                    6,
                    159,
                    0
                ],
                "published": "2024-08-29T02:31:28Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    31,
                    28,
                    3,
                    242,
                    0
                ],
                "title": "Place Protections at the Right Place: Targeted Hardening for\n  Cryptographic Code against Spectre v1",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Place Protections at the Right Place: Targeted Hardening for\n  Cryptographic Code against Spectre v1"
                },
                "summary": "Spectre v1 attacks pose a substantial threat to security-critical software,\nparticularly cryptographic implementations. Existing software mitigations,\nhowever, often introduce excessive overhead by indiscriminately hardening\ninstructions without assessing their vulnerability. We propose an analysis\nframework that employs a novel fixpoint algorithm to detect Spectre\nvulnerabilities and apply targeted hardening. The fixpoint algorithm accounts\nfor program behavior changes induced by stepwise hardening, enabling precise,\nsound and efficient vulnerability detection. This framework also provides\nflexibility for diverse hardening strategies and attacker models, enabling\ncustomized targeted hardening. We instantiate the framework as LightSLH, which\nhardens program with provable security.\n  We evaluate LightSLH on cryptographic algorithms from OpenSSL, Libsodium,\nNaCL and PQClean. Across all experimental cases, LightSLH provides the lowest\noverhead among current provable protection strategies, including 0\\% overhead\nin 50\\% cases. Notably, the analysis of LightSLH reveals two previously unknown\nsecurity issues: (1) The compiler can introduce risks overlooked by LLSCT, a\nhardening method proven secure at the LLVM IR level. We successfully construct\na side channel by exploiting compiler-inserted stack loads, confirming this\nrisk. (2) Memory access patterns generated by the scatter-gather algorithm\nstill depend on secrets, even for observers with cache line granularity. These\nfindings and results highlight the importance of applying accurate protections\nto specific instructions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spectre v1 attacks pose a substantial threat to security-critical software,\nparticularly cryptographic implementations. Existing software mitigations,\nhowever, often introduce excessive overhead by indiscriminately hardening\ninstructions without assessing their vulnerability. We propose an analysis\nframework that employs a novel fixpoint algorithm to detect Spectre\nvulnerabilities and apply targeted hardening. The fixpoint algorithm accounts\nfor program behavior changes induced by stepwise hardening, enabling precise,\nsound and efficient vulnerability detection. This framework also provides\nflexibility for diverse hardening strategies and attacker models, enabling\ncustomized targeted hardening. We instantiate the framework as LightSLH, which\nhardens program with provable security.\n  We evaluate LightSLH on cryptographic algorithms from OpenSSL, Libsodium,\nNaCL and PQClean. Across all experimental cases, LightSLH provides the lowest\noverhead among current provable protection strategies, including 0\\% overhead\nin 50\\% cases. Notably, the analysis of LightSLH reveals two previously unknown\nsecurity issues: (1) The compiler can introduce risks overlooked by LLSCT, a\nhardening method proven secure at the LLVM IR level. We successfully construct\na side channel by exploiting compiler-inserted stack loads, confirming this\nrisk. (2) Memory access patterns generated by the scatter-gather algorithm\nstill depend on secrets, even for observers with cache line granularity. These\nfindings and results highlight the importance of applying accurate protections\nto specific instructions."
                },
                "authors": [
                    {
                        "name": "Yiming Zhu"
                    },
                    {
                        "name": "Wenchao Huang"
                    },
                    {
                        "name": "Yan Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Yan Xiong"
                },
                "author": "Yan Xiong",
                "arxiv_comment": "Accepted to appear at USENIX Security 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16220v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16220v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18300v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18300v2",
                "updated": "2025-06-08T00:52:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    8,
                    0,
                    52,
                    33,
                    6,
                    159,
                    0
                ],
                "published": "2025-05-23T18:46:10Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    18,
                    46,
                    10,
                    4,
                    143,
                    0
                ],
                "title": "Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient\n  Nonlinear MCMC on General Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient\n  Nonlinear MCMC on General Graphs"
                },
                "summary": "We propose a history-driven target (HDT) framework in Markov Chain Monte\nCarlo (MCMC) to improve any random walk algorithm on discrete state spaces,\nsuch as general undirected graphs, for efficient sampling from target\ndistribution $\\boldsymbol{\\mu}$. With broad applications in network science and\ndistributed optimization, recent innovations like the self-repellent random\nwalk (SRRW) achieve near-zero variance by prioritizing under-sampled states\nthrough transition kernel modifications based on past visit frequencies.\nHowever, SRRW's reliance on explicit computation of transition probabilities\nfor all neighbors at each step introduces substantial computational overhead,\nwhile its strict dependence on time-reversible Markov chains excludes advanced\nnon-reversible MCMC methods. To overcome these limitations, instead of direct\nmodification of transition kernel, HDT introduces a history-dependent target\ndistribution $\\boldsymbol{\\pi}[\\mathbf{x}]$ to replace the original target\n$\\boldsymbol{\\mu}$ in any graph sampler, where $\\mathbf{x}$ represents the\nempirical measure of past visits. This design preserves lightweight\nimplementation by requiring only local information between the current and\nproposed states and achieves compatibility with both reversible and\nnon-reversible MCMC samplers, while retaining unbiased samples with target\ndistribution $\\boldsymbol{\\mu}$ and near-zero variance performance. Extensive\nexperiments in graph sampling demonstrate consistent performance gains, and a\nmemory-efficient Least Recently Used (LRU) cache ensures scalability to large\ngeneral graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a history-driven target (HDT) framework in Markov Chain Monte\nCarlo (MCMC) to improve any random walk algorithm on discrete state spaces,\nsuch as general undirected graphs, for efficient sampling from target\ndistribution $\\boldsymbol{\\mu}$. With broad applications in network science and\ndistributed optimization, recent innovations like the self-repellent random\nwalk (SRRW) achieve near-zero variance by prioritizing under-sampled states\nthrough transition kernel modifications based on past visit frequencies.\nHowever, SRRW's reliance on explicit computation of transition probabilities\nfor all neighbors at each step introduces substantial computational overhead,\nwhile its strict dependence on time-reversible Markov chains excludes advanced\nnon-reversible MCMC methods. To overcome these limitations, instead of direct\nmodification of transition kernel, HDT introduces a history-dependent target\ndistribution $\\boldsymbol{\\pi}[\\mathbf{x}]$ to replace the original target\n$\\boldsymbol{\\mu}$ in any graph sampler, where $\\mathbf{x}$ represents the\nempirical measure of past visits. This design preserves lightweight\nimplementation by requiring only local information between the current and\nproposed states and achieves compatibility with both reversible and\nnon-reversible MCMC samplers, while retaining unbiased samples with target\ndistribution $\\boldsymbol{\\mu}$ and near-zero variance performance. Extensive\nexperiments in graph sampling demonstrate consistent performance gains, and a\nmemory-efficient Least Recently Used (LRU) cache ensures scalability to large\ngeneral graphs."
                },
                "authors": [
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Yi-Ting Ma"
                    },
                    {
                        "name": "Do Young Eun"
                    }
                ],
                "author_detail": {
                    "name": "Do Young Eun"
                },
                "author": "Do Young Eun",
                "arxiv_comment": "Accepted at ICML 2025 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18300v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18300v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17132v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17132v2",
                "updated": "2025-06-07T19:22:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    7,
                    19,
                    22,
                    5,
                    5,
                    158,
                    0
                ],
                "published": "2025-05-22T03:00:39Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    3,
                    0,
                    39,
                    3,
                    142,
                    0
                ],
                "title": "Robustifying Vision-Language Models via Dynamic Token Reweighting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustifying Vision-Language Models via Dynamic Token Reweighting"
                },
                "summary": "Large vision-language models (VLMs) are highly vulnerable to jailbreak\nattacks that exploit visual-textual interactions to bypass safety guardrails.\nIn this paper, we present DTR, a novel inference-time defense that mitigates\nmultimodal jailbreak attacks through optimizing the model's key-value (KV)\ncaches. Rather than relying on curated safety-specific data or costly\nimage-to-text conversion, we introduce a new formulation of the safety-relevant\ndistributional shift induced by the visual modality. This formulation enables\nDTR to dynamically adjust visual token weights, minimizing the impact of\nadversarial visual inputs while preserving the model's general capabilities and\ninference efficiency. Extensive evaluation across diverse VLMs and attack\nbenchmarks demonstrates that \\sys outperforms existing defenses in both attack\nrobustness and benign task performance, marking the first successful\napplication of KV cache optimization for safety enhancement in multimodal\nfoundation models. (warning: this paper contains potentially harmful content\ngenerated by VLMs.)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large vision-language models (VLMs) are highly vulnerable to jailbreak\nattacks that exploit visual-textual interactions to bypass safety guardrails.\nIn this paper, we present DTR, a novel inference-time defense that mitigates\nmultimodal jailbreak attacks through optimizing the model's key-value (KV)\ncaches. Rather than relying on curated safety-specific data or costly\nimage-to-text conversion, we introduce a new formulation of the safety-relevant\ndistributional shift induced by the visual modality. This formulation enables\nDTR to dynamically adjust visual token weights, minimizing the impact of\nadversarial visual inputs while preserving the model's general capabilities and\ninference efficiency. Extensive evaluation across diverse VLMs and attack\nbenchmarks demonstrates that \\sys outperforms existing defenses in both attack\nrobustness and benign task performance, marking the first successful\napplication of KV cache optimization for safety enhancement in multimodal\nfoundation models. (warning: this paper contains potentially harmful content\ngenerated by VLMs.)"
                },
                "authors": [
                    {
                        "name": "Tanqiu Jiang"
                    },
                    {
                        "name": "Jiacheng Liang"
                    },
                    {
                        "name": "Rongyi Zhu"
                    },
                    {
                        "name": "Jiawei Zhou"
                    },
                    {
                        "name": "Fenglong Ma"
                    },
                    {
                        "name": "Ting Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ting Wang"
                },
                "author": "Ting Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17132v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17132v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00979v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00979v2",
                "updated": "2025-06-07T14:03:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    7,
                    14,
                    3,
                    6,
                    5,
                    158,
                    0
                ],
                "published": "2025-03-02T18:12:50Z",
                "published_parsed": [
                    2025,
                    3,
                    2,
                    18,
                    12,
                    50,
                    6,
                    61,
                    0
                ],
                "title": "Dialogue Without Limits: Constant-Sized KV Caches for Extended Responses\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dialogue Without Limits: Constant-Sized KV Caches for Extended Responses\n  in LLMs"
                },
                "summary": "Autoregressive Transformers rely on Key-Value (KV) caching to accelerate\ninference. However, the linear growth of the KV cache with context length leads\nto excessive memory consumption and bandwidth constraints. This bottleneck is\nparticularly problematic in real-time applications -- such as chatbots and\ninteractive assistants -- where low latency and high memory efficiency are\ncritical. Existing methods drop distant tokens or compress states in a lossy\nmanner, sacrificing accuracy by discarding vital context or introducing bias.\n  We propose MorphKV, an inference-time technique that maintains a\nconstant-sized KV cache while preserving accuracy. MorphKV balances long-range\ndependencies and local coherence during text generation. It eliminates\nearly-token bias while retaining high-fidelity context by adaptively ranking\ntokens through correlation-aware selection. Unlike heuristic retention or lossy\ncompression, MorphKV iteratively refines the KV cache via lightweight updates\nguided by attention patterns of recent tokens. This approach captures\ninter-token correlation with greater accuracy, crucial for tasks like content\ncreation and code generation. Our studies on long-response tasks show 52.9$\\%$\nmemory savings and 18.2$\\%$ higher accuracy on average compared to\nstate-of-the-art prior works, enabling efficient real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Transformers rely on Key-Value (KV) caching to accelerate\ninference. However, the linear growth of the KV cache with context length leads\nto excessive memory consumption and bandwidth constraints. This bottleneck is\nparticularly problematic in real-time applications -- such as chatbots and\ninteractive assistants -- where low latency and high memory efficiency are\ncritical. Existing methods drop distant tokens or compress states in a lossy\nmanner, sacrificing accuracy by discarding vital context or introducing bias.\n  We propose MorphKV, an inference-time technique that maintains a\nconstant-sized KV cache while preserving accuracy. MorphKV balances long-range\ndependencies and local coherence during text generation. It eliminates\nearly-token bias while retaining high-fidelity context by adaptively ranking\ntokens through correlation-aware selection. Unlike heuristic retention or lossy\ncompression, MorphKV iteratively refines the KV cache via lightweight updates\nguided by attention patterns of recent tokens. This approach captures\ninter-token correlation with greater accuracy, crucial for tasks like content\ncreation and code generation. Our studies on long-response tasks show 52.9$\\%$\nmemory savings and 18.2$\\%$ higher accuracy on average compared to\nstate-of-the-art prior works, enabling efficient real-world deployment."
                },
                "authors": [
                    {
                        "name": "Ravi Ghadia"
                    },
                    {
                        "name": "Avinash Kumar"
                    },
                    {
                        "name": "Gaurav Jain"
                    },
                    {
                        "name": "Prashant Nair"
                    },
                    {
                        "name": "Poulami Das"
                    }
                ],
                "author_detail": {
                    "name": "Poulami Das"
                },
                "author": "Poulami Das",
                "arxiv_comment": "Published in the Proceedings of the 42nd International Conference on\n  Machine Learning (ICML), Vancouver, Canada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00979v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00979v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06773v1",
                "updated": "2025-06-07T11:50:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    7,
                    11,
                    50,
                    11,
                    5,
                    158,
                    0
                ],
                "published": "2025-06-07T11:50:11Z",
                "published_parsed": [
                    2025,
                    6,
                    7,
                    11,
                    50,
                    11,
                    5,
                    158,
                    0
                ],
                "title": "Taming Wild Branches: Overcoming Hard-to-Predict Branches using the\n  Bullseye Predictor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Taming Wild Branches: Overcoming Hard-to-Predict Branches using the\n  Bullseye Predictor"
                },
                "summary": "Branch prediction is key to the performance of out-of-order processors. While\nthe CBP-2016 winner TAGE-SC-L combines geometric-history tables, a statistical\ncorrector, and a loop predictor, over half of its remaining mispredictions stem\nfrom a small set of hard-to-predict (H2P) branches. These branches occur under\ndiverse global histories, causing repeated thrashing in TAGE and eviction\nbefore usefulness counters can mature. Prior work shows that simply enlarging\nthe tables offers only marginal improvement.\n  We augment a 159 KB TAGE-SC-L predictor with a 28 KB H2P-targeted subsystem\ncalled the Bullseye predictor. It identifies problematic PCs using a\nset-associative H2P Identification Table (HIT) and steers them to one of two\nbranch-specific perceptrons, one indexed by hashed local history and the other\nby folded global history. A short trial phase tracks head-to-head accuracy in\nan H2P cache. A branch becomes perceptron-resident only if the perceptron's\nsustained accuracy and output magnitude exceed dynamic thresholds, after which\nTAGE updates for that PC are suppressed to reduce pollution. The HIT, cache,\nand perceptron operate fully in parallel with TAGE-SC-L, providing higher\nfidelity on the H2P tail. This achieves an average MPKI of 3.4045 and CycWpPKI\nof 145.09.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Branch prediction is key to the performance of out-of-order processors. While\nthe CBP-2016 winner TAGE-SC-L combines geometric-history tables, a statistical\ncorrector, and a loop predictor, over half of its remaining mispredictions stem\nfrom a small set of hard-to-predict (H2P) branches. These branches occur under\ndiverse global histories, causing repeated thrashing in TAGE and eviction\nbefore usefulness counters can mature. Prior work shows that simply enlarging\nthe tables offers only marginal improvement.\n  We augment a 159 KB TAGE-SC-L predictor with a 28 KB H2P-targeted subsystem\ncalled the Bullseye predictor. It identifies problematic PCs using a\nset-associative H2P Identification Table (HIT) and steers them to one of two\nbranch-specific perceptrons, one indexed by hashed local history and the other\nby folded global history. A short trial phase tracks head-to-head accuracy in\nan H2P cache. A branch becomes perceptron-resident only if the perceptron's\nsustained accuracy and output magnitude exceed dynamic thresholds, after which\nTAGE updates for that PC are suppressed to reduce pollution. The HIT, cache,\nand perceptron operate fully in parallel with TAGE-SC-L, providing higher\nfidelity on the H2P tail. This achieves an average MPKI of 3.4045 and CycWpPKI\nof 145.09."
                },
                "authors": [
                    {
                        "name": "Emet Behrendt"
                    },
                    {
                        "name": "Shing Wai Pun"
                    },
                    {
                        "name": "Prashant J. Nair"
                    }
                ],
                "author_detail": {
                    "name": "Prashant J. Nair"
                },
                "author": "Prashant J. Nair",
                "arxiv_comment": "Paper accepted and presented at the 6th Championship Branch\n  Prediction (CBP) workshop, co-held with ISCA 2025, on June 21, 2025, Tokyo,\n  Japan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.2; B.2.1; C.4; C.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03296v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03296v2",
                "updated": "2025-06-07T01:36:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    7,
                    1,
                    36,
                    34,
                    5,
                    158,
                    0
                ],
                "published": "2025-06-03T18:35:56Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    18,
                    35,
                    56,
                    1,
                    154,
                    0
                ],
                "title": "Parallel CPU-GPU Execution for LLM Inference on Constrained GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel CPU-GPU Execution for LLM Inference on Constrained GPUs"
                },
                "summary": "Deploying large language models (LLMs) for online inference is often\nconstrained by limited GPU memory, particularly due to the growing KV cache\nduring auto-regressive decoding. Hybrid GPU-CPU execution has emerged as a\npromising solution by offloading KV cache management and parts of attention\ncomputation to the CPU. However, a key bottleneck remains: existing schedulers\nfail to effectively overlap CPU-offloaded tasks with GPU execution during the\nlatency-critical, bandwidth-bound decode phase. This particularly penalizes\nreal-time, decode-heavy applications (e.g., chat, Chain-of-Thought reasoning)\nwhich are currently underserved by existing systems, especially under memory\npressure typical of edge or low-cost deployments.\n  We present APEX, a novel, profiling-informed scheduling strategy that\nmaximizes CPU-GPU parallelism during hybrid LLM inference. Unlike systems\nrelying on static rules or purely heuristic approaches, APEX dynamically\ndispatches compute across heterogeneous resources by predicting execution times\nof CPU and GPU subtasks to maximize overlap while avoiding scheduling\noverheads. We evaluate APEX on diverse workloads and GPU architectures (NVIDIA\nT4, A10), using LLaMa-2-7B and LLaMa-3.1-8B models. Compared to GPU-only\nschedulers like VLLM, APEX improves throughput by 84% - 96% on T4 and 11% - 89%\non A10 GPUs, while preserving latency. Against the best existing hybrid\nschedulers, it delivers up to 49% (T4) and 37% (A10) higher throughput in\nlong-output settings. APEX significantly advances hybrid LLM inference\nefficiency on such memory-constrained hardware and provides a blueprint for\nscheduling in heterogeneous AI systems, filling a critical gap for efficient\nreal-time LLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying large language models (LLMs) for online inference is often\nconstrained by limited GPU memory, particularly due to the growing KV cache\nduring auto-regressive decoding. Hybrid GPU-CPU execution has emerged as a\npromising solution by offloading KV cache management and parts of attention\ncomputation to the CPU. However, a key bottleneck remains: existing schedulers\nfail to effectively overlap CPU-offloaded tasks with GPU execution during the\nlatency-critical, bandwidth-bound decode phase. This particularly penalizes\nreal-time, decode-heavy applications (e.g., chat, Chain-of-Thought reasoning)\nwhich are currently underserved by existing systems, especially under memory\npressure typical of edge or low-cost deployments.\n  We present APEX, a novel, profiling-informed scheduling strategy that\nmaximizes CPU-GPU parallelism during hybrid LLM inference. Unlike systems\nrelying on static rules or purely heuristic approaches, APEX dynamically\ndispatches compute across heterogeneous resources by predicting execution times\nof CPU and GPU subtasks to maximize overlap while avoiding scheduling\noverheads. We evaluate APEX on diverse workloads and GPU architectures (NVIDIA\nT4, A10), using LLaMa-2-7B and LLaMa-3.1-8B models. Compared to GPU-only\nschedulers like VLLM, APEX improves throughput by 84% - 96% on T4 and 11% - 89%\non A10 GPUs, while preserving latency. Against the best existing hybrid\nschedulers, it delivers up to 49% (T4) and 37% (A10) higher throughput in\nlong-output settings. APEX significantly advances hybrid LLM inference\nefficiency on such memory-constrained hardware and provides a blueprint for\nscheduling in heterogeneous AI systems, filling a critical gap for efficient\nreal-time LLM applications."
                },
                "authors": [
                    {
                        "name": "Jiakun Fan"
                    },
                    {
                        "name": "Yanglin Zhang"
                    },
                    {
                        "name": "Xiangchen Li"
                    },
                    {
                        "name": "Dimitrios S. Nikolopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios S. Nikolopoulos"
                },
                "author": "Dimitrios S. Nikolopoulos",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03296v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03296v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06444v1",
                "updated": "2025-06-06T18:05:45Z",
                "updated_parsed": [
                    2025,
                    6,
                    6,
                    18,
                    5,
                    45,
                    4,
                    157,
                    0
                ],
                "published": "2025-06-06T18:05:45Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    18,
                    5,
                    45,
                    4,
                    157,
                    0
                ],
                "title": "Saffron-1: Towards an Inference Scaling Paradigm for LLM Safety\n  Assurance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Saffron-1: Towards an Inference Scaling Paradigm for LLM Safety\n  Assurance"
                },
                "summary": "Existing safety assurance research has primarily focused on training-phase\nalignment to instill safe behaviors into LLMs. However, recent studies have\nexposed these methods' susceptibility to diverse jailbreak attacks.\nConcurrently, inference scaling has significantly advanced LLM reasoning\ncapabilities but remains unexplored in the context of safety assurance.\nAddressing this gap, our work pioneers inference scaling for robust and\neffective LLM safety against emerging threats. We reveal that conventional\ninference scaling techniques, despite their success in reasoning tasks, perform\npoorly in safety contexts, even falling short of basic approaches like\nBest-of-N Sampling. We attribute this inefficiency to a newly identified\nchallenge, the exploration--efficiency dilemma, arising from the high\ncomputational overhead associated with frequent process reward model (PRM)\nevaluations. To overcome this dilemma, we propose SAFFRON, a novel inference\nscaling paradigm tailored explicitly for safety assurance. Central to our\napproach is the introduction of a multifurcation reward model (MRM) that\nsignificantly reduces the required number of reward model evaluations. To\noperationalize this paradigm, we further propose: (i) a partial supervision\ntraining objective for MRM, (ii) a conservative exploration constraint to\nprevent out-of-distribution explorations, and (iii) a Trie-based key--value\ncaching strategy that facilitates cache sharing across sequences during tree\nsearch. Extensive experiments validate the effectiveness of our method.\nAdditionally, we publicly release our trained multifurcation reward model\n(Saffron-1) and the accompanying token-level safety reward dataset (Safety4M)\nto accelerate future research in LLM safety. Our code, model, and data are\npublicly available at https://github.com/q-rz/saffron , and our project\nhomepage is at https://q-rz.github.io/p/saffron .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing safety assurance research has primarily focused on training-phase\nalignment to instill safe behaviors into LLMs. However, recent studies have\nexposed these methods' susceptibility to diverse jailbreak attacks.\nConcurrently, inference scaling has significantly advanced LLM reasoning\ncapabilities but remains unexplored in the context of safety assurance.\nAddressing this gap, our work pioneers inference scaling for robust and\neffective LLM safety against emerging threats. We reveal that conventional\ninference scaling techniques, despite their success in reasoning tasks, perform\npoorly in safety contexts, even falling short of basic approaches like\nBest-of-N Sampling. We attribute this inefficiency to a newly identified\nchallenge, the exploration--efficiency dilemma, arising from the high\ncomputational overhead associated with frequent process reward model (PRM)\nevaluations. To overcome this dilemma, we propose SAFFRON, a novel inference\nscaling paradigm tailored explicitly for safety assurance. Central to our\napproach is the introduction of a multifurcation reward model (MRM) that\nsignificantly reduces the required number of reward model evaluations. To\noperationalize this paradigm, we further propose: (i) a partial supervision\ntraining objective for MRM, (ii) a conservative exploration constraint to\nprevent out-of-distribution explorations, and (iii) a Trie-based key--value\ncaching strategy that facilitates cache sharing across sequences during tree\nsearch. Extensive experiments validate the effectiveness of our method.\nAdditionally, we publicly release our trained multifurcation reward model\n(Saffron-1) and the accompanying token-level safety reward dataset (Safety4M)\nto accelerate future research in LLM safety. Our code, model, and data are\npublicly available at https://github.com/q-rz/saffron , and our project\nhomepage is at https://q-rz.github.io/p/saffron ."
                },
                "authors": [
                    {
                        "name": "Ruizhong Qiu"
                    },
                    {
                        "name": "Gaotang Li"
                    },
                    {
                        "name": "Tianxin Wei"
                    },
                    {
                        "name": "Jingrui He"
                    },
                    {
                        "name": "Hanghang Tong"
                    }
                ],
                "author_detail": {
                    "name": "Hanghang Tong"
                },
                "author": "Hanghang Tong",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05930v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05930v1",
                "updated": "2025-06-06T09:55:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    6,
                    9,
                    55,
                    59,
                    4,
                    157,
                    0
                ],
                "published": "2025-06-06T09:55:59Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    9,
                    55,
                    59,
                    4,
                    157,
                    0
                ],
                "title": "Neural Visibility Cache for Real-Time Light Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Visibility Cache for Real-Time Light Sampling"
                },
                "summary": "Direct illumination with many lights is an inherent component of\nphysically-based rendering, remaining challenging, especially in real-time\nscenarios. We propose an online-trained neural cache that stores visibility\nbetween lights and 3D positions. We feed light visibility to weighted reservoir\nsampling (WRS) to sample a light source. The cache is implemented as a\nfully-fused multilayer perceptron (MLP) with multi-resolution hash-grid\nencoding, enabling online training and efficient inference on modern GPUs in\nreal-time frame rates. The cache can be seamlessly integrated into existing\nrendering frameworks and can be used in combination with other real-time\ntechniques such as spatiotemporal reservoir sampling (ReSTIR).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct illumination with many lights is an inherent component of\nphysically-based rendering, remaining challenging, especially in real-time\nscenarios. We propose an online-trained neural cache that stores visibility\nbetween lights and 3D positions. We feed light visibility to weighted reservoir\nsampling (WRS) to sample a light source. The cache is implemented as a\nfully-fused multilayer perceptron (MLP) with multi-resolution hash-grid\nencoding, enabling online training and efficient inference on modern GPUs in\nreal-time frame rates. The cache can be seamlessly integrated into existing\nrendering frameworks and can be used in combination with other real-time\ntechniques such as spatiotemporal reservoir sampling (ReSTIR)."
                },
                "authors": [
                    {
                        "name": "Jakub Bokansk"
                    },
                    {
                        "name": "Daniel Meister"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Meister"
                },
                "author": "Daniel Meister",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05930v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05930v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05811v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05811v1",
                "updated": "2025-06-06T07:20:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    6,
                    7,
                    20,
                    25,
                    4,
                    157,
                    0
                ],
                "published": "2025-06-06T07:20:25Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    7,
                    20,
                    25,
                    4,
                    157,
                    0
                ],
                "title": "Synchronous Clock and RF Carrier Transmission for Radio Access Network\n  Fronthaul",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synchronous Clock and RF Carrier Transmission for Radio Access Network\n  Fronthaul"
                },
                "summary": "We simultaneously achieve clock synchronisation, clock-synchronised data\ntransmission and ultra-low noise RF carrier generation by combining clock phase\ncaching and frequency comb transmission in radio access networks (RAN). We\ndemonstrate <100fs jitter for 25GHz RF carrier and 2.5GHz clock, and 16-hour\n6.6ps RMS wander.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We simultaneously achieve clock synchronisation, clock-synchronised data\ntransmission and ultra-low noise RF carrier generation by combining clock phase\ncaching and frequency comb transmission in radio access networks (RAN). We\ndemonstrate <100fs jitter for 25GHz RF carrier and 2.5GHz clock, and 16-hour\n6.6ps RMS wander."
                },
                "authors": [
                    {
                        "name": "Kari Aaron Clark"
                    },
                    {
                        "name": "Zun Htay"
                    },
                    {
                        "name": "Zichuan Zhou"
                    },
                    {
                        "name": "Amany Kassem"
                    },
                    {
                        "name": "Andrea Pertoldi"
                    },
                    {
                        "name": "Benjamin Rudin"
                    },
                    {
                        "name": "Florian Emaury"
                    },
                    {
                        "name": "Izzat Darwazeh"
                    },
                    {
                        "name": "Zhixin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhixin Liu"
                },
                "author": "Zhixin Liu",
                "arxiv_comment": "Conference manuscript submitted to the European Conference on Optical\n  Communication 2025 (ECOC 2025) on 2nd May 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05811v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05811v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.16800v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.16800v2",
                "updated": "2025-06-06T06:35:52Z",
                "updated_parsed": [
                    2025,
                    6,
                    6,
                    6,
                    35,
                    52,
                    4,
                    157,
                    0
                ],
                "published": "2023-05-26T10:29:25Z",
                "published_parsed": [
                    2023,
                    5,
                    26,
                    10,
                    29,
                    25,
                    4,
                    146,
                    0
                ],
                "title": "Joint Optimization of Triangle Mesh, Material, and Light from Neural\n  Fields with Neural Radiance Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Optimization of Triangle Mesh, Material, and Light from Neural\n  Fields with Neural Radiance Cache"
                },
                "summary": "Traditional inverse rendering techniques are based on textured meshes, which\nnaturally adapts to modern graphics pipelines, but costly differentiable\nmulti-bounce Monte Carlo (MC) ray tracing poses challenges for modeling global\nillumination. Recently, neural fields has demonstrated impressive\nreconstruction quality but falls short in modeling indirect illumination. In\nthis paper, we introduce a simple yet efficient inverse rendering framework\nthat combines the strengths of both methods. Specifically, given pre-trained\nneural field representing the scene, we can obtain an initial estimate of the\nsigned distance field (SDF) and create a Neural Radiance Cache (NRC), an\nenhancement over the traditional radiance cache used in real-time rendering. By\nusing the former to initialize differentiable marching tetrahedrons (DMTet) and\nthe latter to model indirect illumination, we can compute the global\nillumination via single-bounce differentiable MC ray tracing and jointly\noptimize the geometry, material, and light through back propagation.\nExperiments demonstrate that, compared to previous methods, our approach\neffectively prevents indirect illumination effects from being baked into\nmaterials, thus obtaining the high-quality reconstruction of triangle mesh,\nPhysically-Based (PBR) materials, and High Dynamic Range (HDR) light probe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional inverse rendering techniques are based on textured meshes, which\nnaturally adapts to modern graphics pipelines, but costly differentiable\nmulti-bounce Monte Carlo (MC) ray tracing poses challenges for modeling global\nillumination. Recently, neural fields has demonstrated impressive\nreconstruction quality but falls short in modeling indirect illumination. In\nthis paper, we introduce a simple yet efficient inverse rendering framework\nthat combines the strengths of both methods. Specifically, given pre-trained\nneural field representing the scene, we can obtain an initial estimate of the\nsigned distance field (SDF) and create a Neural Radiance Cache (NRC), an\nenhancement over the traditional radiance cache used in real-time rendering. By\nusing the former to initialize differentiable marching tetrahedrons (DMTet) and\nthe latter to model indirect illumination, we can compute the global\nillumination via single-bounce differentiable MC ray tracing and jointly\noptimize the geometry, material, and light through back propagation.\nExperiments demonstrate that, compared to previous methods, our approach\neffectively prevents indirect illumination effects from being baked into\nmaterials, thus obtaining the high-quality reconstruction of triangle mesh,\nPhysically-Based (PBR) materials, and High Dynamic Range (HDR) light probe."
                },
                "authors": [
                    {
                        "name": "Jiakai Sun"
                    },
                    {
                        "name": "Weijing Zhang"
                    },
                    {
                        "name": "Zhanjie Zhang"
                    },
                    {
                        "name": "Tianyi Chu"
                    },
                    {
                        "name": "Guangyuan Li"
                    },
                    {
                        "name": "Lei Zhao"
                    },
                    {
                        "name": "Wei Xing"
                    }
                ],
                "author_detail": {
                    "name": "Wei Xing"
                },
                "author": "Wei Xing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.16800v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.16800v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09003v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09003v3",
                "updated": "2025-06-06T02:29:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    6,
                    2,
                    29,
                    18,
                    4,
                    157,
                    0
                ],
                "published": "2025-02-13T06:44:33Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    6,
                    44,
                    33,
                    3,
                    44,
                    0
                ],
                "title": "RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach\n  for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach\n  for Large Language Models"
                },
                "summary": "Supervised fine-tuning is a standard method for adapting pre-trained large\nlanguage models (LLMs) to downstream tasks. Quantization has been recently\nstudied as a post-training technique for efficient LLM deployment. To obtain\nquantized fine-tuned LLMs, conventional pipelines would first fine-tune the\npre-trained models, followed by post-training quantization. This often yields\nsuboptimal performance as it fails to leverage the synergy between fine-tuning\nand quantization. To effectively realize low-bit quantization of weights,\nactivations and KV caches in LLMs, we propose an algorithm named Rotated\nStraight-Through-Estimator (RoSTE), which combines quantization-aware\nsupervised fine-tuning (QA-SFT) with an adaptive rotation strategy that\nidentifies an effective rotation configuration to reduce activation outliers.\nWe provide theoretical insights on RoSTE by analyzing its prediction error when\napplied to an overparameterized least square quantized training problem. Our\nfindings reveal that the prediction error is directly proportional to the\nquantization error of the converged weights, which can be effectively managed\nthrough an optimized rotation configuration. Experiments on Pythia, Qwen and\nLlama models of different sizes demonstrate the effectiveness of RoSTE.\nCompared to existing post-SFT quantization baselines, our method consistently\nachieves superior performances across various tasks and different LLM\narchitectures. Our code is available at https://github.com/OptimAI-Lab/RoSTE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised fine-tuning is a standard method for adapting pre-trained large\nlanguage models (LLMs) to downstream tasks. Quantization has been recently\nstudied as a post-training technique for efficient LLM deployment. To obtain\nquantized fine-tuned LLMs, conventional pipelines would first fine-tune the\npre-trained models, followed by post-training quantization. This often yields\nsuboptimal performance as it fails to leverage the synergy between fine-tuning\nand quantization. To effectively realize low-bit quantization of weights,\nactivations and KV caches in LLMs, we propose an algorithm named Rotated\nStraight-Through-Estimator (RoSTE), which combines quantization-aware\nsupervised fine-tuning (QA-SFT) with an adaptive rotation strategy that\nidentifies an effective rotation configuration to reduce activation outliers.\nWe provide theoretical insights on RoSTE by analyzing its prediction error when\napplied to an overparameterized least square quantized training problem. Our\nfindings reveal that the prediction error is directly proportional to the\nquantization error of the converged weights, which can be effectively managed\nthrough an optimized rotation configuration. Experiments on Pythia, Qwen and\nLlama models of different sizes demonstrate the effectiveness of RoSTE.\nCompared to existing post-SFT quantization baselines, our method consistently\nachieves superior performances across various tasks and different LLM\narchitectures. Our code is available at https://github.com/OptimAI-Lab/RoSTE."
                },
                "authors": [
                    {
                        "name": "Quan Wei"
                    },
                    {
                        "name": "Chung-Yiu Yau"
                    },
                    {
                        "name": "Hoi-To Wai"
                    },
                    {
                        "name": "Yang Katie Zhao"
                    },
                    {
                        "name": "Dongyeop Kang"
                    },
                    {
                        "name": "Youngsuk Park"
                    },
                    {
                        "name": "Mingyi Hong"
                    }
                ],
                "author_detail": {
                    "name": "Mingyi Hong"
                },
                "author": "Mingyi Hong",
                "arxiv_comment": "accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09003v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09003v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05682v1",
                "updated": "2025-06-06T02:20:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    6,
                    2,
                    20,
                    49,
                    4,
                    157,
                    0
                ],
                "published": "2025-06-06T02:20:49Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    2,
                    20,
                    49,
                    4,
                    157,
                    0
                ],
                "title": "Lumina: Real-Time Mobile Neural Rendering by Exploiting Computational\n  Redundancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lumina: Real-Time Mobile Neural Rendering by Exploiting Computational\n  Redundancy"
                },
                "summary": "3D Gaussian Splatting (3DGS) has vastly advanced the pace of neural\nrendering, but it remains computationally demanding on today's mobile SoCs. To\naddress this challenge, we propose Lumina, a hardware-algorithm co-designed\nsystem, which integrates two principal optimizations: a novel algorithm, S^2,\nand a radiance caching mechanism, RC, to improve the efficiency of neural\nrendering. S2 algorithm exploits temporal coherence in rendering to reduce the\ncomputational overhead, while RC leverages the color integration process of\n3DGS to decrease the frequency of intensive rasterization computations. Coupled\nwith these techniques, we propose an accelerator architecture, LuminCore, to\nfurther accelerate cache lookup and address the fundamental inefficiencies in\nRasterization. We show that Lumina achieves 4.5x speedup and 5.3x energy\nreduction against a mobile Volta GPU, with a marginal quality loss (< 0.2 dB\npeak signal-to-noise ratio reduction) across synthetic and real-world datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Gaussian Splatting (3DGS) has vastly advanced the pace of neural\nrendering, but it remains computationally demanding on today's mobile SoCs. To\naddress this challenge, we propose Lumina, a hardware-algorithm co-designed\nsystem, which integrates two principal optimizations: a novel algorithm, S^2,\nand a radiance caching mechanism, RC, to improve the efficiency of neural\nrendering. S2 algorithm exploits temporal coherence in rendering to reduce the\ncomputational overhead, while RC leverages the color integration process of\n3DGS to decrease the frequency of intensive rasterization computations. Coupled\nwith these techniques, we propose an accelerator architecture, LuminCore, to\nfurther accelerate cache lookup and address the fundamental inefficiencies in\nRasterization. We show that Lumina achieves 4.5x speedup and 5.3x energy\nreduction against a mobile Volta GPU, with a marginal quality loss (< 0.2 dB\npeak signal-to-noise ratio reduction) across synthetic and real-world datasets."
                },
                "authors": [
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Weikai Lin"
                    },
                    {
                        "name": "Yuge Cheng"
                    },
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Shixuan Sun"
                    },
                    {
                        "name": "Yuhao Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yuhao Zhu"
                },
                "author": "Yuhao Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22118v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22118v3",
                "updated": "2025-06-05T20:50:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    20,
                    50,
                    51,
                    3,
                    156,
                    0
                ],
                "published": "2024-10-29T15:19:13Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "title": "The Impact of Inference Acceleration on Bias of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Inference Acceleration on Bias of LLMs"
                },
                "summary": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to benefit a vast array of\napplication domains. However, due to their immense size, performing inference\nwith LLMs is both costly and slow. Consequently, a plethora of recent work has\nproposed strategies to enhance inference efficiency, e.g., quantization,\npruning, and caching. These acceleration strategies reduce the inference cost\nand latency, often by several factors, while maintaining much of the predictive\nperformance measured via common benchmarks. In this work, we explore another\ncritical aspect of LLM performance: demographic bias in model generations due\nto inference acceleration optimizations. Using a wide range of metrics, we\nprobe bias in model outputs from a number of angles. Analysis of outputs before\nand after inference acceleration shows significant change in bias. Worryingly,\nthese bias effects are complex and unpredictable. A combination of an\nacceleration strategy and bias type may show little bias change in one model\nbut may lead to a large effect in another. Our results highlight a need for\nin-depth and case-by-case evaluation of model bias after it has been modified\nto accelerate inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to benefit a vast array of\napplication domains. However, due to their immense size, performing inference\nwith LLMs is both costly and slow. Consequently, a plethora of recent work has\nproposed strategies to enhance inference efficiency, e.g., quantization,\npruning, and caching. These acceleration strategies reduce the inference cost\nand latency, often by several factors, while maintaining much of the predictive\nperformance measured via common benchmarks. In this work, we explore another\ncritical aspect of LLM performance: demographic bias in model generations due\nto inference acceleration optimizations. Using a wide range of metrics, we\nprobe bias in model outputs from a number of angles. Analysis of outputs before\nand after inference acceleration shows significant change in bias. Worryingly,\nthese bias effects are complex and unpredictable. A combination of an\nacceleration strategy and bias type may show little bias change in one model\nbut may lead to a large effect in another. Our results highlight a need for\nin-depth and case-by-case evaluation of model bias after it has been modified\nto accelerate inference."
                },
                "authors": [
                    {
                        "name": "Elisabeth Kirsten"
                    },
                    {
                        "name": "Ivan Habernal"
                    },
                    {
                        "name": "Vedant Nanda"
                    },
                    {
                        "name": "Muhammad Bilal Zafar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Bilal Zafar"
                },
                "author": "Muhammad Bilal Zafar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22118v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22118v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05344v1",
                "updated": "2025-06-05T17:59:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    55,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    55,
                    3,
                    156,
                    0
                ],
                "title": "SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs"
                },
                "summary": "Multimodal Large Language Models (MLLMs) are commonly derived by extending\npre-trained Large Language Models (LLMs) with visual capabilities. In this\nwork, we investigate how MLLMs process visual inputs by analyzing their\nattention mechanisms. We reveal a surprising sparsity phenomenon: only a small\nsubset (approximately less than 5%) of attention heads in LLMs actively\ncontribute to visual understanding, termed visual heads. To identify these\nheads efficiently, we design a training-free framework that quantifies\nhead-level visual relevance through targeted response analysis. Building on\nthis discovery, we introduce SparseMM, a KV-Cache optimization strategy that\nallocates asymmetric computation budgets to heads in LLMs based on their visual\nscores, leveraging the sparity of visual heads for accelerating the inference\nof MLLMs. Compared with prior KV-Cache acceleration methods that ignore the\nparticularity of visual, SparseMM prioritizes stress and retaining visual\nsemantics during decoding. Extensive evaluations across mainstream multimodal\nbenchmarks demonstrate that SparseMM achieves superior accuracy-efficiency\ntrade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52%\nmemory reduction during generation while maintaining performance parity on\nefficiency test. Our project is open sourced at\nhttps://github.com/CR400AF-A/SparseMM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) are commonly derived by extending\npre-trained Large Language Models (LLMs) with visual capabilities. In this\nwork, we investigate how MLLMs process visual inputs by analyzing their\nattention mechanisms. We reveal a surprising sparsity phenomenon: only a small\nsubset (approximately less than 5%) of attention heads in LLMs actively\ncontribute to visual understanding, termed visual heads. To identify these\nheads efficiently, we design a training-free framework that quantifies\nhead-level visual relevance through targeted response analysis. Building on\nthis discovery, we introduce SparseMM, a KV-Cache optimization strategy that\nallocates asymmetric computation budgets to heads in LLMs based on their visual\nscores, leveraging the sparity of visual heads for accelerating the inference\nof MLLMs. Compared with prior KV-Cache acceleration methods that ignore the\nparticularity of visual, SparseMM prioritizes stress and retaining visual\nsemantics during decoding. Extensive evaluations across mainstream multimodal\nbenchmarks demonstrate that SparseMM achieves superior accuracy-efficiency\ntrade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52%\nmemory reduction during generation while maintaining performance parity on\nefficiency test. Our project is open sourced at\nhttps://github.com/CR400AF-A/SparseMM."
                },
                "authors": [
                    {
                        "name": "Jiahui Wang"
                    },
                    {
                        "name": "Zuyan Liu"
                    },
                    {
                        "name": "Yongming Rao"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05345v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05345v1",
                "updated": "2025-06-05T17:59:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    55,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    55,
                    3,
                    156,
                    0
                ],
                "title": "Inference-Time Hyper-Scaling with KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Time Hyper-Scaling with KV Cache Compression"
                },
                "summary": "Inference-time scaling trades efficiency for increased reasoning accuracy by\ngenerating longer or more parallel sequences. However, in Transformer LLMs,\ngeneration cost is bottlenecked by the size of the key-value (KV) cache, rather\nthan the number of generated tokens. Hence, we explore inference-time\nhyper-scaling: by compressing the KV cache, we can generate more tokens within\nthe same compute budget and further improve the accuracy of scaled inference.\nThe success of this approach, however, hinges on the ability of compression\nmethods to preserve accuracy even at high compression ratios. To make\nhyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a\nnovel method for sparsifying KV caches that only requires 1K training steps to\nachieve 8$\\times$ compression, while maintaining better accuracy than\ntraining-free sparse attention. Instead of prematurely discarding cached\ntokens, DMS delays token eviction, implicitly merging representations and\npreserving critical information. We demonstrate the effectiveness of\ninference-time hyper-scaling with DMS on multiple families of LLMs, showing\nthat it boosts accuracy for comparable inference runtime and memory load. For\ninstance, we enhance Qwen-R1 32B by an average of 9.1 points on AIME 24, 7.6 on\nGPQA, and 9.6 on LiveCodeBench across compute budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-time scaling trades efficiency for increased reasoning accuracy by\ngenerating longer or more parallel sequences. However, in Transformer LLMs,\ngeneration cost is bottlenecked by the size of the key-value (KV) cache, rather\nthan the number of generated tokens. Hence, we explore inference-time\nhyper-scaling: by compressing the KV cache, we can generate more tokens within\nthe same compute budget and further improve the accuracy of scaled inference.\nThe success of this approach, however, hinges on the ability of compression\nmethods to preserve accuracy even at high compression ratios. To make\nhyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a\nnovel method for sparsifying KV caches that only requires 1K training steps to\nachieve 8$\\times$ compression, while maintaining better accuracy than\ntraining-free sparse attention. Instead of prematurely discarding cached\ntokens, DMS delays token eviction, implicitly merging representations and\npreserving critical information. We demonstrate the effectiveness of\ninference-time hyper-scaling with DMS on multiple families of LLMs, showing\nthat it boosts accuracy for comparable inference runtime and memory load. For\ninstance, we enhance Qwen-R1 32B by an average of 9.1 points on AIME 24, 7.6 on\nGPQA, and 9.6 on LiveCodeBench across compute budgets."
                },
                "authors": [
                    {
                        "name": "Adrian acucki"
                    },
                    {
                        "name": "Konrad Staniszewski"
                    },
                    {
                        "name": "Piotr Nawrot"
                    },
                    {
                        "name": "Edoardo M. Ponti"
                    }
                ],
                "author_detail": {
                    "name": "Edoardo M. Ponti"
                },
                "author": "Edoardo M. Ponti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05345v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05345v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05347v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05347v1",
                "updated": "2025-06-05T17:59:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    55,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    55,
                    3,
                    156,
                    0
                ],
                "title": "Neural Inverse Rendering from Propagating Light",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Inverse Rendering from Propagating Light"
                },
                "summary": "We present the first system for physically based, neural inverse rendering\nfrom multi-viewpoint videos of propagating light. Our approach relies on a\ntime-resolved extension of neural radiance caching -- a technique that\naccelerates inverse rendering by storing infinite-bounce radiance arriving at\nany point from any direction. The resulting model accurately accounts for\ndirect and indirect light transport effects and, when applied to captured\nmeasurements from a flash lidar system, enables state-of-the-art 3D\nreconstruction in the presence of strong indirect light. Further, we\ndemonstrate view synthesis of propagating light, automatic decomposition of\ncaptured measurements into direct and indirect components, as well as novel\ncapabilities such as multi-view time-resolved relighting of captured scenes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the first system for physically based, neural inverse rendering\nfrom multi-viewpoint videos of propagating light. Our approach relies on a\ntime-resolved extension of neural radiance caching -- a technique that\naccelerates inverse rendering by storing infinite-bounce radiance arriving at\nany point from any direction. The resulting model accurately accounts for\ndirect and indirect light transport effects and, when applied to captured\nmeasurements from a flash lidar system, enables state-of-the-art 3D\nreconstruction in the presence of strong indirect light. Further, we\ndemonstrate view synthesis of propagating light, automatic decomposition of\ncaptured measurements into direct and indirect components, as well as novel\ncapabilities such as multi-view time-resolved relighting of captured scenes."
                },
                "authors": [
                    {
                        "name": "Anagh Malik"
                    },
                    {
                        "name": "Benjamin Attal"
                    },
                    {
                        "name": "Andrew Xie"
                    },
                    {
                        "name": "Matthew O'Toole"
                    },
                    {
                        "name": "David B. Lindell"
                    }
                ],
                "author_detail": {
                    "name": "David B. Lindell"
                },
                "author": "David B. Lindell",
                "arxiv_comment": "Website: https://anaghmalik.com/InvProp/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05347v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05347v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05332v1",
                "updated": "2025-06-05T17:59:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    4,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:59:04Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    4,
                    3,
                    156,
                    0
                ],
                "title": "Unleashing Hour-Scale Video Training for Long Video-Language\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing Hour-Scale Video Training for Long Video-Language\n  Understanding"
                },
                "summary": "Recent long-form video-language understanding benchmarks have driven progress\nin video large multimodal models (Video-LMMs). However, the scarcity of\nwell-annotated long videos has left the training of hour-long Video-LLMs\nunderexplored. To close this gap, we present VideoMarathon, a large-scale\nhour-long video instruction-following dataset. This dataset includes around\n9,700 hours of long videos sourced from diverse domains, ranging from 3 to 60\nminutes per video. Specifically, it contains 3.3M high-quality QA pairs,\nspanning six fundamental topics: temporality, spatiality, object, action,\nscene, and event. Compared to existing video instruction datasets,\nVideoMarathon significantly extends training video durations up to 1 hour, and\nsupports 22 diverse tasks requiring both short- and long-term video\ncomprehension. Building on VideoMarathon, we propose Hour-LLaVA, a powerful and\nefficient Video-LMM for hour-scale video-language modeling. It enables\nhour-long video training and inference at 1-FPS sampling by leveraging a memory\naugmentation module, which adaptively integrates user question-relevant and\nspatiotemporal-informative semantics from a cached full video context. In our\nexperiments, Hour-LLaVA achieves the best performance on multiple long\nvideo-language benchmarks, demonstrating the high quality of the VideoMarathon\ndataset and the superiority of the Hour-LLaVA model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent long-form video-language understanding benchmarks have driven progress\nin video large multimodal models (Video-LMMs). However, the scarcity of\nwell-annotated long videos has left the training of hour-long Video-LLMs\nunderexplored. To close this gap, we present VideoMarathon, a large-scale\nhour-long video instruction-following dataset. This dataset includes around\n9,700 hours of long videos sourced from diverse domains, ranging from 3 to 60\nminutes per video. Specifically, it contains 3.3M high-quality QA pairs,\nspanning six fundamental topics: temporality, spatiality, object, action,\nscene, and event. Compared to existing video instruction datasets,\nVideoMarathon significantly extends training video durations up to 1 hour, and\nsupports 22 diverse tasks requiring both short- and long-term video\ncomprehension. Building on VideoMarathon, we propose Hour-LLaVA, a powerful and\nefficient Video-LMM for hour-scale video-language modeling. It enables\nhour-long video training and inference at 1-FPS sampling by leveraging a memory\naugmentation module, which adaptively integrates user question-relevant and\nspatiotemporal-informative semantics from a cached full video context. In our\nexperiments, Hour-LLaVA achieves the best performance on multiple long\nvideo-language benchmarks, demonstrating the high quality of the VideoMarathon\ndataset and the superiority of the Hour-LLaVA model."
                },
                "authors": [
                    {
                        "name": "Jingyang Lin"
                    },
                    {
                        "name": "Jialian Wu"
                    },
                    {
                        "name": "Ximeng Sun"
                    },
                    {
                        "name": "Ze Wang"
                    },
                    {
                        "name": "Jiang Liu"
                    },
                    {
                        "name": "Yusheng Su"
                    },
                    {
                        "name": "Xiaodong Yu"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Jiebo Luo"
                    },
                    {
                        "name": "Zicheng Liu"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "arxiv_comment": "Project page: https://videomarathon.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05071v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05071v1",
                "updated": "2025-06-05T14:19:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    19,
                    5,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T14:19:05Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    19,
                    5,
                    3,
                    156,
                    0
                ],
                "title": "Memory Hierarchy Design for Caching Middleware in the Age of NVM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory Hierarchy Design for Caching Middleware in the Age of NVM"
                },
                "summary": "Advances in storage technology have introduced Non-Volatile Memory, NVM, as a\nnew storage medium. NVM, along with Dynamic Random Access Memory (DRAM), Solid\nState Disk (SSD), and Disk present a system designer with a wide array of\noptions in designing caching middleware. Moreover, design decisions to\nreplicate a data item in more than one level of a caching memory hierarchy may\nenhance the overall system performance with a faster recovery time in the event\nof a memory failure. Given a fixed budget, the key configuration questions are:\nWhich storage media should constitute the memory hierarchy? What is the storage\ncapacity of each hierarchy? Should data be replicated or partitioned across the\ndifferent levels of the hierarchy? We model these cache configuration questions\nas an instance of the Multiple Choice Knapsack Problem (MCKP). This model is\nguided by the specification of each type of memory along with an application's\ndatabase characteristics and its workload. Although MCKP is NP-complete, its\nlinear programming relaxation is efficiently solvable and can be used to\nclosely approximate the optimal solution. We use the resulting simple algorithm\nto evaluate design tradeoffs in the context of a memory hierarchy for a\nKey-Value Store (e.g., memcached) as well as a host-side cache (e.g.,\nFlashcache). The results show selective replication is appropriate with certain\nfailure rates and workload characteristics. With a slim failure rate and\nfrequent data updates, tiering of data across the different storage media that\nconstitute the cache is superior to replication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advances in storage technology have introduced Non-Volatile Memory, NVM, as a\nnew storage medium. NVM, along with Dynamic Random Access Memory (DRAM), Solid\nState Disk (SSD), and Disk present a system designer with a wide array of\noptions in designing caching middleware. Moreover, design decisions to\nreplicate a data item in more than one level of a caching memory hierarchy may\nenhance the overall system performance with a faster recovery time in the event\nof a memory failure. Given a fixed budget, the key configuration questions are:\nWhich storage media should constitute the memory hierarchy? What is the storage\ncapacity of each hierarchy? Should data be replicated or partitioned across the\ndifferent levels of the hierarchy? We model these cache configuration questions\nas an instance of the Multiple Choice Knapsack Problem (MCKP). This model is\nguided by the specification of each type of memory along with an application's\ndatabase characteristics and its workload. Although MCKP is NP-complete, its\nlinear programming relaxation is efficiently solvable and can be used to\nclosely approximate the optimal solution. We use the resulting simple algorithm\nto evaluate design tradeoffs in the context of a memory hierarchy for a\nKey-Value Store (e.g., memcached) as well as a host-side cache (e.g.,\nFlashcache). The results show selective replication is appropriate with certain\nfailure rates and workload characteristics. With a slim failure rate and\nfrequent data updates, tiering of data across the different storage media that\nconstitute the cache is superior to replication."
                },
                "authors": [
                    {
                        "name": "Shahram Ghandeharizadeh"
                    },
                    {
                        "name": "Sandy Irani"
                    },
                    {
                        "name": "Jenny Lam"
                    }
                ],
                "author_detail": {
                    "name": "Jenny Lam"
                },
                "author": "Jenny Lam",
                "arxiv_doi": "10.1109/ICDE.2018.00155",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ICDE.2018.00155",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.05071v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05071v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "A shorter version appeared in the IEEE 34th International Conference\n  on Data Engineering (ICDE), Paris, France, 2018, pp. 1380-1383, doi:\n  10.1109/ICDE.2018.00155",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16950v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16950v2",
                "updated": "2025-06-05T13:38:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    13,
                    38,
                    34,
                    3,
                    156,
                    0
                ],
                "published": "2025-05-22T17:33:49Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    33,
                    49,
                    3,
                    142,
                    0
                ],
                "title": "Bottlenecked Transformers: Periodic KV Cache Abstraction for Generalised\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bottlenecked Transformers: Periodic KV Cache Abstraction for Generalised\n  Reasoning"
                },
                "summary": "Despite their impressive capabilities, Large Language Models struggle with\ngeneralisation beyond their training distribution, often exhibiting\nsophisticated pattern interpolation rather than true abstract reasoning\n(extrapolation). In this work, we approach this limitation through the lens of\nInformation Bottleneck (IB) theory, which posits that model generalisation\nemerges from an optimal balance between input compression and retention of\npredictive information in latent representations. We prove using IB theory that\ndecoder-only Transformers are inherently constrained in their ability to form\ntask-optimal sequence representations. We then use this result to demonstrate\nthat periodic global transformation of the internal sequence-level\nrepresentations (KV cache) is a necessary computational step for improving\nTransformer generalisation in reasoning tasks. Based on these theoretical\ninsights, we propose a modification to the Transformer architecture, in the\nform of an additional module that globally rewrites the KV cache at periodic\nintervals, shifting its capacity away from memorising input prefixes and toward\nencoding features most useful for predicting future tokens. Our model delivers\nsubstantial gains on mathematical reasoning benchmarks, outperforming both\nvanilla Transformers with up to 3.5x more parameters, as well as\nheuristic-driven pruning mechanisms for cache compression. Our approach can be\nseen as a principled generalisation of existing KV-cache compression methods;\nwhereas such methods focus solely on compressing input representations, they\noften do so at the expense of retaining predictive information, and thus their\ncapabilities are inherently bounded by those of an unconstrained model. This\nestablishes a principled framework to manipulate Transformer memory using\ninformation theory, addressing fundamental reasoning limitations that scaling\nalone cannot overcome.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their impressive capabilities, Large Language Models struggle with\ngeneralisation beyond their training distribution, often exhibiting\nsophisticated pattern interpolation rather than true abstract reasoning\n(extrapolation). In this work, we approach this limitation through the lens of\nInformation Bottleneck (IB) theory, which posits that model generalisation\nemerges from an optimal balance between input compression and retention of\npredictive information in latent representations. We prove using IB theory that\ndecoder-only Transformers are inherently constrained in their ability to form\ntask-optimal sequence representations. We then use this result to demonstrate\nthat periodic global transformation of the internal sequence-level\nrepresentations (KV cache) is a necessary computational step for improving\nTransformer generalisation in reasoning tasks. Based on these theoretical\ninsights, we propose a modification to the Transformer architecture, in the\nform of an additional module that globally rewrites the KV cache at periodic\nintervals, shifting its capacity away from memorising input prefixes and toward\nencoding features most useful for predicting future tokens. Our model delivers\nsubstantial gains on mathematical reasoning benchmarks, outperforming both\nvanilla Transformers with up to 3.5x more parameters, as well as\nheuristic-driven pruning mechanisms for cache compression. Our approach can be\nseen as a principled generalisation of existing KV-cache compression methods;\nwhereas such methods focus solely on compressing input representations, they\noften do so at the expense of retaining predictive information, and thus their\ncapabilities are inherently bounded by those of an unconstrained model. This\nestablishes a principled framework to manipulate Transformer memory using\ninformation theory, addressing fundamental reasoning limitations that scaling\nalone cannot overcome."
                },
                "authors": [
                    {
                        "name": "Adnan Oomerjee"
                    },
                    {
                        "name": "Zafeirios Fountas"
                    },
                    {
                        "name": "Zhongwei Yu"
                    },
                    {
                        "name": "Haitham Bou-Ammar"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16950v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16950v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13063v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13063v2",
                "updated": "2025-06-05T13:20:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    13,
                    20,
                    9,
                    3,
                    156,
                    0
                ],
                "published": "2025-02-18T17:08:45Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    8,
                    45,
                    1,
                    49,
                    0
                ],
                "title": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity"
                },
                "summary": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches are focused on\nreduction of the amount of compute in existing language models rather than\nminimization of number of bits needed to store text. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches are focused on\nreduction of the amount of compute in existing language models rather than\nminimization of number of bits needed to store text. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign."
                },
                "authors": [
                    {
                        "name": "Yuri Kuratov"
                    },
                    {
                        "name": "Mikhail Arkhipov"
                    },
                    {
                        "name": "Aydar Bulatov"
                    },
                    {
                        "name": "Mikhail Burtsev"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Burtsev"
                },
                "author": "Mikhail Burtsev",
                "arxiv_comment": "ACL 2025 (main conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13063v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13063v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04920v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04920v1",
                "updated": "2025-06-05T11:53:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    11,
                    53,
                    4,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T11:53:04Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    11,
                    53,
                    4,
                    3,
                    156,
                    0
                ],
                "title": "Simulating LLM-to-LLM Tutoring for Multilingual Math Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating LLM-to-LLM Tutoring for Multilingual Math Feedback"
                },
                "summary": "Large language models (LLMs) have demonstrated the ability to generate\nformative feedback and instructional hints in English, making them increasingly\nrelevant for AI-assisted education. However, their ability to provide effective\ninstructional support across different languages, especially for mathematically\ngrounded reasoning tasks, remains largely unexamined. In this work, we present\nthe first large-scale simulation of multilingual tutor-student interactions\nusing LLMs. A stronger model plays the role of the tutor, generating feedback\nin the form of hints, while a weaker model simulates the student. We explore\n352 experimental settings across 11 typologically diverse languages, four\nstate-of-the-art LLMs, and multiple prompting strategies to assess whether\nlanguage-specific feedback leads to measurable learning gains. Our study\nexamines how student input language, teacher feedback language, model choice,\nand language resource level jointly influence performance. Results show that\nmultilingual hints can significantly improve learning outcomes, particularly in\nlow-resource languages when feedback is aligned with the student's native\nlanguage. These findings offer practical insights for developing multilingual,\nLLM-based educational tools that are both effective and inclusive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated the ability to generate\nformative feedback and instructional hints in English, making them increasingly\nrelevant for AI-assisted education. However, their ability to provide effective\ninstructional support across different languages, especially for mathematically\ngrounded reasoning tasks, remains largely unexamined. In this work, we present\nthe first large-scale simulation of multilingual tutor-student interactions\nusing LLMs. A stronger model plays the role of the tutor, generating feedback\nin the form of hints, while a weaker model simulates the student. We explore\n352 experimental settings across 11 typologically diverse languages, four\nstate-of-the-art LLMs, and multiple prompting strategies to assess whether\nlanguage-specific feedback leads to measurable learning gains. Our study\nexamines how student input language, teacher feedback language, model choice,\nand language resource level jointly influence performance. Results show that\nmultilingual hints can significantly improve learning outcomes, particularly in\nlow-resource languages when feedback is aligned with the student's native\nlanguage. These findings offer practical insights for developing multilingual,\nLLM-based educational tools that are both effective and inclusive."
                },
                "authors": [
                    {
                        "name": "Junior Cedric Tonga"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "Fajri Koto"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "Preprint, in submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04920v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04920v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04844v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04844v1",
                "updated": "2025-06-05T10:11:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    10,
                    11,
                    4,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T10:11:04Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    10,
                    11,
                    4,
                    3,
                    156,
                    0
                ],
                "title": "Characterization of the Hamamatsu R12699-406-M4 Photomultiplier Tube in\n  Cold Xenon Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterization of the Hamamatsu R12699-406-M4 Photomultiplier Tube in\n  Cold Xenon Environments"
                },
                "summary": "The Hamamatsu R12699-406-M2 is a $2\\times2$ multi-anode 2-inch\nphotomultiplier tube that offers a compact form factor, low intrinsic\nradioactivity, and high photocathode coverage. These characteristics make it a\npromising candidate for next-generation xenon-based direct detection dark\nmatter experiments, such as XLZD and PandaX-xT. We present a detailed\ncharacterization of this photosensor operated in cold xenon environments,\nfocusing on its single photoelectron response, dark count rate, light emission,\nand afterpulsing behavior. The device demonstrated a gain exceeding $2\\cdot\n10^6$ at the nominal voltage of -1.0 kV, along with a low dark count rate of\n$(0.4\\pm0.2)\\;\\text{Hz/cm}^2$. Due to the compact design, afterpulses exhibited\nshort delay times, resulting in some cases in an overlap with the light-induced\nsignal. To evaluate its applicability in a realistic detector environment, two\nR12699-406-M2 units were deployed in a small-scale dual-phase xenon time\nprojection chamber. The segmented $2\\times2$ anode structure enabled lateral\nposition reconstruction using a single photomultiplier tube, highlighting the\npotential of the sensor for effective event localization in future detectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hamamatsu R12699-406-M2 is a $2\\times2$ multi-anode 2-inch\nphotomultiplier tube that offers a compact form factor, low intrinsic\nradioactivity, and high photocathode coverage. These characteristics make it a\npromising candidate for next-generation xenon-based direct detection dark\nmatter experiments, such as XLZD and PandaX-xT. We present a detailed\ncharacterization of this photosensor operated in cold xenon environments,\nfocusing on its single photoelectron response, dark count rate, light emission,\nand afterpulsing behavior. The device demonstrated a gain exceeding $2\\cdot\n10^6$ at the nominal voltage of -1.0 kV, along with a low dark count rate of\n$(0.4\\pm0.2)\\;\\text{Hz/cm}^2$. Due to the compact design, afterpulses exhibited\nshort delay times, resulting in some cases in an overlap with the light-induced\nsignal. To evaluate its applicability in a realistic detector environment, two\nR12699-406-M2 units were deployed in a small-scale dual-phase xenon time\nprojection chamber. The segmented $2\\times2$ anode structure enabled lateral\nposition reconstruction using a single photomultiplier tube, highlighting the\npotential of the sensor for effective event localization in future detectors."
                },
                "authors": [
                    {
                        "name": "M. Adrover"
                    },
                    {
                        "name": "L. Baudis"
                    },
                    {
                        "name": "A. Bismark"
                    },
                    {
                        "name": "A. P. Colijn"
                    },
                    {
                        "name": "J. J. Cuenca-Garca"
                    },
                    {
                        "name": "M. P. Decowski"
                    },
                    {
                        "name": "M. Flierman"
                    },
                    {
                        "name": "T. den Hollander"
                    }
                ],
                "author_detail": {
                    "name": "T. den Hollander"
                },
                "author": "T. den Hollander",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04844v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04844v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04826v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04826v1",
                "updated": "2025-06-05T09:49:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    9,
                    49,
                    1,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T09:49:01Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    9,
                    49,
                    1,
                    3,
                    156,
                    0
                ],
                "title": "Discharge dynamics in a cylindrical SDBD prototype reactor under\n  ns-pulsed and sinusoidal AC operation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discharge dynamics in a cylindrical SDBD prototype reactor under\n  ns-pulsed and sinusoidal AC operation"
                },
                "summary": "We developed a prototype reactor generating surface dielectric barrier\ndischarges (SDBDs) in ambient air, designed for consistent operation while\npreventing constructive material degradation. It features detachable stainless\nsteel electrodes and quartz dielectric to ensure precise fabrication. The\ngrounded electrode is fully immersed into transformer oil drastically\nsuppressing undesired parasitic discharges. The device efficiently sustains\nns-pulsed and AC discharges at 10 kHz, enabling fundamental studies of their\nelectrical characteristics (applied voltage, induced current, electric power)\nand spatiotemporal dynamics (morphology, propagation length and velocity). The\nelectric power (P) consumed exhibits a dissimilar non-linear increase with the\nrising peak voltage (Vp) in each case: P$\\approx$0.8-2.5 W for ns-pulsed\n(Vp=7-9 kV) and P$\\approx$0.9-5.3 W (Vp=7-10 kV) for AC operation. Using ICCD\nimaging, distinct ionization channels are recorded in the rising part of the\npulsed voltage being detached from the driven electrode; during the voltage\ndecrease, a glow-like discharge is formed remaining anchored on the driven\nelectrode. The rising part of the AC voltage is characterized by erratic,\nelongated ionization channels in a filamentary form, the voltage drop featuring\na glow-like behavior. During the rising and falling parts of the AC voltage,\nthe discharge reaches maximum propagation lengths (Lmax) of $\\approx$12 mm and\n$\\approx$7 mm, respectively, while remaining attached to the driven electrode.\nThe corresponding maximum discharge velocities (vmax) are about 5x10 2 m/s and\n3x10 2 m/s. For the ns-pulsed operation, Lmax$\\approx$5 mm (vmax$\\approx$5x10 5\nm/s) and Lmax$\\approx$3.5 mm (vmax$\\approx$1.5x10 5 m/s) during the rising and\nfalling parts of the voltage pulse, respectively. The SDBD dynamics generated\nwith a ns-pulsed voltage is more reproducible than for the AC case allowing for\nthe use of a 500 times smaller ICCD gate width (2 ns) and a more accurate\ndescription of the discharge's spatiotemporal development. This reactor is\nsuitable for performing fundamental studies and understanding key SDBD features\nfor various applications such as flow control, biomedicine and agriculture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We developed a prototype reactor generating surface dielectric barrier\ndischarges (SDBDs) in ambient air, designed for consistent operation while\npreventing constructive material degradation. It features detachable stainless\nsteel electrodes and quartz dielectric to ensure precise fabrication. The\ngrounded electrode is fully immersed into transformer oil drastically\nsuppressing undesired parasitic discharges. The device efficiently sustains\nns-pulsed and AC discharges at 10 kHz, enabling fundamental studies of their\nelectrical characteristics (applied voltage, induced current, electric power)\nand spatiotemporal dynamics (morphology, propagation length and velocity). The\nelectric power (P) consumed exhibits a dissimilar non-linear increase with the\nrising peak voltage (Vp) in each case: P$\\approx$0.8-2.5 W for ns-pulsed\n(Vp=7-9 kV) and P$\\approx$0.9-5.3 W (Vp=7-10 kV) for AC operation. Using ICCD\nimaging, distinct ionization channels are recorded in the rising part of the\npulsed voltage being detached from the driven electrode; during the voltage\ndecrease, a glow-like discharge is formed remaining anchored on the driven\nelectrode. The rising part of the AC voltage is characterized by erratic,\nelongated ionization channels in a filamentary form, the voltage drop featuring\na glow-like behavior. During the rising and falling parts of the AC voltage,\nthe discharge reaches maximum propagation lengths (Lmax) of $\\approx$12 mm and\n$\\approx$7 mm, respectively, while remaining attached to the driven electrode.\nThe corresponding maximum discharge velocities (vmax) are about 5x10 2 m/s and\n3x10 2 m/s. For the ns-pulsed operation, Lmax$\\approx$5 mm (vmax$\\approx$5x10 5\nm/s) and Lmax$\\approx$3.5 mm (vmax$\\approx$1.5x10 5 m/s) during the rising and\nfalling parts of the voltage pulse, respectively. The SDBD dynamics generated\nwith a ns-pulsed voltage is more reproducible than for the AC case allowing for\nthe use of a 500 times smaller ICCD gate width (2 ns) and a more accurate\ndescription of the discharge's spatiotemporal development. This reactor is\nsuitable for performing fundamental studies and understanding key SDBD features\nfor various applications such as flow control, biomedicine and agriculture."
                },
                "authors": [
                    {
                        "name": "Konstantinos Giotis"
                    },
                    {
                        "name": "Dimitrios Stefas"
                    },
                    {
                        "name": "Yanis Agha"
                    },
                    {
                        "name": "Hans Hft"
                    },
                    {
                        "name": "Xavier Duten"
                    },
                    {
                        "name": "Panagiotis Svarnas"
                    },
                    {
                        "name": "Guillaume Lombardi"
                    },
                    {
                        "name": "Kristaq Gazeli"
                    }
                ],
                "author_detail": {
                    "name": "Kristaq Gazeli"
                },
                "arxiv_affiliation": "LSPM",
                "author": "Kristaq Gazeli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04826v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04826v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04108v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04108v2",
                "updated": "2025-06-05T05:39:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    5,
                    39,
                    48,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-04T16:01:48Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    1,
                    48,
                    2,
                    155,
                    0
                ],
                "title": "Rectified Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rectified Sparse Attention"
                },
                "summary": "Efficient long-sequence generation is a critical challenge for Large Language\nModels. While recent sparse decoding methods improve efficiency, they suffer\nfrom KV cache misalignment, where approximation errors accumulate and degrade\ngeneration quality. In this work, we propose Rectified Sparse Attention (ReSA),\na simple yet effective method that combines block-sparse attention with\nperiodic dense rectification. By refreshing the KV cache at fixed intervals\nusing a dense forward pass, ReSA bounds error accumulation and preserves\nalignment with the pretraining distribution. Experiments across math reasoning,\nlanguage modeling, and retrieval tasks demonstrate that ReSA achieves\nnear-lossless generation quality with significantly improved efficiency.\nNotably, ReSA delivers up to 2.42$\\times$ end-to-end speedup under decoding at\n256K sequence length, making it a practical solution for scalable long-context\ninference. Code is available at https://aka.ms/ReSA-LM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient long-sequence generation is a critical challenge for Large Language\nModels. While recent sparse decoding methods improve efficiency, they suffer\nfrom KV cache misalignment, where approximation errors accumulate and degrade\ngeneration quality. In this work, we propose Rectified Sparse Attention (ReSA),\na simple yet effective method that combines block-sparse attention with\nperiodic dense rectification. By refreshing the KV cache at fixed intervals\nusing a dense forward pass, ReSA bounds error accumulation and preserves\nalignment with the pretraining distribution. Experiments across math reasoning,\nlanguage modeling, and retrieval tasks demonstrate that ReSA achieves\nnear-lossless generation quality with significantly improved efficiency.\nNotably, ReSA delivers up to 2.42$\\times$ end-to-end speedup under decoding at\n256K sequence length, making it a practical solution for scalable long-context\ninference. Code is available at https://aka.ms/ReSA-LM."
                },
                "authors": [
                    {
                        "name": "Yutao Sun"
                    },
                    {
                        "name": "Tianzhu Ye"
                    },
                    {
                        "name": "Li Dong"
                    },
                    {
                        "name": "Yuqing Xia"
                    },
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Yizhao Gao"
                    },
                    {
                        "name": "Shijie Cao"
                    },
                    {
                        "name": "Jianyong Wang"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04108v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04642v1",
                "updated": "2025-06-05T05:23:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    5,
                    23,
                    38,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T05:23:38Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    5,
                    23,
                    38,
                    3,
                    156,
                    0
                ],
                "title": "TaDA: Training-free recipe for Decoding with Adaptive KV Cache\n  Compression and Mean-centering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TaDA: Training-free recipe for Decoding with Adaptive KV Cache\n  Compression and Mean-centering"
                },
                "summary": "The key-value (KV) cache in transformer models is a critical component for\nefficient decoding or inference, yet its memory demands scale poorly with\nsequence length, posing a major challenge for scalable deployment of large\nlanguage models. Among several approaches to KV cache compression, quantization\nof key and value activations has been widely explored. Most KV cache\nquantization methods still need to manage sparse and noncontiguous outliers\nseparately. To address this, we introduce TaDA, a training-free recipe for KV\ncache compression with quantization precision that adapts to error sensitivity\nacross layers and a mean centering to eliminate separate outlier handling. Our\napproach yields substantial accuracy improvements for multiple models\nsupporting various context lengths. Moreover, our approach does not need to\nseparately manage outlier elements -- a persistent hurdle in most traditional\nquantization methods. Experiments on standard benchmarks demonstrate that our\ntechnique reduces KV cache memory footprint to 27% of the original 16-bit\nbaseline while achieving comparable accuracy. Our method paves the way for\nscalable and high-performance reasoning in language models by potentially\nenabling inference for longer context length models, reasoning models, and\nlonger chain of thoughts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key-value (KV) cache in transformer models is a critical component for\nefficient decoding or inference, yet its memory demands scale poorly with\nsequence length, posing a major challenge for scalable deployment of large\nlanguage models. Among several approaches to KV cache compression, quantization\nof key and value activations has been widely explored. Most KV cache\nquantization methods still need to manage sparse and noncontiguous outliers\nseparately. To address this, we introduce TaDA, a training-free recipe for KV\ncache compression with quantization precision that adapts to error sensitivity\nacross layers and a mean centering to eliminate separate outlier handling. Our\napproach yields substantial accuracy improvements for multiple models\nsupporting various context lengths. Moreover, our approach does not need to\nseparately manage outlier elements -- a persistent hurdle in most traditional\nquantization methods. Experiments on standard benchmarks demonstrate that our\ntechnique reduces KV cache memory footprint to 27% of the original 16-bit\nbaseline while achieving comparable accuracy. Our method paves the way for\nscalable and high-performance reasoning in language models by potentially\nenabling inference for longer context length models, reasoning models, and\nlonger chain of thoughts."
                },
                "authors": [
                    {
                        "name": "Vinay Joshi"
                    },
                    {
                        "name": "Pratik Prabhanjan Brahma"
                    },
                    {
                        "name": "Zicheng Liu"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "arxiv_comment": "ACL-2025 industry-track accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05460v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05460v3",
                "updated": "2025-06-05T04:21:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    4,
                    21,
                    30,
                    3,
                    156,
                    0
                ],
                "published": "2024-12-25T10:11:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    10,
                    11,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "Efficiently Serving Large Multimodal Models Using EPD Disaggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently Serving Large Multimodal Models Using EPD Disaggregation"
                },
                "summary": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step negatively affects key Service Level Objectives (SLOs),\nsuch as time to first token (TTFT) and time per output token (TPOT). We\nintroduce Encode-Prefill-Decode (EPD) Disaggregation, a novel framework that\nseparates the encoding, prefill, and decode stages onto dedicated resources.\nUnlike current systems, which bundle encoding and prefill together, our\napproach decouples these steps, unlocking new opportunities and optimizations.\nThese include a mechanism to cache multimedia tokens for efficient transfer, a\nnovel way to parallelize the encoding load within a request, a module for\noptimal resource allocation for disaggregated serving, and a novel\nrole-switching method to handle changing workload characteristics. Experimental\nevaluations with popular LMMs show substantial gains in memory efficiency (up\nto 15x lower peak memory utilization), batch sizes (up to 22x larger), 10x more\nimages per request, and 2.2x larger KV caches. Furthermore, it leads to\nsignificant improvements in SLO attainment (up to 90-100% improvement) and TTFT\n(up to 71% reduction), compared to systems that do not disaggregate. The code\nis available at https://github.com/vbdi/epdserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step negatively affects key Service Level Objectives (SLOs),\nsuch as time to first token (TTFT) and time per output token (TPOT). We\nintroduce Encode-Prefill-Decode (EPD) Disaggregation, a novel framework that\nseparates the encoding, prefill, and decode stages onto dedicated resources.\nUnlike current systems, which bundle encoding and prefill together, our\napproach decouples these steps, unlocking new opportunities and optimizations.\nThese include a mechanism to cache multimedia tokens for efficient transfer, a\nnovel way to parallelize the encoding load within a request, a module for\noptimal resource allocation for disaggregated serving, and a novel\nrole-switching method to handle changing workload characteristics. Experimental\nevaluations with popular LMMs show substantial gains in memory efficiency (up\nto 15x lower peak memory utilization), batch sizes (up to 22x larger), 10x more\nimages per request, and 2.2x larger KV caches. Furthermore, it leads to\nsignificant improvements in SLO attainment (up to 90-100% improvement) and TTFT\n(up to 71% reduction), compared to systems that do not disaggregate. The code\nis available at https://github.com/vbdi/epdserve."
                },
                "authors": [
                    {
                        "name": "Gursimran Singh"
                    },
                    {
                        "name": "Xinglu Wang"
                    },
                    {
                        "name": "Yifan Hu"
                    },
                    {
                        "name": "Timothy Yu"
                    },
                    {
                        "name": "Linzi Xing"
                    },
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "arxiv_comment": "17 pages, 12 figures, 9 tables",
                "arxiv_journal_ref": "International Conference on Machine Proceedings of the 42nd\n  International Conference on Machine Learning, Vancouver, Canada. PMLR 267,\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05460v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05460v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04213v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04213v2",
                "updated": "2025-06-05T03:35:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    3,
                    35,
                    21,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-04T17:57:09Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    57,
                    9,
                    2,
                    155,
                    0
                ],
                "title": "FullDiT2: Efficient In-Context Conditioning for Video Diffusion\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FullDiT2: Efficient In-Context Conditioning for Video Diffusion\n  Transformers"
                },
                "summary": "Fine-grained and efficient controllability on video diffusion transformers\nhas raised increasing desires for the applicability. Recently, In-context\nConditioning emerged as a powerful paradigm for unified conditional video\ngeneration, which enables diverse controls by concatenating varying context\nconditioning signals with noisy video latents into a long unified token\nsequence and jointly processing them via full-attention, e.g., FullDiT. Despite\ntheir effectiveness, these methods face quadratic computation overhead as task\ncomplexity increases, hindering practical deployment. In this paper, we study\nthe efficiency bottleneck neglected in original in-context conditioning video\ngeneration framework. We begin with systematic analysis to identify two key\nsources of the computation inefficiencies: the inherent redundancy within\ncontext condition tokens and the computational redundancy in context-latent\ninteractions throughout the diffusion process. Based on these insights, we\npropose FullDiT2, an efficient in-context conditioning framework for general\ncontrollability in both video generation and editing tasks, which innovates\nfrom two key perspectives. Firstly, to address the token redundancy, FullDiT2\nleverages a dynamic token selection mechanism to adaptively identify important\ncontext tokens, reducing the sequence length for unified full-attention.\nAdditionally, a selective context caching mechanism is devised to minimize\nredundant interactions between condition tokens and video latents. Extensive\nexperiments on six diverse conditional video editing and generation tasks\ndemonstrate that FullDiT2 achieves significant computation reduction and 2-3\ntimes speedup in averaged time cost per diffusion step, with minimal\ndegradation or even higher performance in video generation quality. The project\npage is at \\href{https://fulldit2.github.io/}{https://fulldit2.github.io/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-grained and efficient controllability on video diffusion transformers\nhas raised increasing desires for the applicability. Recently, In-context\nConditioning emerged as a powerful paradigm for unified conditional video\ngeneration, which enables diverse controls by concatenating varying context\nconditioning signals with noisy video latents into a long unified token\nsequence and jointly processing them via full-attention, e.g., FullDiT. Despite\ntheir effectiveness, these methods face quadratic computation overhead as task\ncomplexity increases, hindering practical deployment. In this paper, we study\nthe efficiency bottleneck neglected in original in-context conditioning video\ngeneration framework. We begin with systematic analysis to identify two key\nsources of the computation inefficiencies: the inherent redundancy within\ncontext condition tokens and the computational redundancy in context-latent\ninteractions throughout the diffusion process. Based on these insights, we\npropose FullDiT2, an efficient in-context conditioning framework for general\ncontrollability in both video generation and editing tasks, which innovates\nfrom two key perspectives. Firstly, to address the token redundancy, FullDiT2\nleverages a dynamic token selection mechanism to adaptively identify important\ncontext tokens, reducing the sequence length for unified full-attention.\nAdditionally, a selective context caching mechanism is devised to minimize\nredundant interactions between condition tokens and video latents. Extensive\nexperiments on six diverse conditional video editing and generation tasks\ndemonstrate that FullDiT2 achieves significant computation reduction and 2-3\ntimes speedup in averaged time cost per diffusion step, with minimal\ndegradation or even higher performance in video generation quality. The project\npage is at \\href{https://fulldit2.github.io/}{https://fulldit2.github.io/}."
                },
                "authors": [
                    {
                        "name": "Xuanhua He"
                    },
                    {
                        "name": "Quande Liu"
                    },
                    {
                        "name": "Zixuan Ye"
                    },
                    {
                        "name": "Weicai Ye"
                    },
                    {
                        "name": "Qiulin Wang"
                    },
                    {
                        "name": "Xintao Wang"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Kun Gai"
                    }
                ],
                "author_detail": {
                    "name": "Kun Gai"
                },
                "author": "Kun Gai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04213v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04213v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24357v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24357v2",
                "updated": "2025-06-05T02:27:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    2,
                    27,
                    34,
                    3,
                    156,
                    0
                ],
                "published": "2025-05-30T08:49:27Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    8,
                    49,
                    27,
                    4,
                    150,
                    0
                ],
                "title": "ReCalKV: Low-Rank KV Cache Compression via Head Reordering and Offline\n  Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReCalKV: Low-Rank KV Cache Compression via Head Reordering and Offline\n  Calibration"
                },
                "summary": "Large language models (LLMs) have achieved remarkable performance, yet their\ncapability on long-context reasoning is often constrained by the excessive\nmemory required to store the Key-Value (KV) cache. This makes KV cache\ncompression an essential step toward enabling efficient long-context reasoning.\nRecent methods have explored reducing the hidden dimensions of the KV cache,\nbut many introduce additional computation through projection layers or suffer\nfrom significant performance degradation under high compression ratios. To\naddress these challenges, we propose ReCalKV, a post-training KV cache\ncompression method that reduces the hidden dimensions of the KV cache. We\ndevelop distinct compression strategies for Keys and Values based on their\ndifferent roles and varying importance in the attention mechanism. For Keys, we\npropose Head-wise Similarity-aware Reordering (HSR), which clusters similar\nheads and applies grouped SVD to the key projection matrix, reducing additional\ncomputation while preserving accuracy. For Values, we propose Offline\nCalibration and Matrix Fusion (OCMF) to preserve accuracy without extra\ncomputational overhead. Experiments show that ReCalKV outperforms existing\nlow-rank compression methods, achieving high compression ratios with minimal\nperformance loss. The code and models will be available at:\nhttps://github.com/XIANGLONGYAN/ReCalKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable performance, yet their\ncapability on long-context reasoning is often constrained by the excessive\nmemory required to store the Key-Value (KV) cache. This makes KV cache\ncompression an essential step toward enabling efficient long-context reasoning.\nRecent methods have explored reducing the hidden dimensions of the KV cache,\nbut many introduce additional computation through projection layers or suffer\nfrom significant performance degradation under high compression ratios. To\naddress these challenges, we propose ReCalKV, a post-training KV cache\ncompression method that reduces the hidden dimensions of the KV cache. We\ndevelop distinct compression strategies for Keys and Values based on their\ndifferent roles and varying importance in the attention mechanism. For Keys, we\npropose Head-wise Similarity-aware Reordering (HSR), which clusters similar\nheads and applies grouped SVD to the key projection matrix, reducing additional\ncomputation while preserving accuracy. For Values, we propose Offline\nCalibration and Matrix Fusion (OCMF) to preserve accuracy without extra\ncomputational overhead. Experiments show that ReCalKV outperforms existing\nlow-rank compression methods, achieving high compression ratios with minimal\nperformance loss. The code and models will be available at:\nhttps://github.com/XIANGLONGYAN/ReCalKV."
                },
                "authors": [
                    {
                        "name": "Xianglong Yan"
                    },
                    {
                        "name": "Zhiteng Li"
                    },
                    {
                        "name": "Tianao Zhang"
                    },
                    {
                        "name": "Linghe Kong"
                    },
                    {
                        "name": "Yulun Zhang"
                    },
                    {
                        "name": "Xiaokang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaokang Yang"
                },
                "author": "Xiaokang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24357v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24357v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02488v2",
                "updated": "2025-06-04T23:47:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    23,
                    47,
                    53,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-03T06:02:50Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    6,
                    2,
                    50,
                    1,
                    154,
                    0
                ],
                "title": "Flexiffusion: Training-Free Segment-Wise Neural Architecture Search for\n  Efficient Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flexiffusion: Training-Free Segment-Wise Neural Architecture Search for\n  Efficient Diffusion Models"
                },
                "summary": "Diffusion models (DMs) are powerful generative models capable of producing\nhigh-fidelity images but are constrained by high computational costs due to\niterative multi-step inference. While Neural Architecture Search (NAS) can\noptimize DMs, existing methods are hindered by retraining requirements,\nexponential search complexity from step-wise optimization, and slow evaluation\nrelying on massive image generation. To address these challenges, we propose\nFlexiffusion, a training-free NAS framework that jointly optimizes generation\nschedules and model architectures without modifying pre-trained parameters. Our\nkey insight is to decompose the generation process into flexible segments of\nequal length, where each segment dynamically combines three step types: full\n(complete computation), partial (cache-reused computation), and null (skipped\ncomputation). This segment-wise search space reduces the candidate pool\nexponentially compared to step-wise NAS while preserving architectural\ndiversity. Further, we introduce relative FID (rFID), a lightweight evaluation\nmetric for NAS that measures divergence from a teacher model's outputs instead\nof ground truth, slashing evaluation time by over $90\\%$. In practice,\nFlexiffusion achieves at least $2\\times$ acceleration across LDMs, Stable\nDiffusion, and DDPMs on ImageNet and MS-COCO, with FID degradation under $5\\%$,\noutperforming prior NAS and caching methods. Notably, it attains $5.1\\times$\nspeedup on Stable Diffusion with near-identical CLIP scores. Our work pioneers\na resource-efficient paradigm for searching high-speed DMs without sacrificing\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models (DMs) are powerful generative models capable of producing\nhigh-fidelity images but are constrained by high computational costs due to\niterative multi-step inference. While Neural Architecture Search (NAS) can\noptimize DMs, existing methods are hindered by retraining requirements,\nexponential search complexity from step-wise optimization, and slow evaluation\nrelying on massive image generation. To address these challenges, we propose\nFlexiffusion, a training-free NAS framework that jointly optimizes generation\nschedules and model architectures without modifying pre-trained parameters. Our\nkey insight is to decompose the generation process into flexible segments of\nequal length, where each segment dynamically combines three step types: full\n(complete computation), partial (cache-reused computation), and null (skipped\ncomputation). This segment-wise search space reduces the candidate pool\nexponentially compared to step-wise NAS while preserving architectural\ndiversity. Further, we introduce relative FID (rFID), a lightweight evaluation\nmetric for NAS that measures divergence from a teacher model's outputs instead\nof ground truth, slashing evaluation time by over $90\\%$. In practice,\nFlexiffusion achieves at least $2\\times$ acceleration across LDMs, Stable\nDiffusion, and DDPMs on ImageNet and MS-COCO, with FID degradation under $5\\%$,\noutperforming prior NAS and caching methods. Notably, it attains $5.1\\times$\nspeedup on Stable Diffusion with near-identical CLIP scores. Our work pioneers\na resource-efficient paradigm for searching high-speed DMs without sacrificing\nquality."
                },
                "authors": [
                    {
                        "name": "Hongtao Huang"
                    },
                    {
                        "name": "Xiaojun Chang"
                    },
                    {
                        "name": "Lina Yao"
                    }
                ],
                "author_detail": {
                    "name": "Lina Yao"
                },
                "author": "Lina Yao",
                "arxiv_comment": "This paper was intended to be a v2 version of my previous paper\n  (arXiv:2409.17566), but it was submitted as a new paper by mistake",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16187v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16187v3",
                "updated": "2025-06-04T22:37:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    22,
                    37,
                    29,
                    2,
                    155,
                    0
                ],
                "published": "2024-12-13T06:00:27Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    6,
                    0,
                    27,
                    4,
                    348,
                    0
                ],
                "title": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using\n  Locality-Sensitive Hashing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using\n  Locality-Sensitive Hashing"
                },
                "summary": "Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce HashEvict, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache\nthat are cosine dissimilar to the current query token. This is achieved by\ncomputing the Hamming distance between binarized Gaussian projections of the\ncurrent token query and cached token keys, with a projection length much\nsmaller than the embedding dimension. We maintain a lightweight binary\nstructure in GPU memory to facilitate these calculations. Unlike existing\ncompression strategies that compute attention to determine token retention,\nHashEvict makes these decisions pre-attention, thereby reducing computational\ncosts. Additionally, HashEvict is dynamic - at every decoding step, the key and\nvalue of the current token replace the embeddings of a token expected to\nproduce the lowest attention score. We demonstrate that HashEvict can compress\nthe KV cache by 30%-70% while maintaining high performance across reasoning,\nmultiple-choice, long-context retrieval and summarization tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce HashEvict, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache\nthat are cosine dissimilar to the current query token. This is achieved by\ncomputing the Hamming distance between binarized Gaussian projections of the\ncurrent token query and cached token keys, with a projection length much\nsmaller than the embedding dimension. We maintain a lightweight binary\nstructure in GPU memory to facilitate these calculations. Unlike existing\ncompression strategies that compute attention to determine token retention,\nHashEvict makes these decisions pre-attention, thereby reducing computational\ncosts. Additionally, HashEvict is dynamic - at every decoding step, the key and\nvalue of the current token replace the embeddings of a token expected to\nproduce the lowest attention score. We demonstrate that HashEvict can compress\nthe KV cache by 30%-70% while maintaining high performance across reasoning,\nmultiple-choice, long-context retrieval and summarization tasks."
                },
                "authors": [
                    {
                        "name": "Minghui Liu"
                    },
                    {
                        "name": "Tahseen Rabbani"
                    },
                    {
                        "name": "Tony O'Halloran"
                    },
                    {
                        "name": "Ananth Sankaralingam"
                    },
                    {
                        "name": "Mary-Anne Hartley"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Cornelia Fermller"
                    },
                    {
                        "name": "Yiannis Aloimonos"
                    }
                ],
                "author_detail": {
                    "name": "Yiannis Aloimonos"
                },
                "author": "Yiannis Aloimonos",
                "arxiv_comment": "10 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16187v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16187v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00839v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00839v2",
                "updated": "2025-06-04T18:10:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    18,
                    10,
                    39,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-01T05:04:56Z",
                "published_parsed": [
                    2025,
                    6,
                    1,
                    5,
                    4,
                    56,
                    6,
                    152,
                    0
                ],
                "title": "Neural Path Guiding with Distribution Factorization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Path Guiding with Distribution Factorization"
                },
                "summary": "In this paper, we present a neural path guiding method to aid with Monte\nCarlo (MC) integration in rendering. Existing neural methods utilize\ndistribution representations that are either fast or expressive, but not both.\nWe propose a simple, but effective, representation that is sufficiently\nexpressive and reasonably fast. Specifically, we break down the 2D distribution\nover the directional domain into two 1D probability distribution functions\n(PDF). We propose to model each 1D PDF using a neural network that estimates\nthe distribution at a set of discrete coordinates. The PDF at an arbitrary\nlocation can then be evaluated and sampled through interpolation. To train the\nnetwork, we maximize the similarity of the learned and target distributions. To\nreduce the variance of the gradient during optimizations and estimate the\nnormalization factor, we propose to cache the incoming radiance using an\nadditional network. Through extensive experiments, we demonstrate that our\napproach is better than the existing methods, particularly in challenging\nscenes with complex light transport.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a neural path guiding method to aid with Monte\nCarlo (MC) integration in rendering. Existing neural methods utilize\ndistribution representations that are either fast or expressive, but not both.\nWe propose a simple, but effective, representation that is sufficiently\nexpressive and reasonably fast. Specifically, we break down the 2D distribution\nover the directional domain into two 1D probability distribution functions\n(PDF). We propose to model each 1D PDF using a neural network that estimates\nthe distribution at a set of discrete coordinates. The PDF at an arbitrary\nlocation can then be evaluated and sampled through interpolation. To train the\nnetwork, we maximize the similarity of the learned and target distributions. To\nreduce the variance of the gradient during optimizations and estimate the\nnormalization factor, we propose to cache the incoming radiance using an\nadditional network. Through extensive experiments, we demonstrate that our\napproach is better than the existing methods, particularly in challenging\nscenes with complex light transport."
                },
                "authors": [
                    {
                        "name": "Pedro Figueiredo"
                    },
                    {
                        "name": "Qihao He"
                    },
                    {
                        "name": "Nima Khademi Kalantari"
                    }
                ],
                "author_detail": {
                    "name": "Nima Khademi Kalantari"
                },
                "author": "Nima Khademi Kalantari",
                "arxiv_comment": "11 pages, 11 figures. Accepted to EGSR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00839v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00839v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04225v1",
                "updated": "2025-06-04T17:59:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    59,
                    4,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T17:59:04Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    59,
                    4,
                    2,
                    155,
                    0
                ],
                "title": "Voyager: Long-Range and World-Consistent Video Diffusion for Explorable\n  3D Scene Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Voyager: Long-Range and World-Consistent Video Diffusion for Explorable\n  3D Scene Generation"
                },
                "summary": "Real-world applications like video gaming and virtual reality often demand\nthe ability to model 3D scenes that users can explore along custom camera\ntrajectories. While significant progress has been made in generating 3D objects\nfrom text or images, creating long-range, 3D-consistent, explorable 3D scenes\nremains a complex and challenging problem. In this work, we present Voyager, a\nnovel video diffusion framework that generates world-consistent 3D point-cloud\nsequences from a single image with user-defined camera path. Unlike existing\napproaches, Voyager achieves end-to-end scene generation and reconstruction\nwith inherent consistency across frames, eliminating the need for 3D\nreconstruction pipelines (e.g., structure-from-motion or multi-view stereo).\nOur method integrates three key components: 1) World-Consistent Video\nDiffusion: A unified architecture that jointly generates aligned RGB and depth\nvideo sequences, conditioned on existing world observation to ensure global\ncoherence 2) Long-Range World Exploration: An efficient world cache with point\nculling and an auto-regressive inference with smooth video sampling for\niterative scene extension with context-aware consistency, and 3) Scalable Data\nEngine: A video reconstruction pipeline that automates camera pose estimation\nand metric depth prediction for arbitrary videos, enabling large-scale, diverse\ntraining data curation without manual 3D annotations. Collectively, these\ndesigns result in a clear improvement over existing methods in visual quality\nand geometric accuracy, with versatile applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world applications like video gaming and virtual reality often demand\nthe ability to model 3D scenes that users can explore along custom camera\ntrajectories. While significant progress has been made in generating 3D objects\nfrom text or images, creating long-range, 3D-consistent, explorable 3D scenes\nremains a complex and challenging problem. In this work, we present Voyager, a\nnovel video diffusion framework that generates world-consistent 3D point-cloud\nsequences from a single image with user-defined camera path. Unlike existing\napproaches, Voyager achieves end-to-end scene generation and reconstruction\nwith inherent consistency across frames, eliminating the need for 3D\nreconstruction pipelines (e.g., structure-from-motion or multi-view stereo).\nOur method integrates three key components: 1) World-Consistent Video\nDiffusion: A unified architecture that jointly generates aligned RGB and depth\nvideo sequences, conditioned on existing world observation to ensure global\ncoherence 2) Long-Range World Exploration: An efficient world cache with point\nculling and an auto-regressive inference with smooth video sampling for\niterative scene extension with context-aware consistency, and 3) Scalable Data\nEngine: A video reconstruction pipeline that automates camera pose estimation\nand metric depth prediction for arbitrary videos, enabling large-scale, diverse\ntraining data curation without manual 3D annotations. Collectively, these\ndesigns result in a clear improvement over existing methods in visual quality\nand geometric accuracy, with versatile applications."
                },
                "authors": [
                    {
                        "name": "Tianyu Huang"
                    },
                    {
                        "name": "Wangguandong Zheng"
                    },
                    {
                        "name": "Tengfei Wang"
                    },
                    {
                        "name": "Yuhao Liu"
                    },
                    {
                        "name": "Zhenwei Wang"
                    },
                    {
                        "name": "Junta Wu"
                    },
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Hui Li"
                    },
                    {
                        "name": "Rynson W. H. Lau"
                    },
                    {
                        "name": "Wangmeng Zuo"
                    },
                    {
                        "name": "Chunchao Guo"
                    }
                ],
                "author_detail": {
                    "name": "Chunchao Guo"
                },
                "author": "Chunchao Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05410v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05410v1",
                "updated": "2025-06-04T16:10:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    10,
                    44,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T16:10:44Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    10,
                    44,
                    2,
                    155,
                    0
                ],
                "title": "Homogeneous Keys, Heterogeneous Values: Exploiting Local KV Cache\n  Asymmetry for Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Homogeneous Keys, Heterogeneous Values: Exploiting Local KV Cache\n  Asymmetry for Long-Context LLMs"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have highlighted the critical\nimportance of extending context length, yet the quadratic complexity of\nattention mechanisms poses significant challenges for efficient long-context\nmodeling. KV cache compression has emerged as a key approach to address this\nchallenge. Through extensive empirical analysis, we reveal a fundamental yet\npreviously overlooked asymmetry in KV caches: while adjacent keys receive\nsimilar attention weights (local homogeneity), adjacent values demonstrate\ndistinct heterogeneous distributions. This key-value asymmetry reveals a\ncritical limitation in existing compression methods that treat keys and values\nuniformly. To address the limitation, we propose a training-free compression\nframework (AsymKV) that combines homogeneity-based key merging with a\nmathematically proven lossless value compression. Extensive experiments\ndemonstrate that AsymKV consistently outperforms existing long-context methods\nacross various tasks and base models. For example, on LLaMA3.1-8B, AsymKV\nachieves an average score of 43.95 on LongBench, surpassing SOTA methods like\nH$_2$O (38.89) by a large margin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have highlighted the critical\nimportance of extending context length, yet the quadratic complexity of\nattention mechanisms poses significant challenges for efficient long-context\nmodeling. KV cache compression has emerged as a key approach to address this\nchallenge. Through extensive empirical analysis, we reveal a fundamental yet\npreviously overlooked asymmetry in KV caches: while adjacent keys receive\nsimilar attention weights (local homogeneity), adjacent values demonstrate\ndistinct heterogeneous distributions. This key-value asymmetry reveals a\ncritical limitation in existing compression methods that treat keys and values\nuniformly. To address the limitation, we propose a training-free compression\nframework (AsymKV) that combines homogeneity-based key merging with a\nmathematically proven lossless value compression. Extensive experiments\ndemonstrate that AsymKV consistently outperforms existing long-context methods\nacross various tasks and base models. For example, on LLaMA3.1-8B, AsymKV\nachieves an average score of 43.95 on LongBench, surpassing SOTA methods like\nH$_2$O (38.89) by a large margin."
                },
                "authors": [
                    {
                        "name": "Wanyun Cui"
                    },
                    {
                        "name": "Mingwei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Mingwei Xu"
                },
                "author": "Mingwei Xu",
                "arxiv_comment": "14 pages,7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05410v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05410v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17089v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17089v2",
                "updated": "2025-06-04T16:08:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    8,
                    50,
                    2,
                    155,
                    0
                ],
                "published": "2024-11-26T04:03:14Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    4,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "title": "KVPR: Efficient LLM Inference with I/O-Aware KV Cache Partial\n  Recomputation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVPR: Efficient LLM Inference with I/O-Aware KV Cache Partial\n  Recomputation"
                },
                "summary": "Inference for Large Language Models (LLMs) is computationally demanding. To\nreduce the cost of auto-regressive decoding, Key-Value (KV) cache is used to\nstore intermediate activations, which significantly lowers the computational\noverhead for token generation. However, the memory required for the KV cache\ngrows rapidly, often exceeding the capacity of GPU memory. A cost-effective\nalternative is to offload KV cache to CPU memory, which alleviates GPU memory\npressure, but shifts the bottleneck to the limited bandwidth of the PCIe\nconnection between the CPU and GPU. Existing methods attempt to address these\nissues by overlapping GPU computation with I/O or employing CPU-GPU\nheterogeneous execution, but they are hindered by excessive data movement and\ndependence on CPU capabilities. Fully overlapping PCIe communication latency\ngets challenging as the size of the KV cache grows and/or the GPU compute\ncapabilities increase. In this paper, we introduce KVPR, an efficient I/O-aware\nLLM inference method where the CPU first transfers a partial set of\nactivations, from which the GPU can start recomputing the KV cache values.\nWhile the GPU recomputes the partial KV cache, the remaining portion of the KV\ncache is transferred concurrently from the CPU. This approach overlaps GPU\nrecomputation with KV cache transfer to minimize idle GPU time and maximize\ninference performance. KVPR is fully automated by integrating a profiler module\nthat utilizes input characteristics and system hardware information, a\nscheduler module to optimize the distribution of computation and communication\nworkloads, and a runtime module to efficiently execute the derived execution\nplan. Experimental results show that KVPR achieves up to 35.8% lower latency\nand 46.2% higher throughput during decoding compared to state-of-the-art\napproaches. The code is available at https://github.com/chaoyij/KVPR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference for Large Language Models (LLMs) is computationally demanding. To\nreduce the cost of auto-regressive decoding, Key-Value (KV) cache is used to\nstore intermediate activations, which significantly lowers the computational\noverhead for token generation. However, the memory required for the KV cache\ngrows rapidly, often exceeding the capacity of GPU memory. A cost-effective\nalternative is to offload KV cache to CPU memory, which alleviates GPU memory\npressure, but shifts the bottleneck to the limited bandwidth of the PCIe\nconnection between the CPU and GPU. Existing methods attempt to address these\nissues by overlapping GPU computation with I/O or employing CPU-GPU\nheterogeneous execution, but they are hindered by excessive data movement and\ndependence on CPU capabilities. Fully overlapping PCIe communication latency\ngets challenging as the size of the KV cache grows and/or the GPU compute\ncapabilities increase. In this paper, we introduce KVPR, an efficient I/O-aware\nLLM inference method where the CPU first transfers a partial set of\nactivations, from which the GPU can start recomputing the KV cache values.\nWhile the GPU recomputes the partial KV cache, the remaining portion of the KV\ncache is transferred concurrently from the CPU. This approach overlaps GPU\nrecomputation with KV cache transfer to minimize idle GPU time and maximize\ninference performance. KVPR is fully automated by integrating a profiler module\nthat utilizes input characteristics and system hardware information, a\nscheduler module to optimize the distribution of computation and communication\nworkloads, and a runtime module to efficiently execute the derived execution\nplan. Experimental results show that KVPR achieves up to 35.8% lower latency\nand 46.2% higher throughput during decoding compared to state-of-the-art\napproaches. The code is available at https://github.com/chaoyij/KVPR."
                },
                "authors": [
                    {
                        "name": "Chaoyi Jiang"
                    },
                    {
                        "name": "Lei Gao"
                    },
                    {
                        "name": "Hossein Entezari Zarch"
                    },
                    {
                        "name": "Murali Annavaram"
                    }
                ],
                "author_detail": {
                    "name": "Murali Annavaram"
                },
                "author": "Murali Annavaram",
                "arxiv_comment": "ACL Findings 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17089v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17089v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03854v1",
                "updated": "2025-06-04T11:37:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    11,
                    37,
                    51,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T11:37:51Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    11,
                    37,
                    51,
                    2,
                    155,
                    0
                ],
                "title": "Analysis of Server Throughput For Managed Big Data Analytics Frameworks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis of Server Throughput For Managed Big Data Analytics Frameworks"
                },
                "summary": "Managed big data frameworks, such as Apache Spark and Giraph demand a large\namount of memory per core to process massive volume datasets effectively. The\nmemory pressure that arises from the big data processing leads to high garbage\ncollection (GC) overhead. Big data analytics frameworks attempt to remove this\noverhead by offloading objects to storage devices. At the same time,\ninfrastructure providers, trying to address the same problem, attribute more\nmemory to increase memory per instance leaving cores underutilized. For\nframeworks, trying to avoid GC through offloading to storage devices leads to\nhigh Serialization/Deserialization (S/D) overhead. For infrastructure, the\nresult is that resource usage is decreased. These limitations prevent managed\nbig data frameworks from effectively utilizing the CPU thus leading to low\nserver throughput.\n  We conduct a methodological analysis of server throughput for managed big\ndata analytics frameworks. More specifically, we examine, whether reducing GC\nand S/D can help increase the effective CPU utilization of the server. We use a\nsystem called TeraHeap that moves objects from the Java managed heap (H1) to a\nsecondary heap over a fast storage device (H2) to reduce the GC overhead and\neliminate S/D over data. We focus on analyzing the system's performance under\nthe co-location of multiple memory-bound instances to utilize all available\nDRAM and study server throughput. Our detailed methodology includes choosing\nthe DRAM budget for each instance and how to distribute this budget among H1\nand Page Cache (PC). We try two different distributions for the DRAM budget,\none with more H1 and one with more PC to study the needs of both approaches. We\nevaluate both techniques under 3 different memory-per-core scenarios using\nSpark and Giraph with native JVM or JVM with TeraHeap. We do this to check\nthroughput changes when memory capacity increases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Managed big data frameworks, such as Apache Spark and Giraph demand a large\namount of memory per core to process massive volume datasets effectively. The\nmemory pressure that arises from the big data processing leads to high garbage\ncollection (GC) overhead. Big data analytics frameworks attempt to remove this\noverhead by offloading objects to storage devices. At the same time,\ninfrastructure providers, trying to address the same problem, attribute more\nmemory to increase memory per instance leaving cores underutilized. For\nframeworks, trying to avoid GC through offloading to storage devices leads to\nhigh Serialization/Deserialization (S/D) overhead. For infrastructure, the\nresult is that resource usage is decreased. These limitations prevent managed\nbig data frameworks from effectively utilizing the CPU thus leading to low\nserver throughput.\n  We conduct a methodological analysis of server throughput for managed big\ndata analytics frameworks. More specifically, we examine, whether reducing GC\nand S/D can help increase the effective CPU utilization of the server. We use a\nsystem called TeraHeap that moves objects from the Java managed heap (H1) to a\nsecondary heap over a fast storage device (H2) to reduce the GC overhead and\neliminate S/D over data. We focus on analyzing the system's performance under\nthe co-location of multiple memory-bound instances to utilize all available\nDRAM and study server throughput. Our detailed methodology includes choosing\nthe DRAM budget for each instance and how to distribute this budget among H1\nand Page Cache (PC). We try two different distributions for the DRAM budget,\none with more H1 and one with more PC to study the needs of both approaches. We\nevaluate both techniques under 3 different memory-per-core scenarios using\nSpark and Giraph with native JVM or JVM with TeraHeap. We do this to check\nthroughput changes when memory capacity increases."
                },
                "authors": [
                    {
                        "name": "Emmanouil Anagnostakis"
                    },
                    {
                        "name": "Polyvios Pratikakis"
                    }
                ],
                "author_detail": {
                    "name": "Polyvios Pratikakis"
                },
                "author": "Polyvios Pratikakis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03762v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03762v1",
                "updated": "2025-06-04T09:25:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    9,
                    25,
                    53,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T09:25:53Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    9,
                    25,
                    53,
                    2,
                    155,
                    0
                ],
                "title": "AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for\n  Efficient Inference of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for\n  Efficient Inference of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have significantly advanced the field of\nArtificial Intelligence. However, their deployment is resource-intensive, not\nonly due to the large number of model parameters but also because the\n(Key-Value) KV cache consumes a lot of memory during inference. While several\nworks propose reducing the KV cache by evicting the unnecessary tokens, these\napproaches rely on accumulated attention score as eviction score to quantify\nthe importance of the token. We identify the accumulated attention score is\nbiased and it decreases with the position of the tokens in the mathematical\nexpectation. As a result, the retained tokens concentrate on the initial\npositions, limiting model's access to global contextual information. To address\nthis issue, we propose Adaptive holistic attention KV (AhaKV), it addresses the\nbias of the accumulated attention score by adaptively tuning the scale of\nsoftmax according the expectation of information entropy of attention scores.\nTo make use of the holistic attention information in self-attention mechanism,\nAhaKV utilize the information of value vectors, which is overlooked in previous\nworks, to refine the adaptive score. We show theoretically that our method is\nwell suited for bias reduction. We deployed AhaKV on different models with a\nfixed cache budget. Experiments show that AhaKV successfully mitigates bias and\nretains crucial tokens across global context and achieve state-of-the-art\nresults against other related work on several benchmark tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have significantly advanced the field of\nArtificial Intelligence. However, their deployment is resource-intensive, not\nonly due to the large number of model parameters but also because the\n(Key-Value) KV cache consumes a lot of memory during inference. While several\nworks propose reducing the KV cache by evicting the unnecessary tokens, these\napproaches rely on accumulated attention score as eviction score to quantify\nthe importance of the token. We identify the accumulated attention score is\nbiased and it decreases with the position of the tokens in the mathematical\nexpectation. As a result, the retained tokens concentrate on the initial\npositions, limiting model's access to global contextual information. To address\nthis issue, we propose Adaptive holistic attention KV (AhaKV), it addresses the\nbias of the accumulated attention score by adaptively tuning the scale of\nsoftmax according the expectation of information entropy of attention scores.\nTo make use of the holistic attention information in self-attention mechanism,\nAhaKV utilize the information of value vectors, which is overlooked in previous\nworks, to refine the adaptive score. We show theoretically that our method is\nwell suited for bias reduction. We deployed AhaKV on different models with a\nfixed cache budget. Experiments show that AhaKV successfully mitigates bias and\nretains crucial tokens across global context and achieve state-of-the-art\nresults against other related work on several benchmark tasks."
                },
                "authors": [
                    {
                        "name": "Yifeng Gu"
                    },
                    {
                        "name": "Zicong Jiang"
                    },
                    {
                        "name": "Jianxiu Jin"
                    },
                    {
                        "name": "Kailing Guo"
                    },
                    {
                        "name": "Ziyang Zhang"
                    },
                    {
                        "name": "Xiangmin Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xiangmin Xu"
                },
                "author": "Xiangmin Xu",
                "arxiv_comment": "14 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03762v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03700v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03700v1",
                "updated": "2025-06-04T08:32:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    8,
                    32,
                    30,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T08:32:30Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    8,
                    32,
                    30,
                    2,
                    155,
                    0
                ],
                "title": "AdaDecode: Accelerating LLM Decoding with Adaptive Layer Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaDecode: Accelerating LLM Decoding with Adaptive Layer Parallelism"
                },
                "summary": "Large language models (LLMs) are increasingly used for long-content\ngeneration (e.g., long Chain-of-Thought reasoning) where decoding efficiency\nbecomes a critical bottleneck: Autoregressive decoding is inherently limited by\nits sequential token generation process, where each token must be generated\nbefore the next can be processed. This sequential dependency restricts the\nability to fully leverage modern hardware's parallel processing capabilities.\nExisting methods like speculative decoding and layer skipping offer potential\nspeedups but have notable drawbacks: speculative decoding relies on an\nauxiliary \"drafter\" model, which can be challenging to acquire and increases\nmemory overhead, while layer skipping may introduce discrepancies in the\noutputs due to the missing key-value cache at skipped layers. In this work, we\npropose AdaDecode, which accelerates LLM decoding without requiring auxiliary\nmodels or changes to the original model parameters, while ensuring output\nconsistency. AdaDecode leverages the insight that many tokens can accurately be\ngenerated at intermediate layers, as further layers often do not significantly\nalter predictions once the model reaches a certain confidence. By adaptively\ngenerating tokens at intermediate layers when confidence is high, AdaDecode\nenables the next token's computation to begin immediately. The remaining layer\ncomputations for early-predicted tokens are deferred and executed in parallel\nwith subsequent tokens when needed, maximizing hardware utilization and\nreducing decoding latency. A final verification step ensures that early\npredictions match the results of standard autoregressive decoding, preserving\noutput parity. Experiments across diverse generation tasks shows that AdaDecode\nconsistently achieves superior decoding throughput with up to 1.73x speedup,\nwhile guaranteeing output parity with standard autoregressive decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used for long-content\ngeneration (e.g., long Chain-of-Thought reasoning) where decoding efficiency\nbecomes a critical bottleneck: Autoregressive decoding is inherently limited by\nits sequential token generation process, where each token must be generated\nbefore the next can be processed. This sequential dependency restricts the\nability to fully leverage modern hardware's parallel processing capabilities.\nExisting methods like speculative decoding and layer skipping offer potential\nspeedups but have notable drawbacks: speculative decoding relies on an\nauxiliary \"drafter\" model, which can be challenging to acquire and increases\nmemory overhead, while layer skipping may introduce discrepancies in the\noutputs due to the missing key-value cache at skipped layers. In this work, we\npropose AdaDecode, which accelerates LLM decoding without requiring auxiliary\nmodels or changes to the original model parameters, while ensuring output\nconsistency. AdaDecode leverages the insight that many tokens can accurately be\ngenerated at intermediate layers, as further layers often do not significantly\nalter predictions once the model reaches a certain confidence. By adaptively\ngenerating tokens at intermediate layers when confidence is high, AdaDecode\nenables the next token's computation to begin immediately. The remaining layer\ncomputations for early-predicted tokens are deferred and executed in parallel\nwith subsequent tokens when needed, maximizing hardware utilization and\nreducing decoding latency. A final verification step ensures that early\npredictions match the results of standard autoregressive decoding, preserving\noutput parity. Experiments across diverse generation tasks shows that AdaDecode\nconsistently achieves superior decoding throughput with up to 1.73x speedup,\nwhile guaranteeing output parity with standard autoregressive decoding."
                },
                "authors": [
                    {
                        "name": "Zhepei Wei"
                    },
                    {
                        "name": "Wei-Lin Chen"
                    },
                    {
                        "name": "Xinyu Zhu"
                    },
                    {
                        "name": "Yu Meng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Meng"
                },
                "author": "Yu Meng",
                "arxiv_comment": "ICML 2025. Code: https://github.com/weizhepei/AdaDecode",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03700v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01969v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01969v2",
                "updated": "2025-06-04T03:20:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    3,
                    20,
                    26,
                    2,
                    155,
                    0
                ],
                "published": "2025-05-13T17:45:34Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    17,
                    45,
                    34,
                    1,
                    133,
                    0
                ],
                "title": "FlashMLA-ETAP: Efficient Transpose Attention Pipeline for Accelerating\n  MLA Inference on NVIDIA H20 GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashMLA-ETAP: Efficient Transpose Attention Pipeline for Accelerating\n  MLA Inference on NVIDIA H20 GPUs"
                },
                "summary": "Efficient inference of Multi-Head Latent Attention (MLA) is challenged by\ndeploying the DeepSeek-R1 671B model on a single Multi-GPU server. This paper\nintroduces FlashMLA-ETAP, a novel framework that enhances MLA inference for the\nsingle-instance deployment scenario on NVIDIA H20 GPUs. We propose the\nEfficient Transpose Attention Pipeline (ETAP), which reconfigures attention\ncomputation through transposition to align the KV context length with the\n\\(M\\)-dimension in WGMMA operations, significantly reducing redundant\ncomputations. FlashMLA-ETAP achieves a 2.78x speedup over FlashMLA at 64K\nsequence length (batch size 16), with 5.24x and 4.94x improvements over\nFlashAttention-3 and FlashInfer, respectively, while maintaining numerical\nstability with a 15.2x lower RMSE (\\(1.25 \\times 10^{-5}\\)) than\nFlashAttention-3. Furthermore, ETAP's design enables seamless integration into\nframeworks like FlashAttention-3 and FlashInfer, supported by a detailed\ntheoretical analysis. Our work addresses a critical gap in resource-constrained\ninference, offering a scalable solution for mid-tier GPUs and paving the way\nfor broader adoption in hardware-aware optimization. Code is available at\nhttps://github.com/pengcuo/FlashMLA-ETAP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient inference of Multi-Head Latent Attention (MLA) is challenged by\ndeploying the DeepSeek-R1 671B model on a single Multi-GPU server. This paper\nintroduces FlashMLA-ETAP, a novel framework that enhances MLA inference for the\nsingle-instance deployment scenario on NVIDIA H20 GPUs. We propose the\nEfficient Transpose Attention Pipeline (ETAP), which reconfigures attention\ncomputation through transposition to align the KV context length with the\n\\(M\\)-dimension in WGMMA operations, significantly reducing redundant\ncomputations. FlashMLA-ETAP achieves a 2.78x speedup over FlashMLA at 64K\nsequence length (batch size 16), with 5.24x and 4.94x improvements over\nFlashAttention-3 and FlashInfer, respectively, while maintaining numerical\nstability with a 15.2x lower RMSE (\\(1.25 \\times 10^{-5}\\)) than\nFlashAttention-3. Furthermore, ETAP's design enables seamless integration into\nframeworks like FlashAttention-3 and FlashInfer, supported by a detailed\ntheoretical analysis. Our work addresses a critical gap in resource-constrained\ninference, offering a scalable solution for mid-tier GPUs and paving the way\nfor broader adoption in hardware-aware optimization. Code is available at\nhttps://github.com/pengcuo/FlashMLA-ETAP."
                },
                "authors": [
                    {
                        "name": "Pengcuo Dege"
                    },
                    {
                        "name": "Qiuming Luo"
                    },
                    {
                        "name": "Rui Mao"
                    },
                    {
                        "name": "Chang Kong"
                    }
                ],
                "author_detail": {
                    "name": "Chang Kong"
                },
                "author": "Chang Kong",
                "arxiv_comment": "15 pages, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01969v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01969v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03275v1",
                "updated": "2025-06-03T18:03:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    18,
                    3,
                    32,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T18:03:32Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    18,
                    3,
                    32,
                    1,
                    154,
                    0
                ],
                "title": "Chipmunk: Training-Free Acceleration of Diffusion Transformers with\n  Dynamic Column-Sparse Deltas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chipmunk: Training-Free Acceleration of Diffusion Transformers with\n  Dynamic Column-Sparse Deltas"
                },
                "summary": "Diffusion Transformers (DiTs) have achieved state-of-the-art performance in\nhigh-quality image and video generation but incur substantial compute cost at\ninference. A common observation is that DiT latent noise vectors change slowly\nacross inference steps, which suggests that the DiT compute may be redundant\nacross steps. In this paper, we aim to speed up inference by reducing this\nredundancy, without additional training. We first study how activations change\nbetween steps in two state-of-the-art open-source DiTs. We find that just 5-25%\nof the values in attention and MLP explain 70-90% of the change in activations\nacross steps. This finding motivates our approach, Chipmunk, which uses dynamic\nsparsity at inference time to recompute only the fastest-changing intermediate\nactivations, while caching the rest. Dynamic sparsity introduces two systems\nchallenges: (1) sparse attention and MLP operations tend to underutilize GPU\ntensor cores; and (2) computing dynamic sparsity patterns at runtime and\ncaching activations both introduce overhead. To address these challenges,\nChipmunk first uses a voxel-based reordering of input tokens to introduce\ncolumn-wise sparsity. We implement column-sparse kernels utilizing efficient\nsparse gathers from global to shared GPU memory, achieving a 9.3x speedup at\n93% sparsity compared to highly-optimized dense baselines. Second, Chipmunk\noverlaps the computation of sparsity patterns and cache updates with other\nparts of the computation (e.g., second layer of the MLP) to hide the extra\nlatency. Chipmunk achieves up to 2.16x speedup on HunyuanVideo and 1.41x on\nFLUX.1-dev without compromising generation quality. Furthermore, we show that\nChipmunk can be stacked on top of full step caching, achieving a 3.72x speedup\non HunyuanVideo, a 2.67x speedup on WAN2.1, and a 2.25x speedup on FLUX.1-dev\nwith minimal quality impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have achieved state-of-the-art performance in\nhigh-quality image and video generation but incur substantial compute cost at\ninference. A common observation is that DiT latent noise vectors change slowly\nacross inference steps, which suggests that the DiT compute may be redundant\nacross steps. In this paper, we aim to speed up inference by reducing this\nredundancy, without additional training. We first study how activations change\nbetween steps in two state-of-the-art open-source DiTs. We find that just 5-25%\nof the values in attention and MLP explain 70-90% of the change in activations\nacross steps. This finding motivates our approach, Chipmunk, which uses dynamic\nsparsity at inference time to recompute only the fastest-changing intermediate\nactivations, while caching the rest. Dynamic sparsity introduces two systems\nchallenges: (1) sparse attention and MLP operations tend to underutilize GPU\ntensor cores; and (2) computing dynamic sparsity patterns at runtime and\ncaching activations both introduce overhead. To address these challenges,\nChipmunk first uses a voxel-based reordering of input tokens to introduce\ncolumn-wise sparsity. We implement column-sparse kernels utilizing efficient\nsparse gathers from global to shared GPU memory, achieving a 9.3x speedup at\n93% sparsity compared to highly-optimized dense baselines. Second, Chipmunk\noverlaps the computation of sparsity patterns and cache updates with other\nparts of the computation (e.g., second layer of the MLP) to hide the extra\nlatency. Chipmunk achieves up to 2.16x speedup on HunyuanVideo and 1.41x on\nFLUX.1-dev without compromising generation quality. Furthermore, we show that\nChipmunk can be stacked on top of full step caching, achieving a 3.72x speedup\non HunyuanVideo, a 2.67x speedup on WAN2.1, and a 2.25x speedup on FLUX.1-dev\nwith minimal quality impact."
                },
                "authors": [
                    {
                        "name": "Austin Silveria"
                    },
                    {
                        "name": "Soham V. Govande"
                    },
                    {
                        "name": "Daniel Y. Fu"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Y. Fu"
                },
                "author": "Daniel Y. Fu",
                "arxiv_comment": "10 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12665v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12665v2",
                "updated": "2025-06-03T17:18:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    18,
                    23,
                    1,
                    154,
                    0
                ],
                "published": "2025-02-18T09:11:51Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    9,
                    11,
                    51,
                    1,
                    49,
                    0
                ],
                "title": "A$^2$ATS: Retrieval-Based KV Cache Reduction via Windowed Rotary\n  Position Embedding and Query-Aware Vector Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A$^2$ATS: Retrieval-Based KV Cache Reduction via Windowed Rotary\n  Position Embedding and Query-Aware Vector Quantization"
                },
                "summary": "Long context large language models (LLMs) pose significant challenges for\nefficient serving due to the large memory footprint and high access overhead of\nKV cache. Retrieval-based KV cache reduction methods can mitigate these\nchallenges, typically by offloading the complete KV cache to CPU and retrieving\nnecessary tokens on demand during inference. However, these methods still\nsuffer from unsatisfactory accuracy degradation and extra retrieval overhead.\nTo address these limitations, this paper proposes A$^2$ATS, a novel\nretrieval-based KV cache reduction method. A$^2$ATS aims to obtain an accurate\napproximation of attention scores by applying the vector quantization technique\nto key states, thereby enabling efficient and precise retrieval of the top-K\ntokens. First, we propose Windowed Rotary Position Embedding, which decouples\nthe positional dependency from query and key states after position embedding.\nThen, we propose query-aware vector quantization that optimizes the objective\nof attention score approximation directly. Finally, we design the heterogeneous\ninference architecture for KV cache offloading, enabling long context serving\nwith larger batch sizes. Experimental results demonstrate that A$^2$ATS can\nachieve a lower performance degradation with similar or lower overhead compared\nto existing methods, thereby increasing long context serving throughput by up\nto $2.7 \\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context large language models (LLMs) pose significant challenges for\nefficient serving due to the large memory footprint and high access overhead of\nKV cache. Retrieval-based KV cache reduction methods can mitigate these\nchallenges, typically by offloading the complete KV cache to CPU and retrieving\nnecessary tokens on demand during inference. However, these methods still\nsuffer from unsatisfactory accuracy degradation and extra retrieval overhead.\nTo address these limitations, this paper proposes A$^2$ATS, a novel\nretrieval-based KV cache reduction method. A$^2$ATS aims to obtain an accurate\napproximation of attention scores by applying the vector quantization technique\nto key states, thereby enabling efficient and precise retrieval of the top-K\ntokens. First, we propose Windowed Rotary Position Embedding, which decouples\nthe positional dependency from query and key states after position embedding.\nThen, we propose query-aware vector quantization that optimizes the objective\nof attention score approximation directly. Finally, we design the heterogeneous\ninference architecture for KV cache offloading, enabling long context serving\nwith larger batch sizes. Experimental results demonstrate that A$^2$ATS can\nachieve a lower performance degradation with similar or lower overhead compared\nto existing methods, thereby increasing long context serving throughput by up\nto $2.7 \\times$."
                },
                "authors": [
                    {
                        "name": "Junhui He"
                    },
                    {
                        "name": "Junna Xing"
                    },
                    {
                        "name": "Nan Wang"
                    },
                    {
                        "name": "Rui Xu"
                    },
                    {
                        "name": "Shangyu Wu"
                    },
                    {
                        "name": "Peng Zhou"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Chun Jason Xue"
                    },
                    {
                        "name": "Qingan Li"
                    }
                ],
                "author_detail": {
                    "name": "Qingan Li"
                },
                "author": "Qingan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12665v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12665v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02850v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02850v1",
                "updated": "2025-06-03T13:19:41Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    13,
                    19,
                    41,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T13:19:41Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    13,
                    19,
                    41,
                    1,
                    154,
                    0
                ],
                "title": "METok: Multi-Stage Event-based Token Compression for Efficient Long\n  Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "METok: Multi-Stage Event-based Token Compression for Efficient Long\n  Video Understanding"
                },
                "summary": "Recent advances in Video Large Language Models (VLLMs) have significantly\nenhanced their ability to understand video content. Nonetheless, processing\nlong videos remains challenging due to high computational demands and the\nredundancy present in the visual data. In this work, we propose METok, a\ntraining-free, Multi-stage Event-based Token compression framework designed to\naccelerate VLLMs' inference while preserving accuracy. METok progressively\neliminates redundant visual tokens across three critical stages: (1)\nevent-aware compression during vision encoding, (2) hierarchical token pruning\nin the prefilling stage based on semantic alignment and event importance, and\n(3) a decoding-stage KV Cache optimization that further reduces memory\nconsumption. Our experiments on diverse video benchmarks demonstrate that METok\nachieves an optimal trade-off between efficiency and accuracy by dynamically\nselecting informative visual tokens. For instance, equipping LongVA-7B with\nMETok realizes an 80.6% FLOPs reduction and 93.5% KV Cache memory savings, all\nwhile maintaining comparable or even superior accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Video Large Language Models (VLLMs) have significantly\nenhanced their ability to understand video content. Nonetheless, processing\nlong videos remains challenging due to high computational demands and the\nredundancy present in the visual data. In this work, we propose METok, a\ntraining-free, Multi-stage Event-based Token compression framework designed to\naccelerate VLLMs' inference while preserving accuracy. METok progressively\neliminates redundant visual tokens across three critical stages: (1)\nevent-aware compression during vision encoding, (2) hierarchical token pruning\nin the prefilling stage based on semantic alignment and event importance, and\n(3) a decoding-stage KV Cache optimization that further reduces memory\nconsumption. Our experiments on diverse video benchmarks demonstrate that METok\nachieves an optimal trade-off between efficiency and accuracy by dynamically\nselecting informative visual tokens. For instance, equipping LongVA-7B with\nMETok realizes an 80.6% FLOPs reduction and 93.5% KV Cache memory savings, all\nwhile maintaining comparable or even superior accuracy."
                },
                "authors": [
                    {
                        "name": "Mengyue Wang"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Kristian Kersting"
                    },
                    {
                        "name": "Volker Tresp"
                    },
                    {
                        "name": "Yunpu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yunpu Ma"
                },
                "author": "Yunpu Ma",
                "arxiv_comment": "14 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02850v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02850v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02671v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02671v1",
                "updated": "2025-06-03T09:16:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    9,
                    16,
                    51,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T09:16:51Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    9,
                    16,
                    51,
                    1,
                    154,
                    0
                ],
                "title": "Small Aid, Big Leap: Efficient Test-Time Adaptation for Vision-Language\n  Models with AdaptNet",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small Aid, Big Leap: Efficient Test-Time Adaptation for Vision-Language\n  Models with AdaptNet"
                },
                "summary": "Test-time adaptation (TTA) has emerged as a critical technique for enhancing\nthe generalization capability of vision-language models (VLMs) during\ninference. However, existing approaches often incur substantial computational\ncosts and exhibit poor scalability, primarily due to sample-wise adaptation\ngranularity and reliance on costly auxiliary designs such as data augmentation.\nTo address these limitations, we introduce SAIL (Small Aid, Big Leap), a novel\nadapter-based TTA framework that leverages a lightweight, learnable AdaptNet to\nenable efficient and scalable model adaptation. As SAIL's core, a frozen\npre-trained VLM collaborates with AdaptNet through a confidence-based\ninterpolation weight, generating robust predictions during inference. These\npredictions serve as self-supervised targets to align AdaptNet's outputs\nthrough efficient batch-wise processing, dramatically reducing computational\ncosts without modifying the VLM or requiring memory caches. To mitigate\ncatastrophic forgetting during continual adaptation, we propose a\ngradient-aware reset strategy driven by a gradient drift indicator (GDI), which\ndynamically detects domain transitions and strategically resets AdaptNet for\nstable adaptation. Extensive experiments across diverse benchmarks on two\nscenarios demonstrate that SAIL achieves state-of-the-art performance while\nmaintaining low computational costs. These results highlight SAIL's\neffectiveness, efficiency and scalability for real-world deployment. The code\nwill be released upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time adaptation (TTA) has emerged as a critical technique for enhancing\nthe generalization capability of vision-language models (VLMs) during\ninference. However, existing approaches often incur substantial computational\ncosts and exhibit poor scalability, primarily due to sample-wise adaptation\ngranularity and reliance on costly auxiliary designs such as data augmentation.\nTo address these limitations, we introduce SAIL (Small Aid, Big Leap), a novel\nadapter-based TTA framework that leverages a lightweight, learnable AdaptNet to\nenable efficient and scalable model adaptation. As SAIL's core, a frozen\npre-trained VLM collaborates with AdaptNet through a confidence-based\ninterpolation weight, generating robust predictions during inference. These\npredictions serve as self-supervised targets to align AdaptNet's outputs\nthrough efficient batch-wise processing, dramatically reducing computational\ncosts without modifying the VLM or requiring memory caches. To mitigate\ncatastrophic forgetting during continual adaptation, we propose a\ngradient-aware reset strategy driven by a gradient drift indicator (GDI), which\ndynamically detects domain transitions and strategically resets AdaptNet for\nstable adaptation. Extensive experiments across diverse benchmarks on two\nscenarios demonstrate that SAIL achieves state-of-the-art performance while\nmaintaining low computational costs. These results highlight SAIL's\neffectiveness, efficiency and scalability for real-world deployment. The code\nwill be released upon acceptance."
                },
                "authors": [
                    {
                        "name": "Xiao Chen"
                    },
                    {
                        "name": "Jiazhen Huang"
                    },
                    {
                        "name": "Qinting Jiang"
                    },
                    {
                        "name": "Fanding Huang"
                    },
                    {
                        "name": "Xianghua Fu"
                    },
                    {
                        "name": "Jingyan Jiang"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02671v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02671v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02634v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02634v1",
                "updated": "2025-06-03T08:51:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    8,
                    51,
                    38,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T08:51:38Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    8,
                    51,
                    38,
                    1,
                    154,
                    0
                ],
                "title": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider"
                },
                "summary": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity."
                },
                "authors": [
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Jinbo Han"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Sijie Shen"
                    },
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Chenguang Fang"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by USENIX ATC'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02634v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02634v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02523v1",
                "updated": "2025-06-03T06:53:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    6,
                    53,
                    4,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T06:53:04Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    6,
                    53,
                    4,
                    1,
                    154,
                    0
                ],
                "title": "Hardware-Centric Analysis of DeepSeek's Multi-Head Latent Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hardware-Centric Analysis of DeepSeek's Multi-Head Latent Attention"
                },
                "summary": "Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, improves the\nefficiency of large language models by projecting query, key, and value tensors\ninto a compact latent space. This architectural change reduces the KV-cache\nsize and significantly lowers memory bandwidth demands, particularly in the\nautoregressive decode phase. This letter presents the first hardware-centric\nanalysis of MLA, comparing it to conventional Multi-Head Attention (MHA) and\nevaluating its implications for accelerator performance. We identify two\nalternative execution schemes of MLA--reusing, resp. recomputing latent\nprojection matrices--which offer distinct trade-offs between compute and memory\naccess. Using the Stream design space exploration framework, we model their\nthroughput and energy cost across a range of hardware platforms and find that\nMLA can shift attention workloads toward the compute-bound regime.\n  Our results show that MLA not only reduces bandwidth usage but also enables\nadaptable execution strategies aligned with hardware constraints. Compared to\nMHA, it provides more stable and efficient performance, particularly on\nbandwidth-limited hardware platforms. These findings emphasize MLA's relevance\nas a co-design opportunity for future AI accelerators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, improves the\nefficiency of large language models by projecting query, key, and value tensors\ninto a compact latent space. This architectural change reduces the KV-cache\nsize and significantly lowers memory bandwidth demands, particularly in the\nautoregressive decode phase. This letter presents the first hardware-centric\nanalysis of MLA, comparing it to conventional Multi-Head Attention (MHA) and\nevaluating its implications for accelerator performance. We identify two\nalternative execution schemes of MLA--reusing, resp. recomputing latent\nprojection matrices--which offer distinct trade-offs between compute and memory\naccess. Using the Stream design space exploration framework, we model their\nthroughput and energy cost across a range of hardware platforms and find that\nMLA can shift attention workloads toward the compute-bound regime.\n  Our results show that MLA not only reduces bandwidth usage but also enables\nadaptable execution strategies aligned with hardware constraints. Compared to\nMHA, it provides more stable and efficient performance, particularly on\nbandwidth-limited hardware platforms. These findings emphasize MLA's relevance\nas a co-design opportunity for future AI accelerators."
                },
                "authors": [
                    {
                        "name": "Robin Geens"
                    },
                    {
                        "name": "Marian Verhelst"
                    }
                ],
                "author_detail": {
                    "name": "Marian Verhelst"
                },
                "author": "Marian Verhelst",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19475v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19475v3",
                "updated": "2025-06-03T06:43:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    6,
                    43,
                    53,
                    1,
                    154,
                    0
                ],
                "published": "2025-04-28T04:31:24Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    31,
                    24,
                    0,
                    118,
                    0
                ],
                "title": "Prisma: An Open Source Toolkit for Mechanistic Interpretability in\n  Vision and Video",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prisma: An Open Source Toolkit for Mechanistic Interpretability in\n  Vision and Video"
                },
                "summary": "Robust tooling and publicly available pre-trained models have helped drive\nrecent advances in mechanistic interpretability for language models. However,\nsimilar progress in vision mechanistic interpretability has been hindered by\nthe lack of accessible frameworks and pre-trained weights. We present Prisma\n(Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an\nopen-source framework designed to accelerate vision mechanistic\ninterpretability research, providing a unified toolkit for accessing 75+ vision\nand video transformers; support for sparse autoencoder (SAE), transcoder, and\ncrosscoder training; a suite of 80+ pre-trained SAE weights; activation\ncaching, circuit analysis tools, and visualization tools; and educational\nresources. Our analysis reveals surprising findings, including that effective\nvision SAEs can exhibit substantially lower sparsity patterns than language\nSAEs, and that in some instances, SAE reconstructions can decrease model loss.\nPrisma enables new research directions for understanding vision model internals\nwhile lowering barriers to entry in this emerging field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust tooling and publicly available pre-trained models have helped drive\nrecent advances in mechanistic interpretability for language models. However,\nsimilar progress in vision mechanistic interpretability has been hindered by\nthe lack of accessible frameworks and pre-trained weights. We present Prisma\n(Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an\nopen-source framework designed to accelerate vision mechanistic\ninterpretability research, providing a unified toolkit for accessing 75+ vision\nand video transformers; support for sparse autoencoder (SAE), transcoder, and\ncrosscoder training; a suite of 80+ pre-trained SAE weights; activation\ncaching, circuit analysis tools, and visualization tools; and educational\nresources. Our analysis reveals surprising findings, including that effective\nvision SAEs can exhibit substantially lower sparsity patterns than language\nSAEs, and that in some instances, SAE reconstructions can decrease model loss.\nPrisma enables new research directions for understanding vision model internals\nwhile lowering barriers to entry in this emerging field."
                },
                "authors": [
                    {
                        "name": "Sonia Joseph"
                    },
                    {
                        "name": "Praneet Suresh"
                    },
                    {
                        "name": "Lorenz Hufe"
                    },
                    {
                        "name": "Edward Stevinson"
                    },
                    {
                        "name": "Robert Graham"
                    },
                    {
                        "name": "Yash Vadi"
                    },
                    {
                        "name": "Danilo Bzdok"
                    },
                    {
                        "name": "Sebastian Lapuschkin"
                    },
                    {
                        "name": "Lee Sharkey"
                    },
                    {
                        "name": "Blake Aaron Richards"
                    }
                ],
                "author_detail": {
                    "name": "Blake Aaron Richards"
                },
                "author": "Blake Aaron Richards",
                "arxiv_comment": "4 pages, 3 figures, 9 tables. Oral and Tutorial at the CVPR\n  Mechanistic Interpretability for Vision (MIV) Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19475v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19475v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24133v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24133v2",
                "updated": "2025-06-03T03:32:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    3,
                    32,
                    10,
                    1,
                    154,
                    0
                ],
                "published": "2025-05-30T02:03:24Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    2,
                    3,
                    24,
                    4,
                    150,
                    0
                ],
                "title": "R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning\n  Models Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning\n  Models Acceleration"
                },
                "summary": "Reasoning models have demonstrated impressive performance in self-reflection\nand chain-of-thought reasoning. However, they often produce excessively long\noutputs, leading to prohibitively large key-value (KV) caches during inference.\nWhile chain-of-thought inference significantly improves performance on complex\nreasoning tasks, it can also lead to reasoning failures when deployed with\nexisting KV cache compression approaches. To address this, we propose\nRedundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel\nmethod specifically targeting redundant tokens in reasoning models. Our method\npreserves nearly 100% of the full KV cache performance using only 10% of the KV\ncache, substantially outperforming existing KV cache baselines, which reach\nonly 60% of the performance. Remarkably, R-KV even achieves 105% of full KV\ncache performance with 16% of the KV cache. This KV-cache reduction also leads\nto a 90% memory saving and a 6.6X throughput over standard chain-of-thought\nreasoning inference. Experimental results show that R-KV consistently\noutperforms existing KV cache compression baselines across two mathematical\nreasoning datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning models have demonstrated impressive performance in self-reflection\nand chain-of-thought reasoning. However, they often produce excessively long\noutputs, leading to prohibitively large key-value (KV) caches during inference.\nWhile chain-of-thought inference significantly improves performance on complex\nreasoning tasks, it can also lead to reasoning failures when deployed with\nexisting KV cache compression approaches. To address this, we propose\nRedundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel\nmethod specifically targeting redundant tokens in reasoning models. Our method\npreserves nearly 100% of the full KV cache performance using only 10% of the KV\ncache, substantially outperforming existing KV cache baselines, which reach\nonly 60% of the performance. Remarkably, R-KV even achieves 105% of full KV\ncache performance with 16% of the KV cache. This KV-cache reduction also leads\nto a 90% memory saving and a 6.6X throughput over standard chain-of-thought\nreasoning inference. Experimental results show that R-KV consistently\noutperforms existing KV cache compression baselines across two mathematical\nreasoning datasets."
                },
                "authors": [
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Wen Xiao"
                    },
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Cheng Luo"
                    },
                    {
                        "name": "Yikai Zhang"
                    },
                    {
                        "name": "Ke Wan"
                    },
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Yeyang Zhou"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    },
                    {
                        "name": "Zhen Dong"
                    },
                    {
                        "name": "Anima Anandkumar"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Junjie Hu"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Hu"
                },
                "author": "Junjie Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24133v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24133v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13649v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13649v3",
                "updated": "2025-06-03T01:55:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    1,
                    55,
                    18,
                    1,
                    154,
                    0
                ],
                "published": "2024-12-18T09:27:33Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    27,
                    33,
                    2,
                    353,
                    0
                ],
                "title": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation"
                },
                "summary": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods."
                },
                "authors": [
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Zhenglin Wang"
                    },
                    {
                        "name": "Linhai Zhang"
                    },
                    {
                        "name": "Yilong Lai"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Deyu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Deyu Zhou"
                },
                "author": "Deyu Zhou",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13649v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13649v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04005v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04005v2",
                "updated": "2025-06-03T01:51:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    1,
                    51,
                    37,
                    1,
                    154,
                    0
                ],
                "published": "2025-04-05T00:59:52Z",
                "published_parsed": [
                    2025,
                    4,
                    5,
                    0,
                    59,
                    52,
                    5,
                    95,
                    0
                ],
                "title": "Learning Cache Coherence Traffic for NoC Routing Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Cache Coherence Traffic for NoC Routing Design"
                },
                "summary": "The rapid growth of multi-core systems highlights the need for efficient\nNetwork-on-Chip (NoC) design to ensure seamless communication. Cache coherence,\nessential for data consistency, substantially reduces task computation time by\nenabling data sharing among caches. As a result, routing serves two roles:\nfacilitating data sharing (influenced by topology) and managing NoC-level\ncommunication. However, cache coherence is often overlooked in routing, causing\nmismatches between design expectations and evaluation outcomes. Two main\nchallenges are the lack of specialized tools to assess cache coherence's impact\nand the neglect of topology selection in routing. In this work, we propose a\ncache coherence-aware routing approach with integrated topology selection,\nguided by our Cache Coherence Traffic Analyzer (CCTA). Our method achieves up\nto 10.52% lower packet latency, 55.51% faster execution time, and 49.02% total\nenergy savings, underscoring the critical role of cache coherence in NoC design\nand enabling effective co-design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of multi-core systems highlights the need for efficient\nNetwork-on-Chip (NoC) design to ensure seamless communication. Cache coherence,\nessential for data consistency, substantially reduces task computation time by\nenabling data sharing among caches. As a result, routing serves two roles:\nfacilitating data sharing (influenced by topology) and managing NoC-level\ncommunication. However, cache coherence is often overlooked in routing, causing\nmismatches between design expectations and evaluation outcomes. Two main\nchallenges are the lack of specialized tools to assess cache coherence's impact\nand the neglect of topology selection in routing. In this work, we propose a\ncache coherence-aware routing approach with integrated topology selection,\nguided by our Cache Coherence Traffic Analyzer (CCTA). Our method achieves up\nto 10.52% lower packet latency, 55.51% faster execution time, and 49.02% total\nenergy savings, underscoring the critical role of cache coherence in NoC design\nand enabling effective co-design."
                },
                "authors": [
                    {
                        "name": "Guochu Xiong"
                    },
                    {
                        "name": "Xiangzhong Luo"
                    },
                    {
                        "name": "Weichen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Weichen Liu"
                },
                "author": "Weichen Liu",
                "arxiv_doi": "10.1145/3716368.3735166",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3716368.3735166",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.04005v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04005v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "7 pages, 14 figures",
                "arxiv_journal_ref": "Great Lakes Symposium on VLSI 2025 (GLSVLSI '25)",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04975v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04975v2",
                "updated": "2025-06-02T19:27:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    19,
                    27,
                    12,
                    0,
                    153,
                    0
                ],
                "published": "2024-11-07T18:49:33Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    49,
                    33,
                    3,
                    312,
                    0
                ],
                "title": "SuffixDecoding: Extreme Speculative Decoding for Emerging AI\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuffixDecoding: Extreme Speculative Decoding for Emerging AI\n  Applications"
                },
                "summary": "Speculative decoding is widely adopted to reduce latency in large language\nmodel (LLM) inference by leveraging smaller draft models capable of handling\ndiverse user tasks. However, emerging AI applications, such as LLM-based\nagents, present unique workload characteristics: instead of diverse independent\nrequests, agentic frameworks typically submit repetitive inference requests,\nsuch as multi-agent pipelines performing similar subtasks or self-refinement\nloops iteratively enhancing outputs. These workloads result in long and highly\npredictable sequences, which current speculative decoding methods do not\neffectively exploit. To address this gap, we introduce \\emph{SuffixDecoding}, a\nnovel method that utilizes efficient suffix trees to cache long token sequences\nfrom prompts and previous outputs. By adaptively speculating more tokens when\nacceptance likelihood is high and fewer when it is low, SuffixDecoding\neffectively exploits opportunities for longer speculations while conserving\ncomputation when those opportunities are limited. Evaluations on agentic\nbenchmarks, including SWE-Bench and Text-to-SQL, demonstrate that\nSuffixDecoding achieves speedups of up to 5.3$\\times$, outperforming\nstate-of-the-art methods -- 2.8$\\times$ faster than model-based approaches like\nEAGLE-2/3 and 1.9$\\times$ faster than model-free approaches such as Token\nRecycling. SuffixDecoding is open-sourced at\nhttps://github.com/snowflakedb/ArcticInference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is widely adopted to reduce latency in large language\nmodel (LLM) inference by leveraging smaller draft models capable of handling\ndiverse user tasks. However, emerging AI applications, such as LLM-based\nagents, present unique workload characteristics: instead of diverse independent\nrequests, agentic frameworks typically submit repetitive inference requests,\nsuch as multi-agent pipelines performing similar subtasks or self-refinement\nloops iteratively enhancing outputs. These workloads result in long and highly\npredictable sequences, which current speculative decoding methods do not\neffectively exploit. To address this gap, we introduce \\emph{SuffixDecoding}, a\nnovel method that utilizes efficient suffix trees to cache long token sequences\nfrom prompts and previous outputs. By adaptively speculating more tokens when\nacceptance likelihood is high and fewer when it is low, SuffixDecoding\neffectively exploits opportunities for longer speculations while conserving\ncomputation when those opportunities are limited. Evaluations on agentic\nbenchmarks, including SWE-Bench and Text-to-SQL, demonstrate that\nSuffixDecoding achieves speedups of up to 5.3$\\times$, outperforming\nstate-of-the-art methods -- 2.8$\\times$ faster than model-based approaches like\nEAGLE-2/3 and 1.9$\\times$ faster than model-free approaches such as Token\nRecycling. SuffixDecoding is open-sourced at\nhttps://github.com/snowflakedb/ArcticInference."
                },
                "authors": [
                    {
                        "name": "Gabriele Oliaro"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Daniel Campos"
                    },
                    {
                        "name": "Aurick Qiao"
                    }
                ],
                "author_detail": {
                    "name": "Aurick Qiao"
                },
                "author": "Aurick Qiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04975v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04975v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01928v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01928v1",
                "updated": "2025-06-02T17:47:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    17,
                    47,
                    27,
                    0,
                    153,
                    0
                ],
                "published": "2025-06-02T17:47:27Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    17,
                    47,
                    27,
                    0,
                    153,
                    0
                ],
                "title": "Esoteric Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Esoteric Language Models"
                },
                "summary": "Diffusion-based language models offer a compelling alternative to\nautoregressive (AR) models by enabling parallel and controllable generation.\nAmong this family of models, Masked Diffusion Models (MDMs) achieve the\nstrongest performance but still underperform AR models in perplexity and lack\nkey inference-time efficiency features--most notably, KV caching. In this work,\nwe introduce Eso-LMs, a new family of models that fuses AR and MDM paradigms,\nenabling smooth interpolation between their perplexities while overcoming their\nrespective limitations. Eso-LMs set a new state of the art on standard language\nmodeling benchmarks. Crucially, we are the **first to introduce KV caching for\nMDMs** while preserving parallel generation, significantly improving inference\nefficiency. Combined with an optimized sampling schedule, our method achieves\nup to **65x** faster inference than standard MDMs and **4x** faster inference\nthan prior semi-autoregressive approaches. We provide the code and model\ncheckpoints on the project page:\n[http://s-sahoo.github.io/Eso-LMs](http://s-sahoo.github.io/Eso-LMs)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based language models offer a compelling alternative to\nautoregressive (AR) models by enabling parallel and controllable generation.\nAmong this family of models, Masked Diffusion Models (MDMs) achieve the\nstrongest performance but still underperform AR models in perplexity and lack\nkey inference-time efficiency features--most notably, KV caching. In this work,\nwe introduce Eso-LMs, a new family of models that fuses AR and MDM paradigms,\nenabling smooth interpolation between their perplexities while overcoming their\nrespective limitations. Eso-LMs set a new state of the art on standard language\nmodeling benchmarks. Crucially, we are the **first to introduce KV caching for\nMDMs** while preserving parallel generation, significantly improving inference\nefficiency. Combined with an optimized sampling schedule, our method achieves\nup to **65x** faster inference than standard MDMs and **4x** faster inference\nthan prior semi-autoregressive approaches. We provide the code and model\ncheckpoints on the project page:\n[http://s-sahoo.github.io/Eso-LMs](http://s-sahoo.github.io/Eso-LMs)"
                },
                "authors": [
                    {
                        "name": "Subham Sekhar Sahoo"
                    },
                    {
                        "name": "Zhihan Yang"
                    },
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Johnna Liu"
                    },
                    {
                        "name": "Deepansha Singh"
                    },
                    {
                        "name": "Zhoujun Cheng"
                    },
                    {
                        "name": "Zhengzhong Liu"
                    },
                    {
                        "name": "Eric Xing"
                    },
                    {
                        "name": "John Thickstun"
                    },
                    {
                        "name": "Arash Vahdat"
                    }
                ],
                "author_detail": {
                    "name": "Arash Vahdat"
                },
                "author": "Arash Vahdat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01928v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01928v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v8",
                "updated": "2025-06-02T17:46:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    17,
                    46,
                    50,
                    0,
                    153,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "arxiv_comment": "Added additional variations in appendix, at the request of\n  collaborators who want to prove various properties",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01880v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01880v1",
                "updated": "2025-06-02T17:09:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    17,
                    9,
                    59,
                    0,
                    153,
                    0
                ],
                "published": "2025-06-02T17:09:59Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    17,
                    9,
                    59,
                    0,
                    153,
                    0
                ],
                "title": "Pearl: Automatic Code Optimization Using Deep Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pearl: Automatic Code Optimization Using Deep Reinforcement Learning"
                },
                "summary": "Compilers are crucial in optimizing programs and accelerating their\nexecution. However, optimizing programs automatically using compilers is not\ntrivial. Recent work has attempted to use reinforcement learning (RL) to solve\nthis problem. It has limitations though. Current methods either do not support\nthe optimization of general loop nests or can only be used to optimize loop\nnests seen during training. In this paper, we propose Pearl, a novel framework\nthat uses deep reinforcement learning to automate compiler code optimization.\nIt uses an RL agent to select the sequence of code optimizations a compiler\nshould apply to make the input code run faster. This agent can optimize general\nloop nests and can generalize to programs unseen during training. To enable the\noptimization of general loop nests, we propose a novel representation of the\naction space that allows the RL agent to select on which part of the loop nest\na given code optimization should be applied. Training RL agents for loop nest\noptimization is slow and data-intensive. We accelerate this process by caching\nresults and pre-training the agent. Integrated with the Tiramisu compiler, our\napproach streamlines optimization and outperforms existing methods. To the best\nof our knowledge, Pearl is the first RL-based system to support general\nprograms composed of loop nests manipulating tensors while still being able to\ngeneralize to programs unseen during training. It is also the first to support\nthe class of polyhedral optimizations, a class of advanced loop nest\noptimizations. We evaluate Pearl on a set of benchmarks, and demonstrate\ncompetitive performance improvements over state-of-the-art compilers. Notably,\nPearl achieves a geometric mean speedup of 2.02x compared to Tiramisu and 3.36x\ncompared to Pluto.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compilers are crucial in optimizing programs and accelerating their\nexecution. However, optimizing programs automatically using compilers is not\ntrivial. Recent work has attempted to use reinforcement learning (RL) to solve\nthis problem. It has limitations though. Current methods either do not support\nthe optimization of general loop nests or can only be used to optimize loop\nnests seen during training. In this paper, we propose Pearl, a novel framework\nthat uses deep reinforcement learning to automate compiler code optimization.\nIt uses an RL agent to select the sequence of code optimizations a compiler\nshould apply to make the input code run faster. This agent can optimize general\nloop nests and can generalize to programs unseen during training. To enable the\noptimization of general loop nests, we propose a novel representation of the\naction space that allows the RL agent to select on which part of the loop nest\na given code optimization should be applied. Training RL agents for loop nest\noptimization is slow and data-intensive. We accelerate this process by caching\nresults and pre-training the agent. Integrated with the Tiramisu compiler, our\napproach streamlines optimization and outperforms existing methods. To the best\nof our knowledge, Pearl is the first RL-based system to support general\nprograms composed of loop nests manipulating tensors while still being able to\ngeneralize to programs unseen during training. It is also the first to support\nthe class of polyhedral optimizations, a class of advanced loop nest\noptimizations. We evaluate Pearl on a set of benchmarks, and demonstrate\ncompetitive performance improvements over state-of-the-art compilers. Notably,\nPearl achieves a geometric mean speedup of 2.02x compared to Tiramisu and 3.36x\ncompared to Pluto."
                },
                "authors": [
                    {
                        "name": "Djamel Rassem Lamouri"
                    },
                    {
                        "name": "Iheb Nassim Aouadj"
                    },
                    {
                        "name": "Smail Kourta"
                    },
                    {
                        "name": "Riyadh Baghdadi"
                    }
                ],
                "author_detail": {
                    "name": "Riyadh Baghdadi"
                },
                "author": "Riyadh Baghdadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01880v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01880v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01827v1",
                "updated": "2025-06-02T16:12:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    16,
                    12,
                    22,
                    0,
                    153,
                    0
                ],
                "published": "2025-06-02T16:12:22Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    16,
                    12,
                    22,
                    0,
                    153,
                    0
                ],
                "title": "Memory Access Characterization of Large Language Models in CPU\n  Environment and its Potential Impacts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory Access Characterization of Large Language Models in CPU\n  Environment and its Potential Impacts"
                },
                "summary": "As machine learning algorithms are shown to be an increasingly valuable tool,\nthe demand for their access has grown accordingly. Oftentimes, it is infeasible\nto run inference with larger models without an accelerator, which may be\nunavailable in environments that have constraints such as energy consumption,\nsecurity, or cost. To increase the availability of these models, we aim to\nimprove the LLM inference speed on a CPU-only environment by modifying the\ncache architecture. To determine what improvements could be made, we conducted\ntwo experiments using Llama.cpp and the QWEN model: running various cache\nconfigurations and evaluating their performance, and outputting a trace of the\nmemory footprint. Using these experiments, we investigate the memory access\npatterns and performance characteristics to identify potential optimizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As machine learning algorithms are shown to be an increasingly valuable tool,\nthe demand for their access has grown accordingly. Oftentimes, it is infeasible\nto run inference with larger models without an accelerator, which may be\nunavailable in environments that have constraints such as energy consumption,\nsecurity, or cost. To increase the availability of these models, we aim to\nimprove the LLM inference speed on a CPU-only environment by modifying the\ncache architecture. To determine what improvements could be made, we conducted\ntwo experiments using Llama.cpp and the QWEN model: running various cache\nconfigurations and evaluating their performance, and outputting a trace of the\nmemory footprint. Using these experiments, we investigate the memory access\npatterns and performance characteristics to identify potential optimizations."
                },
                "authors": [
                    {
                        "name": "Spencer Banasik"
                    }
                ],
                "author_detail": {
                    "name": "Spencer Banasik"
                },
                "author": "Spencer Banasik",
                "arxiv_comment": "34 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01643v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01643v1",
                "updated": "2025-06-02T13:16:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    13,
                    16,
                    14,
                    0,
                    153,
                    0
                ],
                "published": "2025-06-02T13:16:14Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    13,
                    16,
                    14,
                    0,
                    153,
                    0
                ],
                "title": "A Low Power Monolithic Active Pixel Sensor Prototype for the STCF Inner\n  Tracker",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Low Power Monolithic Active Pixel Sensor Prototype for the STCF Inner\n  Tracker"
                },
                "summary": "The Super Tau-Charm Facility (STCF) is a proposed $e^+e^-$ collider with a\npeak luminosity 100 times higher than that of the present tau-charm factory.\nThe inner tracker (ITK) of STCF should feature a low material budget and high\nreadout speed. Under these requirements, the monolithic active pixel sensor\n(MAPS) is considered as a promising candidate for the ITK. To minimize the\npower consumption of MAPS (for low material budget), larger-size sensors are\nproposed to reduce the scale of the readout circuitry while preserving the\nrequired position resolution. Multiple sensors with varying dimensions and\nstructures were designed and integrated in several prototype chips for\nperformance comparison, fabricated in a 180~nm CIS process. The in-pixel\nreadout circuit can also provide time of arrival (ToA) and time-over-threshold\n(ToT) of the hit signal, with a least significant bit (LSB) of 50 ns. The\nperipheral readout circuit performs operations including timestamp correction,\ndata aggregation, caching, framing, 8b/10b encoding, and serialization.\nAccording to simulation, the power consumption for a full-scale chip is about\n55.7 mW/cm2. Preliminary measurements have been conducted on the prototype\nchips.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Super Tau-Charm Facility (STCF) is a proposed $e^+e^-$ collider with a\npeak luminosity 100 times higher than that of the present tau-charm factory.\nThe inner tracker (ITK) of STCF should feature a low material budget and high\nreadout speed. Under these requirements, the monolithic active pixel sensor\n(MAPS) is considered as a promising candidate for the ITK. To minimize the\npower consumption of MAPS (for low material budget), larger-size sensors are\nproposed to reduce the scale of the readout circuitry while preserving the\nrequired position resolution. Multiple sensors with varying dimensions and\nstructures were designed and integrated in several prototype chips for\nperformance comparison, fabricated in a 180~nm CIS process. The in-pixel\nreadout circuit can also provide time of arrival (ToA) and time-over-threshold\n(ToT) of the hit signal, with a least significant bit (LSB) of 50 ns. The\nperipheral readout circuit performs operations including timestamp correction,\ndata aggregation, caching, framing, 8b/10b encoding, and serialization.\nAccording to simulation, the power consumption for a full-scale chip is about\n55.7 mW/cm2. Preliminary measurements have been conducted on the prototype\nchips."
                },
                "authors": [
                    {
                        "name": "Dongwei Xuan"
                    },
                    {
                        "name": "Ruiyang Zhang"
                    },
                    {
                        "name": "Jiajun Qin"
                    },
                    {
                        "name": "Hao Han"
                    },
                    {
                        "name": "Xinyu Bin"
                    },
                    {
                        "name": "Zihan Xu"
                    },
                    {
                        "name": "Lei Zhao"
                    },
                    {
                        "name": "Jianbei Liu"
                    },
                    {
                        "name": "Liang Zhang"
                    },
                    {
                        "name": "Anqing Wang"
                    },
                    {
                        "name": "Aodong Song"
                    },
                    {
                        "name": "Xiangming Sun"
                    },
                    {
                        "name": "Le Xiao"
                    },
                    {
                        "name": "Lailin Xu"
                    }
                ],
                "author_detail": {
                    "name": "Lailin Xu"
                },
                "author": "Lailin Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01643v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01643v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v6",
                "updated": "2025-06-02T11:46:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    11,
                    46,
                    43,
                    0,
                    153,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless separator tokens (i.e.,\npunctuations) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless separator tokens (i.e.,\npunctuations) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "Accepted to ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03960v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03960v3",
                "updated": "2025-06-02T02:08:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    2,
                    8,
                    6,
                    0,
                    153,
                    0
                ],
                "published": "2024-10-04T22:45:26Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    22,
                    45,
                    26,
                    4,
                    278,
                    0
                ],
                "title": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation"
                },
                "summary": "LLM inference for enterprise applications, such as summarization, RAG, and\ncode-generation, typically observe much longer prompt than generations, leading\nto high prefill cost and response latency. We present SwiftKV, a novel model\ntransformation and distillation procedure targeted at reducing the prefill\ncompute (in FLOPs) of prompt tokens while preserving high generation quality.\nFirst, SwiftKV prefills later layers' KV cache using an earlier layer's output,\nallowing prompt tokens to skip those later layers. Second, SwiftKV employs a\nlightweight knowledge-preserving distillation procedure that can adapt existing\nLLMs with minimal accuracy impact. Third, SwiftKV can naturally incorporate KV\ncache compression to improve inference performance in low-memory scenarios. Our\ncomprehensive experiments show that SwiftKV can effectively reduce prefill\ncomputation by 25-50% across several LLM families while incurring minimum\nquality degradation. In the end-to-end inference serving, SwiftKV realizes up\nto 2x higher aggregate throughput and 60% lower time per output token. It can\nachieve a staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B. SwiftKV is open-sourced at\nhttps://github.com/snowflakedb/arctictraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference for enterprise applications, such as summarization, RAG, and\ncode-generation, typically observe much longer prompt than generations, leading\nto high prefill cost and response latency. We present SwiftKV, a novel model\ntransformation and distillation procedure targeted at reducing the prefill\ncompute (in FLOPs) of prompt tokens while preserving high generation quality.\nFirst, SwiftKV prefills later layers' KV cache using an earlier layer's output,\nallowing prompt tokens to skip those later layers. Second, SwiftKV employs a\nlightweight knowledge-preserving distillation procedure that can adapt existing\nLLMs with minimal accuracy impact. Third, SwiftKV can naturally incorporate KV\ncache compression to improve inference performance in low-memory scenarios. Our\ncomprehensive experiments show that SwiftKV can effectively reduce prefill\ncomputation by 25-50% across several LLM families while incurring minimum\nquality degradation. In the end-to-end inference serving, SwiftKV realizes up\nto 2x higher aggregate throughput and 60% lower time per output token. It can\nachieve a staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B. SwiftKV is open-sourced at\nhttps://github.com/snowflakedb/arctictraining."
                },
                "authors": [
                    {
                        "name": "Aurick Qiao"
                    },
                    {
                        "name": "Zhewei Yao"
                    },
                    {
                        "name": "Samyam Rajbhandari"
                    },
                    {
                        "name": "Yuxiong He"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiong He"
                },
                "author": "Yuxiong He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03960v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03960v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24584v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24584v2",
                "updated": "2025-06-02T01:08:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    1,
                    8,
                    24,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-30T13:32:00Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    13,
                    32,
                    0,
                    4,
                    150,
                    0
                ],
                "title": "AutoChemSchematic AI: A Closed-Loop, Physics-Aware Agentic Framework for\n  Auto-Generating Chemical Process and Instrumentation Diagrams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoChemSchematic AI: A Closed-Loop, Physics-Aware Agentic Framework for\n  Auto-Generating Chemical Process and Instrumentation Diagrams"
                },
                "summary": "Recent advancements in generative AI have accelerated the discovery of novel\nchemicals and materials; however, transitioning these discoveries to\nindustrial-scale production remains a critical bottleneck, as it requires the\ndevelopment of entirely new chemical manufacturing processes. Current AI\nmethods cannot auto-generate PFDs or PIDs, despite their critical role in\nscaling chemical processes, while adhering to engineering constraints. We\npresent a closed loop, physics aware framework for the automated generation of\nindustrially viable PFDs and PIDs. The framework integrates domain specialized\nsmall scale language models (SLMs) (trained for chemical process QA tasks) with\nfirst principles simulation, leveraging three key components: (1) a\nhierarchical knowledge graph of process flow and instrumentation descriptions\nfor 1,020+ chemicals, (2) a multi-stage training pipeline that fine tunes\ndomain specialized SLMs on synthetic datasets via Supervised Fine-Tuning (SFT),\nDirect Preference Optimization (DPO), and Retrieval-Augmented Instruction\nTuning (RAIT), and (3) DWSIM based simulator in the loop validation to ensure\nfeasibility. To improve both runtime efficiency and model compactness, the\nframework incorporates advanced inference time optimizations including\nFlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization,\nand Test Time Inference Scaling and independently applies structural pruning\ntechniques (width and depth) guided by importance heuristics to reduce model\nsize with minimal accuracy loss. Experiments demonstrate that the framework\ngenerates simulator-validated process descriptions with high fidelity,\noutperforms baseline methods in correctness, and generalizes to unseen\nchemicals. By bridging AI-driven design with industrial-scale feasibility, this\nwork significantly reduces R&D timelines from lab discovery to plant\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in generative AI have accelerated the discovery of novel\nchemicals and materials; however, transitioning these discoveries to\nindustrial-scale production remains a critical bottleneck, as it requires the\ndevelopment of entirely new chemical manufacturing processes. Current AI\nmethods cannot auto-generate PFDs or PIDs, despite their critical role in\nscaling chemical processes, while adhering to engineering constraints. We\npresent a closed loop, physics aware framework for the automated generation of\nindustrially viable PFDs and PIDs. The framework integrates domain specialized\nsmall scale language models (SLMs) (trained for chemical process QA tasks) with\nfirst principles simulation, leveraging three key components: (1) a\nhierarchical knowledge graph of process flow and instrumentation descriptions\nfor 1,020+ chemicals, (2) a multi-stage training pipeline that fine tunes\ndomain specialized SLMs on synthetic datasets via Supervised Fine-Tuning (SFT),\nDirect Preference Optimization (DPO), and Retrieval-Augmented Instruction\nTuning (RAIT), and (3) DWSIM based simulator in the loop validation to ensure\nfeasibility. To improve both runtime efficiency and model compactness, the\nframework incorporates advanced inference time optimizations including\nFlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization,\nand Test Time Inference Scaling and independently applies structural pruning\ntechniques (width and depth) guided by importance heuristics to reduce model\nsize with minimal accuracy loss. Experiments demonstrate that the framework\ngenerates simulator-validated process descriptions with high fidelity,\noutperforms baseline methods in correctness, and generalizes to unseen\nchemicals. By bridging AI-driven design with industrial-scale feasibility, this\nwork significantly reduces R&D timelines from lab discovery to plant\ndeployment."
                },
                "authors": [
                    {
                        "name": "Sakhinana Sagar Srinivas"
                    },
                    {
                        "name": "Shivam Gupta"
                    },
                    {
                        "name": "Venkataramana Runkana"
                    }
                ],
                "author_detail": {
                    "name": "Venkataramana Runkana"
                },
                "author": "Venkataramana Runkana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24584v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24584v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01215v1",
                "updated": "2025-06-01T23:49:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    1,
                    23,
                    49,
                    14,
                    6,
                    152,
                    0
                ],
                "published": "2025-06-01T23:49:14Z",
                "published_parsed": [
                    2025,
                    6,
                    1,
                    23,
                    49,
                    14,
                    6,
                    152,
                    0
                ],
                "title": "Compress, Gather, and Recompute: REFORMing Long-Context Processing in\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compress, Gather, and Recompute: REFORMing Long-Context Processing in\n  Transformers"
                },
                "summary": "As large language models increasingly gain popularity in real-world\napplications, processing extremely long contexts, often exceeding the model's\npre-trained context limits, has emerged as a critical challenge. While existing\napproaches to efficient long-context processing show promise, recurrent\ncompression-based methods struggle with information preservation, whereas\nrandom access approaches require substantial memory resources. We introduce\nREFORM, a novel inference framework that efficiently handles long contexts\nthrough a two-phase approach. First, it incrementally processes input chunks\nwhile maintaining a compressed KV cache, constructs cross-layer context\nembeddings, and utilizes early exit strategy for improved efficiency. Second,\nit identifies and gathers essential tokens via similarity matching and\nselectively recomputes the KV cache. Compared to baselines, REFORM achieves\nover 50% and 27% performance gains on RULER and BABILong respectively at 1M\ncontext length. It also outperforms baselines on Infinite-Bench and MM-NIAH,\ndemonstrating flexibility across diverse tasks and domains. Additionally,\nREFORM reduces inference time by 30% and peak memory usage by 5%, achieving\nboth efficiency and superior performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models increasingly gain popularity in real-world\napplications, processing extremely long contexts, often exceeding the model's\npre-trained context limits, has emerged as a critical challenge. While existing\napproaches to efficient long-context processing show promise, recurrent\ncompression-based methods struggle with information preservation, whereas\nrandom access approaches require substantial memory resources. We introduce\nREFORM, a novel inference framework that efficiently handles long contexts\nthrough a two-phase approach. First, it incrementally processes input chunks\nwhile maintaining a compressed KV cache, constructs cross-layer context\nembeddings, and utilizes early exit strategy for improved efficiency. Second,\nit identifies and gathers essential tokens via similarity matching and\nselectively recomputes the KV cache. Compared to baselines, REFORM achieves\nover 50% and 27% performance gains on RULER and BABILong respectively at 1M\ncontext length. It also outperforms baselines on Infinite-Bench and MM-NIAH,\ndemonstrating flexibility across diverse tasks and domains. Additionally,\nREFORM reduces inference time by 30% and peak memory usage by 5%, achieving\nboth efficiency and superior performance."
                },
                "authors": [
                    {
                        "name": "Woomin Song"
                    },
                    {
                        "name": "Sai Muralidhar Jayanthi"
                    },
                    {
                        "name": "Srikanth Ronanki"
                    },
                    {
                        "name": "Kanthashree Mysore Sathyendra"
                    },
                    {
                        "name": "Jinwoo Shin"
                    },
                    {
                        "name": "Aram Galstyan"
                    },
                    {
                        "name": "Shubham Katiyar"
                    },
                    {
                        "name": "Sravan Babu Bodapati"
                    }
                ],
                "author_detail": {
                    "name": "Sravan Babu Bodapati"
                },
                "author": "Sravan Babu Bodapati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01151v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01151v1",
                "updated": "2025-06-01T20:05:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    1,
                    20,
                    5,
                    30,
                    6,
                    152,
                    0
                ],
                "published": "2025-06-01T20:05:30Z",
                "published_parsed": [
                    2025,
                    6,
                    1,
                    20,
                    5,
                    30,
                    6,
                    152,
                    0
                ],
                "title": "Earley-Driven Dynamic Pruning for Efficient Structured Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Earley-Driven Dynamic Pruning for Efficient Structured Decoding"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable capabilities, yet ensuring\ntheir outputs conform to strict structural or grammatical constraints remains\nchallenging, which is critical in function calls and domain-specific language\n(DSL) generation. Constrained decoding with context-free grammar is a flexible\napproach to guarantee LLMs' adherence to a specific format by dynamically\nbuilding a token logits mask. However, creating this mask requires checking the\nvalidity of all tokens in the LLM vocabulary at every decoding step, which\noften incurs significant overheads in existing constrained decoding engines. To\naddress this challenge, we propose $\\textbf{ZapFormat}$, a novel\n$\\textbf{dynamic pruning}$ strategy based on the Earley algorithm that\nidentifies and eliminates invalid or redundant Earley states in real-time,\nsignificantly reducing memory occupation of the Earley algorithm's states. This\nfurther enables us to use a state cache to speed up structured generations on a\nlarge number of queries. We implemented ZapFormat in a new constrained decoding\nengine called Formatron which also incorporates existing optimizations. Through\ncomprehensive experiments on structured generation tasks, including JSON\ngeneration, JSON Schema validation, and semantic parsing, we demonstrate that\nFormatron not only $\\textbf{consistently maintains}$ high-precision compliant\noutputs but also achieves $\\textbf{significant improvements}$ in inference\nspeed up to 2x compared to state-of-the-art implementations. More importantly,\nFormatron is generally applicable across various LLM architectures. We release\nFormatron as open source at https://github.com/Dan-wanna-M/formatron.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable capabilities, yet ensuring\ntheir outputs conform to strict structural or grammatical constraints remains\nchallenging, which is critical in function calls and domain-specific language\n(DSL) generation. Constrained decoding with context-free grammar is a flexible\napproach to guarantee LLMs' adherence to a specific format by dynamically\nbuilding a token logits mask. However, creating this mask requires checking the\nvalidity of all tokens in the LLM vocabulary at every decoding step, which\noften incurs significant overheads in existing constrained decoding engines. To\naddress this challenge, we propose $\\textbf{ZapFormat}$, a novel\n$\\textbf{dynamic pruning}$ strategy based on the Earley algorithm that\nidentifies and eliminates invalid or redundant Earley states in real-time,\nsignificantly reducing memory occupation of the Earley algorithm's states. This\nfurther enables us to use a state cache to speed up structured generations on a\nlarge number of queries. We implemented ZapFormat in a new constrained decoding\nengine called Formatron which also incorporates existing optimizations. Through\ncomprehensive experiments on structured generation tasks, including JSON\ngeneration, JSON Schema validation, and semantic parsing, we demonstrate that\nFormatron not only $\\textbf{consistently maintains}$ high-precision compliant\noutputs but also achieves $\\textbf{significant improvements}$ in inference\nspeed up to 2x compared to state-of-the-art implementations. More importantly,\nFormatron is generally applicable across various LLM architectures. We release\nFormatron as open source at https://github.com/Dan-wanna-M/formatron."
                },
                "authors": [
                    {
                        "name": "Xintong Sun"
                    },
                    {
                        "name": "Chi Wei"
                    },
                    {
                        "name": "Minghao Tian"
                    },
                    {
                        "name": "Shiwen Ni"
                    }
                ],
                "author_detail": {
                    "name": "Shiwen Ni"
                },
                "author": "Shiwen Ni",
                "arxiv_comment": "ICML2025 poster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01151v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01151v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18458v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18458v3",
                "updated": "2025-06-01T16:00:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    1,
                    16,
                    0,
                    34,
                    6,
                    152,
                    0
                ],
                "published": "2025-05-24T01:57:12Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    1,
                    57,
                    12,
                    5,
                    144,
                    0
                ],
                "title": "A Survey of LLM $\\times$ DATA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of LLM $\\times$ DATA"
                },
                "summary": "The integration of large language model (LLM) and data management (DATA) is\nrapidly redefining both domains. In this survey, we comprehensively review the\nbidirectional relationships. On the one hand, DATA4LLM, spanning large-scale\ndata processing, storage, and serving, feeds LLMs with high quality, diversity,\nand timeliness of data required for stages like pre-training, post-training,\nretrieval-augmented generation, and agentic workflows: (i) Data processing for\nLLMs includes scalable acquisition, deduplication, filtering, selection, domain\nmixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on\nefficient data and model formats, distributed and heterogeneous storage\nhierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data\nserving for LLMs tackles challenges in RAG (e.g., knowledge post-processing),\nLLM inference (e.g., prompt compression, data provenance), and training\nstrategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA,\nLLMs are emerging as general-purpose engines for data management. We review\nrecent advances in (i) data manipulation, including automatic data cleaning,\nintegration, discovery; (ii) data analysis, covering reasoning over structured,\nsemi-structured, and unstructured data, and (iii) system optimization (e.g.,\nconfiguration tuning, query rewriting, anomaly diagnosis), powered by LLM\ntechniques like retrieval-augmented prompting, task-specialized fine-tuning,\nand multi-agent collaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of large language model (LLM) and data management (DATA) is\nrapidly redefining both domains. In this survey, we comprehensively review the\nbidirectional relationships. On the one hand, DATA4LLM, spanning large-scale\ndata processing, storage, and serving, feeds LLMs with high quality, diversity,\nand timeliness of data required for stages like pre-training, post-training,\nretrieval-augmented generation, and agentic workflows: (i) Data processing for\nLLMs includes scalable acquisition, deduplication, filtering, selection, domain\nmixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on\nefficient data and model formats, distributed and heterogeneous storage\nhierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data\nserving for LLMs tackles challenges in RAG (e.g., knowledge post-processing),\nLLM inference (e.g., prompt compression, data provenance), and training\nstrategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA,\nLLMs are emerging as general-purpose engines for data management. We review\nrecent advances in (i) data manipulation, including automatic data cleaning,\nintegration, discovery; (ii) data analysis, covering reasoning over structured,\nsemi-structured, and unstructured data, and (iii) system optimization (e.g.,\nconfiguration tuning, query rewriting, anomaly diagnosis), powered by LLM\ntechniques like retrieval-augmented prompting, task-specialized fine-tuning,\nand multi-agent collaboration."
                },
                "authors": [
                    {
                        "name": "Xuanhe Zhou"
                    },
                    {
                        "name": "Junxuan He"
                    },
                    {
                        "name": "Wei Zhou"
                    },
                    {
                        "name": "Haodong Chen"
                    },
                    {
                        "name": "Zirui Tang"
                    },
                    {
                        "name": "Haoyu Zhao"
                    },
                    {
                        "name": "Xin Tong"
                    },
                    {
                        "name": "Guoliang Li"
                    },
                    {
                        "name": "Youmin Chen"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Zhaojun Sun"
                    },
                    {
                        "name": "Binyuan Hui"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Jingren Zhou"
                    },
                    {
                        "name": "Fan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fan Wu"
                },
                "author": "Fan Wu",
                "arxiv_comment": "Please refer to the paper list at:\n  https://github.com/weAIDB/awesome-data-llm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18458v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18458v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06594v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06594v2",
                "updated": "2025-06-01T10:36:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    1,
                    10,
                    36,
                    7,
                    6,
                    152,
                    0
                ],
                "published": "2025-03-09T12:54:05Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    12,
                    54,
                    5,
                    6,
                    68,
                    0
                ],
                "title": "Beyond Decoder-only: Large Language Models Can be Good Encoders for\n  Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Decoder-only: Large Language Models Can be Good Encoders for\n  Machine Translation"
                },
                "summary": "The field of neural machine translation (NMT) has changed with the advent of\nlarge language models (LLMs). Much of the recent emphasis in natural language\nprocessing (NLP) has been on modeling machine translation and many other\nproblems using a single pre-trained Transformer decoder, while encoder-decoder\narchitectures, which were the standard in earlier NMT models, have received\nrelatively less attention. In this paper, we explore translation models that\nare universal, efficient, and easy to optimize, by marrying the world of LLMs\nwith the world of NMT. We apply LLMs to NMT encoding and leave the NMT decoder\nunchanged. We also develop methods for adapting LLMs to work better with the\nNMT decoder. Furthermore, we construct a new dataset involving multiple tasks\nto assess how well the machine translation system generalizes across various\ntasks. Evaluations on the WMT and our datasets show that results using our\nmethod match or surpass a range of baselines in terms of translation quality,\nbut achieve $2.4 \\sim 6.5 \\times$ inference speedups and a $75\\%$ reduction in\nthe memory footprint of the KV cache. It also demonstrates strong\ngeneralization across a variety of translation-related tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The field of neural machine translation (NMT) has changed with the advent of\nlarge language models (LLMs). Much of the recent emphasis in natural language\nprocessing (NLP) has been on modeling machine translation and many other\nproblems using a single pre-trained Transformer decoder, while encoder-decoder\narchitectures, which were the standard in earlier NMT models, have received\nrelatively less attention. In this paper, we explore translation models that\nare universal, efficient, and easy to optimize, by marrying the world of LLMs\nwith the world of NMT. We apply LLMs to NMT encoding and leave the NMT decoder\nunchanged. We also develop methods for adapting LLMs to work better with the\nNMT decoder. Furthermore, we construct a new dataset involving multiple tasks\nto assess how well the machine translation system generalizes across various\ntasks. Evaluations on the WMT and our datasets show that results using our\nmethod match or surpass a range of baselines in terms of translation quality,\nbut achieve $2.4 \\sim 6.5 \\times$ inference speedups and a $75\\%$ reduction in\nthe memory footprint of the KV cache. It also demonstrates strong\ngeneralization across a variety of translation-related tasks."
                },
                "authors": [
                    {
                        "name": "Yingfeng Luo"
                    },
                    {
                        "name": "Tong Zheng"
                    },
                    {
                        "name": "Yongyu Mu"
                    },
                    {
                        "name": "Bei Li"
                    },
                    {
                        "name": "Qinghong Zhang"
                    },
                    {
                        "name": "Yongqi Gao"
                    },
                    {
                        "name": "Ziqiang Xu"
                    },
                    {
                        "name": "Peinan Feng"
                    },
                    {
                        "name": "Xiaoqian Liu"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "Accepted to ACL Findings 2025. Please cite the ACL version. Code and\n  data are available at: https://github.com/NiuTrans/LaMaTE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06594v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06594v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00744v1",
                "updated": "2025-05-31T23:16:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    31,
                    23,
                    16,
                    53,
                    5,
                    151,
                    0
                ],
                "published": "2025-05-31T23:16:53Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    23,
                    16,
                    53,
                    5,
                    151,
                    0
                ],
                "title": "Blending Complementary Memory Systems in Hybrid Quadratic-Linear\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Blending Complementary Memory Systems in Hybrid Quadratic-Linear\n  Transformers"
                },
                "summary": "We develop hybrid memory architectures for general-purpose sequence\nprocessing neural networks, that combine key-value memory using softmax\nattention (KV-memory) with dynamic synaptic memory through fast-weight\nprogramming (FW-memory) -- the core principles of quadratic and linear\ntransformers, respectively. These two memory systems have complementary but\nindividually limited properties: KV-memory offers precise retrieval but is\nconstrained by quadratic complexity in sequence length, while FW-memory\nsupports arbitrarily long sequences and enables more expressive computation but\nsacrifices precise recall. We propose and compare three methods to blend these\ntwo systems into a single memory system to leverage the strengths of both. We\nconduct experiments on general language modeling and retrieval tasks by\ntraining 340M- and 1.3B-parameter models from scratch, as well as on synthetic\nalgorithmic tasks designed to precisely illustrate the benefits of certain\nhybrid methods over others. We also evaluate our hybrid memory systems on\nreinforcement learning in partially observable environments. Overall, we\ndemonstrate how a well-designed hybrid can overcome the limitations of its\nindividual components, offering new insights into the design principle of\nneural memory systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop hybrid memory architectures for general-purpose sequence\nprocessing neural networks, that combine key-value memory using softmax\nattention (KV-memory) with dynamic synaptic memory through fast-weight\nprogramming (FW-memory) -- the core principles of quadratic and linear\ntransformers, respectively. These two memory systems have complementary but\nindividually limited properties: KV-memory offers precise retrieval but is\nconstrained by quadratic complexity in sequence length, while FW-memory\nsupports arbitrarily long sequences and enables more expressive computation but\nsacrifices precise recall. We propose and compare three methods to blend these\ntwo systems into a single memory system to leverage the strengths of both. We\nconduct experiments on general language modeling and retrieval tasks by\ntraining 340M- and 1.3B-parameter models from scratch, as well as on synthetic\nalgorithmic tasks designed to precisely illustrate the benefits of certain\nhybrid methods over others. We also evaluate our hybrid memory systems on\nreinforcement learning in partially observable environments. Overall, we\ndemonstrate how a well-designed hybrid can overcome the limitations of its\nindividual components, offering new insights into the design principle of\nneural memory systems."
                },
                "authors": [
                    {
                        "name": "Kazuki Irie"
                    },
                    {
                        "name": "Morris Yau"
                    },
                    {
                        "name": "Samuel J. Gershman"
                    }
                ],
                "author_detail": {
                    "name": "Samuel J. Gershman"
                },
                "author": "Samuel J. Gershman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17491v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17491v2",
                "updated": "2025-05-31T23:01:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    31,
                    23,
                    1,
                    0,
                    5,
                    151,
                    0
                ],
                "published": "2024-07-04T02:35:00Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    2,
                    35,
                    0,
                    3,
                    186,
                    0
                ],
                "title": "Robust Adaptation of Foundation Models with Black-Box Visual Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Adaptation of Foundation Models with Black-Box Visual Prompting"
                },
                "summary": "With a surge of large-scale pre-trained models, parameter-efficient transfer\nlearning (PETL) of large models has garnered significant attention. While\npromising, they commonly rely on two optimistic assumptions: 1) full access to\nthe parameters of a PTM, and 2) sufficient memory capacity to cache all\nintermediate activations for gradient computation. However, in most real-world\napplications, PTMs serve as black-box APIs or proprietary software without full\nparameter accessibility. Besides, it is hard to meet a large memory requirement\nfor modern PTMs. This work proposes black-box visual prompting (BlackVIP),\nwhich efficiently adapts the PTMs without knowledge of their architectures or\nparameters. BlackVIP has two components: 1) Coordinator and 2) simultaneous\nperturbation stochastic approximation with gradient correction (SPSA-GC). The\nCoordinator designs input-dependent visual prompts, which allow the target PTM\nto adapt in the wild. SPSA-GC efficiently estimates the gradient of PTM to\nupdate Coordinator. Besides, we introduce a variant, BlackVIP-SE, which\nsignificantly reduces the runtime and computational cost of BlackVIP. Extensive\nexperiments on 19 datasets demonstrate that BlackVIPs enable robust adaptation\nto diverse domains and tasks with minimal memory requirements. We further\nprovide a theoretical analysis on the generalization of visual prompting\nmethods by presenting their connection to the certified robustness of\nrandomized smoothing, and presenting an empirical support for improved\nrobustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With a surge of large-scale pre-trained models, parameter-efficient transfer\nlearning (PETL) of large models has garnered significant attention. While\npromising, they commonly rely on two optimistic assumptions: 1) full access to\nthe parameters of a PTM, and 2) sufficient memory capacity to cache all\nintermediate activations for gradient computation. However, in most real-world\napplications, PTMs serve as black-box APIs or proprietary software without full\nparameter accessibility. Besides, it is hard to meet a large memory requirement\nfor modern PTMs. This work proposes black-box visual prompting (BlackVIP),\nwhich efficiently adapts the PTMs without knowledge of their architectures or\nparameters. BlackVIP has two components: 1) Coordinator and 2) simultaneous\nperturbation stochastic approximation with gradient correction (SPSA-GC). The\nCoordinator designs input-dependent visual prompts, which allow the target PTM\nto adapt in the wild. SPSA-GC efficiently estimates the gradient of PTM to\nupdate Coordinator. Besides, we introduce a variant, BlackVIP-SE, which\nsignificantly reduces the runtime and computational cost of BlackVIP. Extensive\nexperiments on 19 datasets demonstrate that BlackVIPs enable robust adaptation\nto diverse domains and tasks with minimal memory requirements. We further\nprovide a theoretical analysis on the generalization of visual prompting\nmethods by presenting their connection to the certified robustness of\nrandomized smoothing, and presenting an empirical support for improved\nrobustness."
                },
                "authors": [
                    {
                        "name": "Changdae Oh"
                    },
                    {
                        "name": "Gyeongdeok Seo"
                    },
                    {
                        "name": "Geunyoung Jung"
                    },
                    {
                        "name": "Zhi-Qi Cheng"
                    },
                    {
                        "name": "Hosik Choi"
                    },
                    {
                        "name": "Jiyoung Jung"
                    },
                    {
                        "name": "Kyungwoo Song"
                    }
                ],
                "author_detail": {
                    "name": "Kyungwoo Song"
                },
                "author": "Kyungwoo Song",
                "arxiv_comment": "Extended work from the CVPR'23 paper: arxiv:2303.14773; This paper\n  has been submitted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI) for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17491v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17491v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12942v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12942v2",
                "updated": "2025-05-31T22:12:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    31,
                    22,
                    12,
                    10,
                    5,
                    151,
                    0
                ],
                "published": "2025-05-19T10:29:32Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    10,
                    29,
                    32,
                    0,
                    139,
                    0
                ],
                "title": "A3 : an Analytical Low-Rank Approximation Framework for Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A3 : an Analytical Low-Rank Approximation Framework for Attention"
                },
                "summary": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance."
                },
                "authors": [
                    {
                        "name": "Jeffrey T. H. Wong"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Xinye Cao"
                    },
                    {
                        "name": "Pedro Gimenes"
                    },
                    {
                        "name": "George A. Constantinides"
                    },
                    {
                        "name": "Wayne Luk"
                    },
                    {
                        "name": "Yiren Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yiren Zhao"
                },
                "author": "Yiren Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12942v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12942v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01723v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01723v6",
                "updated": "2025-05-31T17:58:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    31,
                    17,
                    58,
                    24,
                    5,
                    151,
                    0
                ],
                "published": "2024-10-02T16:34:29Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    34,
                    29,
                    2,
                    276,
                    0
                ],
                "title": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method. Our code is available at\nhttps://github.com/ModelTC/HarmoniCa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method. Our code is available at\nhttps://github.com/ModelTC/HarmoniCa."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Zining Wang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Xinjie Zhang"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "Accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01723v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01723v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16175v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16175v2",
                "updated": "2025-05-31T13:43:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    31,
                    13,
                    43,
                    36,
                    5,
                    151,
                    0
                ],
                "published": "2025-05-22T03:26:50Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    3,
                    26,
                    50,
                    3,
                    142,
                    0
                ],
                "title": "QuickVideo: Real-Time Long Video Understanding with System Algorithm\n  Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuickVideo: Real-Time Long Video Understanding with System Algorithm\n  Co-Design"
                },
                "summary": "Long-video understanding has emerged as a crucial capability in real-world\napplications such as video surveillance, meeting summarization, educational\nlecture analysis, and sports broadcasting. However, it remains computationally\nprohibitive for VideoLLMs, primarily due to two bottlenecks: 1) sequential\nvideo decoding, the process of converting the raw bit stream to RGB frames can\ntake up to a minute for hour-long video inputs, and 2) costly prefilling of up\nto several million tokens for LLM inference, resulting in high latency and\nmemory use. To address these challenges, we propose QuickVideo, a\nsystem-algorithm co-design that substantially accelerates long-video\nunderstanding to support real-time downstream applications. It comprises three\nkey innovations: QuickDecoder, a parallelized CPU-based video decoder that\nachieves 2-3 times speedup by splitting videos into keyframe-aligned intervals\nprocessed concurrently; QuickPrefill, a memory-efficient prefilling method\nusing KV-cache pruning to support more frames with less GPU memory; and an\noverlapping scheme that overlaps CPU video decoding with GPU inference.\nTogether, these components infernece time reduce by a minute on long video\ninputs, enabling scalable, high-quality video understanding even on limited\nhardware. Experiments show that QuickVideo generalizes across durations and\nsampling rates, making long video processing feasible in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-video understanding has emerged as a crucial capability in real-world\napplications such as video surveillance, meeting summarization, educational\nlecture analysis, and sports broadcasting. However, it remains computationally\nprohibitive for VideoLLMs, primarily due to two bottlenecks: 1) sequential\nvideo decoding, the process of converting the raw bit stream to RGB frames can\ntake up to a minute for hour-long video inputs, and 2) costly prefilling of up\nto several million tokens for LLM inference, resulting in high latency and\nmemory use. To address these challenges, we propose QuickVideo, a\nsystem-algorithm co-design that substantially accelerates long-video\nunderstanding to support real-time downstream applications. It comprises three\nkey innovations: QuickDecoder, a parallelized CPU-based video decoder that\nachieves 2-3 times speedup by splitting videos into keyframe-aligned intervals\nprocessed concurrently; QuickPrefill, a memory-efficient prefilling method\nusing KV-cache pruning to support more frames with less GPU memory; and an\noverlapping scheme that overlaps CPU video decoding with GPU inference.\nTogether, these components infernece time reduce by a minute on long video\ninputs, enabling scalable, high-quality video understanding even on limited\nhardware. Experiments show that QuickVideo generalizes across durations and\nsampling rates, making long video processing feasible in practice."
                },
                "authors": [
                    {
                        "name": "Benjamin Schneider"
                    },
                    {
                        "name": "Dongfu Jiang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Wenhu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenhu Chen"
                },
                "author": "Wenhu Chen",
                "arxiv_comment": "19 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16175v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16175v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02038v1",
                "updated": "2025-05-31T06:58:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    31,
                    6,
                    58,
                    52,
                    5,
                    151,
                    0
                ],
                "published": "2025-05-31T06:58:52Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    6,
                    58,
                    52,
                    5,
                    151,
                    0
                ],
                "title": "Blockchain Powered Edge Intelligence for U-Healthcare in Privacy\n  Critical and Time Sensitive Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Blockchain Powered Edge Intelligence for U-Healthcare in Privacy\n  Critical and Time Sensitive Environment"
                },
                "summary": "Edge Intelligence (EI) serves as a critical enabler for privacy-preserving\nsystems by providing AI-empowered computation and distributed caching services\nat the edge, thereby minimizing latency and enhancing data privacy. The\nintegration of blockchain technology further augments EI frameworks by ensuring\ntransactional transparency, auditability, and system-wide reliability through a\ndecentralized network model. However, the operational architecture of such\nsystems introduces inherent vulnerabilities, particularly due to the extensive\ndata interactions between edge gateways (EGs) and the distributed nature of\ninformation storage during service provisioning. To address these challenges,\nwe propose an autonomous computing model along with its interaction topologies\ntailored for privacy-critical and time-sensitive health applications. The\nsystem supports continuous monitoring, real-time alert notifications, disease\ndetection, and robust data processing and aggregation. It also includes a data\ntransaction handler and mechanisms for ensuring privacy at the EGs. Moreover, a\nresource-efficient one-dimensional convolutional neural network (1D-CNN) is\nproposed for the multiclass classification of arrhythmia, enabling accurate and\nreal-time analysis of constrained EGs. Furthermore, a secure access scheme is\ndefined to manage both off-chain and on-chain data sharing and storage. To\nvalidate the proposed model, comprehensive security, performance, and cost\nanalyses are conducted, demonstrating the efficiency and reliability of the\nfine-grained access control scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Intelligence (EI) serves as a critical enabler for privacy-preserving\nsystems by providing AI-empowered computation and distributed caching services\nat the edge, thereby minimizing latency and enhancing data privacy. The\nintegration of blockchain technology further augments EI frameworks by ensuring\ntransactional transparency, auditability, and system-wide reliability through a\ndecentralized network model. However, the operational architecture of such\nsystems introduces inherent vulnerabilities, particularly due to the extensive\ndata interactions between edge gateways (EGs) and the distributed nature of\ninformation storage during service provisioning. To address these challenges,\nwe propose an autonomous computing model along with its interaction topologies\ntailored for privacy-critical and time-sensitive health applications. The\nsystem supports continuous monitoring, real-time alert notifications, disease\ndetection, and robust data processing and aggregation. It also includes a data\ntransaction handler and mechanisms for ensuring privacy at the EGs. Moreover, a\nresource-efficient one-dimensional convolutional neural network (1D-CNN) is\nproposed for the multiclass classification of arrhythmia, enabling accurate and\nreal-time analysis of constrained EGs. Furthermore, a secure access scheme is\ndefined to manage both off-chain and on-chain data sharing and storage. To\nvalidate the proposed model, comprehensive security, performance, and cost\nanalyses are conducted, demonstrating the efficiency and reliability of the\nfine-grained access control scheme."
                },
                "authors": [
                    {
                        "name": "Anum Nawaz"
                    },
                    {
                        "name": "Hafiz Humza Mahmood Ramzan"
                    },
                    {
                        "name": "Xianjia Yu"
                    },
                    {
                        "name": "Zhuo Zou"
                    },
                    {
                        "name": "Tomi Westerlund"
                    }
                ],
                "author_detail": {
                    "name": "Tomi Westerlund"
                },
                "author": "Tomi Westerlund",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00420v1",
                "updated": "2025-05-31T06:50:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    31,
                    6,
                    50,
                    5,
                    5,
                    151,
                    0
                ],
                "published": "2025-05-31T06:50:05Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    6,
                    50,
                    5,
                    5,
                    151,
                    0
                ],
                "title": "A New Spatiotemporal Correlation Anomaly Detection Method that\n  Integrates Contrastive Learning and Few-Shot Learning in Wireless Sensor\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Spatiotemporal Correlation Anomaly Detection Method that\n  Integrates Contrastive Learning and Few-Shot Learning in Wireless Sensor\n  Networks"
                },
                "summary": "Detecting anomalies in the data collected by WSNs can provide crucial\nevidence for assessing the reliability and stability of WSNs. Existing methods\nfor WSN anomaly detection often face challenges such as the limited extraction\nof spatiotemporal correlation features, the absence of sample labels, few\nanomaly samples, and an imbalanced sample distribution. To address these\nissues, a spatiotemporal correlation detection model (MTAD-RD) considering both\nmodel architecture and a two-stage training strategy perspective is proposed.\nIn terms of model structure design, the proposed MTAD-RD backbone network\nincludes a retentive network (RetNet) enhanced by a cross-retention (CR)\nmodule, a multigranular feature fusion module, and a graph attention network\nmodule to extract internode correlation information. This proposed model can\nintegrate the intermodal correlation features and spatial features of WSN\nneighbor nodes while extracting global information from time series data.\nMoreover, its serialized inference characteristic can remarkably reduce\ninference overhead. For model training, a two-stage training approach was\ndesigned. First, a contrastive learning proxy task was designed for time series\ndata with graph structure information in WSNs, enabling the backbone network to\nlearn transferable features from unlabeled data using unsupervised contrastive\nlearning methods, thereby addressing the issue of missing sample labels in the\ndataset. Then, a caching-based sample sampler was designed to divide samples\ninto few-shot and contrastive learning data. A specific joint loss function was\ndeveloped to jointly train the dual-graph discriminator network to address the\nproblem of sample imbalance effectively. In experiments carried out on real\npublic datasets, the designed MTAD-RD anomaly detection method achieved an F1\nscore of 90.97%, outperforming existing supervised WSN anomaly detection\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting anomalies in the data collected by WSNs can provide crucial\nevidence for assessing the reliability and stability of WSNs. Existing methods\nfor WSN anomaly detection often face challenges such as the limited extraction\nof spatiotemporal correlation features, the absence of sample labels, few\nanomaly samples, and an imbalanced sample distribution. To address these\nissues, a spatiotemporal correlation detection model (MTAD-RD) considering both\nmodel architecture and a two-stage training strategy perspective is proposed.\nIn terms of model structure design, the proposed MTAD-RD backbone network\nincludes a retentive network (RetNet) enhanced by a cross-retention (CR)\nmodule, a multigranular feature fusion module, and a graph attention network\nmodule to extract internode correlation information. This proposed model can\nintegrate the intermodal correlation features and spatial features of WSN\nneighbor nodes while extracting global information from time series data.\nMoreover, its serialized inference characteristic can remarkably reduce\ninference overhead. For model training, a two-stage training approach was\ndesigned. First, a contrastive learning proxy task was designed for time series\ndata with graph structure information in WSNs, enabling the backbone network to\nlearn transferable features from unlabeled data using unsupervised contrastive\nlearning methods, thereby addressing the issue of missing sample labels in the\ndataset. Then, a caching-based sample sampler was designed to divide samples\ninto few-shot and contrastive learning data. A specific joint loss function was\ndeveloped to jointly train the dual-graph discriminator network to address the\nproblem of sample imbalance effectively. In experiments carried out on real\npublic datasets, the designed MTAD-RD anomaly detection method achieved an F1\nscore of 90.97%, outperforming existing supervised WSN anomaly detection\nmethods."
                },
                "authors": [
                    {
                        "name": "Miao Ye"
                    },
                    {
                        "name": "Suxiao Wang"
                    },
                    {
                        "name": "Jiaguang Han"
                    },
                    {
                        "name": "Yong Wang"
                    },
                    {
                        "name": "Xiaoli Wang"
                    },
                    {
                        "name": "Jingxuan Wei"
                    },
                    {
                        "name": "Peng Wen"
                    },
                    {
                        "name": "Jing Cui"
                    }
                ],
                "author_detail": {
                    "name": "Jing Cui"
                },
                "author": "Jing Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00413v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00413v1",
                "updated": "2025-05-31T06:10:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    31,
                    6,
                    10,
                    10,
                    5,
                    151,
                    0
                ],
                "published": "2025-05-31T06:10:10Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    6,
                    10,
                    10,
                    5,
                    151,
                    0
                ],
                "title": "Accelerating Diffusion LLMs via Adaptive Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion LLMs via Adaptive Parallel Decoding"
                },
                "summary": "The generation speed of LLMs are bottlenecked by autoregressive decoding,\nwhere tokens are predicted sequentially one by one. Alternatively, diffusion\nlarge language models (dLLMs) theoretically allow for parallel token\ngeneration, but in practice struggle to achieve the speed of autoregressive\nmodels without significantly sacrificing quality. We therefore introduce\nadaptive parallel decoding (APD), a novel method that dynamically adjusts the\nnumber of tokens sampled in parallel. We achieve this by defining a\nmultiplicative mixture between the dLLM marginal probabilities and the joint\nprobability of sequences under a small auxiliary autoregressive model. This\ninverts the standard setup of speculative decoding, where the goal is to sample\nfrom a large autoregressive verifier by drafting from a smaller model. We\nfurther optimize APD by enabling KV caching and limiting the size of the masked\ninput. Altogether, our method puts forward three tunable parameters to flexibly\ntradeoff throughput and quality. We show that APD provides markedly higher\nthroughput with minimal quality degradations on downstream benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generation speed of LLMs are bottlenecked by autoregressive decoding,\nwhere tokens are predicted sequentially one by one. Alternatively, diffusion\nlarge language models (dLLMs) theoretically allow for parallel token\ngeneration, but in practice struggle to achieve the speed of autoregressive\nmodels without significantly sacrificing quality. We therefore introduce\nadaptive parallel decoding (APD), a novel method that dynamically adjusts the\nnumber of tokens sampled in parallel. We achieve this by defining a\nmultiplicative mixture between the dLLM marginal probabilities and the joint\nprobability of sequences under a small auxiliary autoregressive model. This\ninverts the standard setup of speculative decoding, where the goal is to sample\nfrom a large autoregressive verifier by drafting from a smaller model. We\nfurther optimize APD by enabling KV caching and limiting the size of the masked\ninput. Altogether, our method puts forward three tunable parameters to flexibly\ntradeoff throughput and quality. We show that APD provides markedly higher\nthroughput with minimal quality degradations on downstream benchmarks."
                },
                "authors": [
                    {
                        "name": "Daniel Israel"
                    },
                    {
                        "name": "Guy Van den Broeck"
                    },
                    {
                        "name": "Aditya Grover"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Grover"
                },
                "author": "Aditya Grover",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00413v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00413v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04420v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04420v4",
                "updated": "2025-05-31T04:45:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    31,
                    4,
                    45,
                    23,
                    5,
                    151,
                    0
                ],
                "published": "2025-02-06T15:26:26Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    26,
                    26,
                    3,
                    37,
                    0
                ],
                "title": "KVTuner: Sensitivity-Aware Layer-Wise Mixed-Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVTuner: Sensitivity-Aware Layer-Wise Mixed-Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference"
                },
                "summary": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we theoretically analyze the\ninherent correlation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is generally more important than\nvalue cache for quantization error reduction. We further propose a simple yet\neffective framework KVTuner to adaptively search for the optimal\nhardware-friendly layer-wise KV quantization precision pairs for coarse-grained\nKV cache with multi-objective optimization and directly utilize the offline\nsearched configurations during online inference. To reduce the computational\ncost of offline calibration, we utilize the intra-layer KV precision pair\npruning and inter-layer clustering to reduce the search space. Experimental\nresults show that we can achieve nearly lossless 3.25-bit mixed precision KV\ncache quantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for\nsensitive models like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The\nmaximum inference throughput can be improved by 21.25\\% compared with KIVI-KV8\nquantization over various context lengths. Our code and searched configurations\nare available at https://github.com/cmd2001/KVTuner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we theoretically analyze the\ninherent correlation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is generally more important than\nvalue cache for quantization error reduction. We further propose a simple yet\neffective framework KVTuner to adaptively search for the optimal\nhardware-friendly layer-wise KV quantization precision pairs for coarse-grained\nKV cache with multi-objective optimization and directly utilize the offline\nsearched configurations during online inference. To reduce the computational\ncost of offline calibration, we utilize the intra-layer KV precision pair\npruning and inter-layer clustering to reduce the search space. Experimental\nresults show that we can achieve nearly lossless 3.25-bit mixed precision KV\ncache quantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for\nsensitive models like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The\nmaximum inference throughput can be improved by 21.25\\% compared with KIVI-KV8\nquantization over various context lengths. Our code and searched configurations\nare available at https://github.com/cmd2001/KVTuner."
                },
                "authors": [
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zeyu Xing"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Linping Qu"
                    },
                    {
                        "name": "Hui-Ling Zhen"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Sinno Jialin Pan"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "arxiv_comment": "Accepted by ICML25. Code: https://github.com/cmd2001/KVTuner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04420v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04420v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2506.09996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09996v1",
                "updated": "2025-06-11T17:59:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    59,
                    58,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T17:59:58Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    59,
                    58,
                    2,
                    162,
                    0
                ],
                "title": "From Judgment to Interference: Early Stopping LLM Harmful Outputs via\n  Streaming Content Monitoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Judgment to Interference: Early Stopping LLM Harmful Outputs via\n  Streaming Content Monitoring"
                },
                "summary": "Though safety alignment has been applied to most large language models\n(LLMs), LLM service providers generally deploy a subsequent moderation as the\nexternal safety guardrail in real-world products. Existing moderators mainly\npractice a conventional full detection, which determines the harmfulness based\non the complete LLM output, causing high service latency. Recent works pay more\nattention to partial detection where moderators oversee the generation midway\nand early stop the output if harmfulness is detected, but they directly apply\nmoderators trained with the full detection paradigm to incomplete outputs,\nintroducing a training-inference gap that lowers the performance. In this\npaper, we explore how to form a data-and-model solution that natively supports\npartial detection. For the data, we construct FineHarm, a dataset consisting of\n29K prompt-response pairs with fine-grained annotations to provide reasonable\nsupervision for token-level training. Then, we propose the streaming content\nmonitor, which is trained with dual supervision of response- and token-level\nlabels and can follow the output stream of LLM to make a timely judgment of\nharmfulness. Experiments show that SCM gains 0.95+ in macro F1 score that is\ncomparable to full detection, by only seeing the first 18% of tokens in\nresponses on average. Moreover, the SCM can serve as a pseudo-harmfulness\nannotator for improving safety alignment and lead to a higher harmlessness\nscore than DPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Though safety alignment has been applied to most large language models\n(LLMs), LLM service providers generally deploy a subsequent moderation as the\nexternal safety guardrail in real-world products. Existing moderators mainly\npractice a conventional full detection, which determines the harmfulness based\non the complete LLM output, causing high service latency. Recent works pay more\nattention to partial detection where moderators oversee the generation midway\nand early stop the output if harmfulness is detected, but they directly apply\nmoderators trained with the full detection paradigm to incomplete outputs,\nintroducing a training-inference gap that lowers the performance. In this\npaper, we explore how to form a data-and-model solution that natively supports\npartial detection. For the data, we construct FineHarm, a dataset consisting of\n29K prompt-response pairs with fine-grained annotations to provide reasonable\nsupervision for token-level training. Then, we propose the streaming content\nmonitor, which is trained with dual supervision of response- and token-level\nlabels and can follow the output stream of LLM to make a timely judgment of\nharmfulness. Experiments show that SCM gains 0.95+ in macro F1 score that is\ncomparable to full detection, by only seeing the first 18% of tokens in\nresponses on average. Moreover, the SCM can serve as a pseudo-harmfulness\nannotator for improving safety alignment and lead to a higher harmlessness\nscore than DPO."
                },
                "authors": [
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Qiang Sheng"
                    },
                    {
                        "name": "Yehan Yang"
                    },
                    {
                        "name": "Xueyao Zhang"
                    },
                    {
                        "name": "Juan Cao"
                    }
                ],
                "author_detail": {
                    "name": "Juan Cao"
                },
                "author": "Juan Cao",
                "arxiv_comment": "22 pages, 7 figures, and 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09998v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09998v1",
                "updated": "2025-06-11T17:59:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    59,
                    58,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T17:59:58Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    59,
                    58,
                    2,
                    162,
                    0
                ],
                "title": "Flipping Against All Odds: Reducing LLM Coin Flip Bias via Verbalized\n  Rejection Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flipping Against All Odds: Reducing LLM Coin Flip Bias via Verbalized\n  Rejection Sampling"
                },
                "summary": "Large language models (LLMs) can often accurately describe probability\ndistributions using natural language, yet they still struggle to generate\nfaithful samples from them. This mismatch limits their use in tasks requiring\nreliable stochasticity, such as Monte Carlo methods, agent-based simulations,\nand randomized decision-making. We investigate this gap between knowledge and\nsampling in the context of Bernoulli distributions. We introduce Verbalized\nRejection Sampling (VRS), a natural-language adaptation of classical rejection\nsampling that prompts the LLM to reason about and accept or reject proposed\nsamples. Despite relying on the same Bernoulli mechanism internally, VRS\nsubstantially reduces sampling bias across models. We provide theoretical\nanalysis showing that, under mild assumptions, VRS improves over direct\nsampling, with gains attributable to both the algorithm and prompt design. More\nbroadly, our results show how classical probabilistic tools can be verbalized\nand embedded into LLM workflows to improve reliability, without requiring\naccess to model internals or heavy prompt engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can often accurately describe probability\ndistributions using natural language, yet they still struggle to generate\nfaithful samples from them. This mismatch limits their use in tasks requiring\nreliable stochasticity, such as Monte Carlo methods, agent-based simulations,\nand randomized decision-making. We investigate this gap between knowledge and\nsampling in the context of Bernoulli distributions. We introduce Verbalized\nRejection Sampling (VRS), a natural-language adaptation of classical rejection\nsampling that prompts the LLM to reason about and accept or reject proposed\nsamples. Despite relying on the same Bernoulli mechanism internally, VRS\nsubstantially reduces sampling bias across models. We provide theoretical\nanalysis showing that, under mild assumptions, VRS improves over direct\nsampling, with gains attributable to both the algorithm and prompt design. More\nbroadly, our results show how classical probabilistic tools can be verbalized\nand embedded into LLM workflows to improve reliability, without requiring\naccess to model internals or heavy prompt engineering."
                },
                "authors": [
                    {
                        "name": "Tim Z. Xiao"
                    },
                    {
                        "name": "Johannes Zenn"
                    },
                    {
                        "name": "Zhen Liu"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Robert Bamler"
                    },
                    {
                        "name": "Bernhard Schlkopf"
                    }
                ],
                "author_detail": {
                    "name": "Bernhard Schlkopf"
                },
                "author": "Bernhard Schlkopf",
                "arxiv_comment": "Technical Report v1 (21 pages, 14 figures)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09998v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09998v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09991v1",
                "updated": "2025-06-11T17:59:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    59,
                    23,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T17:59:23Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    59,
                    23,
                    2,
                    162,
                    0
                ],
                "title": "Multiverse: Your Language Models Secretly Decide How to Parallelize and\n  Merge Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiverse: Your Language Models Secretly Decide How to Parallelize and\n  Merge Generation"
                },
                "summary": "Autoregressive Large Language Models (AR-LLMs) frequently exhibit implicit\nparallelism in sequential generation. Inspired by this, we introduce\nMultiverse, a new generative model that enables natively parallel generation.\nMultiverse internalizes a MapReduce paradigm, generating automatically through\nthree stages: (i) a Map stage for adaptive task decomposition, (ii) a Process\nstage for parallel subtask execution, and (iii) a Reduce stage for lossless\nresult synthesis. Next, we build a real-world Multiverse reasoning model with\nco-design of data, algorithm, and system, enabling rapid and seamless transfer\nfrom frontier AR-LLMs. Starting from sequential reasoning chains, we create\nMultiverse 1K by converting them into structured training data using an\nautomated LLM-assisted pipeline, avoiding costly human annotations.\nAlgorithmically, we design Multiverse Attention to separate parallel reasoning\nsteps while keeping compatibility with causal attention for efficient training.\nSystematically, we implement Multiverse Engine to enable parallel inference. It\nfeatures a dedicated scheduler that dynamically switches between sequential and\nparallel generation, triggered directly by the model. After a 3-hour\nfine-tuning with 1K examples, our Multiverse-32B stands as the only\nopen-sourced non-AR model achieving performance on par with leading AR-LLMs of\nthe same scale, evidenced by AIME24 & 25 scores of 54% and 46%, respectively.\nMoreover, our budget control experiments show that Multiverse-32B exhibits\nsuperior scaling, outperforming AR-LLMs by 1.87% on average using the same\ncontext length. Such scaling further leads to practical efficiency gain,\nachieving up to 2x speedup across varying batch sizes. We have open-sourced the\nentire Multiverse ecosystem, including data, model weights, engine, supporting\ntools, as well as complete data curation prompts and detailed training and\nevaluation recipes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Large Language Models (AR-LLMs) frequently exhibit implicit\nparallelism in sequential generation. Inspired by this, we introduce\nMultiverse, a new generative model that enables natively parallel generation.\nMultiverse internalizes a MapReduce paradigm, generating automatically through\nthree stages: (i) a Map stage for adaptive task decomposition, (ii) a Process\nstage for parallel subtask execution, and (iii) a Reduce stage for lossless\nresult synthesis. Next, we build a real-world Multiverse reasoning model with\nco-design of data, algorithm, and system, enabling rapid and seamless transfer\nfrom frontier AR-LLMs. Starting from sequential reasoning chains, we create\nMultiverse 1K by converting them into structured training data using an\nautomated LLM-assisted pipeline, avoiding costly human annotations.\nAlgorithmically, we design Multiverse Attention to separate parallel reasoning\nsteps while keeping compatibility with causal attention for efficient training.\nSystematically, we implement Multiverse Engine to enable parallel inference. It\nfeatures a dedicated scheduler that dynamically switches between sequential and\nparallel generation, triggered directly by the model. After a 3-hour\nfine-tuning with 1K examples, our Multiverse-32B stands as the only\nopen-sourced non-AR model achieving performance on par with leading AR-LLMs of\nthe same scale, evidenced by AIME24 & 25 scores of 54% and 46%, respectively.\nMoreover, our budget control experiments show that Multiverse-32B exhibits\nsuperior scaling, outperforming AR-LLMs by 1.87% on average using the same\ncontext length. Such scaling further leads to practical efficiency gain,\nachieving up to 2x speedup across varying batch sizes. We have open-sourced the\nentire Multiverse ecosystem, including data, model weights, engine, supporting\ntools, as well as complete data curation prompts and detailed training and\nevaluation recipes."
                },
                "authors": [
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Yuwei An"
                    },
                    {
                        "name": "Hongyi Liu"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09984v1",
                "updated": "2025-06-11T17:57:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    57,
                    9,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T17:57:09Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    57,
                    9,
                    2,
                    162,
                    0
                ],
                "title": "InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio\n  Conditions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio\n  Conditions"
                },
                "summary": "End-to-end human animation with rich multi-modal conditions, e.g., text,\nimage and audio has achieved remarkable advancements in recent years. However,\nmost existing methods could only animate a single subject and inject conditions\nin a global manner, ignoring scenarios that multiple concepts could appears in\nthe same video with rich human-human interactions and human-object\ninteractions. Such global assumption prevents precise and per-identity control\nof multiple concepts including humans and objects, therefore hinders\napplications. In this work, we discard the single-entity assumption and\nintroduce a novel framework that enforces strong, region-specific binding of\nconditions from modalities to each identity's spatiotemporal footprint. Given\nreference images of multiple concepts, our method could automatically infer\nlayout information by leveraging a mask predictor to match appearance cues\nbetween the denoised video and each reference appearance. Furthermore, we\ninject local audio condition into its corresponding region to ensure\nlayout-aligned modality matching in a iterative manner. This design enables the\nhigh-quality generation of controllable multi-concept human-centric videos.\nEmpirical results and ablation studies validate the effectiveness of our\nexplicit layout control for multi-modal conditions compared to implicit\ncounterparts and other existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-end human animation with rich multi-modal conditions, e.g., text,\nimage and audio has achieved remarkable advancements in recent years. However,\nmost existing methods could only animate a single subject and inject conditions\nin a global manner, ignoring scenarios that multiple concepts could appears in\nthe same video with rich human-human interactions and human-object\ninteractions. Such global assumption prevents precise and per-identity control\nof multiple concepts including humans and objects, therefore hinders\napplications. In this work, we discard the single-entity assumption and\nintroduce a novel framework that enforces strong, region-specific binding of\nconditions from modalities to each identity's spatiotemporal footprint. Given\nreference images of multiple concepts, our method could automatically infer\nlayout information by leveraging a mask predictor to match appearance cues\nbetween the denoised video and each reference appearance. Furthermore, we\ninject local audio condition into its corresponding region to ensure\nlayout-aligned modality matching in a iterative manner. This design enables the\nhigh-quality generation of controllable multi-concept human-centric videos.\nEmpirical results and ablation studies validate the effectiveness of our\nexplicit layout control for multi-modal conditions compared to implicit\ncounterparts and other existing methods."
                },
                "authors": [
                    {
                        "name": "Zhenzhi Wang"
                    },
                    {
                        "name": "Jiaqi Yang"
                    },
                    {
                        "name": "Jianwen Jiang"
                    },
                    {
                        "name": "Chao Liang"
                    },
                    {
                        "name": "Gaojie Lin"
                    },
                    {
                        "name": "Zerong Zheng"
                    },
                    {
                        "name": "Ceyuan Yang"
                    },
                    {
                        "name": "Dahua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Dahua Lin"
                },
                "author": "Dahua Lin",
                "arxiv_comment": "TL;DR: The first multi-person dialogue video generation method from\n  pairs of reference image and audio via explicit layout-aligned condition\n  injection. See project page https://zhenzhiwang.github.io/interacthuman/ for\n  more details",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09983v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09983v1",
                "updated": "2025-06-11T17:56:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    56,
                    10,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T17:56:10Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    56,
                    10,
                    2,
                    162,
                    0
                ],
                "title": "Step-by-step Instructions and a Simple Tabular Output Format Improve the\n  Dependency Parsing Accuracy of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Step-by-step Instructions and a Simple Tabular Output Format Improve the\n  Dependency Parsing Accuracy of LLMs"
                },
                "summary": "Recent advances in large language models (LLMs) have enabled impressive\nperformance in various tasks. However, standard prompting often struggles to\nproduce structurally valid and accurate outputs, especially in dependency\nparsing. We propose a novel step-by-step instruction strategy, where universal\npart-of-speech tagging precedes the prediction of syntactic heads and\ndependency labels, and a simplified CoNLL-U like output format, our method\nachieves state-of-the-art accuracy on Universal Dependencies datasets across 17\nlanguages without hallucination or contamination. We further show that\nmultilingual fine-tuning simultaneously improves cross-language generalization\nperformance. Our results highlight the effectiveness of explicit reasoning\nsteps in LLM-based parsing and offer a scalable, format-consistent alternative\nto bracket-based approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have enabled impressive\nperformance in various tasks. However, standard prompting often struggles to\nproduce structurally valid and accurate outputs, especially in dependency\nparsing. We propose a novel step-by-step instruction strategy, where universal\npart-of-speech tagging precedes the prediction of syntactic heads and\ndependency labels, and a simplified CoNLL-U like output format, our method\nachieves state-of-the-art accuracy on Universal Dependencies datasets across 17\nlanguages without hallucination or contamination. We further show that\nmultilingual fine-tuning simultaneously improves cross-language generalization\nperformance. Our results highlight the effectiveness of explicit reasoning\nsteps in LLM-based parsing and offer a scalable, format-consistent alternative\nto bracket-based approaches."
                },
                "authors": [
                    {
                        "name": "Hiroshi Matsuda"
                    },
                    {
                        "name": "Chunpeng Ma"
                    },
                    {
                        "name": "Masayuki Asahara"
                    }
                ],
                "author_detail": {
                    "name": "Masayuki Asahara"
                },
                "author": "Masayuki Asahara",
                "arxiv_comment": "9 pages, 2 figures, accepted for SyntaxFest 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09983v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09983v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09975v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09975v1",
                "updated": "2025-06-11T17:51:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    51,
                    28,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T17:51:28Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    51,
                    28,
                    2,
                    162,
                    0
                ],
                "title": "When Detection Fails: The Power of Fine-Tuned Models to Generate\n  Human-Like Social Media Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Detection Fails: The Power of Fine-Tuned Models to Generate\n  Human-Like Social Media Text"
                },
                "summary": "Detecting AI-generated text is a difficult problem to begin with; detecting\nAI-generated text on social media is made even more difficult due to the short\ntext length and informal, idiosyncratic language of the internet. It is\nnonetheless important to tackle this problem, as social media represents a\nsignificant attack vector in online influence campaigns, which may be bolstered\nthrough the use of mass-produced AI-generated posts supporting (or opposing)\nparticular policies, decisions, or events. We approach this problem with the\nmindset and resources of a reasonably sophisticated threat actor, and create a\ndataset of 505,159 AI-generated social media posts from a combination of\nopen-source, closed-source, and fine-tuned LLMs, covering 11 different\ncontroversial topics. We show that while the posts can be detected under\ntypical research assumptions about knowledge of and access to the generating\nmodels, under the more realistic assumption that an attacker will not release\ntheir fine-tuned model to the public, detectability drops dramatically. This\nresult is confirmed with a human study. Ablation experiments highlight the\nvulnerability of various detection algorithms to fine-tuned LLMs. This result\nhas implications across all detection domains, since fine-tuning is a generally\napplicable and realistic LLM use case.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting AI-generated text is a difficult problem to begin with; detecting\nAI-generated text on social media is made even more difficult due to the short\ntext length and informal, idiosyncratic language of the internet. It is\nnonetheless important to tackle this problem, as social media represents a\nsignificant attack vector in online influence campaigns, which may be bolstered\nthrough the use of mass-produced AI-generated posts supporting (or opposing)\nparticular policies, decisions, or events. We approach this problem with the\nmindset and resources of a reasonably sophisticated threat actor, and create a\ndataset of 505,159 AI-generated social media posts from a combination of\nopen-source, closed-source, and fine-tuned LLMs, covering 11 different\ncontroversial topics. We show that while the posts can be detected under\ntypical research assumptions about knowledge of and access to the generating\nmodels, under the more realistic assumption that an attacker will not release\ntheir fine-tuned model to the public, detectability drops dramatically. This\nresult is confirmed with a human study. Ablation experiments highlight the\nvulnerability of various detection algorithms to fine-tuned LLMs. This result\nhas implications across all detection domains, since fine-tuning is a generally\napplicable and realistic LLM use case."
                },
                "authors": [
                    {
                        "name": "Hillary Dawkins"
                    },
                    {
                        "name": "Kathleen C. Fraser"
                    },
                    {
                        "name": "Svetlana Kiritchenko"
                    }
                ],
                "author_detail": {
                    "name": "Svetlana Kiritchenko"
                },
                "author": "Svetlana Kiritchenko",
                "arxiv_comment": "to appear in ACL Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09975v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09975v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12372v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12372v6",
                "updated": "2025-06-11T17:49:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    49,
                    37,
                    2,
                    162,
                    0
                ],
                "published": "2025-01-21T18:52:15Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    18,
                    52,
                    15,
                    1,
                    21,
                    0
                ],
                "title": "Is Long Context All You Need? Leveraging LLM's Extended Context for\n  NL2SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Long Context All You Need? Leveraging LLM's Extended Context for\n  NL2SQL"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities across\na range of natural language processing tasks. In particular, improvements in\nreasoning abilities and the expansion of context windows have opened new\navenues for leveraging these powerful models. NL2SQL is challenging in that the\nnatural language question is inherently ambiguous, while the SQL generation\nrequires a precise understanding of complex data schema and semantics. One\napproach to this semantic ambiguous problem is to provide more and sufficient\ncontextual information.\n  In this work, we explore the performance and the latency trade-offs of the\nextended context window (a.k.a., long context) offered by Google's\nstate-of-the-art LLM (\\textit{gemini-1.5-pro}). We study the impact of various\ncontextual information, including column example values, question and SQL query\npairs, user-provided hints, SQL documentation, and schema. To the best of our\nknowledge, this is the first work to study how the extended context window and\nextra contextual information can help NL2SQL generation with respect to both\naccuracy and latency cost. We show that long context LLMs are robust and do not\nget lost in the extended contextual information. Additionally, our long-context\nNL2SQL pipeline based on Google's \\textit{gemini-pro-1.5} achieve strong\nperformances on various benchmark datasets without finetuning and expensive\nself-consistency based techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities across\na range of natural language processing tasks. In particular, improvements in\nreasoning abilities and the expansion of context windows have opened new\navenues for leveraging these powerful models. NL2SQL is challenging in that the\nnatural language question is inherently ambiguous, while the SQL generation\nrequires a precise understanding of complex data schema and semantics. One\napproach to this semantic ambiguous problem is to provide more and sufficient\ncontextual information.\n  In this work, we explore the performance and the latency trade-offs of the\nextended context window (a.k.a., long context) offered by Google's\nstate-of-the-art LLM (\\textit{gemini-1.5-pro}). We study the impact of various\ncontextual information, including column example values, question and SQL query\npairs, user-provided hints, SQL documentation, and schema. To the best of our\nknowledge, this is the first work to study how the extended context window and\nextra contextual information can help NL2SQL generation with respect to both\naccuracy and latency cost. We show that long context LLMs are robust and do not\nget lost in the extended contextual information. Additionally, our long-context\nNL2SQL pipeline based on Google's \\textit{gemini-pro-1.5} achieve strong\nperformances on various benchmark datasets without finetuning and expensive\nself-consistency based techniques."
                },
                "authors": [
                    {
                        "name": "Yeounoh Chung"
                    },
                    {
                        "name": "Gaurav T. Kakkar"
                    },
                    {
                        "name": "Yu Gan"
                    },
                    {
                        "name": "Brenton Milne"
                    },
                    {
                        "name": "Fatma Ozcan"
                    }
                ],
                "author_detail": {
                    "name": "Fatma Ozcan"
                },
                "author": "Fatma Ozcan",
                "arxiv_doi": "10.14778/3742728.3742761",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.14778/3742728.3742761",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.12372v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12372v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "13 pages, 6 figures, VLDB 2025",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.16998v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.16998v5",
                "updated": "2025-06-11T17:46:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    46,
                    56,
                    2,
                    162,
                    0
                ],
                "published": "2024-03-25T17:59:09Z",
                "published_parsed": [
                    2024,
                    3,
                    25,
                    17,
                    59,
                    9,
                    0,
                    85,
                    0
                ],
                "title": "Understanding Long Videos with Multimodal Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Long Videos with Multimodal Language Models"
                },
                "summary": "Large Language Models (LLMs) have allowed recent LLM-based approaches to\nachieve excellent performance on long-video understanding benchmarks. We\ninvestigate how extensive world knowledge and strong reasoning skills of\nunderlying LLMs influence this strong performance. Surprisingly, we discover\nthat LLM-based approaches can yield surprisingly good accuracy on long-video\ntasks with limited video information, sometimes even with no video specific\ninformation. Building on this, we explore injecting video-specific information\ninto an LLM-based framework. We utilize off-the-shelf vision tools to extract\nthree object-centric information modalities from videos, and then leverage\nnatural language as a medium for fusing this information. Our resulting\nMultimodal Video Understanding (MVU) framework demonstrates state-of-the-art\nperformance across multiple video understanding benchmarks. Strong performance\nalso on robotics domain tasks establish its strong generality. Code:\nhttps://github.com/kahnchana/mvu",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have allowed recent LLM-based approaches to\nachieve excellent performance on long-video understanding benchmarks. We\ninvestigate how extensive world knowledge and strong reasoning skills of\nunderlying LLMs influence this strong performance. Surprisingly, we discover\nthat LLM-based approaches can yield surprisingly good accuracy on long-video\ntasks with limited video information, sometimes even with no video specific\ninformation. Building on this, we explore injecting video-specific information\ninto an LLM-based framework. We utilize off-the-shelf vision tools to extract\nthree object-centric information modalities from videos, and then leverage\nnatural language as a medium for fusing this information. Our resulting\nMultimodal Video Understanding (MVU) framework demonstrates state-of-the-art\nperformance across multiple video understanding benchmarks. Strong performance\nalso on robotics domain tasks establish its strong generality. Code:\nhttps://github.com/kahnchana/mvu"
                },
                "authors": [
                    {
                        "name": "Kanchana Ranasinghe"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Kumara Kahatapitiya"
                    },
                    {
                        "name": "Michael S. Ryoo"
                    }
                ],
                "author_detail": {
                    "name": "Michael S. Ryoo"
                },
                "author": "Michael S. Ryoo",
                "arxiv_comment": "17 pages (main paper), 7 pages appendix. ICLR 2025 conference paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.16998v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.16998v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09968v1",
                "updated": "2025-06-11T17:45:03Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    45,
                    3,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T17:45:03Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    45,
                    3,
                    2,
                    162,
                    0
                ],
                "title": "SRLAgent: Enhancing Self-Regulated Learning Skills through Gamification\n  and LLM Assistance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SRLAgent: Enhancing Self-Regulated Learning Skills through Gamification\n  and LLM Assistance"
                },
                "summary": "Self-regulated learning (SRL) is crucial for college students navigating\nincreased academic demands and independence. Insufficient SRL skills can lead\nto disorganized study habits, low motivation, and poor time management,\nundermining learners ability to thrive in challenging environments. Through a\nformative study involving 59 college students, we identified key challenges\nstudents face in developing SRL skills, including difficulties with\ngoal-setting, time management, and reflective learning. To address these\nchallenges, we introduce SRLAgent, an LLM-assisted system that fosters SRL\nskills through gamification and adaptive support from large language models\n(LLMs). Grounded in Zimmermans three-phase SRL framework, SRLAgent enables\nstudents to engage in goal-setting, strategy execution, and self-reflection\nwithin an interactive game-based environment. The system offers real-time\nfeedback and scaffolding powered by LLMs to support students independent study\nefforts. We evaluated SRLAgent using a between-subjects design, comparing it to\na baseline system (SRL without Agent features) and a traditional multimedia\nlearning condition. Results showed significant improvements in SRL skills\nwithin the SRLAgent group (p < .001, Cohens d = 0.234) and higher engagement\ncompared to the baselines. This work highlights the value of embedding SRL\nscaffolding and real-time AI support within gamified environments, offering\ndesign implications for educational technologies that aim to promote deeper\nlearning and metacognitive skill development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-regulated learning (SRL) is crucial for college students navigating\nincreased academic demands and independence. Insufficient SRL skills can lead\nto disorganized study habits, low motivation, and poor time management,\nundermining learners ability to thrive in challenging environments. Through a\nformative study involving 59 college students, we identified key challenges\nstudents face in developing SRL skills, including difficulties with\ngoal-setting, time management, and reflective learning. To address these\nchallenges, we introduce SRLAgent, an LLM-assisted system that fosters SRL\nskills through gamification and adaptive support from large language models\n(LLMs). Grounded in Zimmermans three-phase SRL framework, SRLAgent enables\nstudents to engage in goal-setting, strategy execution, and self-reflection\nwithin an interactive game-based environment. The system offers real-time\nfeedback and scaffolding powered by LLMs to support students independent study\nefforts. We evaluated SRLAgent using a between-subjects design, comparing it to\na baseline system (SRL without Agent features) and a traditional multimedia\nlearning condition. Results showed significant improvements in SRL skills\nwithin the SRLAgent group (p < .001, Cohens d = 0.234) and higher engagement\ncompared to the baselines. This work highlights the value of embedding SRL\nscaffolding and real-time AI support within gamified environments, offering\ndesign implications for educational technologies that aim to promote deeper\nlearning and metacognitive skill development."
                },
                "authors": [
                    {
                        "name": "Wentao Ge"
                    },
                    {
                        "name": "Yuqing Sun"
                    },
                    {
                        "name": "Ziyan Wang"
                    },
                    {
                        "name": "Haoyue Zheng"
                    },
                    {
                        "name": "Weiyang He"
                    },
                    {
                        "name": "Piaohong Wang"
                    },
                    {
                        "name": "Qianyu Zhu"
                    },
                    {
                        "name": "Benyou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Benyou Wang"
                },
                "author": "Benyou Wang",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.1; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11171v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11171v2",
                "updated": "2025-06-11T17:44:41Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    44,
                    41,
                    2,
                    162,
                    0
                ],
                "published": "2025-04-15T13:17:39Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    17,
                    39,
                    1,
                    105,
                    0
                ],
                "title": "TerraMind: Large-Scale Generative Multimodality for Earth Observation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TerraMind: Large-Scale Generative Multimodality for Earth Observation"
                },
                "summary": "We present TerraMind, the first any-to-any generative, multimodal foundation\nmodel for Earth observation (EO). Unlike other multimodal models, TerraMind is\npretrained on dual-scale representations combining both token-level and\npixel-level data across modalities. On a token level, TerraMind encodes\nhigh-level contextual information to learn cross-modal relationships, while on\na pixel level, TerraMind leverages fine-grained representations to capture\ncritical spatial nuances. We pretrained TerraMind on nine geospatial modalities\nof a global, large-scale dataset. In this paper, we demonstrate that (i)\nTerraMind's dual-scale early fusion approach unlocks a range of zero-shot and\nfew-shot applications for Earth observation, (ii) TerraMind introduces\n\"Thinking-in-Modalities\" (TiM) -- the capability of generating additional\nartificial data during finetuning and inference to improve the model output --\nand (iii) TerraMind achieves beyond state-of-the-art performance in\ncommunity-standard benchmarks for EO like PANGAEA. The pretraining dataset, the\nmodel weights, and our code are open-sourced under a permissive license.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present TerraMind, the first any-to-any generative, multimodal foundation\nmodel for Earth observation (EO). Unlike other multimodal models, TerraMind is\npretrained on dual-scale representations combining both token-level and\npixel-level data across modalities. On a token level, TerraMind encodes\nhigh-level contextual information to learn cross-modal relationships, while on\na pixel level, TerraMind leverages fine-grained representations to capture\ncritical spatial nuances. We pretrained TerraMind on nine geospatial modalities\nof a global, large-scale dataset. In this paper, we demonstrate that (i)\nTerraMind's dual-scale early fusion approach unlocks a range of zero-shot and\nfew-shot applications for Earth observation, (ii) TerraMind introduces\n\"Thinking-in-Modalities\" (TiM) -- the capability of generating additional\nartificial data during finetuning and inference to improve the model output --\nand (iii) TerraMind achieves beyond state-of-the-art performance in\ncommunity-standard benchmarks for EO like PANGAEA. The pretraining dataset, the\nmodel weights, and our code are open-sourced under a permissive license."
                },
                "authors": [
                    {
                        "name": "Johannes Jakubik"
                    },
                    {
                        "name": "Felix Yang"
                    },
                    {
                        "name": "Benedikt Blumenstiel"
                    },
                    {
                        "name": "Erik Scheurer"
                    },
                    {
                        "name": "Rocco Sedona"
                    },
                    {
                        "name": "Stefano Maurogiovanni"
                    },
                    {
                        "name": "Jente Bosmans"
                    },
                    {
                        "name": "Nikolaos Dionelis"
                    },
                    {
                        "name": "Valerio Marsocci"
                    },
                    {
                        "name": "Niklas Kopp"
                    },
                    {
                        "name": "Rahul Ramachandran"
                    },
                    {
                        "name": "Paolo Fraccaro"
                    },
                    {
                        "name": "Thomas Brunschwiler"
                    },
                    {
                        "name": "Gabriele Cavallaro"
                    },
                    {
                        "name": "Juan Bernabe-Moreno"
                    },
                    {
                        "name": "Nicolas Longp"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Longp"
                },
                "author": "Nicolas Longp",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11171v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11171v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10450v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10450v2",
                "updated": "2025-06-11T17:44:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    44,
                    34,
                    2,
                    162,
                    0
                ],
                "published": "2025-02-11T20:08:42Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    20,
                    8,
                    42,
                    1,
                    42,
                    0
                ],
                "title": "Trustworthy AI: Safety, Bias, and Privacy -- A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trustworthy AI: Safety, Bias, and Privacy -- A Survey"
                },
                "summary": "The capabilities of artificial intelligence systems have been advancing to a\ngreat extent, but these systems still struggle with failure modes,\nvulnerabilities, and biases. In this paper, we study the current state of the\nfield, and present promising insights and perspectives regarding concerns that\nchallenge the trustworthiness of AI models. In particular, this paper\ninvestigates the issues regarding three thrusts: safety, privacy, and bias,\nwhich hurt models' trustworthiness. For safety, we discuss safety alignment in\nthe context of large language models, preventing them from generating toxic or\nharmful content. For bias, we focus on spurious biases that can mislead a\nnetwork. Lastly, for privacy, we cover membership inference attacks in deep\nneural networks. The discussions addressed in this paper reflect our own\nexperiments and observations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The capabilities of artificial intelligence systems have been advancing to a\ngreat extent, but these systems still struggle with failure modes,\nvulnerabilities, and biases. In this paper, we study the current state of the\nfield, and present promising insights and perspectives regarding concerns that\nchallenge the trustworthiness of AI models. In particular, this paper\ninvestigates the issues regarding three thrusts: safety, privacy, and bias,\nwhich hurt models' trustworthiness. For safety, we discuss safety alignment in\nthe context of large language models, preventing them from generating toxic or\nharmful content. For bias, we focus on spurious biases that can mislead a\nnetwork. Lastly, for privacy, we cover membership inference attacks in deep\nneural networks. The discussions addressed in this paper reflect our own\nexperiments and observations."
                },
                "authors": [
                    {
                        "name": "Xingli Fang"
                    },
                    {
                        "name": "Jianwei Li"
                    },
                    {
                        "name": "Varun Mulchandani"
                    },
                    {
                        "name": "Jung-Eun Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jung-Eun Kim"
                },
                "author": "Jung-Eun Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10450v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10450v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09965v1",
                "updated": "2025-06-11T17:41:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    41,
                    50,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T17:41:50Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    41,
                    50,
                    2,
                    162,
                    0
                ],
                "title": "Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven\n  Thinking and Visual Drawing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven\n  Thinking and Visual Drawing"
                },
                "summary": "As textual reasoning with large language models (LLMs) has advanced\nsignificantly, there has been growing interest in enhancing the multimodal\nreasoning capabilities of large vision-language models (LVLMs). However,\nexisting methods primarily approach multimodal reasoning in a straightforward,\ntext-centric manner, where both reasoning and answer derivation are conducted\npurely through text, with the only difference being the presence of multimodal\ninput. As a result, these methods often encounter fundamental limitations in\nspatial reasoning tasks that demand precise geometric understanding and\ncontinuous spatial tracking-capabilities that humans achieve through mental\nvisualization and manipulation. To address the limitations, we propose drawing\nto reason in space, a novel paradigm that enables LVLMs to reason through\nelementary drawing operations in the visual space. By equipping models with\nbasic drawing operations, including annotating bounding boxes and drawing\nauxiliary lines, we empower them to express and analyze spatial relationships\nthrough direct visual manipulation, meanwhile avoiding the performance ceiling\nimposed by specialized perception tools in previous tool-integrated reasoning\napproaches. To cultivate this capability, we develop a three-stage training\nframework: cold-start training with synthetic data to establish basic drawing\nabilities, reflective rejection sampling to enhance self-reflection behaviors,\nand reinforcement learning to directly optimize for target rewards. Extensive\nexperiments demonstrate that our model, named VILASR, consistently outperforms\nexisting methods across diverse spatial reasoning benchmarks, involving maze\nnavigation, static spatial reasoning, video-based reasoning, and\nmulti-view-based reasoning tasks, with an average improvement of 18.4%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As textual reasoning with large language models (LLMs) has advanced\nsignificantly, there has been growing interest in enhancing the multimodal\nreasoning capabilities of large vision-language models (LVLMs). However,\nexisting methods primarily approach multimodal reasoning in a straightforward,\ntext-centric manner, where both reasoning and answer derivation are conducted\npurely through text, with the only difference being the presence of multimodal\ninput. As a result, these methods often encounter fundamental limitations in\nspatial reasoning tasks that demand precise geometric understanding and\ncontinuous spatial tracking-capabilities that humans achieve through mental\nvisualization and manipulation. To address the limitations, we propose drawing\nto reason in space, a novel paradigm that enables LVLMs to reason through\nelementary drawing operations in the visual space. By equipping models with\nbasic drawing operations, including annotating bounding boxes and drawing\nauxiliary lines, we empower them to express and analyze spatial relationships\nthrough direct visual manipulation, meanwhile avoiding the performance ceiling\nimposed by specialized perception tools in previous tool-integrated reasoning\napproaches. To cultivate this capability, we develop a three-stage training\nframework: cold-start training with synthetic data to establish basic drawing\nabilities, reflective rejection sampling to enhance self-reflection behaviors,\nand reinforcement learning to directly optimize for target rewards. Extensive\nexperiments demonstrate that our model, named VILASR, consistently outperforms\nexisting methods across diverse spatial reasoning benchmarks, involving maze\nnavigation, static spatial reasoning, video-based reasoning, and\nmulti-view-based reasoning tasks, with an average improvement of 18.4%."
                },
                "authors": [
                    {
                        "name": "Junfei Wu"
                    },
                    {
                        "name": "Jian Guan"
                    },
                    {
                        "name": "Kaituo Feng"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Shu Wu"
                    },
                    {
                        "name": "Liang Wang"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Tieniu Tan"
                    }
                ],
                "author_detail": {
                    "name": "Tieniu Tan"
                },
                "author": "Tieniu Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13909v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13909v4",
                "updated": "2025-06-11T17:41:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    41,
                    16,
                    2,
                    162,
                    0
                ],
                "published": "2025-02-19T17:41:09Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    41,
                    9,
                    2,
                    50,
                    0
                ],
                "title": "Lost in Sequence: Do Large Language Models Understand Sequential\n  Recommendation?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lost in Sequence: Do Large Language Models Understand Sequential\n  Recommendation?"
                },
                "summary": "Large Language Models (LLMs) have recently emerged as promising tools for\nrecommendation thanks to their advanced textual understanding ability and\ncontext-awareness. Despite the current practice of training and evaluating\nLLM-based recommendation (LLM4Rec) models under a sequential recommendation\nscenario, we found that whether these models understand the sequential\ninformation inherent in users' item interaction sequences has been largely\noverlooked. In this paper, we first demonstrate through a series of experiments\nthat existing LLM4Rec models do not fully capture sequential information both\nduring training and inference. Then, we propose a simple yet effective\nLLM-based sequential recommender, called LLM-SRec, a method that enhances the\nintegration of sequential information into LLMs by distilling the user\nrepresentations extracted from a pre-trained CF-SRec model into LLMs. Our\nextensive experiments show that LLM-SRec enhances LLMs' ability to understand\nusers' item interaction sequences, ultimately leading to improved\nrecommendation performance. Furthermore, unlike existing LLM4Rec models that\nrequire fine-tuning of LLMs, LLM-SRec achieves state-of-the-art performance by\ntraining only a few lightweight MLPs, highlighting its practicality in\nreal-world applications. Our code is available at\nhttps://github.com/Sein-Kim/LLM-SRec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently emerged as promising tools for\nrecommendation thanks to their advanced textual understanding ability and\ncontext-awareness. Despite the current practice of training and evaluating\nLLM-based recommendation (LLM4Rec) models under a sequential recommendation\nscenario, we found that whether these models understand the sequential\ninformation inherent in users' item interaction sequences has been largely\noverlooked. In this paper, we first demonstrate through a series of experiments\nthat existing LLM4Rec models do not fully capture sequential information both\nduring training and inference. Then, we propose a simple yet effective\nLLM-based sequential recommender, called LLM-SRec, a method that enhances the\nintegration of sequential information into LLMs by distilling the user\nrepresentations extracted from a pre-trained CF-SRec model into LLMs. Our\nextensive experiments show that LLM-SRec enhances LLMs' ability to understand\nusers' item interaction sequences, ultimately leading to improved\nrecommendation performance. Furthermore, unlike existing LLM4Rec models that\nrequire fine-tuning of LLMs, LLM-SRec achieves state-of-the-art performance by\ntraining only a few lightweight MLPs, highlighting its practicality in\nreal-world applications. Our code is available at\nhttps://github.com/Sein-Kim/LLM-SRec."
                },
                "authors": [
                    {
                        "name": "Sein Kim"
                    },
                    {
                        "name": "Hongseok Kang"
                    },
                    {
                        "name": "Kibum Kim"
                    },
                    {
                        "name": "Jiwan Kim"
                    },
                    {
                        "name": "Donghyun Kim"
                    },
                    {
                        "name": "Minchul Yang"
                    },
                    {
                        "name": "Kwangjin Oh"
                    },
                    {
                        "name": "Julian McAuley"
                    },
                    {
                        "name": "Chanyoung Park"
                    }
                ],
                "author_detail": {
                    "name": "Chanyoung Park"
                },
                "author": "Chanyoung Park",
                "arxiv_comment": "KDD 2025 Research Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13909v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13909v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09958v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09958v1",
                "updated": "2025-06-11T17:31:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    31,
                    38,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T17:31:38Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    31,
                    38,
                    2,
                    162,
                    0
                ],
                "title": "Kvasir-VQA-x1: A Multimodal Dataset for Medical Reasoning and Robust\n  MedVQA in Gastrointestinal Endoscopy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kvasir-VQA-x1: A Multimodal Dataset for Medical Reasoning and Robust\n  MedVQA in Gastrointestinal Endoscopy"
                },
                "summary": "Medical Visual Question Answering (MedVQA) is a promising field for\ndeveloping clinical decision support systems, yet progress is often limited by\nthe available datasets, which can lack clinical complexity and visual\ndiversity. To address these gaps, we introduce Kvasir-VQA-x1, a new,\nlarge-scale dataset for gastrointestinal (GI) endoscopy. Our work significantly\nexpands upon the original Kvasir-VQA by incorporating 159,549 new\nquestion-answer pairs that are designed to test deeper clinical reasoning. We\ndeveloped a systematic method using large language models to generate these\nquestions, which are stratified by complexity to better assess a model's\ninference capabilities. To ensure our dataset prepares models for real-world\nclinical scenarios, we have also introduced a variety of visual augmentations\nthat mimic common imaging artifacts. The dataset is structured to support two\nmain evaluation tracks: one for standard VQA performance and another to test\nmodel robustness against these visual perturbations. By providing a more\nchallenging and clinically relevant benchmark, Kvasir-VQA-x1 aims to accelerate\nthe development of more reliable and effective multimodal AI systems for use in\nclinical settings. The dataset is fully accessible and adheres to FAIR data\nprinciples, making it a valuable resource for the wider research community.\nCode and data: https://github.com/Simula/Kvasir-VQA-x1 and\nhttps://huggingface.co/datasets/SimulaMet/Kvasir-VQA-x1",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical Visual Question Answering (MedVQA) is a promising field for\ndeveloping clinical decision support systems, yet progress is often limited by\nthe available datasets, which can lack clinical complexity and visual\ndiversity. To address these gaps, we introduce Kvasir-VQA-x1, a new,\nlarge-scale dataset for gastrointestinal (GI) endoscopy. Our work significantly\nexpands upon the original Kvasir-VQA by incorporating 159,549 new\nquestion-answer pairs that are designed to test deeper clinical reasoning. We\ndeveloped a systematic method using large language models to generate these\nquestions, which are stratified by complexity to better assess a model's\ninference capabilities. To ensure our dataset prepares models for real-world\nclinical scenarios, we have also introduced a variety of visual augmentations\nthat mimic common imaging artifacts. The dataset is structured to support two\nmain evaluation tracks: one for standard VQA performance and another to test\nmodel robustness against these visual perturbations. By providing a more\nchallenging and clinically relevant benchmark, Kvasir-VQA-x1 aims to accelerate\nthe development of more reliable and effective multimodal AI systems for use in\nclinical settings. The dataset is fully accessible and adheres to FAIR data\nprinciples, making it a valuable resource for the wider research community.\nCode and data: https://github.com/Simula/Kvasir-VQA-x1 and\nhttps://huggingface.co/datasets/SimulaMet/Kvasir-VQA-x1"
                },
                "authors": [
                    {
                        "name": "Sushant Gautam"
                    },
                    {
                        "name": "Michael A. Riegler"
                    },
                    {
                        "name": "Pl Halvorsen"
                    }
                ],
                "author_detail": {
                    "name": "Pl Halvorsen"
                },
                "author": "Pl Halvorsen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09958v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09958v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T45 (Machine learning), 92C55 (Biomedical imaging and signal\n  processing) 68T45 (Machine learning), 92C55 (Biomedical imaging and signal\n  processing)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10; I.2.6; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09956v1",
                "updated": "2025-06-11T17:30:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    30,
                    7,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T17:30:07Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    30,
                    7,
                    2,
                    162,
                    0
                ],
                "title": "LLMail-Inject: A Dataset from a Realistic Adaptive Prompt Injection\n  Challenge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMail-Inject: A Dataset from a Realistic Adaptive Prompt Injection\n  Challenge"
                },
                "summary": "Indirect Prompt Injection attacks exploit the inherent limitation of Large\nLanguage Models (LLMs) to distinguish between instructions and data in their\ninputs. Despite numerous defense proposals, the systematic evaluation against\nadaptive adversaries remains limited, even when successful attacks can have\nwide security and privacy implications, and many real-world LLM-based\napplications remain vulnerable. We present the results of LLMail-Inject, a\npublic challenge simulating a realistic scenario in which participants\nadaptively attempted to inject malicious instructions into emails in order to\ntrigger unauthorized tool calls in an LLM-based email assistant. The challenge\nspanned multiple defense strategies, LLM architectures, and retrieval\nconfigurations, resulting in a dataset of 208,095 unique attack submissions\nfrom 839 participants. We release the challenge code, the full dataset of\nsubmissions, and our analysis demonstrating how this data can provide new\ninsights into the instruction-data separation problem. We hope this will serve\nas a foundation for future research towards practical structural solutions to\nprompt injection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Indirect Prompt Injection attacks exploit the inherent limitation of Large\nLanguage Models (LLMs) to distinguish between instructions and data in their\ninputs. Despite numerous defense proposals, the systematic evaluation against\nadaptive adversaries remains limited, even when successful attacks can have\nwide security and privacy implications, and many real-world LLM-based\napplications remain vulnerable. We present the results of LLMail-Inject, a\npublic challenge simulating a realistic scenario in which participants\nadaptively attempted to inject malicious instructions into emails in order to\ntrigger unauthorized tool calls in an LLM-based email assistant. The challenge\nspanned multiple defense strategies, LLM architectures, and retrieval\nconfigurations, resulting in a dataset of 208,095 unique attack submissions\nfrom 839 participants. We release the challenge code, the full dataset of\nsubmissions, and our analysis demonstrating how this data can provide new\ninsights into the instruction-data separation problem. We hope this will serve\nas a foundation for future research towards practical structural solutions to\nprompt injection."
                },
                "authors": [
                    {
                        "name": "Sahar Abdelnabi"
                    },
                    {
                        "name": "Aideen Fay"
                    },
                    {
                        "name": "Ahmed Salem"
                    },
                    {
                        "name": "Egor Zverev"
                    },
                    {
                        "name": "Kai-Chieh Liao"
                    },
                    {
                        "name": "Chi-Huang Liu"
                    },
                    {
                        "name": "Chun-Chih Kuo"
                    },
                    {
                        "name": "Jannis Weigend"
                    },
                    {
                        "name": "Danyael Manlangit"
                    },
                    {
                        "name": "Alex Apostolov"
                    },
                    {
                        "name": "Haris Umair"
                    },
                    {
                        "name": "Joo Donato"
                    },
                    {
                        "name": "Masayuki Kawakita"
                    },
                    {
                        "name": "Athar Mahboob"
                    },
                    {
                        "name": "Tran Huu Bach"
                    },
                    {
                        "name": "Tsun-Han Chiang"
                    },
                    {
                        "name": "Myeongjin Cho"
                    },
                    {
                        "name": "Hajin Choi"
                    },
                    {
                        "name": "Byeonghyeon Kim"
                    },
                    {
                        "name": "Hyeonjin Lee"
                    },
                    {
                        "name": "Benjamin Pannell"
                    },
                    {
                        "name": "Conor McCauley"
                    },
                    {
                        "name": "Mark Russinovich"
                    },
                    {
                        "name": "Andrew Paverd"
                    },
                    {
                        "name": "Giovanni Cherubin"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Cherubin"
                },
                "author": "Giovanni Cherubin",
                "arxiv_comment": "Dataset at:\n  https://huggingface.co/datasets/microsoft/llmail-inject-challenge",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06144v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06144v4",
                "updated": "2025-06-11T17:23:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    23,
                    47,
                    2,
                    162,
                    0
                ],
                "published": "2024-06-10T10:03:16Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    10,
                    3,
                    16,
                    0,
                    162,
                    0
                ],
                "title": "Language Models Resist Alignment: Evidence From Data Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models Resist Alignment: Evidence From Data Compression"
                },
                "summary": "Large language models (LLMs) may exhibit unintended or undesirable behaviors.\nRecent works have concentrated on aligning LLMs to mitigate harmful outputs.\nDespite these efforts, some anomalies indicate that even a well-conducted\nalignment process can be easily circumvented, whether intentionally or\naccidentally. Does alignment fine-tuning yield have robust effects on models,\nor are its impacts merely superficial? In this work, we make the first\nexploration of this phenomenon from both theoretical and empirical\nperspectives. Empirically, we demonstrate the $\\mathbf{elasticity}$ of\npost-alignment models, i.e., the tendency to revert to the behavior\ndistribution formed during the pre-training phase upon further fine-tuning.\nLeveraging compression theory, we formally deduce that fine-tuning\ndisproportionately undermines alignment relative to pre-training, potentially\nby orders of magnitude. We validate the presence of elasticity through\nexperiments on models of varying types and scales. Specifically, we find that\nmodel performance declines rapidly before reverting to the pre-training\ndistribution, after which the rate of decline drops significantly. Furthermore,\nwe further reveal that elasticity positively correlates with the increased\nmodel size and the expansion of pre-training data. Our findings underscore the\nneed to address the inherent elasticity of LLMs to mitigate their resistance to\nalignment. The model weight and code are available at\npku-lm-resist-alignment.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) may exhibit unintended or undesirable behaviors.\nRecent works have concentrated on aligning LLMs to mitigate harmful outputs.\nDespite these efforts, some anomalies indicate that even a well-conducted\nalignment process can be easily circumvented, whether intentionally or\naccidentally. Does alignment fine-tuning yield have robust effects on models,\nor are its impacts merely superficial? In this work, we make the first\nexploration of this phenomenon from both theoretical and empirical\nperspectives. Empirically, we demonstrate the $\\mathbf{elasticity}$ of\npost-alignment models, i.e., the tendency to revert to the behavior\ndistribution formed during the pre-training phase upon further fine-tuning.\nLeveraging compression theory, we formally deduce that fine-tuning\ndisproportionately undermines alignment relative to pre-training, potentially\nby orders of magnitude. We validate the presence of elasticity through\nexperiments on models of varying types and scales. Specifically, we find that\nmodel performance declines rapidly before reverting to the pre-training\ndistribution, after which the rate of decline drops significantly. Furthermore,\nwe further reveal that elasticity positively correlates with the increased\nmodel size and the expansion of pre-training data. Our findings underscore the\nneed to address the inherent elasticity of LLMs to mitigate their resistance to\nalignment. The model weight and code are available at\npku-lm-resist-alignment.github.io."
                },
                "authors": [
                    {
                        "name": "Jiaming Ji"
                    },
                    {
                        "name": "Kaile Wang"
                    },
                    {
                        "name": "Tianyi Qiu"
                    },
                    {
                        "name": "Boyuan Chen"
                    },
                    {
                        "name": "Jiayi Zhou"
                    },
                    {
                        "name": "Changye Li"
                    },
                    {
                        "name": "Hantao Lou"
                    },
                    {
                        "name": "Juntao Dai"
                    },
                    {
                        "name": "Yunhuai Liu"
                    },
                    {
                        "name": "Yaodong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yaodong Yang"
                },
                "author": "Yaodong Yang",
                "arxiv_comment": "Accepted by ACL2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06144v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06144v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03047v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03047v2",
                "updated": "2025-06-11T17:18:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    18,
                    47,
                    2,
                    162,
                    0
                ],
                "published": "2025-03-04T23:08:51Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    23,
                    8,
                    51,
                    1,
                    63,
                    0
                ],
                "title": "Stochastic block models with many communities and the Kesten--Stigum\n  bound",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic block models with many communities and the Kesten--Stigum\n  bound"
                },
                "summary": "We study the inference of communities in stochastic block models with a\ngrowing number of communities. For block models with $n$ vertices and a fixed\nnumber of communities $q$, it was predicted in Decelle et al. (2011) that there\nare computationally efficient algorithms for recovering the communities above\nthe Kesten--Stigum (KS) bound and that efficient recovery is impossible below\nthe KS bound. This conjecture has since stimulated a lot of interest, with the\nachievability side proven in a line of research that culminated in the work of\nAbbe and Sandon (2018). Conversely, recent work by Sohn and Wein (2025)\nprovides evidence for the hardness part using the low-degree paradigm.\n  In this paper we investigate community recovery in the regime $q=q_n \\to\n\\infty$ as $n\\to\\infty$ where no such predictions exist. We show that efficient\ninference of communities remains possible above the KS bound. Furthermore, we\nshow that recovery of block models is low-degree hard below the KS bound when\nthe number of communities satisfies $q\\ll \\sqrt{n}$. Perhaps surprisingly, we\nfind that when $q \\gg \\sqrt{n}$, there is an efficient algorithm based on\nnon-backtracking walks for recovery even below the KS bound. We identify a new\nthreshold and ask if it is the threshold for efficient recovery in this regime.\nFinally, we show that detection is easy and identify (up to a constant) the\ninformation-theoretic threshold for community recovery as the number of\ncommunities $q$ diverges.\n  Our low-degree hardness results also naturally have consequences for graphon\nestimation, improving results of Luo and Gao (2024).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the inference of communities in stochastic block models with a\ngrowing number of communities. For block models with $n$ vertices and a fixed\nnumber of communities $q$, it was predicted in Decelle et al. (2011) that there\nare computationally efficient algorithms for recovering the communities above\nthe Kesten--Stigum (KS) bound and that efficient recovery is impossible below\nthe KS bound. This conjecture has since stimulated a lot of interest, with the\nachievability side proven in a line of research that culminated in the work of\nAbbe and Sandon (2018). Conversely, recent work by Sohn and Wein (2025)\nprovides evidence for the hardness part using the low-degree paradigm.\n  In this paper we investigate community recovery in the regime $q=q_n \\to\n\\infty$ as $n\\to\\infty$ where no such predictions exist. We show that efficient\ninference of communities remains possible above the KS bound. Furthermore, we\nshow that recovery of block models is low-degree hard below the KS bound when\nthe number of communities satisfies $q\\ll \\sqrt{n}$. Perhaps surprisingly, we\nfind that when $q \\gg \\sqrt{n}$, there is an efficient algorithm based on\nnon-backtracking walks for recovery even below the KS bound. We identify a new\nthreshold and ask if it is the threshold for efficient recovery in this regime.\nFinally, we show that detection is easy and identify (up to a constant) the\ninformation-theoretic threshold for community recovery as the number of\ncommunities $q$ diverges.\n  Our low-degree hardness results also naturally have consequences for graphon\nestimation, improving results of Luo and Gao (2024)."
                },
                "authors": [
                    {
                        "name": "Byron Chin"
                    },
                    {
                        "name": "Elchanan Mossel"
                    },
                    {
                        "name": "Youngtak Sohn"
                    },
                    {
                        "name": "Alexander S. Wein"
                    }
                ],
                "author_detail": {
                    "name": "Alexander S. Wein"
                },
                "author": "Alexander S. Wein",
                "arxiv_comment": "46 pages, 1 figure, added discussion and minor corrections, extended\n  abstract in COLT 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03047v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03047v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.PR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09944v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09944v1",
                "updated": "2025-06-11T17:12:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    12,
                    6,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T17:12:06Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    12,
                    6,
                    2,
                    162,
                    0
                ],
                "title": "Query-Focused Retrieval Heads Improve Long-Context Reasoning and\n  Re-ranking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query-Focused Retrieval Heads Improve Long-Context Reasoning and\n  Re-ranking"
                },
                "summary": "Recent work has identified retrieval heads (Wu et al., 2025b), a subset of\nattention heads responsible for retrieving salient information in long-context\nlanguage models (LMs), as measured by their copy-paste behavior in\nNeedle-in-a-Haystack tasks. In this paper, we introduce QRHEAD (Query-Focused\nRetrieval Head), an improved set of attention heads that enhance retrieval from\nlong context. We identify QRHEAD by aggregating attention scores with respect\nto the input query, using a handful of examples from real-world tasks (e.g.,\nlong-context QA). We further introduce QR- RETRIEVER, an efficient and\neffective retriever that uses the accumulated attention mass of QRHEAD as\nretrieval scores. We use QR- RETRIEVER for long-context reasoning by selecting\nthe most relevant parts with the highest retrieval scores. On multi-hop\nreasoning tasks LongMemEval and CLIPPER, this yields over 10% performance gains\nover full context and outperforms strong dense retrievers. We also evaluate\nQRRETRIEVER as a re-ranker on the BEIR benchmark and find that it achieves\nstrong zero-shot performance, outperforming other LLM-based re-rankers such as\nRankGPT. Further analysis shows that both the querycontext attention scoring\nand task selection are crucial for identifying QRHEAD with strong downstream\nutility. Overall, our work contributes a general-purpose retriever and offers\ninterpretability insights into the long-context capabilities of LMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work has identified retrieval heads (Wu et al., 2025b), a subset of\nattention heads responsible for retrieving salient information in long-context\nlanguage models (LMs), as measured by their copy-paste behavior in\nNeedle-in-a-Haystack tasks. In this paper, we introduce QRHEAD (Query-Focused\nRetrieval Head), an improved set of attention heads that enhance retrieval from\nlong context. We identify QRHEAD by aggregating attention scores with respect\nto the input query, using a handful of examples from real-world tasks (e.g.,\nlong-context QA). We further introduce QR- RETRIEVER, an efficient and\neffective retriever that uses the accumulated attention mass of QRHEAD as\nretrieval scores. We use QR- RETRIEVER for long-context reasoning by selecting\nthe most relevant parts with the highest retrieval scores. On multi-hop\nreasoning tasks LongMemEval and CLIPPER, this yields over 10% performance gains\nover full context and outperforms strong dense retrievers. We also evaluate\nQRRETRIEVER as a re-ranker on the BEIR benchmark and find that it achieves\nstrong zero-shot performance, outperforming other LLM-based re-rankers such as\nRankGPT. Further analysis shows that both the querycontext attention scoring\nand task selection are crucial for identifying QRHEAD with strong downstream\nutility. Overall, our work contributes a general-purpose retriever and offers\ninterpretability insights into the long-context capabilities of LMs."
                },
                "authors": [
                    {
                        "name": "Wuwei Zhang"
                    },
                    {
                        "name": "Fangcong Yin"
                    },
                    {
                        "name": "Howard Yen"
                    },
                    {
                        "name": "Danqi Chen"
                    },
                    {
                        "name": "Xi Ye"
                    }
                ],
                "author_detail": {
                    "name": "Xi Ye"
                },
                "author": "Xi Ye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09944v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09944v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06845v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06845v5",
                "updated": "2025-06-11T17:10:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    10,
                    59,
                    2,
                    162,
                    0
                ],
                "published": "2024-12-08T02:01:46Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    2,
                    1,
                    46,
                    6,
                    343,
                    0
                ],
                "title": "7B Fully Open Source Moxin-LLM/VLM -- From Pretraining to GRPO-based\n  Reinforcement Learning Enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "7B Fully Open Source Moxin-LLM/VLM -- From Pretraining to GRPO-based\n  Reinforcement Learning Enhancement"
                },
                "summary": "Recently, Large Language Models (LLMs) have undergone a significant\ntransformation, marked by a rapid rise in both their popularity and\ncapabilities. Leading this evolution are proprietary LLMs like GPT-4 and\nGPT-o1, which have captured widespread attention in the AI community due to\ntheir remarkable performance and versatility. Simultaneously, open-source LLMs,\nsuch as LLaMA, have made great contributions to the ever-increasing popularity\nof LLMs due to the ease to customize and deploy the models across diverse\napplications. Although open-source LLMs present unprecedented opportunities for\ninnovation and research, the commercialization of LLMs has raised concerns\nabout transparency, reproducibility, and safety. Many open-source LLMs fail to\nmeet fundamental transparency requirements by withholding essential components\nlike training code and data, which may hinder further innovations on LLMs. To\nmitigate this issue, we introduce Moxin 7B, a fully open-source LLM developed,\nadhering to principles of open science, open source, open data, and open\naccess. We release the pre-training code and configurations, training and\nfine-tuning datasets, and intermediate and final checkpoints, aiming to make\ncontinuous commitments to fully open-source LLMs. After pre-training the base\nmodel, we finetune the Moxin Base model with SOTA post-training framework and\ninstruction data to obtain Moxin Instruct model. To improve the reasoning\ncapability, we further finetune our Instruct model with chain-of-thought data\ndistilled from DeepSeek R1, and then use Group Relative Policy Optimization\n(GRPO) following DeepSeek R1 to finetune our model, leading to the Moxin\nReasoning model. Moreover, we develop our vision language model based on our\nMoxin model. Experiments show that our models achieve superior performance in\nvarious evaluations such as zero-shot evaluation, few-shot evaluation, and CoT\nevaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Large Language Models (LLMs) have undergone a significant\ntransformation, marked by a rapid rise in both their popularity and\ncapabilities. Leading this evolution are proprietary LLMs like GPT-4 and\nGPT-o1, which have captured widespread attention in the AI community due to\ntheir remarkable performance and versatility. Simultaneously, open-source LLMs,\nsuch as LLaMA, have made great contributions to the ever-increasing popularity\nof LLMs due to the ease to customize and deploy the models across diverse\napplications. Although open-source LLMs present unprecedented opportunities for\ninnovation and research, the commercialization of LLMs has raised concerns\nabout transparency, reproducibility, and safety. Many open-source LLMs fail to\nmeet fundamental transparency requirements by withholding essential components\nlike training code and data, which may hinder further innovations on LLMs. To\nmitigate this issue, we introduce Moxin 7B, a fully open-source LLM developed,\nadhering to principles of open science, open source, open data, and open\naccess. We release the pre-training code and configurations, training and\nfine-tuning datasets, and intermediate and final checkpoints, aiming to make\ncontinuous commitments to fully open-source LLMs. After pre-training the base\nmodel, we finetune the Moxin Base model with SOTA post-training framework and\ninstruction data to obtain Moxin Instruct model. To improve the reasoning\ncapability, we further finetune our Instruct model with chain-of-thought data\ndistilled from DeepSeek R1, and then use Group Relative Policy Optimization\n(GRPO) following DeepSeek R1 to finetune our model, leading to the Moxin\nReasoning model. Moreover, we develop our vision language model based on our\nMoxin model. Experiments show that our models achieve superior performance in\nvarious evaluations such as zero-shot evaluation, few-shot evaluation, and CoT\nevaluation."
                },
                "authors": [
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Xuan Shen"
                    },
                    {
                        "name": "Zhenglun Kong"
                    },
                    {
                        "name": "Yixin Shen"
                    },
                    {
                        "name": "Sung-En Chang"
                    },
                    {
                        "name": "Timothy Rupprecht"
                    },
                    {
                        "name": "Lei Lu"
                    },
                    {
                        "name": "Enfu Nan"
                    },
                    {
                        "name": "Changdi Yang"
                    },
                    {
                        "name": "Yumei He"
                    },
                    {
                        "name": "Weiyan Shi"
                    },
                    {
                        "name": "Xingchen Xu"
                    },
                    {
                        "name": "Yu Huang"
                    },
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Yue Chen"
                    },
                    {
                        "name": "Yong He"
                    },
                    {
                        "name": "Yanzhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yanzhi Wang"
                },
                "author": "Yanzhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06845v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06845v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09942v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09942v1",
                "updated": "2025-06-11T17:10:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    10,
                    36,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T17:10:36Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    10,
                    36,
                    2,
                    162,
                    0
                ],
                "title": "VerIF: Verification Engineering for Reinforcement Learning in\n  Instruction Following",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VerIF: Verification Engineering for Reinforcement Learning in\n  Instruction Following"
                },
                "summary": "Reinforcement learning with verifiable rewards (RLVR) has become a key\ntechnique for enhancing large language models (LLMs), with verification\nengineering playing a central role. However, best practices for RL in\ninstruction following remain underexplored. In this work, we explore the\nverification challenge in RL for instruction following and propose VerIF, a\nverification method that combines rule-based code verification with LLM-based\nverification from a large reasoning model (e.g., QwQ-32B). To support this\napproach, we construct a high-quality instruction-following dataset,\nVerInstruct, containing approximately 22,000 instances with associated\nverification signals. We apply RL training with VerIF to two models, achieving\nsignificant improvements across several representative instruction-following\nbenchmarks. The trained models reach state-of-the-art performance among models\nof comparable size and generalize well to unseen constraints. We further\nobserve that their general capabilities remain unaffected, suggesting that RL\nwith VerIF can be integrated into existing RL recipes to enhance overall model\nperformance. We have released our datasets, codes, and models to facilitate\nfuture research at https://github.com/THU-KEG/VerIF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning with verifiable rewards (RLVR) has become a key\ntechnique for enhancing large language models (LLMs), with verification\nengineering playing a central role. However, best practices for RL in\ninstruction following remain underexplored. In this work, we explore the\nverification challenge in RL for instruction following and propose VerIF, a\nverification method that combines rule-based code verification with LLM-based\nverification from a large reasoning model (e.g., QwQ-32B). To support this\napproach, we construct a high-quality instruction-following dataset,\nVerInstruct, containing approximately 22,000 instances with associated\nverification signals. We apply RL training with VerIF to two models, achieving\nsignificant improvements across several representative instruction-following\nbenchmarks. The trained models reach state-of-the-art performance among models\nof comparable size and generalize well to unseen constraints. We further\nobserve that their general capabilities remain unaffected, suggesting that RL\nwith VerIF can be integrated into existing RL recipes to enhance overall model\nperformance. We have released our datasets, codes, and models to facilitate\nfuture research at https://github.com/THU-KEG/VerIF."
                },
                "authors": [
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Yunjia Qi"
                    },
                    {
                        "name": "Xiaozhi Wang"
                    },
                    {
                        "name": "Bin Xu"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "arxiv_comment": "16 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09942v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09942v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14259v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14259v4",
                "updated": "2025-06-11T17:09:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    9,
                    58,
                    2,
                    162,
                    0
                ],
                "published": "2024-05-23T07:39:42Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    7,
                    39,
                    42,
                    3,
                    144,
                    0
                ],
                "title": "Let's Fuse Step by Step: A Generative Fusion Decoding Algorithm with\n  LLMs for Robust and Instruction-Aware ASR and OCR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let's Fuse Step by Step: A Generative Fusion Decoding Algorithm with\n  LLMs for Robust and Instruction-Aware ASR and OCR"
                },
                "summary": "We propose \"Generative Fusion Decoding\" (GFD), a novel shallow fusion\nframework designed to integrate large language models (LLMs) into cross-modal\ntext recognition systems for automatic speech recognition (ASR) and optical\ncharacter recognition (OCR). We derive the necessary formulations to enable GFD\nto operate across mismatched token spaces of different models by calculating\nlikelihood at the byte level, thereby enabling seamless fusion and synchronous\nprogression during the decoding process. GFD is plug-and-play by design, making\nit readily compatible with various auto-regressive models without the need for\nany re-training. GFD proves effective for general ASR and OCR tasks through\nintermediate and frequent interactions with LLMs, surpassing cascaded methods\nin English and Mandarin benchmarks. In addition, GFD transfers in-context\nlearning abilities of LLMs and allows for adaptive ASR in instruction-aware and\nlong-context settings, yielding significant WER reductions of up to 17.7\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose \"Generative Fusion Decoding\" (GFD), a novel shallow fusion\nframework designed to integrate large language models (LLMs) into cross-modal\ntext recognition systems for automatic speech recognition (ASR) and optical\ncharacter recognition (OCR). We derive the necessary formulations to enable GFD\nto operate across mismatched token spaces of different models by calculating\nlikelihood at the byte level, thereby enabling seamless fusion and synchronous\nprogression during the decoding process. GFD is plug-and-play by design, making\nit readily compatible with various auto-regressive models without the need for\nany re-training. GFD proves effective for general ASR and OCR tasks through\nintermediate and frequent interactions with LLMs, surpassing cascaded methods\nin English and Mandarin benchmarks. In addition, GFD transfers in-context\nlearning abilities of LLMs and allows for adaptive ASR in instruction-aware and\nlong-context settings, yielding significant WER reductions of up to 17.7\\%."
                },
                "authors": [
                    {
                        "name": "Chan-Jan Hsu"
                    },
                    {
                        "name": "Yi-Chang Chen"
                    },
                    {
                        "name": "Feng-Ting Liao"
                    },
                    {
                        "name": "Pei-Chen Ho"
                    },
                    {
                        "name": "Yu-Hsiang Wang"
                    },
                    {
                        "name": "Po-Chun Hsu"
                    },
                    {
                        "name": "Da-shan Shiu"
                    }
                ],
                "author_detail": {
                    "name": "Da-shan Shiu"
                },
                "author": "Da-shan Shiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14259v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14259v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04430v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04430v2",
                "updated": "2025-06-11T17:05:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    5,
                    40,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-04T20:27:17Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    20,
                    27,
                    17,
                    2,
                    155,
                    0
                ],
                "title": "Leveraging Coordinate Momentum in SignSGD and Muon: Memory-Optimized\n  Zero-Order",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Coordinate Momentum in SignSGD and Muon: Memory-Optimized\n  Zero-Order"
                },
                "summary": "Fine-tuning Large Language Models (LLMs) is essential for adapting\npre-trained models to downstream tasks. Yet traditional first-order optimizers\nsuch as Stochastic Gradient Descent (SGD) and Adam incur prohibitive memory and\ncomputational costs that scale poorly with model size. In this paper, we\ninvestigate zero-order (ZO) optimization methods as a memory- and\ncompute-efficient alternative, particularly in the context of\nparameter-efficient fine-tuning techniques like LoRA. We propose\n$\\texttt{JAGUAR SignSGD}$, a ZO momentum-based algorithm that extends ZO\nSignSGD, requiring the same number of parameters as the standard ZO SGD and\nonly $\\mathcal{O}(1)$ function evaluations per iteration. To the best of our\nknowledge, this is the first study to establish rigorous convergence guarantees\nfor SignSGD in the stochastic ZO case. We further propose $\\texttt{JAGUAR\nMuon}$, a novel ZO extension of the Muon optimizer that leverages the matrix\nstructure of model parameters, and we provide its convergence rate under\narbitrary stochastic noise. Through extensive experiments on challenging LLM\nfine-tuning benchmarks, we demonstrate that the proposed algorithms meet or\nexceed the convergence quality of standard first-order methods, achieving\nsignificant memory reduction. Our theoretical and empirical results establish\nnew ZO optimization methods as a practical and theoretically grounded approach\nfor resource-constrained LLM adaptation. Our code is available at\nhttps://github.com/brain-mmo-lab/ZO_LLM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning Large Language Models (LLMs) is essential for adapting\npre-trained models to downstream tasks. Yet traditional first-order optimizers\nsuch as Stochastic Gradient Descent (SGD) and Adam incur prohibitive memory and\ncomputational costs that scale poorly with model size. In this paper, we\ninvestigate zero-order (ZO) optimization methods as a memory- and\ncompute-efficient alternative, particularly in the context of\nparameter-efficient fine-tuning techniques like LoRA. We propose\n$\\texttt{JAGUAR SignSGD}$, a ZO momentum-based algorithm that extends ZO\nSignSGD, requiring the same number of parameters as the standard ZO SGD and\nonly $\\mathcal{O}(1)$ function evaluations per iteration. To the best of our\nknowledge, this is the first study to establish rigorous convergence guarantees\nfor SignSGD in the stochastic ZO case. We further propose $\\texttt{JAGUAR\nMuon}$, a novel ZO extension of the Muon optimizer that leverages the matrix\nstructure of model parameters, and we provide its convergence rate under\narbitrary stochastic noise. Through extensive experiments on challenging LLM\nfine-tuning benchmarks, we demonstrate that the proposed algorithms meet or\nexceed the convergence quality of standard first-order methods, achieving\nsignificant memory reduction. Our theoretical and empirical results establish\nnew ZO optimization methods as a practical and theoretically grounded approach\nfor resource-constrained LLM adaptation. Our code is available at\nhttps://github.com/brain-mmo-lab/ZO_LLM"
                },
                "authors": [
                    {
                        "name": "Egor Petrov"
                    },
                    {
                        "name": "Grigoriy Evseev"
                    },
                    {
                        "name": "Aleksey Antonov"
                    },
                    {
                        "name": "Andrey Veprikov"
                    },
                    {
                        "name": "Pavel Plyusnin"
                    },
                    {
                        "name": "Nikolay Bushkov"
                    },
                    {
                        "name": "Stanislav Moiseev"
                    },
                    {
                        "name": "Aleksandr Beznosikov"
                    }
                ],
                "author_detail": {
                    "name": "Aleksandr Beznosikov"
                },
                "author": "Aleksandr Beznosikov",
                "arxiv_comment": "26 pages, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04430v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04430v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08174v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08174v2",
                "updated": "2025-06-11T17:04:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    4,
                    39,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-09T19:39:09Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    19,
                    39,
                    9,
                    0,
                    160,
                    0
                ],
                "title": "LLM-BT-Terms: Back-Translation as a Framework for Terminology\n  Standardization and Dynamic Semantic Embedding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-BT-Terms: Back-Translation as a Framework for Terminology\n  Standardization and Dynamic Semantic Embedding"
                },
                "summary": "The rapid expansion of English technical terminology presents a significant\nchallenge to traditional expert-based standardization, particularly in rapidly\ndeveloping areas such as artificial intelligence and quantum computing. Manual\napproaches face difficulties in maintaining consistent multilingual\nterminology. To address this, we introduce LLM-BT, a back-translation framework\npowered by large language models (LLMs) designed to automate terminology\nverification and standardization through cross-lingual semantic alignment. Our\nkey contributions include: (1) term-level consistency validation: by performing\nEnglish -> intermediate language -> English back-translation, LLM-BT achieves\nhigh term consistency across different models (such as GPT-4, DeepSeek, and\nGrok). Case studies demonstrate over 90 percent of terms are preserved either\nexactly or semantically; (2) multi-path verification workflow: we develop a\nnovel pipeline described as Retrieve -> Generate -> Verify -> Optimize, which\nsupports both serial paths (e.g., English -> Simplified Chinese -> Traditional\nChinese -> English) and parallel paths (e.g., English -> Chinese / Portuguese\n-> English). BLEU scores and term-level accuracy indicate strong cross-lingual\nrobustness, with BLEU scores exceeding 0.45 and Portuguese term accuracy\nreaching 100 percent; (3) back-translation as semantic embedding: we\nreinterpret back-translation as a form of dynamic semantic embedding that\nuncovers latent trajectories of meaning. In contrast to static embeddings,\nLLM-BT offers transparent, path-based embeddings shaped by the evolution of the\nmodels. This reframing positions back-translation as an active mechanism for\nmultilingual terminology standardization, fostering collaboration between\nmachines and humans - machines preserve semantic integrity, while humans\nprovide cultural interpretation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid expansion of English technical terminology presents a significant\nchallenge to traditional expert-based standardization, particularly in rapidly\ndeveloping areas such as artificial intelligence and quantum computing. Manual\napproaches face difficulties in maintaining consistent multilingual\nterminology. To address this, we introduce LLM-BT, a back-translation framework\npowered by large language models (LLMs) designed to automate terminology\nverification and standardization through cross-lingual semantic alignment. Our\nkey contributions include: (1) term-level consistency validation: by performing\nEnglish -> intermediate language -> English back-translation, LLM-BT achieves\nhigh term consistency across different models (such as GPT-4, DeepSeek, and\nGrok). Case studies demonstrate over 90 percent of terms are preserved either\nexactly or semantically; (2) multi-path verification workflow: we develop a\nnovel pipeline described as Retrieve -> Generate -> Verify -> Optimize, which\nsupports both serial paths (e.g., English -> Simplified Chinese -> Traditional\nChinese -> English) and parallel paths (e.g., English -> Chinese / Portuguese\n-> English). BLEU scores and term-level accuracy indicate strong cross-lingual\nrobustness, with BLEU scores exceeding 0.45 and Portuguese term accuracy\nreaching 100 percent; (3) back-translation as semantic embedding: we\nreinterpret back-translation as a form of dynamic semantic embedding that\nuncovers latent trajectories of meaning. In contrast to static embeddings,\nLLM-BT offers transparent, path-based embeddings shaped by the evolution of the\nmodels. This reframing positions back-translation as an active mechanism for\nmultilingual terminology standardization, fostering collaboration between\nmachines and humans - machines preserve semantic integrity, while humans\nprovide cultural interpretation."
                },
                "authors": [
                    {
                        "name": "Li Weigang"
                    },
                    {
                        "name": "Pedro Carvalho Brom"
                    }
                ],
                "author_detail": {
                    "name": "Pedro Carvalho Brom"
                },
                "author": "Pedro Carvalho Brom",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08174v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08174v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17761v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17761v3",
                "updated": "2025-06-11T16:56:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    16,
                    56,
                    58,
                    2,
                    162,
                    0
                ],
                "published": "2024-06-25T17:45:26Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    17,
                    45,
                    26,
                    1,
                    177,
                    0
                ],
                "title": "CaLMQA: Exploring culturally specific long-form question answering\n  across 23 languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaLMQA: Exploring culturally specific long-form question answering\n  across 23 languages"
                },
                "summary": "Despite rising global usage of large language models (LLMs), their ability to\ngenerate long-form answers to culturally specific questions remains unexplored\nin many languages. To fill this gap, we perform the first study of textual\nmultilingual long-form QA by creating CaLMQA, a dataset of 51.7K culturally\nspecific questions across 23 different languages. We define culturally specific\nquestions as those that refer to concepts unique to one or a few cultures, or\nhave different answers depending on the cultural or regional context. We obtain\nthese questions by crawling naturally-occurring questions from community web\nforums in high-resource languages, and by hiring native speakers to write\nquestions in under-resourced, rarely-studied languages such as Fijian and\nKirundi. Our data collection methodologies are translation-free, enabling the\ncollection of culturally unique questions like \"Kuber iki umwami wa mbere\nw'uburundi yitwa Ntare?\" (Kirundi; English translation: \"Why was the first king\nof Burundi called Ntare (Lion)?\"). We evaluate factuality, relevance and\nsurface-level quality of LLM-generated long-form answers, finding that (1) for\nmany languages, even the best models make critical surface-level errors (e.g.,\nanswering in the wrong language, repetition), especially for low-resource\nlanguages; and (2) answers to culturally specific questions contain more\nfactual errors than answers to culturally agnostic questions -- questions that\nhave consistent meaning and answer across many cultures. We release CaLMQA to\nfacilitate future research in cultural and multilingual long-form QA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite rising global usage of large language models (LLMs), their ability to\ngenerate long-form answers to culturally specific questions remains unexplored\nin many languages. To fill this gap, we perform the first study of textual\nmultilingual long-form QA by creating CaLMQA, a dataset of 51.7K culturally\nspecific questions across 23 different languages. We define culturally specific\nquestions as those that refer to concepts unique to one or a few cultures, or\nhave different answers depending on the cultural or regional context. We obtain\nthese questions by crawling naturally-occurring questions from community web\nforums in high-resource languages, and by hiring native speakers to write\nquestions in under-resourced, rarely-studied languages such as Fijian and\nKirundi. Our data collection methodologies are translation-free, enabling the\ncollection of culturally unique questions like \"Kuber iki umwami wa mbere\nw'uburundi yitwa Ntare?\" (Kirundi; English translation: \"Why was the first king\nof Burundi called Ntare (Lion)?\"). We evaluate factuality, relevance and\nsurface-level quality of LLM-generated long-form answers, finding that (1) for\nmany languages, even the best models make critical surface-level errors (e.g.,\nanswering in the wrong language, repetition), especially for low-resource\nlanguages; and (2) answers to culturally specific questions contain more\nfactual errors than answers to culturally agnostic questions -- questions that\nhave consistent meaning and answer across many cultures. We release CaLMQA to\nfacilitate future research in cultural and multilingual long-form QA."
                },
                "authors": [
                    {
                        "name": "Shane Arora"
                    },
                    {
                        "name": "Marzena Karpinska"
                    },
                    {
                        "name": "Hung-Ting Chen"
                    },
                    {
                        "name": "Ipsita Bhattacharjee"
                    },
                    {
                        "name": "Mohit Iyyer"
                    },
                    {
                        "name": "Eunsol Choi"
                    }
                ],
                "author_detail": {
                    "name": "Eunsol Choi"
                },
                "author": "Eunsol Choi",
                "arxiv_comment": "46 pages, 26 figures. Accepted as a main conference paper at ACL\n  2025. Code and data available at https://github.com/2015aroras/CaLMQA .\n  Dataset expanded to 51.7K questions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17761v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17761v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.08726v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.08726v2",
                "updated": "2025-06-11T16:54:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    16,
                    54,
                    54,
                    2,
                    162,
                    0
                ],
                "published": "2024-06-13T01:08:40Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    1,
                    8,
                    40,
                    3,
                    165,
                    0
                ],
                "title": "Standard Language Ideology in AI-Generated Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Standard Language Ideology in AI-Generated Language"
                },
                "summary": "Standard language ideology is reflected and reinforced in language generated\nby large language models (LLMs). We present a faceted taxonomy of open problems\nthat illustrate how standard language ideology manifests in AI-generated\nlanguage, alongside implications for minoritized language communities and\nsociety more broadly. We introduce the concept of standard AI-generated\nlanguage ideology, a process through which LLMs position \"standard\"\nlanguages--particularly Standard American English (SAE)--as the linguistic\ndefault, reinforcing the perception that SAE is the most \"appropriate\"\nlanguage. We then discuss ongoing tensions around what constitutes desirable\nsystem behavior, as well as advantages and drawbacks of generative AI tools\nattempting, or refusing, to imitate different English language varieties.\nRather than prescribing narrow technical fixes, we offer three recommendations\nfor researchers, practitioners, and funders that focus on shifting structural\nconditions and supporting more emancipatory outcomes for diverse language\ncommunities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Standard language ideology is reflected and reinforced in language generated\nby large language models (LLMs). We present a faceted taxonomy of open problems\nthat illustrate how standard language ideology manifests in AI-generated\nlanguage, alongside implications for minoritized language communities and\nsociety more broadly. We introduce the concept of standard AI-generated\nlanguage ideology, a process through which LLMs position \"standard\"\nlanguages--particularly Standard American English (SAE)--as the linguistic\ndefault, reinforcing the perception that SAE is the most \"appropriate\"\nlanguage. We then discuss ongoing tensions around what constitutes desirable\nsystem behavior, as well as advantages and drawbacks of generative AI tools\nattempting, or refusing, to imitate different English language varieties.\nRather than prescribing narrow technical fixes, we offer three recommendations\nfor researchers, practitioners, and funders that focus on shifting structural\nconditions and supporting more emancipatory outcomes for diverse language\ncommunities."
                },
                "authors": [
                    {
                        "name": "Genevieve Smith"
                    },
                    {
                        "name": "Eve Fleisig"
                    },
                    {
                        "name": "Madeline Bossi"
                    },
                    {
                        "name": "Ishita Rustagi"
                    },
                    {
                        "name": "Xavier Yin"
                    }
                ],
                "author_detail": {
                    "name": "Xavier Yin"
                },
                "author": "Xavier Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.08726v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.08726v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05668v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05668v2",
                "updated": "2025-06-11T16:51:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    16,
                    51,
                    44,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-06T01:32:14Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    1,
                    32,
                    14,
                    4,
                    157,
                    0
                ],
                "title": "RNE: a plug-and-play framework for diffusion density estimation and\n  inference-time control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RNE: a plug-and-play framework for diffusion density estimation and\n  inference-time control"
                },
                "summary": "In this paper, we introduce the Radon-Nikodym Estimator (RNE), a flexible,\nplug-and-play framework for diffusion inference-time density estimation and\ncontrol, based on the concept of the density ratio between path distributions.\nRNE connects and unifies a variety of existing density estimation and\ninference-time control methods under a single and intuitive perspective,\nstemming from basic variational inference and probabilistic principles\ntherefore offering both theoretical clarity and practical versatility.\nExperiments demonstrate that RNE delivers strong results in diffusion density\nestimation, and offers broad applicability to inference-time control tasks --\nsuch as annealing, diffusion model composition, and reward-tilting -- with\npromising inference-time scaling performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce the Radon-Nikodym Estimator (RNE), a flexible,\nplug-and-play framework for diffusion inference-time density estimation and\ncontrol, based on the concept of the density ratio between path distributions.\nRNE connects and unifies a variety of existing density estimation and\ninference-time control methods under a single and intuitive perspective,\nstemming from basic variational inference and probabilistic principles\ntherefore offering both theoretical clarity and practical versatility.\nExperiments demonstrate that RNE delivers strong results in diffusion density\nestimation, and offers broad applicability to inference-time control tasks --\nsuch as annealing, diffusion model composition, and reward-tilting -- with\npromising inference-time scaling performance."
                },
                "authors": [
                    {
                        "name": "Jiajun He"
                    },
                    {
                        "name": "Jos Miguel Hernndez-Lobato"
                    },
                    {
                        "name": "Yuanqi Du"
                    },
                    {
                        "name": "Francisco Vargas"
                    }
                ],
                "author_detail": {
                    "name": "Francisco Vargas"
                },
                "author": "Francisco Vargas",
                "arxiv_comment": "39 pages; 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05668v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05668v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09928v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09928v1",
                "updated": "2025-06-11T16:51:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    16,
                    51,
                    7,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T16:51:07Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    16,
                    51,
                    7,
                    2,
                    162,
                    0
                ],
                "title": "Bayesian Probabilistic Matrix Factorization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Probabilistic Matrix Factorization"
                },
                "summary": "Matrix factorization is a widely used technique in recommendation systems.\nProbabilistic Matrix Factorization (PMF) [1] extends traditional matrix\nfactorization by incorporating probability distributions over latent factors,\nallowing for uncertainty quantification. However, computing the posterior\ndistribution is intractable due to the high-dimensional integral. To address\nthis, we employ two Bayesian inference methods: Markov Chain Monte Carlo (MCMC)\n[2] and Variational Inference (VI) [3] to approximate the posterior. We\nevaluate their performance on MovieLens dataset and compare their convergence\nspeed, predictive accuracy, and computational efficiency. Experimental results\ndemonstrate that VI offers faster convergence, while MCMC provides more\naccurate posterior estimates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrix factorization is a widely used technique in recommendation systems.\nProbabilistic Matrix Factorization (PMF) [1] extends traditional matrix\nfactorization by incorporating probability distributions over latent factors,\nallowing for uncertainty quantification. However, computing the posterior\ndistribution is intractable due to the high-dimensional integral. To address\nthis, we employ two Bayesian inference methods: Markov Chain Monte Carlo (MCMC)\n[2] and Variational Inference (VI) [3] to approximate the posterior. We\nevaluate their performance on MovieLens dataset and compare their convergence\nspeed, predictive accuracy, and computational efficiency. Experimental results\ndemonstrate that VI offers faster convergence, while MCMC provides more\naccurate posterior estimates."
                },
                "authors": [
                    {
                        "name": "Ruixuan Xu"
                    },
                    {
                        "name": "Xiangxiang Weng"
                    }
                ],
                "author_detail": {
                    "name": "Xiangxiang Weng"
                },
                "author": "Xiangxiang Weng",
                "arxiv_comment": "11 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09928v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09928v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06366v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06366v3",
                "updated": "2025-06-12T10:22:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    10,
                    22,
                    1,
                    3,
                    163,
                    0
                ],
                "published": "2025-06-04T08:12:32Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    8,
                    12,
                    32,
                    2,
                    155,
                    0
                ],
                "title": "AI Agent Behavioral Science",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Agent Behavioral Science"
                },
                "summary": "Recent advances in large language models (LLMs) have enabled the development\nof AI agents that exhibit increasingly human-like behaviors, including\nplanning, adaptation, and social dynamics across diverse, interactive, and\nopen-ended scenarios. These behaviors are not solely the product of the\ninternal architectures of the underlying models, but emerge from their\nintegration into agentic systems operating within specific contexts, where\nenvironmental factors, social cues, and interaction feedbacks shape behavior\nover time. This evolution necessitates a new scientific perspective: AI Agent\nBehavioral Science. Rather than focusing only on internal mechanisms, this\nperspective emphasizes the systematic observation of behavior, design of\ninterventions to test hypotheses, and theory-guided interpretation of how AI\nagents act, adapt, and interact over time. We systematize a growing body of\nresearch across individual agent, multi-agent, and human-agent interaction\nsettings, and further demonstrate how this perspective informs responsible AI\nby treating fairness, safety, interpretability, accountability, and privacy as\nbehavioral properties. By unifying recent findings and laying out future\ndirections, we position AI Agent Behavioral Science as a necessary complement\nto traditional model-centric approaches, providing essential tools for\nunderstanding, evaluating, and governing the real-world behavior of\nincreasingly autonomous AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have enabled the development\nof AI agents that exhibit increasingly human-like behaviors, including\nplanning, adaptation, and social dynamics across diverse, interactive, and\nopen-ended scenarios. These behaviors are not solely the product of the\ninternal architectures of the underlying models, but emerge from their\nintegration into agentic systems operating within specific contexts, where\nenvironmental factors, social cues, and interaction feedbacks shape behavior\nover time. This evolution necessitates a new scientific perspective: AI Agent\nBehavioral Science. Rather than focusing only on internal mechanisms, this\nperspective emphasizes the systematic observation of behavior, design of\ninterventions to test hypotheses, and theory-guided interpretation of how AI\nagents act, adapt, and interact over time. We systematize a growing body of\nresearch across individual agent, multi-agent, and human-agent interaction\nsettings, and further demonstrate how this perspective informs responsible AI\nby treating fairness, safety, interpretability, accountability, and privacy as\nbehavioral properties. By unifying recent findings and laying out future\ndirections, we position AI Agent Behavioral Science as a necessary complement\nto traditional model-centric approaches, providing essential tools for\nunderstanding, evaluating, and governing the real-world behavior of\nincreasingly autonomous AI systems."
                },
                "authors": [
                    {
                        "name": "Lin Chen"
                    },
                    {
                        "name": "Yunke Zhang"
                    },
                    {
                        "name": "Jie Feng"
                    },
                    {
                        "name": "Haoye Chai"
                    },
                    {
                        "name": "Honglin Zhang"
                    },
                    {
                        "name": "Bingbing Fan"
                    },
                    {
                        "name": "Yibo Ma"
                    },
                    {
                        "name": "Shiyuan Zhang"
                    },
                    {
                        "name": "Nian Li"
                    },
                    {
                        "name": "Tianhui Liu"
                    },
                    {
                        "name": "Nicholas Sukiennik"
                    },
                    {
                        "name": "Keyu Zhao"
                    },
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Ziyi Liu"
                    },
                    {
                        "name": "Fengli Xu"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06366v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06366v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09925v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09925v1",
                "updated": "2025-06-11T16:49:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    16,
                    49,
                    15,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T16:49:15Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    16,
                    49,
                    15,
                    2,
                    162,
                    0
                ],
                "title": "Emergent anisotropic three-phase order in critically doped\n  superconducting diamond films",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergent anisotropic three-phase order in critically doped\n  superconducting diamond films"
                },
                "summary": "Two decades since its discovery, superconducting heavily boron-doped diamond\n(HBDD) still presents unresolved fundamental questions whose resolution is\nrelevant to the development of this material for quantum technologies. We use\nelectrical magnetotransport measurements of critically-doped homoepitaxial\nsingle crystal HBDD films to reveal signatures of intrinsic (electronic)\ngranular superconductivity. By studying the dependence of electrical\nresistivity on temperature and magnetic field vector, we infer that this\ngranularity arises from electron correlations. This is revealed by a striking\nthree-phase anisotropy in the magnetoresistance, accompanied by a spontaneous\ntransverse voltage (Hall anomaly). Our findings indicate an emergent\nmagnetically tunable intrinsic order in an otherwise isotropic three\ndimensional single crystal HBDD film, offering new insights into the mechanism\nof superconductivity in this quantum material.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two decades since its discovery, superconducting heavily boron-doped diamond\n(HBDD) still presents unresolved fundamental questions whose resolution is\nrelevant to the development of this material for quantum technologies. We use\nelectrical magnetotransport measurements of critically-doped homoepitaxial\nsingle crystal HBDD films to reveal signatures of intrinsic (electronic)\ngranular superconductivity. By studying the dependence of electrical\nresistivity on temperature and magnetic field vector, we infer that this\ngranularity arises from electron correlations. This is revealed by a striking\nthree-phase anisotropy in the magnetoresistance, accompanied by a spontaneous\ntransverse voltage (Hall anomaly). Our findings indicate an emergent\nmagnetically tunable intrinsic order in an otherwise isotropic three\ndimensional single crystal HBDD film, offering new insights into the mechanism\nof superconductivity in this quantum material."
                },
                "authors": [
                    {
                        "name": "Jyotirmay Dwivedi"
                    },
                    {
                        "name": "Jake Morris"
                    },
                    {
                        "name": "Saurav Islam"
                    },
                    {
                        "name": "Kalana D. Halanayake"
                    },
                    {
                        "name": "Gabriel A. Vazquez-Lizardi"
                    },
                    {
                        "name": "David Snyder"
                    },
                    {
                        "name": "Anthony Richardella"
                    },
                    {
                        "name": "Luke Lyle"
                    },
                    {
                        "name": "Danielle Reifsnyder Hickey"
                    },
                    {
                        "name": "Nazar Delegan"
                    },
                    {
                        "name": "F. Joseph Heremans"
                    },
                    {
                        "name": "David D. Awschalom"
                    },
                    {
                        "name": "Nitin Samarth"
                    }
                ],
                "author_detail": {
                    "name": "Nitin Samarth"
                },
                "author": "Nitin Samarth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09925v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09925v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09923v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09923v1",
                "updated": "2025-06-11T16:43:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    16,
                    43,
                    36,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T16:43:36Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    16,
                    43,
                    36,
                    2,
                    162,
                    0
                ],
                "title": "Apollo: A Posteriori Label-Only Membership Inference Attack Towards\n  Machine Unlearning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apollo: A Posteriori Label-Only Membership Inference Attack Towards\n  Machine Unlearning"
                },
                "summary": "Machine Unlearning (MU) aims to update Machine Learning (ML) models following\nrequests to remove training samples and their influences on a trained model\nefficiently without retraining the original ML model from scratch. While MU\nitself has been employed to provide privacy protection and regulatory\ncompliance, it can also increase the attack surface of the model. Existing\nprivacy inference attacks towards MU that aim to infer properties of the\nunlearned set rely on the weaker threat model that assumes the attacker has\naccess to both the unlearned model and the original model, limiting their\nfeasibility toward real-life scenarios. We propose a novel privacy attack, A\nPosteriori Label-Only Membership Inference Attack towards MU, Apollo, that\ninfers whether a data sample has been unlearned, following a strict threat\nmodel where an adversary has access to the label-output of the unlearned model\nonly. We demonstrate that our proposed attack, while requiring less access to\nthe target model compared to previous attacks, can achieve relatively high\nprecision on the membership status of the unlearned samples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine Unlearning (MU) aims to update Machine Learning (ML) models following\nrequests to remove training samples and their influences on a trained model\nefficiently without retraining the original ML model from scratch. While MU\nitself has been employed to provide privacy protection and regulatory\ncompliance, it can also increase the attack surface of the model. Existing\nprivacy inference attacks towards MU that aim to infer properties of the\nunlearned set rely on the weaker threat model that assumes the attacker has\naccess to both the unlearned model and the original model, limiting their\nfeasibility toward real-life scenarios. We propose a novel privacy attack, A\nPosteriori Label-Only Membership Inference Attack towards MU, Apollo, that\ninfers whether a data sample has been unlearned, following a strict threat\nmodel where an adversary has access to the label-output of the unlearned model\nonly. We demonstrate that our proposed attack, while requiring less access to\nthe target model compared to previous attacks, can achieve relatively high\nprecision on the membership status of the unlearned samples."
                },
                "authors": [
                    {
                        "name": "Liou Tang"
                    },
                    {
                        "name": "James Joshi"
                    },
                    {
                        "name": "Ashish Kundu"
                    }
                ],
                "author_detail": {
                    "name": "Ashish Kundu"
                },
                "author": "Ashish Kundu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09923v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24461v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24461v2",
                "updated": "2025-06-11T16:40:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    16,
                    40,
                    39,
                    2,
                    162,
                    0
                ],
                "published": "2025-05-30T10:57:09Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    10,
                    57,
                    9,
                    4,
                    150,
                    0
                ],
                "title": "Logits-Based Finetuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logits-Based Finetuning"
                },
                "summary": "In recent years, developing compact and efficient large language models\n(LLMs) has emerged as a thriving area of research. Traditional Supervised\nFine-Tuning (SFT), which relies on singular ground truth labels, often fails to\ncapture token-level dependencies and linguistic diversity. To address these\nlimitations, we propose a logits-based fine-tuning framework that integrates\nthe strengths of supervised learning and knowledge distillation. Our approach\nconstructs enriched training targets by combining teacher logits with ground\ntruth labels, preserving both correctness and linguistic diversity. This\nensures more reliable and effective training. We constructed a large-scale 1.2M\nlogits dataset and trained a series of science-focused models. Experimental\nresults demonstrate that our method achieves significant improvements, with\naccuracy gains of 18% on Mawps and 22.7% on TabMWP. Across nine widely used\nmathematical benchmarks, our method consistently outperforms prior SFT models,\nachieving an average improvement of 7.28%. Codes are available at\nhttps://github.com/dvlab-research/Logits-Based-Finetuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, developing compact and efficient large language models\n(LLMs) has emerged as a thriving area of research. Traditional Supervised\nFine-Tuning (SFT), which relies on singular ground truth labels, often fails to\ncapture token-level dependencies and linguistic diversity. To address these\nlimitations, we propose a logits-based fine-tuning framework that integrates\nthe strengths of supervised learning and knowledge distillation. Our approach\nconstructs enriched training targets by combining teacher logits with ground\ntruth labels, preserving both correctness and linguistic diversity. This\nensures more reliable and effective training. We constructed a large-scale 1.2M\nlogits dataset and trained a series of science-focused models. Experimental\nresults demonstrate that our method achieves significant improvements, with\naccuracy gains of 18% on Mawps and 22.7% on TabMWP. Across nine widely used\nmathematical benchmarks, our method consistently outperforms prior SFT models,\nachieving an average improvement of 7.28%. Codes are available at\nhttps://github.com/dvlab-research/Logits-Based-Finetuning."
                },
                "authors": [
                    {
                        "name": "Jingyao Li"
                    },
                    {
                        "name": "Senqiao Yang"
                    },
                    {
                        "name": "Sitong Wu"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Chuanyang Zheng"
                    },
                    {
                        "name": "Hong Xu"
                    },
                    {
                        "name": "Jiaya Jia"
                    }
                ],
                "author_detail": {
                    "name": "Jiaya Jia"
                },
                "author": "Jiaya Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24461v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24461v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09916v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09916v1",
                "updated": "2025-06-11T16:33:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    16,
                    33,
                    9,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T16:33:09Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    16,
                    33,
                    9,
                    2,
                    162,
                    0
                ],
                "title": "Only-Style: Stylistic Consistency in Image Generation without Content\n  Leakage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Only-Style: Stylistic Consistency in Image Generation without Content\n  Leakage"
                },
                "summary": "Generating images in a consistent reference visual style remains a\nchallenging computer vision task. State-of-the-art methods aiming for\nstyle-consistent generation struggle to effectively separate semantic content\nfrom stylistic elements, leading to content leakage from the image provided as\na reference to the targets. To address this challenge, we propose Only-Style: a\nmethod designed to mitigate content leakage in a semantically coherent manner\nwhile preserving stylistic consistency. Only-Style works by localizing content\nleakage during inference, allowing the adaptive tuning of a parameter that\ncontrols the style alignment process, specifically within the image patches\ncontaining the subject in the reference image. This adaptive process best\nbalances stylistic consistency with leakage elimination. Moreover, the\nlocalization of content leakage can function as a standalone component, given a\nreference-target image pair, allowing the adaptive tuning of any\nmethod-specific parameter that provides control over the impact of the\nstylistic reference. In addition, we propose a novel evaluation framework to\nquantify the success of style-consistent generations in avoiding undesired\ncontent leakage. Our approach demonstrates a significant improvement over\nstate-of-the-art methods through extensive evaluation across diverse instances,\nconsistently achieving robust stylistic consistency without undesired content\nleakage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating images in a consistent reference visual style remains a\nchallenging computer vision task. State-of-the-art methods aiming for\nstyle-consistent generation struggle to effectively separate semantic content\nfrom stylistic elements, leading to content leakage from the image provided as\na reference to the targets. To address this challenge, we propose Only-Style: a\nmethod designed to mitigate content leakage in a semantically coherent manner\nwhile preserving stylistic consistency. Only-Style works by localizing content\nleakage during inference, allowing the adaptive tuning of a parameter that\ncontrols the style alignment process, specifically within the image patches\ncontaining the subject in the reference image. This adaptive process best\nbalances stylistic consistency with leakage elimination. Moreover, the\nlocalization of content leakage can function as a standalone component, given a\nreference-target image pair, allowing the adaptive tuning of any\nmethod-specific parameter that provides control over the impact of the\nstylistic reference. In addition, we propose a novel evaluation framework to\nquantify the success of style-consistent generations in avoiding undesired\ncontent leakage. Our approach demonstrates a significant improvement over\nstate-of-the-art methods through extensive evaluation across diverse instances,\nconsistently achieving robust stylistic consistency without undesired content\nleakage."
                },
                "authors": [
                    {
                        "name": "Tilemachos Aravanis"
                    },
                    {
                        "name": "Panagiotis Filntisis"
                    },
                    {
                        "name": "Petros Maragos"
                    },
                    {
                        "name": "George Retsinas"
                    }
                ],
                "author_detail": {
                    "name": "George Retsinas"
                },
                "arxiv_affiliation": "HERON - Center of Excellence in Robotics, Athens, Greece",
                "author": "George Retsinas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09916v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09916v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09902v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09902v1",
                "updated": "2025-06-11T16:16:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    16,
                    16,
                    7,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T16:16:07Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    16,
                    16,
                    7,
                    2,
                    162,
                    0
                ],
                "title": "PersonaLens: A Benchmark for Personalization Evaluation in\n  Conversational AI Assistants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PersonaLens: A Benchmark for Personalization Evaluation in\n  Conversational AI Assistants"
                },
                "summary": "Large language models (LLMs) have advanced conversational AI assistants.\nHowever, systematically evaluating how well these assistants apply\npersonalization--adapting to individual user preferences while completing\ntasks--remains challenging. Existing personalization benchmarks focus on\nchit-chat, non-conversational tasks, or narrow domains, failing to capture the\ncomplexities of personalized task-oriented assistance. To address this, we\nintroduce PersonaLens, a comprehensive benchmark for evaluating personalization\nin task-oriented AI assistants. Our benchmark features diverse user profiles\nequipped with rich preferences and interaction histories, along with two\nspecialized LLM-based agents: a user agent that engages in realistic\ntask-oriented dialogues with AI assistants, and a judge agent that employs the\nLLM-as-a-Judge paradigm to assess personalization, response quality, and task\nsuccess. Through extensive experiments with current LLM assistants across\ndiverse tasks, we reveal significant variability in their personalization\ncapabilities, providing crucial insights for advancing conversational AI\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have advanced conversational AI assistants.\nHowever, systematically evaluating how well these assistants apply\npersonalization--adapting to individual user preferences while completing\ntasks--remains challenging. Existing personalization benchmarks focus on\nchit-chat, non-conversational tasks, or narrow domains, failing to capture the\ncomplexities of personalized task-oriented assistance. To address this, we\nintroduce PersonaLens, a comprehensive benchmark for evaluating personalization\nin task-oriented AI assistants. Our benchmark features diverse user profiles\nequipped with rich preferences and interaction histories, along with two\nspecialized LLM-based agents: a user agent that engages in realistic\ntask-oriented dialogues with AI assistants, and a judge agent that employs the\nLLM-as-a-Judge paradigm to assess personalization, response quality, and task\nsuccess. Through extensive experiments with current LLM assistants across\ndiverse tasks, we reveal significant variability in their personalization\ncapabilities, providing crucial insights for advancing conversational AI\nsystems."
                },
                "authors": [
                    {
                        "name": "Zheng Zhao"
                    },
                    {
                        "name": "Clara Vania"
                    },
                    {
                        "name": "Subhradeep Kayal"
                    },
                    {
                        "name": "Naila Khan"
                    },
                    {
                        "name": "Shay B. Cohen"
                    },
                    {
                        "name": "Emine Yilmaz"
                    }
                ],
                "author_detail": {
                    "name": "Emine Yilmaz"
                },
                "author": "Emine Yilmaz",
                "arxiv_comment": "Accepted to ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09902v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.19384v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19384v2",
                "updated": "2025-06-11T16:12:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    16,
                    12,
                    50,
                    2,
                    162,
                    0
                ],
                "published": "2024-06-27T17:57:03Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    17,
                    57,
                    3,
                    3,
                    179,
                    0
                ],
                "title": "The Remarkable Robustness of LLMs: Stages of Inference?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Remarkable Robustness of LLMs: Stages of Inference?"
                },
                "summary": "We investigate the robustness of Large Language Models (LLMs) to structural\ninterventions by deleting and swapping adjacent layers during inference.\nSurprisingly, models retain 72-95% of their original top-1 prediction accuracy\nwithout any fine-tuning. We find that performance degradation is not uniform\nacross layers: interventions to the early and final layers cause the most\ndegradation, while the model is remarkably robust to dropping middle layers.\nThis pattern of localized sensitivity motivates our hypothesis of four stages\nof inference, observed across diverse model families and sizes: (1)\ndetokenization, where local context is integrated to lift raw token embeddings\ninto higher-level representations; (2) feature engineering, where task- and\nentity-specific features are iteratively refined; (3) prediction ensembling,\nwhere hidden states are aggregated into plausible next-token predictions; and\n(4) residual sharpening, where irrelevant features are suppressed to finalize\nthe output distribution. Synthesizing behavioral and mechanistic evidence, we\nprovide a framework for interpreting depth-dependent computations in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the robustness of Large Language Models (LLMs) to structural\ninterventions by deleting and swapping adjacent layers during inference.\nSurprisingly, models retain 72-95% of their original top-1 prediction accuracy\nwithout any fine-tuning. We find that performance degradation is not uniform\nacross layers: interventions to the early and final layers cause the most\ndegradation, while the model is remarkably robust to dropping middle layers.\nThis pattern of localized sensitivity motivates our hypothesis of four stages\nof inference, observed across diverse model families and sizes: (1)\ndetokenization, where local context is integrated to lift raw token embeddings\ninto higher-level representations; (2) feature engineering, where task- and\nentity-specific features are iteratively refined; (3) prediction ensembling,\nwhere hidden states are aggregated into plausible next-token predictions; and\n(4) residual sharpening, where irrelevant features are suppressed to finalize\nthe output distribution. Synthesizing behavioral and mechanistic evidence, we\nprovide a framework for interpreting depth-dependent computations in LLMs."
                },
                "authors": [
                    {
                        "name": "Vedang Lad"
                    },
                    {
                        "name": "Wes Gurnee"
                    },
                    {
                        "name": "Max Tegmark"
                    }
                ],
                "author_detail": {
                    "name": "Max Tegmark"
                },
                "author": "Max Tegmark",
                "arxiv_comment": "For Github code see\n  https://github.com/vdlad/Remarkable-Robustness-of-LLMs. Send all\n  correspondence to the first author",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.19384v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19384v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05387v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05387v2",
                "updated": "2025-06-11T16:08:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    16,
                    8,
                    29,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-03T14:25:23Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    25,
                    23,
                    1,
                    154,
                    0
                ],
                "title": "Advancing Decoding Strategies: Enhancements in Locally Typical Sampling\n  for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Decoding Strategies: Enhancements in Locally Typical Sampling\n  for LLMs"
                },
                "summary": "This chapter explores advancements in decoding strategies for large language\nmodels (LLMs), focusing on enhancing the Locally Typical Sampling (LTS)\nalgorithm. Traditional decoding methods, such as top-k and nucleus sampling,\noften struggle to balance fluency, diversity, and coherence in text generation.\nTo address these challenges, Adaptive Semantic-Aware Typicality Sampling (ASTS)\nis proposed as an improved version of LTS, incorporating dynamic entropy\nthresholding, multi-objective scoring, and reward-penalty adjustments. ASTS\nensures contextually coherent and diverse text generation while maintaining\ncomputational efficiency. Its performance is evaluated across multiple\nbenchmarks, including story generation and abstractive summarization, using\nmetrics such as perplexity, MAUVE, and diversity scores. Experimental results\ndemonstrate that ASTS outperforms existing sampling techniques by reducing\nrepetition, enhancing semantic alignment, and improving fluency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This chapter explores advancements in decoding strategies for large language\nmodels (LLMs), focusing on enhancing the Locally Typical Sampling (LTS)\nalgorithm. Traditional decoding methods, such as top-k and nucleus sampling,\noften struggle to balance fluency, diversity, and coherence in text generation.\nTo address these challenges, Adaptive Semantic-Aware Typicality Sampling (ASTS)\nis proposed as an improved version of LTS, incorporating dynamic entropy\nthresholding, multi-objective scoring, and reward-penalty adjustments. ASTS\nensures contextually coherent and diverse text generation while maintaining\ncomputational efficiency. Its performance is evaluated across multiple\nbenchmarks, including story generation and abstractive summarization, using\nmetrics such as perplexity, MAUVE, and diversity scores. Experimental results\ndemonstrate that ASTS outperforms existing sampling techniques by reducing\nrepetition, enhancing semantic alignment, and improving fluency."
                },
                "authors": [
                    {
                        "name": "Jaydip Sen"
                    },
                    {
                        "name": "Saptarshi Sengupta"
                    },
                    {
                        "name": "Subhasis Dasgupta"
                    }
                ],
                "author_detail": {
                    "name": "Subhasis Dasgupta"
                },
                "author": "Subhasis Dasgupta",
                "arxiv_comment": "This is the accepted but pre-reviewed version of the chapter that has\n  been accepted for publication in the Springer volume 'Decision-Making in\n  Computational Intelligence-Based Systems,' edited by Witold Pedrycz, Gilberto\n  Rivera, Rose Ma Rodriguez, and Salvador Ibarra Martinez. The chapter is 39\n  pages long, and it contains 2 figures and 6 tables. This is NOT the final\n  camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05387v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05387v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00367v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00367v2",
                "updated": "2025-06-11T16:01:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    16,
                    1,
                    30,
                    2,
                    162,
                    0
                ],
                "published": "2025-05-31T03:22:22Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    3,
                    22,
                    22,
                    5,
                    151,
                    0
                ],
                "title": "The ringdown of a black hole surrounded by a thin shell of matter",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ringdown of a black hole surrounded by a thin shell of matter"
                },
                "summary": "Recent studies have shown that far-field perturbations to the curvature\npotential of a black hole spacetime may destabilize its quasinormal mode (QNM)\nspectrum while only mildly affecting time-domain ringdown signals. In this\nwork, we study the QNM spectrum and ringdown behavior of a Schwarzschild black\nhole with a far-field perturbation to its physical environment -- a thin matter\nshell with finite surface tension. After accounting for the dynamics of the\ninteraction between GWs and the shell, we find that the fundamental mode can\nmigrate perturbatively or be destabilized by the appearance of new modes with\nno analogue in the vacuum case, much like studies of ``bumps\" in the curvature\npotential. However, unlike these previous works, we find that the coupling\nbetween metric perturbations and oscillations of the shell also sources\nweakly-damped QNMs which are exclusive to the polar sector. We then study\nwhether the analysis tools of least-squares QNM fits and the full and rational\nringdown filters can clearly identify the signatures of the shell in\nrepresentative ringdown waveforms. We conclude that ringdown at sufficiently\nearly times is insensitive to the shell; weakly-damped QNMs (in the polar\nsector) and echoes, which may enable the analysis methods considered here to\ninfer the presence of a shell, only appear at late times and are generally\nweak.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have shown that far-field perturbations to the curvature\npotential of a black hole spacetime may destabilize its quasinormal mode (QNM)\nspectrum while only mildly affecting time-domain ringdown signals. In this\nwork, we study the QNM spectrum and ringdown behavior of a Schwarzschild black\nhole with a far-field perturbation to its physical environment -- a thin matter\nshell with finite surface tension. After accounting for the dynamics of the\ninteraction between GWs and the shell, we find that the fundamental mode can\nmigrate perturbatively or be destabilized by the appearance of new modes with\nno analogue in the vacuum case, much like studies of ``bumps\" in the curvature\npotential. However, unlike these previous works, we find that the coupling\nbetween metric perturbations and oscillations of the shell also sources\nweakly-damped QNMs which are exclusive to the polar sector. We then study\nwhether the analysis tools of least-squares QNM fits and the full and rational\nringdown filters can clearly identify the signatures of the shell in\nrepresentative ringdown waveforms. We conclude that ringdown at sufficiently\nearly times is insensitive to the shell; weakly-damped QNMs (in the polar\nsector) and echoes, which may enable the analysis methods considered here to\ninfer the presence of a shell, only appear at late times and are generally\nweak."
                },
                "authors": [
                    {
                        "name": "Andrew Laeuger"
                    },
                    {
                        "name": "Colin Weller"
                    },
                    {
                        "name": "Dongjun Li"
                    },
                    {
                        "name": "Yanbei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yanbei Chen"
                },
                "author": "Yanbei Chen",
                "arxiv_comment": "26 pages, 12 figures. Submitted to Physical Review D",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00367v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00367v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09890v1",
                "updated": "2025-06-11T16:00:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    16,
                    0,
                    54,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T16:00:54Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    16,
                    0,
                    54,
                    2,
                    162,
                    0
                ],
                "title": "The Emergence of Abstract Thought in Large Language Models Beyond Any\n  Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Emergence of Abstract Thought in Large Language Models Beyond Any\n  Language"
                },
                "summary": "As large language models (LLMs) continue to advance, their capacity to\nfunction effectively across a diverse range of languages has shown marked\nimprovement. Preliminary studies observe that the hidden activations of LLMs\noften resemble English, even when responding to non-English prompts. This has\nled to the widespread assumption that LLMs may \"think\" in English. However,\nmore recent results showing strong multilingual performance, even surpassing\nEnglish performance on specific tasks in other languages, challenge this view.\nIn this work, we find that LLMs progressively develop a core language-agnostic\nparameter space-a remarkably small subset of parameters whose deactivation\nresults in significant performance degradation across all languages. This\ncompact yet critical set of parameters underlies the model's ability to\ngeneralize beyond individual languages, supporting the emergence of abstract\nthought that is not tied to any specific linguistic system. Specifically, we\nidentify language-related neurons-those are consistently activated during the\nprocessing of particular languages, and categorize them as either shared\n(active across multiple languages) or exclusive (specific to one). As LLMs\nundergo continued development over time, we observe a marked increase in both\nthe proportion and functional importance of shared neurons, while exclusive\nneurons progressively diminish in influence. These shared neurons constitute\nthe backbone of the core language-agnostic parameter space, supporting the\nemergence of abstract thought. Motivated by these insights, we propose\nneuron-specific training strategies tailored to LLMs' language-agnostic levels\nat different development stages. Experiments across diverse LLM families\nsupport our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to advance, their capacity to\nfunction effectively across a diverse range of languages has shown marked\nimprovement. Preliminary studies observe that the hidden activations of LLMs\noften resemble English, even when responding to non-English prompts. This has\nled to the widespread assumption that LLMs may \"think\" in English. However,\nmore recent results showing strong multilingual performance, even surpassing\nEnglish performance on specific tasks in other languages, challenge this view.\nIn this work, we find that LLMs progressively develop a core language-agnostic\nparameter space-a remarkably small subset of parameters whose deactivation\nresults in significant performance degradation across all languages. This\ncompact yet critical set of parameters underlies the model's ability to\ngeneralize beyond individual languages, supporting the emergence of abstract\nthought that is not tied to any specific linguistic system. Specifically, we\nidentify language-related neurons-those are consistently activated during the\nprocessing of particular languages, and categorize them as either shared\n(active across multiple languages) or exclusive (specific to one). As LLMs\nundergo continued development over time, we observe a marked increase in both\nthe proportion and functional importance of shared neurons, while exclusive\nneurons progressively diminish in influence. These shared neurons constitute\nthe backbone of the core language-agnostic parameter space, supporting the\nemergence of abstract thought. Motivated by these insights, we propose\nneuron-specific training strategies tailored to LLMs' language-agnostic levels\nat different development stages. Experiments across diverse LLM families\nsupport our approach."
                },
                "authors": [
                    {
                        "name": "Yuxin Chen"
                    },
                    {
                        "name": "Yiran Zhao"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "An Zhang"
                    },
                    {
                        "name": "Kenji Kawaguchi"
                    },
                    {
                        "name": "Shafiq Joty"
                    },
                    {
                        "name": "Junnan Li"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    },
                    {
                        "name": "Michael Qizhe Shieh"
                    },
                    {
                        "name": "Wenxuan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wenxuan Zhang"
                },
                "author": "Wenxuan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09886v1",
                "updated": "2025-06-11T15:59:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    15,
                    59,
                    15,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T15:59:15Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    15,
                    59,
                    15,
                    2,
                    162,
                    0
                ],
                "title": "Attention Head Embeddings with Trainable Deep Kernels for Hallucination\n  Detection in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Head Embeddings with Trainable Deep Kernels for Hallucination\n  Detection in LLMs"
                },
                "summary": "We present a novel approach for detecting hallucinations in large language\nmodels (LLMs) by analyzing the probabilistic divergence between prompt and\nresponse hidden-state distributions. Counterintuitively, we find that\nhallucinated responses exhibit smaller deviations from their prompts compared\nto grounded responses, suggesting that hallucinations often arise from\nsuperficial rephrasing rather than substantive reasoning. Leveraging this\ninsight, we propose a model-intrinsic detection method that uses distributional\ndistances as principled hallucination scores, eliminating the need for external\nknowledge or auxiliary models. To enhance sensitivity, we employ deep learnable\nkernels that automatically adapt to capture nuanced geometric differences\nbetween distributions. Our approach outperforms existing baselines,\ndemonstrating state-of-the-art performance on several benchmarks. The method\nremains competitive even without kernel training, offering a robust, scalable\nsolution for hallucination detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel approach for detecting hallucinations in large language\nmodels (LLMs) by analyzing the probabilistic divergence between prompt and\nresponse hidden-state distributions. Counterintuitively, we find that\nhallucinated responses exhibit smaller deviations from their prompts compared\nto grounded responses, suggesting that hallucinations often arise from\nsuperficial rephrasing rather than substantive reasoning. Leveraging this\ninsight, we propose a model-intrinsic detection method that uses distributional\ndistances as principled hallucination scores, eliminating the need for external\nknowledge or auxiliary models. To enhance sensitivity, we employ deep learnable\nkernels that automatically adapt to capture nuanced geometric differences\nbetween distributions. Our approach outperforms existing baselines,\ndemonstrating state-of-the-art performance on several benchmarks. The method\nremains competitive even without kernel training, offering a robust, scalable\nsolution for hallucination detection."
                },
                "authors": [
                    {
                        "name": "Rodion Oblovatny"
                    },
                    {
                        "name": "Alexandra Bazarova"
                    },
                    {
                        "name": "Alexey Zaytsev"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Zaytsev"
                },
                "author": "Alexey Zaytsev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08403v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08403v2",
                "updated": "2025-06-11T15:57:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    15,
                    57,
                    34,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-10T03:22:30Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    3,
                    22,
                    30,
                    1,
                    161,
                    0
                ],
                "title": "TACTIC: Translation Agents with Cognitive-Theoretic Interactive\n  Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TACTIC: Translation Agents with Cognitive-Theoretic Interactive\n  Collaboration"
                },
                "summary": "Machine translation has long been a central task in natural language\nprocessing. With the rapid advancement of large language models (LLMs), there\nhas been remarkable progress in translation quality. However, fully realizing\nthe translation potential of LLMs remains an open challenge. Recent studies\nhave explored multi-agent systems to decompose complex translation tasks into\ncollaborative subtasks, showing initial promise in enhancing translation\nquality through agent cooperation and specialization. Nevertheless, existing\nmulti-agent translation frameworks largely neglect foundational insights from\ncognitive translation studies. These insights emphasize how human translators\nemploy different cognitive strategies, such as balancing literal and free\ntranslation, refining expressions based on context, and iteratively evaluating\noutputs. To address this limitation, we propose a cognitively informed\nmulti-agent framework called TACTIC, which stands for T ranslation A gents with\nCognitive- T heoretic Interactive Collaboration. The framework comprises six\nfunctionally distinct agents that mirror key cognitive processes observed in\nhuman translation behavior. These include agents for drafting, refinement,\nevaluation, scoring, context reasoning, and external knowledge gathering. By\nsimulating an interactive and theory-grounded translation workflow, TACTIC\neffectively leverages the full capacity of LLMs for high-quality translation.\nExperimental results on diverse language pairs from the FLORES-200 and WMT24\nbenchmarks show that our method consistently achieves state-of-the-art\nperformance. Using DeepSeek-V3 as the base model, TACTIC surpasses GPT-4.1 by\nan average of +0.6 XCOMET and +1.18 COMETKIWI-23. Compared to DeepSeek-R1, it\nfurther improves by +0.84 XCOMET and +2.99 COMETKIWI-23. Code is available at\nhttps://github.com/weiyali126/TACTIC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine translation has long been a central task in natural language\nprocessing. With the rapid advancement of large language models (LLMs), there\nhas been remarkable progress in translation quality. However, fully realizing\nthe translation potential of LLMs remains an open challenge. Recent studies\nhave explored multi-agent systems to decompose complex translation tasks into\ncollaborative subtasks, showing initial promise in enhancing translation\nquality through agent cooperation and specialization. Nevertheless, existing\nmulti-agent translation frameworks largely neglect foundational insights from\ncognitive translation studies. These insights emphasize how human translators\nemploy different cognitive strategies, such as balancing literal and free\ntranslation, refining expressions based on context, and iteratively evaluating\noutputs. To address this limitation, we propose a cognitively informed\nmulti-agent framework called TACTIC, which stands for T ranslation A gents with\nCognitive- T heoretic Interactive Collaboration. The framework comprises six\nfunctionally distinct agents that mirror key cognitive processes observed in\nhuman translation behavior. These include agents for drafting, refinement,\nevaluation, scoring, context reasoning, and external knowledge gathering. By\nsimulating an interactive and theory-grounded translation workflow, TACTIC\neffectively leverages the full capacity of LLMs for high-quality translation.\nExperimental results on diverse language pairs from the FLORES-200 and WMT24\nbenchmarks show that our method consistently achieves state-of-the-art\nperformance. Using DeepSeek-V3 as the base model, TACTIC surpasses GPT-4.1 by\nan average of +0.6 XCOMET and +1.18 COMETKIWI-23. Compared to DeepSeek-R1, it\nfurther improves by +0.84 XCOMET and +2.99 COMETKIWI-23. Code is available at\nhttps://github.com/weiyali126/TACTIC."
                },
                "authors": [
                    {
                        "name": "Weiya Li"
                    },
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Bei Li"
                    },
                    {
                        "name": "Boyang Liu"
                    },
                    {
                        "name": "Zichen Wen"
                    },
                    {
                        "name": "Nuanqiao Shan"
                    },
                    {
                        "name": "Xiaoqian Liu"
                    },
                    {
                        "name": "Anping Liu"
                    },
                    {
                        "name": "Huajie Liu"
                    },
                    {
                        "name": "Hu Song"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "20 pages, 4 figures, Under review. Code:\n  https://github.com/weiyali126/TACTIC",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08403v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08403v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09885v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09885v1",
                "updated": "2025-06-11T15:57:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    15,
                    57,
                    8,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T15:57:08Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    15,
                    57,
                    8,
                    2,
                    162,
                    0
                ],
                "title": "The Less You Depend, The More You Learn: Synthesizing Novel Views from\n  Sparse, Unposed Images without Any 3D Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Less You Depend, The More You Learn: Synthesizing Novel Views from\n  Sparse, Unposed Images without Any 3D Knowledge"
                },
                "summary": "We consider the problem of generalizable novel view synthesis (NVS), which\naims to generate photorealistic novel views from sparse or even unposed 2D\nimages without per-scene optimization. This task remains fundamentally\nchallenging, as it requires inferring 3D structure from incomplete and\nambiguous 2D observations. Early approaches typically rely on strong 3D\nknowledge, including architectural 3D inductive biases (e.g., embedding\nexplicit 3D representations, such as NeRF or 3DGS, into network design) and\nground-truth camera poses for both input and target views. While recent efforts\nhave sought to reduce the 3D inductive bias or the dependence on known camera\nposes of input views, critical questions regarding the role of 3D knowledge and\nthe necessity of circumventing its use remain under-explored. In this work, we\nconduct a systematic analysis on the 3D knowledge and uncover a critical trend:\nthe performance of methods that requires less 3D knowledge accelerates more as\ndata scales, eventually achieving performance on par with their 3D\nknowledge-driven counterparts, which highlights the increasing importance of\nreducing dependence on 3D knowledge in the era of large-scale data. Motivated\nby and following this trend, we propose a novel NVS framework that minimizes 3D\ninductive bias and pose dependence for both input and target views. By\neliminating this 3D knowledge, our method fully leverages data scaling and\nlearns implicit 3D awareness directly from sparse 2D images, without any 3D\ninductive bias or pose annotation during training. Extensive experiments\ndemonstrate that our model generates photorealistic and 3D-consistent novel\nviews, achieving even comparable performance with methods that rely on posed\ninputs, thereby validating the feasibility and effectiveness of our\ndata-centric paradigm. Project page:\nhttps://pku-vcl-geometry.github.io/Less3Depend/ .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the problem of generalizable novel view synthesis (NVS), which\naims to generate photorealistic novel views from sparse or even unposed 2D\nimages without per-scene optimization. This task remains fundamentally\nchallenging, as it requires inferring 3D structure from incomplete and\nambiguous 2D observations. Early approaches typically rely on strong 3D\nknowledge, including architectural 3D inductive biases (e.g., embedding\nexplicit 3D representations, such as NeRF or 3DGS, into network design) and\nground-truth camera poses for both input and target views. While recent efforts\nhave sought to reduce the 3D inductive bias or the dependence on known camera\nposes of input views, critical questions regarding the role of 3D knowledge and\nthe necessity of circumventing its use remain under-explored. In this work, we\nconduct a systematic analysis on the 3D knowledge and uncover a critical trend:\nthe performance of methods that requires less 3D knowledge accelerates more as\ndata scales, eventually achieving performance on par with their 3D\nknowledge-driven counterparts, which highlights the increasing importance of\nreducing dependence on 3D knowledge in the era of large-scale data. Motivated\nby and following this trend, we propose a novel NVS framework that minimizes 3D\ninductive bias and pose dependence for both input and target views. By\neliminating this 3D knowledge, our method fully leverages data scaling and\nlearns implicit 3D awareness directly from sparse 2D images, without any 3D\ninductive bias or pose annotation during training. Extensive experiments\ndemonstrate that our model generates photorealistic and 3D-consistent novel\nviews, achieving even comparable performance with methods that rely on posed\ninputs, thereby validating the feasibility and effectiveness of our\ndata-centric paradigm. Project page:\nhttps://pku-vcl-geometry.github.io/Less3Depend/ ."
                },
                "authors": [
                    {
                        "name": "Haoru Wang"
                    },
                    {
                        "name": "Kai Ye"
                    },
                    {
                        "name": "Yangyan Li"
                    },
                    {
                        "name": "Wenzheng Chen"
                    },
                    {
                        "name": "Baoquan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Baoquan Chen"
                },
                "author": "Baoquan Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09885v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09885v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2203.11873v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2203.11873v3",
                "updated": "2025-06-11T15:46:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    15,
                    46,
                    47,
                    2,
                    162,
                    0
                ],
                "published": "2022-03-22T16:51:50Z",
                "published_parsed": [
                    2022,
                    3,
                    22,
                    16,
                    51,
                    50,
                    1,
                    81,
                    0
                ],
                "title": "Nonstationary Spatial Process Models with Spatially Varying Covariance\n  Kernels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonstationary Spatial Process Models with Spatially Varying Covariance\n  Kernels"
                },
                "summary": "Building spatial process models that capture nonstationary behavior while\ndelivering computationally efficient inference is challenging. Nonstationary\nspatially varying kernels (see, e.g., Paciorek, 2003) offer flexibility and\nrichness, but computation is impeded by high-dimensional parameter spaces\nresulting from spatially varying process parameters. Matters are exacerbated if\nthe number of locations recording measurements is massive. With limited\ntheoretical tractability, obviating computational bottlenecks requires synergy\nbetween model construction and algorithm development. We build a class of\nscalable nonstationary spatial process models using spatially varying\ncovariance kernels. We implement a Bayesian modeling framework using Hybrid\nMonte Carlo with nested interweaving. We conduct experiments on synthetic data\nsets to explore model selection and parameter identifiability, and assess\ninferential improvements accrued from nonstationary modeling. We illustrate\nstrengths and pitfalls with a data set on remote sensed normalized difference\nvegetation index.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building spatial process models that capture nonstationary behavior while\ndelivering computationally efficient inference is challenging. Nonstationary\nspatially varying kernels (see, e.g., Paciorek, 2003) offer flexibility and\nrichness, but computation is impeded by high-dimensional parameter spaces\nresulting from spatially varying process parameters. Matters are exacerbated if\nthe number of locations recording measurements is massive. With limited\ntheoretical tractability, obviating computational bottlenecks requires synergy\nbetween model construction and algorithm development. We build a class of\nscalable nonstationary spatial process models using spatially varying\ncovariance kernels. We implement a Bayesian modeling framework using Hybrid\nMonte Carlo with nested interweaving. We conduct experiments on synthetic data\nsets to explore model selection and parameter identifiability, and assess\ninferential improvements accrued from nonstationary modeling. We illustrate\nstrengths and pitfalls with a data set on remote sensed normalized difference\nvegetation index."
                },
                "authors": [
                    {
                        "name": "Sbastien Coube-Sisqueille"
                    },
                    {
                        "name": "Sudipto Banerjee"
                    },
                    {
                        "name": "Benot Liquet"
                    }
                ],
                "author_detail": {
                    "name": "Benot Liquet"
                },
                "author": "Benot Liquet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2203.11873v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2203.11873v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03949v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03949v2",
                "updated": "2025-06-11T15:37:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    15,
                    37,
                    7,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-04T13:39:01Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    13,
                    39,
                    1,
                    2,
                    155,
                    0
                ],
                "title": "TableEval: A Real-World Benchmark for Complex, Multilingual, and\n  Multi-Structured Table Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TableEval: A Real-World Benchmark for Complex, Multilingual, and\n  Multi-Structured Table Question Answering"
                },
                "summary": "LLMs have shown impressive progress in natural language processing. However,\nthey still face significant challenges in TableQA, where real-world\ncomplexities such as diverse table structures, multilingual data, and\ndomain-specific reasoning are crucial. Existing TableQA benchmarks are often\nlimited by their focus on simple flat tables and suffer from data leakage.\nFurthermore, most benchmarks are monolingual and fail to capture the\ncross-lingual and cross-domain variability in practical applications. To\naddress these limitations, we introduce TableEval, a new benchmark designed to\nevaluate LLMs on realistic TableQA tasks. Specifically, TableEval includes\ntables with various structures (such as concise, hierarchical, and nested\ntables) collected from four domains (including government, finance, academia,\nand industry reports). Besides, TableEval features cross-lingual scenarios with\ntables in Simplified Chinese, Traditional Chinese, and English. To minimize the\nrisk of data leakage, we collect all data from recent real-world documents.\nConsidering that existing TableQA metrics fail to capture semantic accuracy, we\nfurther propose SEAT, a new evaluation framework that assesses the alignment\nbetween model responses and reference answers at the sub-question level.\nExperimental results have shown that SEAT achieves high agreement with human\njudgment. Extensive experiments on TableEval reveal critical gaps in the\nability of state-of-the-art LLMs to handle these complex, real-world TableQA\ntasks, offering insights for future improvements. We make our dataset available\nhere: https://github.com/wenge-research/TableEval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have shown impressive progress in natural language processing. However,\nthey still face significant challenges in TableQA, where real-world\ncomplexities such as diverse table structures, multilingual data, and\ndomain-specific reasoning are crucial. Existing TableQA benchmarks are often\nlimited by their focus on simple flat tables and suffer from data leakage.\nFurthermore, most benchmarks are monolingual and fail to capture the\ncross-lingual and cross-domain variability in practical applications. To\naddress these limitations, we introduce TableEval, a new benchmark designed to\nevaluate LLMs on realistic TableQA tasks. Specifically, TableEval includes\ntables with various structures (such as concise, hierarchical, and nested\ntables) collected from four domains (including government, finance, academia,\nand industry reports). Besides, TableEval features cross-lingual scenarios with\ntables in Simplified Chinese, Traditional Chinese, and English. To minimize the\nrisk of data leakage, we collect all data from recent real-world documents.\nConsidering that existing TableQA metrics fail to capture semantic accuracy, we\nfurther propose SEAT, a new evaluation framework that assesses the alignment\nbetween model responses and reference answers at the sub-question level.\nExperimental results have shown that SEAT achieves high agreement with human\njudgment. Extensive experiments on TableEval reveal critical gaps in the\nability of state-of-the-art LLMs to handle these complex, real-world TableQA\ntasks, offering insights for future improvements. We make our dataset available\nhere: https://github.com/wenge-research/TableEval."
                },
                "authors": [
                    {
                        "name": "Junnan Zhu"
                    },
                    {
                        "name": "Jingyi Wang"
                    },
                    {
                        "name": "Bohan Yu"
                    },
                    {
                        "name": "Xiaoyu Wu"
                    },
                    {
                        "name": "Junbo Li"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Nan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Nan Xu"
                },
                "author": "Nan Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03949v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03949v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00092v2",
                "updated": "2025-06-11T15:25:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    15,
                    25,
                    46,
                    2,
                    162,
                    0
                ],
                "published": "2025-03-31T18:00:03Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    18,
                    0,
                    3,
                    0,
                    90,
                    0
                ],
                "title": "A generalized method to measure the Lorentz factor from gamma-ray burst\n  photospheric emission",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A generalized method to measure the Lorentz factor from gamma-ray burst\n  photospheric emission"
                },
                "summary": "The properties of gamma-ray bursts (GRBs) that are inferred from observations\ndepend on the value of the bulk Lorentz factor, $\\Gamma$. Consequently,\naccurately estimating it is an important aim. In this work, we present a method\nof measuring $\\Gamma$ based on observed photospheric emission, which can also\nbe used for highly dissipative flows that may lead to non-thermal spectral\nshapes. For the method to be applicable, two conditions need to be met: the\nphoton number should be conserved in the later stages of the jet, and the\noriginal photon temperature must be inferred from the data. The case of\ndissipation via subphotospheric shocks is discussed in detail, and we show that\nthe method is particularly efficient when a low-energy spectral break is\nidentified. We demonstrate the capabilities of the method by applying it to two\ndifferent GRB spectra. From one of the spectra, we obtain a value for $\\Gamma$\nwith statistical uncertainties of only $\\sim 15$\\%, while for the other\nspectrum we only obtain an upper limit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The properties of gamma-ray bursts (GRBs) that are inferred from observations\ndepend on the value of the bulk Lorentz factor, $\\Gamma$. Consequently,\naccurately estimating it is an important aim. In this work, we present a method\nof measuring $\\Gamma$ based on observed photospheric emission, which can also\nbe used for highly dissipative flows that may lead to non-thermal spectral\nshapes. For the method to be applicable, two conditions need to be met: the\nphoton number should be conserved in the later stages of the jet, and the\noriginal photon temperature must be inferred from the data. The case of\ndissipation via subphotospheric shocks is discussed in detail, and we show that\nthe method is particularly efficient when a low-energy spectral break is\nidentified. We demonstrate the capabilities of the method by applying it to two\ndifferent GRB spectra. From one of the spectra, we obtain a value for $\\Gamma$\nwith statistical uncertainties of only $\\sim 15$\\%, while for the other\nspectrum we only obtain an upper limit."
                },
                "authors": [
                    {
                        "name": "Oscar Wistemar"
                    },
                    {
                        "name": "Felix Ryde"
                    },
                    {
                        "name": "Filip Alamaa"
                    }
                ],
                "author_detail": {
                    "name": "Filip Alamaa"
                },
                "author": "Filip Alamaa",
                "arxiv_doi": "10.3847/1538-4357/add52d",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/add52d",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.00092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in ApJ, minor changes. 8 pages, 3 figures",
                "arxiv_journal_ref": "The Astrophysical Journal, 986:118, 2025",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09853v1",
                "updated": "2025-06-11T15:22:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    15,
                    22,
                    9,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T15:22:09Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    15,
                    22,
                    9,
                    2,
                    162,
                    0
                ],
                "title": "Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning"
                },
                "summary": "Chain-of-Thought (CoT) prompting plays an indispensable role in endowing\nlarge language models (LLMs) with complex reasoning capabilities. However, CoT\ncurrently faces two fundamental challenges: (1) Sufficiency, which ensures that\nthe generated intermediate inference steps comprehensively cover and\nsubstantiate the final conclusion; and (2) Necessity, which identifies the\ninference steps that are truly indispensable for the soundness of the resulting\nanswer. We propose a causal framework that characterizes CoT reasoning through\nthe dual lenses of sufficiency and necessity. Incorporating causal Probability\nof Sufficiency and Necessity allows us not only to determine which steps are\nlogically sufficient or necessary to the prediction outcome, but also to\nquantify their actual influence on the final reasoning outcome under different\nintervention scenarios, thereby enabling the automated addition of missing\nsteps and the pruning of redundant ones. Extensive experimental results on\nvarious mathematical and commonsense reasoning benchmarks confirm substantial\nimprovements in reasoning efficiency and reduced token usage without\nsacrificing accuracy. Our work provides a promising direction for improving LLM\nreasoning performance and cost-effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) prompting plays an indispensable role in endowing\nlarge language models (LLMs) with complex reasoning capabilities. However, CoT\ncurrently faces two fundamental challenges: (1) Sufficiency, which ensures that\nthe generated intermediate inference steps comprehensively cover and\nsubstantiate the final conclusion; and (2) Necessity, which identifies the\ninference steps that are truly indispensable for the soundness of the resulting\nanswer. We propose a causal framework that characterizes CoT reasoning through\nthe dual lenses of sufficiency and necessity. Incorporating causal Probability\nof Sufficiency and Necessity allows us not only to determine which steps are\nlogically sufficient or necessary to the prediction outcome, but also to\nquantify their actual influence on the final reasoning outcome under different\nintervention scenarios, thereby enabling the automated addition of missing\nsteps and the pruning of redundant ones. Extensive experimental results on\nvarious mathematical and commonsense reasoning benchmarks confirm substantial\nimprovements in reasoning efficiency and reduced token usage without\nsacrificing accuracy. Our work provides a promising direction for improving LLM\nreasoning performance and cost-effectiveness."
                },
                "authors": [
                    {
                        "name": "Xiangning Yu"
                    },
                    {
                        "name": "Zhuohan Wang"
                    },
                    {
                        "name": "Linyi Yang"
                    },
                    {
                        "name": "Haoxuan Li"
                    },
                    {
                        "name": "Anjie Liu"
                    },
                    {
                        "name": "Xiao Xue"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Mengyue Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mengyue Yang"
                },
                "author": "Mengyue Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09847v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09847v1",
                "updated": "2025-06-11T15:21:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    15,
                    21,
                    5,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T15:21:05Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    15,
                    21,
                    5,
                    2,
                    162,
                    0
                ],
                "title": "Dataset of News Articles with Provenance Metadata for Media Relevance\n  Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dataset of News Articles with Provenance Metadata for Media Relevance\n  Assessment"
                },
                "summary": "Out-of-context and misattributed imagery is the leading form of media\nmanipulation in today's misinformation and disinformation landscape. The\nexisting methods attempting to detect this practice often only consider whether\nthe semantics of the imagery corresponds to the text narrative, missing\nmanipulation so long as the depicted objects or scenes somewhat correspond to\nthe narrative at hand. To tackle this, we introduce News Media Provenance\nDataset, a dataset of news articles with provenance-tagged images. We formulate\ntwo tasks on this dataset, location of origin relevance (LOR) and date and time\nof origin relevance (DTOR), and present baseline results on six large language\nmodels (LLMs). We identify that, while the zero-shot performance on LOR is\npromising, the performance on DTOR hinders, leaving room for specialized\narchitectures and future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-context and misattributed imagery is the leading form of media\nmanipulation in today's misinformation and disinformation landscape. The\nexisting methods attempting to detect this practice often only consider whether\nthe semantics of the imagery corresponds to the text narrative, missing\nmanipulation so long as the depicted objects or scenes somewhat correspond to\nthe narrative at hand. To tackle this, we introduce News Media Provenance\nDataset, a dataset of news articles with provenance-tagged images. We formulate\ntwo tasks on this dataset, location of origin relevance (LOR) and date and time\nof origin relevance (DTOR), and present baseline results on six large language\nmodels (LLMs). We identify that, while the zero-shot performance on LOR is\npromising, the performance on DTOR hinders, leaving room for specialized\narchitectures and future work."
                },
                "authors": [
                    {
                        "name": "Tomas Peterka"
                    },
                    {
                        "name": "Matyas Bohacek"
                    }
                ],
                "author_detail": {
                    "name": "Matyas Bohacek"
                },
                "author": "Matyas Bohacek",
                "arxiv_journal_ref": "Workshop on NLP for Positive Impact @ ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09847v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09847v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09846v1",
                "updated": "2025-06-11T15:20:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    15,
                    20,
                    30,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T15:20:30Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    15,
                    20,
                    30,
                    2,
                    162,
                    0
                ],
                "title": "Learning to Align: Addressing Character Frequency Distribution Shifts in\n  Handwritten Text Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Align: Addressing Character Frequency Distribution Shifts in\n  Handwritten Text Recognition"
                },
                "summary": "Handwritten text recognition aims to convert visual input into\nmachine-readable text, and it remains challenging due to the evolving and\ncontext-dependent nature of handwriting. Character sets change over time, and\ncharacter frequency distributions shift across historical periods or regions,\noften causing models trained on broad, heterogeneous corpora to underperform on\nspecific subsets. To tackle this, we propose a novel loss function that\nincorporates the Wasserstein distance between the character frequency\ndistribution of the predicted text and a target distribution empirically\nderived from training data. By penalizing divergence from expected\ndistributions, our approach enhances both accuracy and robustness under\ntemporal and contextual intra-dataset shifts. Furthermore, we demonstrate that\ncharacter distribution alignment can also improve existing models at inference\ntime without requiring retraining by integrating it as a scoring function in a\nguided decoding scheme. Experimental results across multiple datasets and\narchitectures confirm the effectiveness of our method in boosting\ngeneralization and performance. We open source our code at\nhttps://github.com/pkaliosis/fada.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handwritten text recognition aims to convert visual input into\nmachine-readable text, and it remains challenging due to the evolving and\ncontext-dependent nature of handwriting. Character sets change over time, and\ncharacter frequency distributions shift across historical periods or regions,\noften causing models trained on broad, heterogeneous corpora to underperform on\nspecific subsets. To tackle this, we propose a novel loss function that\nincorporates the Wasserstein distance between the character frequency\ndistribution of the predicted text and a target distribution empirically\nderived from training data. By penalizing divergence from expected\ndistributions, our approach enhances both accuracy and robustness under\ntemporal and contextual intra-dataset shifts. Furthermore, we demonstrate that\ncharacter distribution alignment can also improve existing models at inference\ntime without requiring retraining by integrating it as a scoring function in a\nguided decoding scheme. Experimental results across multiple datasets and\narchitectures confirm the effectiveness of our method in boosting\ngeneralization and performance. We open source our code at\nhttps://github.com/pkaliosis/fada."
                },
                "authors": [
                    {
                        "name": "Panagiotis Kaliosis"
                    },
                    {
                        "name": "John Pavlopoulos"
                    }
                ],
                "author_detail": {
                    "name": "John Pavlopoulos"
                },
                "author": "John Pavlopoulos",
                "arxiv_comment": "17 pages, 10 figures, Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07859v2",
                "updated": "2025-06-11T15:19:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    15,
                    19,
                    33,
                    2,
                    162,
                    0
                ],
                "published": "2025-05-08T11:17:10Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    11,
                    17,
                    10,
                    3,
                    128,
                    0
                ],
                "title": "Product of Experts with LLMs: Boosting Performance on ARC Is a Matter of\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Product of Experts with LLMs: Boosting Performance on ARC Is a Matter of\n  Perspective"
                },
                "summary": "The Abstraction and Reasoning Corpus (ARC-AGI) poses a significant challenge\nfor large language models (LLMs), exposing limitations in their abstract\nreasoning abilities. In this work, we leverage task-specific data augmentations\nthroughout the training, generation, and scoring phases, and employ a\ndepth-first search algorithm to generate diverse, high-probability candidate\nsolutions. Furthermore, we utilize the LLM not only as a generator but also as\na scorer, using its output probabilities to select the most promising\nsolutions. Our method achieves a score of 71.6% (286.5/400 solved tasks) on the\npublic ARC-AGI evaluation set, demonstrating state-of-the-art performance among\npublicly available approaches. While concurrent closed-source work has reported\nhigher scores, our method distinguishes itself through its transparency,\nreproducibility, and remarkably low inference cost, averaging only around 2ct\nper task on readily available hardware (we assume a price of 36ct/hour for a\nNvidia 4090 GPU).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Abstraction and Reasoning Corpus (ARC-AGI) poses a significant challenge\nfor large language models (LLMs), exposing limitations in their abstract\nreasoning abilities. In this work, we leverage task-specific data augmentations\nthroughout the training, generation, and scoring phases, and employ a\ndepth-first search algorithm to generate diverse, high-probability candidate\nsolutions. Furthermore, we utilize the LLM not only as a generator but also as\na scorer, using its output probabilities to select the most promising\nsolutions. Our method achieves a score of 71.6% (286.5/400 solved tasks) on the\npublic ARC-AGI evaluation set, demonstrating state-of-the-art performance among\npublicly available approaches. While concurrent closed-source work has reported\nhigher scores, our method distinguishes itself through its transparency,\nreproducibility, and remarkably low inference cost, averaging only around 2ct\nper task on readily available hardware (we assume a price of 36ct/hour for a\nNvidia 4090 GPU)."
                },
                "authors": [
                    {
                        "name": "Daniel Franzen"
                    },
                    {
                        "name": "Jan Disselhoff"
                    },
                    {
                        "name": "David Hartmann"
                    }
                ],
                "author_detail": {
                    "name": "David Hartmann"
                },
                "author": "David Hartmann",
                "arxiv_comment": "ICML 2025 camera-ready; 15 pages, 6 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17962v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17962v6",
                "updated": "2025-06-11T15:18:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    15,
                    18,
                    44,
                    2,
                    162,
                    0
                ],
                "published": "2024-06-25T22:44:17Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    22,
                    44,
                    17,
                    1,
                    177,
                    0
                ],
                "title": "Crafting Customisable Characters with LLMs: Introducing SimsChat, a\n  Persona-Driven Role-Playing Agent Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crafting Customisable Characters with LLMs: Introducing SimsChat, a\n  Persona-Driven Role-Playing Agent Framework"
                },
                "summary": "Large Language Models (LLMs) demonstrate remarkable ability to comprehend\ninstructions and generate human-like text, enabling sophisticated agent\nsimulation beyond basic behavior replication. However, the potential for\ncreating freely customisable characters remains underexplored. We introduce the\nCustomisable Conversation Agent Framework, which employs LLMs to simulate\nreal-world characters through personalised characteristic feature injection,\nenabling diverse character creation according to user preferences. We propose\nthe SimsConv dataset, comprising 68 customised characters and 13,971 multi-turn\nrole-playing dialogues across 1,360 real-world scenes. Characters are initially\ncustomised using pre-defined elements (career, aspiration, traits, skills),\nthen expanded through personal and social profiles. Building on this, we\npresent SimsChat, a freely customisable role-playing agent incorporating\nvarious realistic settings and topic-specified character interactions.\nExperimental results on both SimsConv and WikiRoleEval datasets demonstrate\nSimsChat's superior performance in maintaining character consistency, knowledge\naccuracy, and appropriate question rejection compared to existing models. Our\nframework provides valuable insights for developing more accurate and\ncustomisable human simulacra. Our data and code are publicly available at\nhttps://github.com/Bernard-Yang/SimsChat.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate remarkable ability to comprehend\ninstructions and generate human-like text, enabling sophisticated agent\nsimulation beyond basic behavior replication. However, the potential for\ncreating freely customisable characters remains underexplored. We introduce the\nCustomisable Conversation Agent Framework, which employs LLMs to simulate\nreal-world characters through personalised characteristic feature injection,\nenabling diverse character creation according to user preferences. We propose\nthe SimsConv dataset, comprising 68 customised characters and 13,971 multi-turn\nrole-playing dialogues across 1,360 real-world scenes. Characters are initially\ncustomised using pre-defined elements (career, aspiration, traits, skills),\nthen expanded through personal and social profiles. Building on this, we\npresent SimsChat, a freely customisable role-playing agent incorporating\nvarious realistic settings and topic-specified character interactions.\nExperimental results on both SimsConv and WikiRoleEval datasets demonstrate\nSimsChat's superior performance in maintaining character consistency, knowledge\naccuracy, and appropriate question rejection compared to existing models. Our\nframework provides valuable insights for developing more accurate and\ncustomisable human simulacra. Our data and code are publicly available at\nhttps://github.com/Bernard-Yang/SimsChat."
                },
                "authors": [
                    {
                        "name": "Bohao Yang"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Chenghao Xiao"
                    },
                    {
                        "name": "Kun Zhao"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Lin Yuan"
                    },
                    {
                        "name": "Guang Yang"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17962v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17962v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09826v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09826v1",
                "updated": "2025-06-11T15:06:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    15,
                    6,
                    26,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T15:06:26Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    15,
                    6,
                    26,
                    2,
                    162,
                    0
                ],
                "title": "ABYSS III: Observing accretion activity in young stars through empirical\n  veiling measurements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ABYSS III: Observing accretion activity in young stars through empirical\n  veiling measurements"
                },
                "summary": "Stellar accretion plays an important role in the early stages of stellar\nevolution, particularly in Classical T Tauri Stars (CTTSs). Accretion of a CTTS\ncan be related to different physical parameters such as effective temperature\n(T$_{\\text{eff}}$), age, abundance of hydrogen, etc. We can infer how accretion\nworks by examining it across different wavelength regions. Accretion can be\ntraced using veiling, a parameter that measures how excess emission from\naccretion affects the photospheric spectrum of CTTS. In this study, we selected\na sample of CTTSs, Weak-line T Tauri Stars (WTTSs), and field stars, observed\nas a part of the SDSS-V Milky Way Mapper using the BOSS spectrograph. We\nmeasured veiling for CTTSs through comparing them to theoretical spectra. Next,\nwe assessed the effect of veiling on different stellar properties, including\nwavelength, H$\\alpha$ emission, effective temperature, and age. We investigated\nhow veiling changes with these parameters and what the physical reasons behind\nthe changes can be. Finally, we evaluated how our findings align with existing\naccretion shock models. This study highlights veiling as a critical diagnostic\ntool for understanding accretion in young stars.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stellar accretion plays an important role in the early stages of stellar\nevolution, particularly in Classical T Tauri Stars (CTTSs). Accretion of a CTTS\ncan be related to different physical parameters such as effective temperature\n(T$_{\\text{eff}}$), age, abundance of hydrogen, etc. We can infer how accretion\nworks by examining it across different wavelength regions. Accretion can be\ntraced using veiling, a parameter that measures how excess emission from\naccretion affects the photospheric spectrum of CTTS. In this study, we selected\na sample of CTTSs, Weak-line T Tauri Stars (WTTSs), and field stars, observed\nas a part of the SDSS-V Milky Way Mapper using the BOSS spectrograph. We\nmeasured veiling for CTTSs through comparing them to theoretical spectra. Next,\nwe assessed the effect of veiling on different stellar properties, including\nwavelength, H$\\alpha$ emission, effective temperature, and age. We investigated\nhow veiling changes with these parameters and what the physical reasons behind\nthe changes can be. Finally, we evaluated how our findings align with existing\naccretion shock models. This study highlights veiling as a critical diagnostic\ntool for understanding accretion in young stars."
                },
                "authors": [
                    {
                        "name": "Serat Saad"
                    },
                    {
                        "name": "Marina Kounkel"
                    },
                    {
                        "name": "Keivan G. Stassun"
                    },
                    {
                        "name": "A. Roman-Lopes"
                    },
                    {
                        "name": "Carlos G. Romn-Ziga"
                    },
                    {
                        "name": "Jinyoung Serena Kim"
                    },
                    {
                        "name": "Jonathan C. Tan"
                    },
                    {
                        "name": "R. Lopez-Valdivia"
                    }
                ],
                "author_detail": {
                    "name": "R. Lopez-Valdivia"
                },
                "author": "R. Lopez-Valdivia",
                "arxiv_comment": "12 pages, 10 figures, accepted to AJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09826v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09826v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01129v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01129v4",
                "updated": "2025-06-11T15:02:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    15,
                    2,
                    28,
                    2,
                    162,
                    0
                ],
                "published": "2024-04-01T14:11:45Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    14,
                    11,
                    45,
                    0,
                    92,
                    0
                ],
                "title": "Emphasising Structured Information: Integrating Abstract Meaning\n  Representation into LLMs for Enhanced Open-Domain Dialogue Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emphasising Structured Information: Integrating Abstract Meaning\n  Representation into LLMs for Enhanced Open-Domain Dialogue Evaluation"
                },
                "summary": "Automatic open-domain dialogue evaluation has attracted increasing attention,\nyet remains challenging due to the complexity of assessing response\nappropriateness. Traditional evaluation metrics, typically trained with true\npositive and randomly selected negative responses, tend to assign higher scores\nto responses that share greater content similarity with contexts. However,\nadversarial negative responses, despite possessing high lexical overlap with\ncontexts, can be semantically incongruous. Consequently, existing metrics\nstruggle to effectively evaluate such responses, resulting in low correlations\nwith human judgments. While recent studies have demonstrated the effectiveness\nof Large Language Models (LLMs) for open-domain dialogue evaluation, they still\nface challenges in handling adversarial negative examples. We propose a novel\nevaluation framework that integrates Abstract Meaning Representation (AMR)\nenhanced domain-specific language models (SLMs) with LLMs. Our SLMs explicitly\nincorporate AMR graph information through a gating mechanism for enhanced\nsemantic representation learning, while both SLM predictions and AMR knowledge\nare integrated into LLM prompts for robust evaluation. Extensive experiments on\nopen-domain dialogue evaluation tasks demonstrate the superiority of our method\ncompared to state-of-the-art baselines. Our comprehensive ablation studies\nreveal that AMR graph information contributes substantially more to performance\nimprovements. Our framework achieves strong correlations with human judgments\nacross multiple datasets, establishing a new benchmark for dialogue evaluation.\nOur code and data are publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic open-domain dialogue evaluation has attracted increasing attention,\nyet remains challenging due to the complexity of assessing response\nappropriateness. Traditional evaluation metrics, typically trained with true\npositive and randomly selected negative responses, tend to assign higher scores\nto responses that share greater content similarity with contexts. However,\nadversarial negative responses, despite possessing high lexical overlap with\ncontexts, can be semantically incongruous. Consequently, existing metrics\nstruggle to effectively evaluate such responses, resulting in low correlations\nwith human judgments. While recent studies have demonstrated the effectiveness\nof Large Language Models (LLMs) for open-domain dialogue evaluation, they still\nface challenges in handling adversarial negative examples. We propose a novel\nevaluation framework that integrates Abstract Meaning Representation (AMR)\nenhanced domain-specific language models (SLMs) with LLMs. Our SLMs explicitly\nincorporate AMR graph information through a gating mechanism for enhanced\nsemantic representation learning, while both SLM predictions and AMR knowledge\nare integrated into LLM prompts for robust evaluation. Extensive experiments on\nopen-domain dialogue evaluation tasks demonstrate the superiority of our method\ncompared to state-of-the-art baselines. Our comprehensive ablation studies\nreveal that AMR graph information contributes substantially more to performance\nimprovements. Our framework achieves strong correlations with human judgments\nacross multiple datasets, establishing a new benchmark for dialogue evaluation.\nOur code and data are publicly available."
                },
                "authors": [
                    {
                        "name": "Bohao Yang"
                    },
                    {
                        "name": "Kun Zhao"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Liang Zhan"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01129v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01129v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00103v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00103v2",
                "updated": "2025-06-11T14:56:19Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    56,
                    19,
                    2,
                    162,
                    0
                ],
                "published": "2025-05-30T14:34:57Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    34,
                    57,
                    4,
                    150,
                    0
                ],
                "title": "Writing-Zero: Bridge the Gap Between Non-verifiable Tasks and Verifiable\n  Rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Writing-Zero: Bridge the Gap Between Non-verifiable Tasks and Verifiable\n  Rewards"
                },
                "summary": "Reinforcement learning with verifiable rewards (RLVR) has enabled large\nlanguage models (LLMs) to achieve remarkable breakthroughs in reasoning tasks\nwith objective ground-truth answers, such as mathematics and code generation.\nHowever, a significant gap remains for non-verifiable tasks, like creative\nwriting and open-ended dialogue, where quality assessment is inherently\nsubjective and lacks definitive references. Existing approaches for these\ndomains often rely on scalar reward models trained with human preferences,\nwhich suffer from limited generalization and are prone to reward hacking, such\nas over-explanation and length bias. In this work, we propose a unified\nRLVR-based training paradigm that bridges the gap between non-verifiable tasks\nand verifiable rewards. We introduce a writing-principle-based pairwise\nGenerative Reward Model (GenRM) and a novel Bootstrapped Relative Policy\nOptimization (BRPO) algorithm. The pairwise writing GenRM leverages\nself-principled critique to transform subjective assessments into reliable,\nverifiable rewards, while BRPO enables dynamic, reference-free pairwise\ncomparison by leveraging a bootstrapped response as temporary reference from\nwithin group rollouts during RL training. Our approach empowers LLMs to develop\nrobust writing capabilities without supervised fine-tuning, as demonstrated by\nWriting-Zero, which shows consistent improvement and strong resistance to\nreward hacking compared to scalar reward baselines. Furthermore, our method\nachieves competitive results on both in-house and open-source writing\nbenchmarks. Our findings suggest the potential to unify rule-based,\nreference-based, and reference-free reward modeling under the RLVR framework,\nthus paving the way for a comprehensive and scalable RL training paradigm\napplicable across all language tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning with verifiable rewards (RLVR) has enabled large\nlanguage models (LLMs) to achieve remarkable breakthroughs in reasoning tasks\nwith objective ground-truth answers, such as mathematics and code generation.\nHowever, a significant gap remains for non-verifiable tasks, like creative\nwriting and open-ended dialogue, where quality assessment is inherently\nsubjective and lacks definitive references. Existing approaches for these\ndomains often rely on scalar reward models trained with human preferences,\nwhich suffer from limited generalization and are prone to reward hacking, such\nas over-explanation and length bias. In this work, we propose a unified\nRLVR-based training paradigm that bridges the gap between non-verifiable tasks\nand verifiable rewards. We introduce a writing-principle-based pairwise\nGenerative Reward Model (GenRM) and a novel Bootstrapped Relative Policy\nOptimization (BRPO) algorithm. The pairwise writing GenRM leverages\nself-principled critique to transform subjective assessments into reliable,\nverifiable rewards, while BRPO enables dynamic, reference-free pairwise\ncomparison by leveraging a bootstrapped response as temporary reference from\nwithin group rollouts during RL training. Our approach empowers LLMs to develop\nrobust writing capabilities without supervised fine-tuning, as demonstrated by\nWriting-Zero, which shows consistent improvement and strong resistance to\nreward hacking compared to scalar reward baselines. Furthermore, our method\nachieves competitive results on both in-house and open-source writing\nbenchmarks. Our findings suggest the potential to unify rule-based,\nreference-based, and reference-free reward modeling under the RLVR framework,\nthus paving the way for a comprehensive and scalable RL training paradigm\napplicable across all language tasks."
                },
                "authors": [
                    {
                        "name": "Ruipeng Jia"
                    },
                    {
                        "name": "Yunyi Yang"
                    },
                    {
                        "name": "Yongbo Gai"
                    },
                    {
                        "name": "Kai Luo"
                    },
                    {
                        "name": "Shihao Huang"
                    },
                    {
                        "name": "Jianhe Lin"
                    },
                    {
                        "name": "Xiaoxi Jiang"
                    },
                    {
                        "name": "Guanjun Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Guanjun Jiang"
                },
                "author": "Guanjun Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00103v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00103v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08877v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08877v2",
                "updated": "2025-06-11T14:54:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    54,
                    8,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-10T15:07:11Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    15,
                    7,
                    11,
                    1,
                    161,
                    0
                ],
                "title": "Nonequilibrium fluctuation-response relations for state-current\n  correlations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonequilibrium fluctuation-response relations for state-current\n  correlations"
                },
                "summary": "Recently, novel exact identities known as Fluctuation-Response Relations\n(FRRs) have been derived for nonequilibrium steady states of Markov jump\nprocesses. These identities link the fluctuations of state or current\nobservables to a combination of responses of these observables to perturbations\nof transition rates. Here, we complement these results by deriving analogous\nFRRs applicable to mixed covariances of one state and one current observable.\nWe further derive novel Inverse FRRs expressing individual state or current\nresponse in terms of a combination of covariances rather than vice versa. Using\nthese relations, we demonstrate that the breaking of the Onsager symmetry can\noccur only in the presence of state-current correlations. On the practical\nside, we demonstrate the applicability of FRRs for simplifying calculations of\nfluctuations in large Markov networks, we use them to explain the behavior of\nfluctuations in quantum dot devices or enzymatic reaction schemes, and discuss\ntheir potential relevance for model inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, novel exact identities known as Fluctuation-Response Relations\n(FRRs) have been derived for nonequilibrium steady states of Markov jump\nprocesses. These identities link the fluctuations of state or current\nobservables to a combination of responses of these observables to perturbations\nof transition rates. Here, we complement these results by deriving analogous\nFRRs applicable to mixed covariances of one state and one current observable.\nWe further derive novel Inverse FRRs expressing individual state or current\nresponse in terms of a combination of covariances rather than vice versa. Using\nthese relations, we demonstrate that the breaking of the Onsager symmetry can\noccur only in the presence of state-current correlations. On the practical\nside, we demonstrate the applicability of FRRs for simplifying calculations of\nfluctuations in large Markov networks, we use them to explain the behavior of\nfluctuations in quantum dot devices or enzymatic reaction schemes, and discuss\ntheir potential relevance for model inference."
                },
                "authors": [
                    {
                        "name": "Krzysztof Ptaszynski"
                    },
                    {
                        "name": "Timur Aslyamov"
                    },
                    {
                        "name": "Massimiliano Esposito"
                    }
                ],
                "author_detail": {
                    "name": "Massimiliano Esposito"
                },
                "author": "Massimiliano Esposito",
                "arxiv_comment": "10 pages, 6 figures. Companion paper to arXiv:2412.10233",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08877v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08877v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09813v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09813v1",
                "updated": "2025-06-11T14:53:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    53,
                    47,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T14:53:47Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    53,
                    47,
                    2,
                    162,
                    0
                ],
                "title": "Metritocracy: Representative Metrics for Lite Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metritocracy: Representative Metrics for Lite Benchmarks"
                },
                "summary": "A common problem in LLM evaluation is how to choose a subset of metrics from\na full suite of possible metrics. Subset selection is usually done for\nefficiency or interpretability reasons, and the goal is often to select a\n``representative'' subset of metrics. However, ``representative'' is rarely\nclearly defined. In this work, we use ideas from social choice theory to\nformalize two notions of representation for the selection of a subset of\nevaluation metrics. We first introduce positional representation, which\nguarantees every alternative is sufficiently represented at every position\ncutoff. We then introduce positional proportionality, which guarantees no\nalternative is proportionally over- or under-represented by more than a small\nerror at any position. We prove upper and lower bounds on the smallest number\nof metrics needed to guarantee either of these properties in the worst case. We\nalso study a generalized form of each property that allows for additional input\non groups of metrics that must be represented. Finally, we tie theory to\npractice through real-world case studies on both LLM evaluation and hospital\nquality evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A common problem in LLM evaluation is how to choose a subset of metrics from\na full suite of possible metrics. Subset selection is usually done for\nefficiency or interpretability reasons, and the goal is often to select a\n``representative'' subset of metrics. However, ``representative'' is rarely\nclearly defined. In this work, we use ideas from social choice theory to\nformalize two notions of representation for the selection of a subset of\nevaluation metrics. We first introduce positional representation, which\nguarantees every alternative is sufficiently represented at every position\ncutoff. We then introduce positional proportionality, which guarantees no\nalternative is proportionally over- or under-represented by more than a small\nerror at any position. We prove upper and lower bounds on the smallest number\nof metrics needed to guarantee either of these properties in the worst case. We\nalso study a generalized form of each property that allows for additional input\non groups of metrics that must be represented. Finally, we tie theory to\npractice through real-world case studies on both LLM evaluation and hospital\nquality evaluation."
                },
                "authors": [
                    {
                        "name": "Ariel Procaccia"
                    },
                    {
                        "name": "Benjamin Schiffer"
                    },
                    {
                        "name": "Serena Wang"
                    },
                    {
                        "name": "Shirley Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shirley Zhang"
                },
                "author": "Shirley Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09813v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09813v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10233v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10233v2",
                "updated": "2025-06-11T14:50:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    50,
                    29,
                    2,
                    162,
                    0
                ],
                "published": "2024-12-13T16:02:38Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    2,
                    38,
                    4,
                    348,
                    0
                ],
                "title": "Nonequilibrium Fluctuation-Response Relations for State Observables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonequilibrium Fluctuation-Response Relations for State Observables"
                },
                "summary": "Time-integrated state observables, which quantify the fraction of time spent\nby the system in a specific pool of states, are important in many fields, such\nas chemical sensing or the theory of fluorescence spectroscopy. We derive exact\nidentities, called Fluctuation-Response Relations (FRRs), that connect the\nfluctuations of such observables to their response to external perturbations in\nnonequilibrium steady state of Markov jump processes. Using these results, we\nderive novel upper and lower bounds for fluctuations. We further demonstrate\ntheir applicability for simplifying calculations of fluctuations in large\nMarkov networks, use them to explain the physical origin of positive and\nnegative correlations of occupation times in a double quantum dot device, and\ndiscuss their relevance for model inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-integrated state observables, which quantify the fraction of time spent\nby the system in a specific pool of states, are important in many fields, such\nas chemical sensing or the theory of fluorescence spectroscopy. We derive exact\nidentities, called Fluctuation-Response Relations (FRRs), that connect the\nfluctuations of such observables to their response to external perturbations in\nnonequilibrium steady state of Markov jump processes. Using these results, we\nderive novel upper and lower bounds for fluctuations. We further demonstrate\ntheir applicability for simplifying calculations of fluctuations in large\nMarkov networks, use them to explain the physical origin of positive and\nnegative correlations of occupation times in a double quantum dot device, and\ndiscuss their relevance for model inference."
                },
                "authors": [
                    {
                        "name": "Krzysztof Ptaszynski"
                    },
                    {
                        "name": "Timur Aslyamov"
                    },
                    {
                        "name": "Massimiliano Esposito"
                    }
                ],
                "author_detail": {
                    "name": "Massimiliano Esposito"
                },
                "author": "Massimiliano Esposito",
                "arxiv_comment": "8 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10233v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10233v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05023v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05023v3",
                "updated": "2025-06-11T14:47:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    47,
                    23,
                    2,
                    162,
                    0
                ],
                "published": "2024-12-06T13:20:57Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    13,
                    20,
                    57,
                    4,
                    341,
                    0
                ],
                "title": "Steps are all you need: Rethinking STEM Education with Prompt\n  Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steps are all you need: Rethinking STEM Education with Prompt\n  Engineering"
                },
                "summary": "Few shot and Chain-of-Thought prompting have shown promise when applied to\nPhysics Question Answering Tasks, but are limited by the lack of mathematical\nability inherent to LLMs, and are prone to hallucination. By utilizing a\nMixture of Experts (MoE) Model, along with analogical prompting, we are able to\nshow improved model performance when compared to the baseline on standard LLMs.\nWe also survey the limits of these prompting techniques and the effects they\nhave on model performance. Additionally, we propose Analogical CoT prompting, a\nprompting technique designed to allow smaller, open source models to leverage\nAnalogical prompting, something they have struggled with, possibly due to a\nlack of specialist training data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few shot and Chain-of-Thought prompting have shown promise when applied to\nPhysics Question Answering Tasks, but are limited by the lack of mathematical\nability inherent to LLMs, and are prone to hallucination. By utilizing a\nMixture of Experts (MoE) Model, along with analogical prompting, we are able to\nshow improved model performance when compared to the baseline on standard LLMs.\nWe also survey the limits of these prompting techniques and the effects they\nhave on model performance. Additionally, we propose Analogical CoT prompting, a\nprompting technique designed to allow smaller, open source models to leverage\nAnalogical prompting, something they have struggled with, possibly due to a\nlack of specialist training data."
                },
                "authors": [
                    {
                        "name": "Krishnasai Addala"
                    },
                    {
                        "name": "Kabir Dev Paul Baghel"
                    },
                    {
                        "name": "Navya Gupta"
                    },
                    {
                        "name": "Rishitej Reddy Vyalla"
                    },
                    {
                        "name": "Chhavi Kirtani"
                    },
                    {
                        "name": "Avinash Anand"
                    },
                    {
                        "name": "Rajiv Ratn Shah"
                    }
                ],
                "author_detail": {
                    "name": "Rajiv Ratn Shah"
                },
                "author": "Rajiv Ratn Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05023v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05023v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05453v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05453v3",
                "updated": "2025-06-11T14:43:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    43,
                    44,
                    2,
                    162,
                    0
                ],
                "published": "2024-12-06T22:25:23Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    22,
                    25,
                    23,
                    4,
                    341,
                    0
                ],
                "title": "Knowledge Graphs are all you need: Leveraging KGs in Physics Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Graphs are all you need: Leveraging KGs in Physics Question\n  Answering"
                },
                "summary": "This study explores the effectiveness of using knowledge graphs generated by\nlarge language models to decompose high school-level physics questions into\nsub-questions. We introduce a pipeline aimed at enhancing model response\nquality for Question Answering tasks. By employing LLMs to construct knowledge\ngraphs that capture the internal logic of the questions, these graphs then\nguide the generation of subquestions. We hypothesize that this method yields\nsub-questions that are more logically consistent with the original questions\ncompared to traditional decomposition techniques. Our results show that\nsub-questions derived from knowledge graphs exhibit significantly improved\nfidelity to the original question's logic. This approach not only enhances the\nlearning experience by providing clearer and more contextually appropriate\nsub-questions but also highlights the potential of LLMs to transform\neducational methodologies. The findings indicate a promising direction for\napplying AI to improve the quality and effectiveness of educational content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the effectiveness of using knowledge graphs generated by\nlarge language models to decompose high school-level physics questions into\nsub-questions. We introduce a pipeline aimed at enhancing model response\nquality for Question Answering tasks. By employing LLMs to construct knowledge\ngraphs that capture the internal logic of the questions, these graphs then\nguide the generation of subquestions. We hypothesize that this method yields\nsub-questions that are more logically consistent with the original questions\ncompared to traditional decomposition techniques. Our results show that\nsub-questions derived from knowledge graphs exhibit significantly improved\nfidelity to the original question's logic. This approach not only enhances the\nlearning experience by providing clearer and more contextually appropriate\nsub-questions but also highlights the potential of LLMs to transform\neducational methodologies. The findings indicate a promising direction for\napplying AI to improve the quality and effectiveness of educational content."
                },
                "authors": [
                    {
                        "name": "Krishnasai Addala"
                    },
                    {
                        "name": "Kabir Dev Paul Baghel"
                    },
                    {
                        "name": "Dhruv Jain"
                    },
                    {
                        "name": "Navya Gupta"
                    },
                    {
                        "name": "Rishitej Reddy Vyalla"
                    },
                    {
                        "name": "Chhavi Kirtani"
                    },
                    {
                        "name": "Avinash Anand"
                    },
                    {
                        "name": "Rajiv Ratn Shah"
                    }
                ],
                "author_detail": {
                    "name": "Rajiv Ratn Shah"
                },
                "author": "Rajiv Ratn Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05453v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05453v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14352v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14352v2",
                "updated": "2025-06-11T14:42:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    42,
                    54,
                    2,
                    162,
                    0
                ],
                "published": "2024-08-26T15:29:34Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    15,
                    29,
                    34,
                    0,
                    239,
                    0
                ],
                "title": "LogProber: Disentangling confidence from contamination in LLM responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogProber: Disentangling confidence from contamination in LLM responses"
                },
                "summary": "In machine learning, contamination refers to situations where testing data\nleak into the training set. The issue is particularly relevant for the\nevaluation of the performance of Large Language Models (LLMs), which are\ngenerally trained on gargantuan, and generally opaque, corpora of text scraped\nfrom the world wide web. Developing tools to detect contamination is therefore\ncrucial to be able to fairly and properly track the evolution of the\nperformance of LLMs. To date, only a few recent studies have attempted to\naddress the issue of quantifying and detecting contamination in short text\nsequences, such as those commonly found in benchmarks. However, these methods\nhave limitations that can sometimes render them impractical.In the present\npaper, we introduce LogProber, a novel, efficient algorithm that we show to be\nable to detect contamination in a black box setting that tries to tackle some\nof these drawbacks by focusing on the familiarity with the question rather than\nthe answer. Here, we explore the properties of the proposed method in\ncomparison with concurrent approaches, identify its advantages and limitations,\nand illustrate how different forms of contamination can go undetected depending\non the design of the detection algorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In machine learning, contamination refers to situations where testing data\nleak into the training set. The issue is particularly relevant for the\nevaluation of the performance of Large Language Models (LLMs), which are\ngenerally trained on gargantuan, and generally opaque, corpora of text scraped\nfrom the world wide web. Developing tools to detect contamination is therefore\ncrucial to be able to fairly and properly track the evolution of the\nperformance of LLMs. To date, only a few recent studies have attempted to\naddress the issue of quantifying and detecting contamination in short text\nsequences, such as those commonly found in benchmarks. However, these methods\nhave limitations that can sometimes render them impractical.In the present\npaper, we introduce LogProber, a novel, efficient algorithm that we show to be\nable to detect contamination in a black box setting that tries to tackle some\nof these drawbacks by focusing on the familiarity with the question rather than\nthe answer. Here, we explore the properties of the proposed method in\ncomparison with concurrent approaches, identify its advantages and limitations,\nand illustrate how different forms of contamination can go undetected depending\non the design of the detection algorithm."
                },
                "authors": [
                    {
                        "name": "Nicolas Yax"
                    },
                    {
                        "name": "Pierre-Yves Oudeyer"
                    },
                    {
                        "name": "Stefano Palminteri"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Palminteri"
                },
                "author": "Stefano Palminteri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14352v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14352v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09796v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09796v1",
                "updated": "2025-06-11T14:41:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    41,
                    10,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T14:41:10Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    41,
                    10,
                    2,
                    162,
                    0
                ],
                "title": "Do LLMs Give Psychometrically Plausible Responses in Educational\n  Assessments?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Give Psychometrically Plausible Responses in Educational\n  Assessments?"
                },
                "summary": "Knowing how test takers answer items in educational assessments is essential\nfor test development, to evaluate item quality, and to improve test validity.\nHowever, this process usually requires extensive pilot studies with human\nparticipants. If large language models (LLMs) exhibit human-like response\nbehavior to test items, this could open up the possibility of using them as\npilot participants to accelerate test development. In this paper, we evaluate\nthe human-likeness or psychometric plausibility of responses from 18\ninstruction-tuned LLMs with two publicly available datasets of multiple-choice\ntest items across three subjects: reading, U.S. history, and economics. Our\nmethodology builds on two theoretical frameworks from psychometrics which are\ncommonly used in educational assessment, classical test theory and item\nresponse theory. The results show that while larger models are excessively\nconfident, their response distributions can be more human-like when calibrated\nwith temperature scaling. In addition, we find that LLMs tend to correlate\nbetter with humans in reading comprehension items compared to other subjects.\nHowever, the correlations are not very strong overall, indicating that LLMs\nshould not be used for piloting educational assessments in a zero-shot setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowing how test takers answer items in educational assessments is essential\nfor test development, to evaluate item quality, and to improve test validity.\nHowever, this process usually requires extensive pilot studies with human\nparticipants. If large language models (LLMs) exhibit human-like response\nbehavior to test items, this could open up the possibility of using them as\npilot participants to accelerate test development. In this paper, we evaluate\nthe human-likeness or psychometric plausibility of responses from 18\ninstruction-tuned LLMs with two publicly available datasets of multiple-choice\ntest items across three subjects: reading, U.S. history, and economics. Our\nmethodology builds on two theoretical frameworks from psychometrics which are\ncommonly used in educational assessment, classical test theory and item\nresponse theory. The results show that while larger models are excessively\nconfident, their response distributions can be more human-like when calibrated\nwith temperature scaling. In addition, we find that LLMs tend to correlate\nbetter with humans in reading comprehension items compared to other subjects.\nHowever, the correlations are not very strong overall, indicating that LLMs\nshould not be used for piloting educational assessments in a zero-shot setting."
                },
                "authors": [
                    {
                        "name": "Andreas Suberli"
                    },
                    {
                        "name": "Diego Frassinelli"
                    },
                    {
                        "name": "Barbara Plank"
                    }
                ],
                "author_detail": {
                    "name": "Barbara Plank"
                },
                "author": "Barbara Plank",
                "arxiv_comment": "Accepted for publication at the 20th Workshop on Innovative Use of\n  NLP for Building Educational Applications (BEA) at ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09796v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09796v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09795v1",
                "updated": "2025-06-11T14:39:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    39,
                    51,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T14:39:51Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    39,
                    51,
                    2,
                    162,
                    0
                ],
                "title": "Learning Quality from Complexity and Structure: A Feature-Fused XGBoost\n  Model for Video Quality Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Quality from Complexity and Structure: A Feature-Fused XGBoost\n  Model for Video Quality Assessment"
                },
                "summary": "This paper presents a novel approach for reduced-reference video quality\nassessment (VQA), developed as part of the recent VQA Grand Challenge. Our\nmethod leverages low-level complexity and structural information from reference\nand test videos to predict perceptual quality scores. Specifically, we extract\nspatio-temporal features using Video Complexity Analyzer (VCA) and compute SSIM\nvalues from the test video to capture both texture and structural\ncharacteristics. These features are aggregated through temporal pooling, and\nresidual features are calculated by comparing the original and distorted\nfeature sets. The combined features are used to train an XGBoost regression\nmodel that estimates the overall video quality. The pipeline is fully\nautomated, interpretable, and highly scalable, requiring no deep neural\nnetworks or GPU inference. Experimental results on the challenge dataset\ndemonstrate that our proposed method achieves competitive correlation with\nsubjective quality scores while maintaining a low computational footprint. The\nmodel's lightweight design and strong generalization performance suit real-time\nstreaming quality monitoring and adaptive encoding scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel approach for reduced-reference video quality\nassessment (VQA), developed as part of the recent VQA Grand Challenge. Our\nmethod leverages low-level complexity and structural information from reference\nand test videos to predict perceptual quality scores. Specifically, we extract\nspatio-temporal features using Video Complexity Analyzer (VCA) and compute SSIM\nvalues from the test video to capture both texture and structural\ncharacteristics. These features are aggregated through temporal pooling, and\nresidual features are calculated by comparing the original and distorted\nfeature sets. The combined features are used to train an XGBoost regression\nmodel that estimates the overall video quality. The pipeline is fully\nautomated, interpretable, and highly scalable, requiring no deep neural\nnetworks or GPU inference. Experimental results on the challenge dataset\ndemonstrate that our proposed method achieves competitive correlation with\nsubjective quality scores while maintaining a low computational footprint. The\nmodel's lightweight design and strong generalization performance suit real-time\nstreaming quality monitoring and adaptive encoding scenarios."
                },
                "authors": [
                    {
                        "name": "Amritha Premkumar"
                    },
                    {
                        "name": "Prajit T Rajendran"
                    },
                    {
                        "name": "Vignesh V Menon"
                    }
                ],
                "author_detail": {
                    "name": "Vignesh V Menon"
                },
                "author": "Vignesh V Menon",
                "arxiv_comment": "ICME 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09792v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09792v1",
                "updated": "2025-06-11T14:36:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    36,
                    26,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T14:36:26Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    36,
                    26,
                    2,
                    162,
                    0
                ],
                "title": "Incorporating Linguistic Constraints from External Knowledge Source for\n  Audio-Visual Target Speech Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incorporating Linguistic Constraints from External Knowledge Source for\n  Audio-Visual Target Speech Extraction"
                },
                "summary": "Audio-visual target speaker extraction (AV-TSE) models primarily rely on\ntarget visual cues to isolate the target speaker's voice from others. We know\nthat humans leverage linguistic knowledge, such as syntax and semantics, to\nsupport speech perception. Inspired by this, we explore the potential of\npre-trained speech-language models (PSLMs) and pre-trained language models\n(PLMs) as auxiliary knowledge sources for AV-TSE. In this study, we propose\nincorporating the linguistic constraints from PSLMs or PLMs for the AV-TSE\nmodel as additional supervision signals. Without introducing any extra\ncomputational cost during inference, the proposed approach consistently\nimproves speech quality and intelligibility. Furthermore, we evaluate our\nmethod in multi-language settings and visual cue-impaired scenarios and show\nrobust performance gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio-visual target speaker extraction (AV-TSE) models primarily rely on\ntarget visual cues to isolate the target speaker's voice from others. We know\nthat humans leverage linguistic knowledge, such as syntax and semantics, to\nsupport speech perception. Inspired by this, we explore the potential of\npre-trained speech-language models (PSLMs) and pre-trained language models\n(PLMs) as auxiliary knowledge sources for AV-TSE. In this study, we propose\nincorporating the linguistic constraints from PSLMs or PLMs for the AV-TSE\nmodel as additional supervision signals. Without introducing any extra\ncomputational cost during inference, the proposed approach consistently\nimproves speech quality and intelligibility. Furthermore, we evaluate our\nmethod in multi-language settings and visual cue-impaired scenarios and show\nrobust performance gains."
                },
                "authors": [
                    {
                        "name": "Wenxuan Wu"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Xixin Wu"
                    },
                    {
                        "name": "Helen Meng"
                    },
                    {
                        "name": "Haizhou Li"
                    }
                ],
                "author_detail": {
                    "name": "Haizhou Li"
                },
                "author": "Haizhou Li",
                "arxiv_comment": "Accepted by Interspeech 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09792v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09792v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17194v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17194v5",
                "updated": "2025-06-11T14:29:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    29,
                    40,
                    2,
                    162,
                    0
                ],
                "published": "2024-10-22T17:13:34Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    17,
                    13,
                    34,
                    1,
                    296,
                    0
                ],
                "title": "Representation Shattering in Transformers: A Synthetic Study with\n  Knowledge Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representation Shattering in Transformers: A Synthetic Study with\n  Knowledge Editing"
                },
                "summary": "Knowledge Editing (KE) algorithms alter models' weights to perform targeted\nupdates to incorrect, outdated, or otherwise unwanted factual associations.\nHowever, recent work has shown that applying KE can adversely affect models'\nbroader factual recall accuracy and diminish their reasoning abilities.\nAlthough these studies give insights into the potential harms of KE algorithms,\ne.g., performance evaluations on benchmarks, little is understood about why\nsuch destructive failures occur. Motivated by this, we define a novel synthetic\ntask in which a Transformer is trained from scratch to internalize a\n\"structured\" knowledge graph. The structure enforces relationships between\nentities of the graph, such that editing a factual association has \"trickling\neffects\" on other entities (e.g., altering X's parent is Y to Z affects who X's\nsiblings' parent is). Through evaluations of edited models on this task, we\nshow that KE inadvertently affects representations of entities beyond the\ntargeted one, distorting relevant structures that allow a model to infer unseen\nknowledge about an entity. We call this phenomenon representation shattering\nand demonstrate that it degrades models' factual recall and reasoning\nperformance. We further corroborate our findings in naturalistic settings with\npre-trained Llama and Mamba models as well. Overall, our work yields a precise\nmechanistic hypothesis to explain why KE has adverse effects on model\nabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Editing (KE) algorithms alter models' weights to perform targeted\nupdates to incorrect, outdated, or otherwise unwanted factual associations.\nHowever, recent work has shown that applying KE can adversely affect models'\nbroader factual recall accuracy and diminish their reasoning abilities.\nAlthough these studies give insights into the potential harms of KE algorithms,\ne.g., performance evaluations on benchmarks, little is understood about why\nsuch destructive failures occur. Motivated by this, we define a novel synthetic\ntask in which a Transformer is trained from scratch to internalize a\n\"structured\" knowledge graph. The structure enforces relationships between\nentities of the graph, such that editing a factual association has \"trickling\neffects\" on other entities (e.g., altering X's parent is Y to Z affects who X's\nsiblings' parent is). Through evaluations of edited models on this task, we\nshow that KE inadvertently affects representations of entities beyond the\ntargeted one, distorting relevant structures that allow a model to infer unseen\nknowledge about an entity. We call this phenomenon representation shattering\nand demonstrate that it degrades models' factual recall and reasoning\nperformance. We further corroborate our findings in naturalistic settings with\npre-trained Llama and Mamba models as well. Overall, our work yields a precise\nmechanistic hypothesis to explain why KE has adverse effects on model\nabilities."
                },
                "authors": [
                    {
                        "name": "Kento Nishi"
                    },
                    {
                        "name": "Rahul Ramesh"
                    },
                    {
                        "name": "Maya Okawa"
                    },
                    {
                        "name": "Mikail Khona"
                    },
                    {
                        "name": "Hidenori Tanaka"
                    },
                    {
                        "name": "Ekdeep Singh Lubana"
                    }
                ],
                "author_detail": {
                    "name": "Ekdeep Singh Lubana"
                },
                "author": "Ekdeep Singh Lubana",
                "arxiv_comment": "Accepted to ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17194v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17194v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09784v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09784v1",
                "updated": "2025-06-11T14:23:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    23,
                    38,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T14:23:38Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    23,
                    38,
                    2,
                    162,
                    0
                ],
                "title": "Accurate and efficient zero-shot 6D pose estimation with frozen\n  foundation models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate and efficient zero-shot 6D pose estimation with frozen\n  foundation models"
                },
                "summary": "Estimating the 6D pose of objects from RGBD data is a fundamental problem in\ncomputer vision, with applications in robotics and augmented reality. A key\nchallenge is achieving generalization to novel objects that were not seen\nduring training. Most existing approaches address this by scaling up training\non synthetic data tailored to the task, a process that demands substantial\ncomputational resources. But is task-specific training really necessary for\naccurate and efficient 6D pose estimation of novel objects? To answer No!, we\nintroduce FreeZeV2, the second generation of FreeZe: a training-free method\nthat achieves strong generalization to unseen objects by leveraging geometric\nand vision foundation models pre-trained on unrelated data. FreeZeV2 improves\nboth accuracy and efficiency over FreeZe through three key contributions: (i) a\nsparse feature extraction strategy that reduces inference-time computation\nwithout sacrificing accuracy; (ii) a feature-aware scoring mechanism that\nimproves both pose selection during RANSAC-based 3D registration and the final\nranking of pose candidates; and (iii) a modular design that supports ensembles\nof instance segmentation models, increasing robustness to segmentation masks\nerrors. We evaluate FreeZeV2 on the seven core datasets of the BOP Benchmark,\nwhere it establishes a new state-of-the-art in 6D pose estimation of unseen\nobjects. When using the same segmentation masks, FreeZeV2 achieves a remarkable\n8x speedup over FreeZe while also improving accuracy by 5%. When using\nensembles of segmentation models, FreeZeV2 gains an additional 8% in accuracy\nwhile still running 2.5x faster than FreeZe. FreeZeV2 was awarded Best Overall\nMethod at the BOP Challenge 2024.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating the 6D pose of objects from RGBD data is a fundamental problem in\ncomputer vision, with applications in robotics and augmented reality. A key\nchallenge is achieving generalization to novel objects that were not seen\nduring training. Most existing approaches address this by scaling up training\non synthetic data tailored to the task, a process that demands substantial\ncomputational resources. But is task-specific training really necessary for\naccurate and efficient 6D pose estimation of novel objects? To answer No!, we\nintroduce FreeZeV2, the second generation of FreeZe: a training-free method\nthat achieves strong generalization to unseen objects by leveraging geometric\nand vision foundation models pre-trained on unrelated data. FreeZeV2 improves\nboth accuracy and efficiency over FreeZe through three key contributions: (i) a\nsparse feature extraction strategy that reduces inference-time computation\nwithout sacrificing accuracy; (ii) a feature-aware scoring mechanism that\nimproves both pose selection during RANSAC-based 3D registration and the final\nranking of pose candidates; and (iii) a modular design that supports ensembles\nof instance segmentation models, increasing robustness to segmentation masks\nerrors. We evaluate FreeZeV2 on the seven core datasets of the BOP Benchmark,\nwhere it establishes a new state-of-the-art in 6D pose estimation of unseen\nobjects. When using the same segmentation masks, FreeZeV2 achieves a remarkable\n8x speedup over FreeZe while also improving accuracy by 5%. When using\nensembles of segmentation models, FreeZeV2 gains an additional 8% in accuracy\nwhile still running 2.5x faster than FreeZe. FreeZeV2 was awarded Best Overall\nMethod at the BOP Challenge 2024."
                },
                "authors": [
                    {
                        "name": "Andrea Caraffa"
                    },
                    {
                        "name": "Davide Boscaini"
                    },
                    {
                        "name": "Fabio Poiesi"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Poiesi"
                },
                "author": "Fabio Poiesi",
                "arxiv_comment": "Technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09784v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09784v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09782v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09782v1",
                "updated": "2025-06-11T14:21:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    21,
                    38,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T14:21:38Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    21,
                    38,
                    2,
                    162,
                    0
                ],
                "title": "Q-SAM2: Accurate Quantization for Segment Anything Model 2",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q-SAM2: Accurate Quantization for Segment Anything Model 2"
                },
                "summary": "The Segment Anything Model 2 (SAM2) has gained significant attention as a\nfoundational approach for promptable image and video segmentation. However, its\nexpensive computational and memory consumption poses a severe challenge for its\napplication in resource-constrained scenarios. In this paper, we propose an\naccurate low-bit quantization method for efficient SAM2, termed Q-SAM2. To\naddress the performance degradation caused by the singularities in weight and\nactivation distributions during quantization, Q-SAM2 introduces two novel\ntechnical contributions. We first introduce a linear layer calibration method\nfor low-bit initialization of SAM2, which minimizes the Frobenius norm over a\nsmall image batch to reposition weight distributions for improved quantization.\nWe then propose a Quantization-Aware Training (QAT) pipeline that applies\nclipping to suppress outliers and allows the network to adapt to quantization\nthresholds during training. Our comprehensive experiments demonstrate that\nQ-SAM2 allows for highly accurate inference while substantially improving\nefficiency. Both quantitative and visual results show that our Q-SAM2 surpasses\nexisting state-of-the-art general quantization schemes, especially for\nultra-low 2-bit quantization. While designed for quantization-aware training,\nour proposed calibration technique also proves effective in post-training\nquantization, achieving up to a 66% mIoU accuracy improvement over\nnon-calibrated models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Segment Anything Model 2 (SAM2) has gained significant attention as a\nfoundational approach for promptable image and video segmentation. However, its\nexpensive computational and memory consumption poses a severe challenge for its\napplication in resource-constrained scenarios. In this paper, we propose an\naccurate low-bit quantization method for efficient SAM2, termed Q-SAM2. To\naddress the performance degradation caused by the singularities in weight and\nactivation distributions during quantization, Q-SAM2 introduces two novel\ntechnical contributions. We first introduce a linear layer calibration method\nfor low-bit initialization of SAM2, which minimizes the Frobenius norm over a\nsmall image batch to reposition weight distributions for improved quantization.\nWe then propose a Quantization-Aware Training (QAT) pipeline that applies\nclipping to suppress outliers and allows the network to adapt to quantization\nthresholds during training. Our comprehensive experiments demonstrate that\nQ-SAM2 allows for highly accurate inference while substantially improving\nefficiency. Both quantitative and visual results show that our Q-SAM2 surpasses\nexisting state-of-the-art general quantization schemes, especially for\nultra-low 2-bit quantization. While designed for quantization-aware training,\nour proposed calibration technique also proves effective in post-training\nquantization, achieving up to a 66% mIoU accuracy improvement over\nnon-calibrated models."
                },
                "authors": [
                    {
                        "name": "Nicola Farronato"
                    },
                    {
                        "name": "Florian Scheidegger"
                    },
                    {
                        "name": "Mattia Rigotti"
                    },
                    {
                        "name": "Cristiano Malossi"
                    },
                    {
                        "name": "Michele Magno"
                    },
                    {
                        "name": "Haotong Qin"
                    }
                ],
                "author_detail": {
                    "name": "Haotong Qin"
                },
                "author": "Haotong Qin",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09782v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09782v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09771v1",
                "updated": "2025-06-11T14:10:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    10,
                    39,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T14:10:39Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    10,
                    39,
                    2,
                    162,
                    0
                ],
                "title": "Where Journalism Silenced Voices: Exploring Discrimination in the\n  Representation of Indigenous Communities in Bangladesh",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Where Journalism Silenced Voices: Exploring Discrimination in the\n  Representation of Indigenous Communities in Bangladesh"
                },
                "summary": "In this paper, we examine the intersections of indigeneity and media\nrepresentation in shaping perceptions of indigenous communities in Bangladesh.\nUsing a mixed-methods approach, we combine quantitative analysis of media data\nwith qualitative insights from focus group discussions (FGD). First, we\nidentify a total of 4,893 indigenous-related articles from our initial dataset\nof 2.2 million newspaper articles, using a combination of keyword-based\nfiltering and LLM, achieving 77% accuracy and an F1-score of 81.9\\%. From\nmanually inspecting 3 prominent Bangla newspapers, we identify 15 genres that\nwe use as our topics for semi-supervised topic modeling using CorEx. Results\nshow indigenous news articles have higher representation of culture and\nentertainment (19%, 10% higher than general news articles), and a\ndisproportionate focus on conflict and protest (9%, 7% higher than general\nnews). On the other hand, sentiment analysis reveals that 57% of articles on\nindigenous topics carry a negative tone, compared to 27% for non-indigenous\nrelated news. Drawing from communication studies, we further analyze framing,\npriming, and agenda-setting (frequency of themes) to support the case for\ndiscrimination in representation of indigenous news coverage. For the\nqualitative part of our analysis, we facilitated FGD, where participants\nfurther validated these findings. Participants unanimously expressed their\nfeeling of being under-represented, and that critical issues affecting their\ncommunities (such as education, healthcare, and land rights) are systematically\nmarginalized in news media coverage. By highlighting 8 cases of discrimination\nand media misrepresentation that were frequently mentioned by participants in\nthe FGD, this study emphasizes the urgent need for more equitable media\npractices that accurately reflect the experiences and struggles of marginalized\ncommunities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we examine the intersections of indigeneity and media\nrepresentation in shaping perceptions of indigenous communities in Bangladesh.\nUsing a mixed-methods approach, we combine quantitative analysis of media data\nwith qualitative insights from focus group discussions (FGD). First, we\nidentify a total of 4,893 indigenous-related articles from our initial dataset\nof 2.2 million newspaper articles, using a combination of keyword-based\nfiltering and LLM, achieving 77% accuracy and an F1-score of 81.9\\%. From\nmanually inspecting 3 prominent Bangla newspapers, we identify 15 genres that\nwe use as our topics for semi-supervised topic modeling using CorEx. Results\nshow indigenous news articles have higher representation of culture and\nentertainment (19%, 10% higher than general news articles), and a\ndisproportionate focus on conflict and protest (9%, 7% higher than general\nnews). On the other hand, sentiment analysis reveals that 57% of articles on\nindigenous topics carry a negative tone, compared to 27% for non-indigenous\nrelated news. Drawing from communication studies, we further analyze framing,\npriming, and agenda-setting (frequency of themes) to support the case for\ndiscrimination in representation of indigenous news coverage. For the\nqualitative part of our analysis, we facilitated FGD, where participants\nfurther validated these findings. Participants unanimously expressed their\nfeeling of being under-represented, and that critical issues affecting their\ncommunities (such as education, healthcare, and land rights) are systematically\nmarginalized in news media coverage. By highlighting 8 cases of discrimination\nand media misrepresentation that were frequently mentioned by participants in\nthe FGD, this study emphasizes the urgent need for more equitable media\npractices that accurately reflect the experiences and struggles of marginalized\ncommunities."
                },
                "authors": [
                    {
                        "name": "Abhijit Paul"
                    },
                    {
                        "name": "Adity Khisa"
                    },
                    {
                        "name": "Zarif Masud"
                    },
                    {
                        "name": "Sharif Md. Abdullah"
                    },
                    {
                        "name": "Ahmedul Kabir"
                    },
                    {
                        "name": "Shebuti Rayana"
                    }
                ],
                "author_detail": {
                    "name": "Shebuti Rayana"
                },
                "author": "Shebuti Rayana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12977v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12977v4",
                "updated": "2025-06-11T14:09:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    9,
                    4,
                    2,
                    162,
                    0
                ],
                "published": "2024-11-20T02:10:44Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    2,
                    10,
                    44,
                    2,
                    325,
                    0
                ],
                "title": "MindForge: Empowering Embodied Agents with Theory of Mind for Lifelong\n  Cultural Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MindForge: Empowering Embodied Agents with Theory of Mind for Lifelong\n  Cultural Learning"
                },
                "summary": "Embodied agents powered by large language models (LLMs), such as Voyager,\npromise open-ended competence in worlds such as Minecraft. However, when\npowered by open-weight LLMs they still falter on elementary tasks after\ndomain-specific fine-tuning. We propose MindForge, a generative-agent framework\nfor cultural lifelong learning through explicit perspective taking. We\nintroduce three key innovations: (1) a structured theory of mind representation\nlinking percepts, beliefs, desires, and actions; (2) natural inter-agent\ncommunication; and (3) a multi-component memory system. Following the cultural\nlearning framework, we test MindForge in both instructive and collaborative\nsettings within Minecraft. In an instructive setting with GPT-4, MindForge\nagents powered by open-weight LLMs significantly outperform their Voyager\ncounterparts in basic tasks yielding $3\\times$ more tech-tree milestones and\ncollecting $2.3\\times$ more unique items than the Voyager baseline.\nFurthermore, in fully \\textit{collaborative} settings, we find that the\nperformance of two underachieving agents improves with more communication\nrounds, echoing the Condorcet Jury Theorem. MindForge agents demonstrate\nsophisticated behaviors, including expert-novice knowledge transfer,\ncollaborative problem solving, and adaptation to out-of-distribution tasks\nthrough accumulated cultural experiences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied agents powered by large language models (LLMs), such as Voyager,\npromise open-ended competence in worlds such as Minecraft. However, when\npowered by open-weight LLMs they still falter on elementary tasks after\ndomain-specific fine-tuning. We propose MindForge, a generative-agent framework\nfor cultural lifelong learning through explicit perspective taking. We\nintroduce three key innovations: (1) a structured theory of mind representation\nlinking percepts, beliefs, desires, and actions; (2) natural inter-agent\ncommunication; and (3) a multi-component memory system. Following the cultural\nlearning framework, we test MindForge in both instructive and collaborative\nsettings within Minecraft. In an instructive setting with GPT-4, MindForge\nagents powered by open-weight LLMs significantly outperform their Voyager\ncounterparts in basic tasks yielding $3\\times$ more tech-tree milestones and\ncollecting $2.3\\times$ more unique items than the Voyager baseline.\nFurthermore, in fully \\textit{collaborative} settings, we find that the\nperformance of two underachieving agents improves with more communication\nrounds, echoing the Condorcet Jury Theorem. MindForge agents demonstrate\nsophisticated behaviors, including expert-novice knowledge transfer,\ncollaborative problem solving, and adaptation to out-of-distribution tasks\nthrough accumulated cultural experiences."
                },
                "authors": [
                    {
                        "name": "Mircea Lic"
                    },
                    {
                        "name": "Ojas Shirekar"
                    },
                    {
                        "name": "Baptiste Colle"
                    },
                    {
                        "name": "Chirag Raman"
                    }
                ],
                "author_detail": {
                    "name": "Chirag Raman"
                },
                "author": "Chirag Raman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12977v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12977v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09755v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09755v1",
                "updated": "2025-06-11T13:57:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    57,
                    26,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T13:57:26Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    57,
                    26,
                    2,
                    162,
                    0
                ],
                "title": "Intelligent Design 4.0: Paradigm Evolution Toward the Agentic AI Era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent Design 4.0: Paradigm Evolution Toward the Agentic AI Era"
                },
                "summary": "Research and practice in Intelligent Design (ID) have significantly enhanced\nengineering innovation, efficiency, quality, and productivity over recent\ndecades, fundamentally reshaping how engineering designers think, behave, and\ninteract with design processes. The recent emergence of Foundation Models\n(FMs), particularly Large Language Models (LLMs), has demonstrated general\nknowledge-based reasoning capabilities, and open new paths and avenues for\nfurther transformation in engineering design. In this context, this paper\nintroduces Intelligent Design 4.0 (ID 4.0) as an emerging paradigm empowered by\nagentic AI systems. We review the historical evolution of ID across four\ndistinct stages: rule-based expert systems, task-specific machine learning\nmodels, large-scale foundation AI models, and the recent emerging paradigm of\nmulti-agent collaboration. We propose a conceptual framework for ID 4.0 and\ndiscuss its potential to support end-to-end automation of engineering design\nprocesses through coordinated, autonomous multi-agent-based systems.\nFurthermore, we discuss future perspectives to enhance and fully realize ID\n4.0's potential, including more complex design scenarios, more practical design\nimplementations, novel agent coordination mechanisms, and autonomous design\ngoal-setting with better human value alignment. In sum, these insights lay a\nfoundation for advancing Intelligent Design toward greater adaptivity,\nautonomy, and effectiveness in addressing increasingly complex design\nchallenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research and practice in Intelligent Design (ID) have significantly enhanced\nengineering innovation, efficiency, quality, and productivity over recent\ndecades, fundamentally reshaping how engineering designers think, behave, and\ninteract with design processes. The recent emergence of Foundation Models\n(FMs), particularly Large Language Models (LLMs), has demonstrated general\nknowledge-based reasoning capabilities, and open new paths and avenues for\nfurther transformation in engineering design. In this context, this paper\nintroduces Intelligent Design 4.0 (ID 4.0) as an emerging paradigm empowered by\nagentic AI systems. We review the historical evolution of ID across four\ndistinct stages: rule-based expert systems, task-specific machine learning\nmodels, large-scale foundation AI models, and the recent emerging paradigm of\nmulti-agent collaboration. We propose a conceptual framework for ID 4.0 and\ndiscuss its potential to support end-to-end automation of engineering design\nprocesses through coordinated, autonomous multi-agent-based systems.\nFurthermore, we discuss future perspectives to enhance and fully realize ID\n4.0's potential, including more complex design scenarios, more practical design\nimplementations, novel agent coordination mechanisms, and autonomous design\ngoal-setting with better human value alignment. In sum, these insights lay a\nfoundation for advancing Intelligent Design toward greater adaptivity,\nautonomy, and effectiveness in addressing increasingly complex design\nchallenges."
                },
                "authors": [
                    {
                        "name": "Shuo Jiang"
                    },
                    {
                        "name": "Min Xie"
                    },
                    {
                        "name": "Frank Youhua Chen"
                    },
                    {
                        "name": "Jian Ma"
                    },
                    {
                        "name": "Jianxi Luo"
                    }
                ],
                "author_detail": {
                    "name": "Jianxi Luo"
                },
                "author": "Jianxi Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09755v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09755v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09749v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09749v1",
                "updated": "2025-06-11T13:53:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    53,
                    35,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T13:53:35Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    53,
                    35,
                    2,
                    162,
                    0
                ],
                "title": "Large Language Models for Design Structure Matrix Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Design Structure Matrix Optimization"
                },
                "summary": "In complex engineering systems, the interdependencies among components or\ndevelopment activities are often modeled and analyzed using Design Structure\nMatrix (DSM). Reorganizing elements within a DSM to minimize feedback loops and\nenhance modularity or process efficiency constitutes a challenging\ncombinatorial optimization (CO) problem in engineering design and operations.\nAs problem sizes increase and dependency networks become more intricate,\ntraditional optimization methods that solely use mathematical heuristics often\nfail to capture the contextual nuances and struggle to deliver effective\nsolutions. In this study, we explore the potential of Large Language Models\n(LLMs) for helping solve such CO problems by leveraging their capabilities for\nadvanced reasoning and contextual understanding. We propose a novel LLM-based\nframework that integrates network topology with contextual domain knowledge for\niterative optimization of DSM element sequencing - a common CO problem.\nExperiments on various DSM cases show that our method consistently achieves\nfaster convergence and superior solution quality compared to both stochastic\nand deterministic baselines. Notably, we find that incorporating contextual\ndomain knowledge significantly enhances optimization performance regardless of\nthe chosen LLM backbone. These findings highlight the potential of LLMs to\nsolve complex engineering CO problems by combining semantic and mathematical\nreasoning. This approach paves the way towards a new paradigm in LLM-based\nengineering design optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In complex engineering systems, the interdependencies among components or\ndevelopment activities are often modeled and analyzed using Design Structure\nMatrix (DSM). Reorganizing elements within a DSM to minimize feedback loops and\nenhance modularity or process efficiency constitutes a challenging\ncombinatorial optimization (CO) problem in engineering design and operations.\nAs problem sizes increase and dependency networks become more intricate,\ntraditional optimization methods that solely use mathematical heuristics often\nfail to capture the contextual nuances and struggle to deliver effective\nsolutions. In this study, we explore the potential of Large Language Models\n(LLMs) for helping solve such CO problems by leveraging their capabilities for\nadvanced reasoning and contextual understanding. We propose a novel LLM-based\nframework that integrates network topology with contextual domain knowledge for\niterative optimization of DSM element sequencing - a common CO problem.\nExperiments on various DSM cases show that our method consistently achieves\nfaster convergence and superior solution quality compared to both stochastic\nand deterministic baselines. Notably, we find that incorporating contextual\ndomain knowledge significantly enhances optimization performance regardless of\nthe chosen LLM backbone. These findings highlight the potential of LLMs to\nsolve complex engineering CO problems by combining semantic and mathematical\nreasoning. This approach paves the way towards a new paradigm in LLM-based\nengineering design optimization."
                },
                "authors": [
                    {
                        "name": "Shuo Jiang"
                    },
                    {
                        "name": "Min Xie"
                    },
                    {
                        "name": "Jianxi Luo"
                    }
                ],
                "author_detail": {
                    "name": "Jianxi Luo"
                },
                "author": "Jianxi Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09749v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09749v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04211v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04211v2",
                "updated": "2025-06-11T13:51:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    51,
                    54,
                    2,
                    162,
                    0
                ],
                "published": "2025-04-05T15:44:57Z",
                "published_parsed": [
                    2025,
                    4,
                    5,
                    15,
                    44,
                    57,
                    5,
                    95,
                    0
                ],
                "title": "Accelerated Bayesian Inference for Pulsar Timing Arrays: Normalizing\n  Flows for Rapid Model Comparison Across Stochastic Gravitational-Wave\n  Background Sources",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerated Bayesian Inference for Pulsar Timing Arrays: Normalizing\n  Flows for Rapid Model Comparison Across Stochastic Gravitational-Wave\n  Background Sources"
                },
                "summary": "The recent detection of nanohertz stochastic gravitational-wave backgrounds\n(SGWBs) by pulsar timing arrays (PTAs) promises unique insights into\nastrophysical and cosmological origins. However, traditional Markov Chain Monte\nCarlo (MCMC) approaches become prohibitively expensive for large datasets. We\nemploy a normalizing flow (NF)-based machine learning framework to accelerate\nBayesian inference in PTA analyses. For the first time, we perform Bayesian\nmodel comparison across SGWB source models in the framework of machine learning\nby training NF architectures on the PTA dataset (NANOGrav 15-year) and enabling\ndirect evidence estimation via learned harmonic mean estimators. Our examples\ninclude 10 conventional SGWB source models such as supermassive black hole\nbinaries, power-law spectrum, cosmic strings, domain walls, scalar-induced GWs,\nfirst-order phase transitions, and dual scenario/inflationary gravitational\nwave. Our approach jointly infers 20 red noise parameters and 2 SGWB parameters\nper model in $\\sim 20$\\,hours (including training), compared to $\\sim 10$\\,days\nwith MCMC. Critically, the NF method preserves rigorous model selection\naccuracy, with small Hellinger distances ($\\lesssim 0.3$) relative to MCMC\nposteriors, and reproduces MCMC-based Bayes factors across all tested\nscenarios. This scalable technique for SGWB source comparison will be essential\nfor future PTA expansions and next-generation arrays such as the SKA, offering\norders-of-magnitude efficiency gains without sacrificing physical\ninterpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent detection of nanohertz stochastic gravitational-wave backgrounds\n(SGWBs) by pulsar timing arrays (PTAs) promises unique insights into\nastrophysical and cosmological origins. However, traditional Markov Chain Monte\nCarlo (MCMC) approaches become prohibitively expensive for large datasets. We\nemploy a normalizing flow (NF)-based machine learning framework to accelerate\nBayesian inference in PTA analyses. For the first time, we perform Bayesian\nmodel comparison across SGWB source models in the framework of machine learning\nby training NF architectures on the PTA dataset (NANOGrav 15-year) and enabling\ndirect evidence estimation via learned harmonic mean estimators. Our examples\ninclude 10 conventional SGWB source models such as supermassive black hole\nbinaries, power-law spectrum, cosmic strings, domain walls, scalar-induced GWs,\nfirst-order phase transitions, and dual scenario/inflationary gravitational\nwave. Our approach jointly infers 20 red noise parameters and 2 SGWB parameters\nper model in $\\sim 20$\\,hours (including training), compared to $\\sim 10$\\,days\nwith MCMC. Critically, the NF method preserves rigorous model selection\naccuracy, with small Hellinger distances ($\\lesssim 0.3$) relative to MCMC\nposteriors, and reproduces MCMC-based Bayes factors across all tested\nscenarios. This scalable technique for SGWB source comparison will be essential\nfor future PTA expansions and next-generation arrays such as the SKA, offering\norders-of-magnitude efficiency gains without sacrificing physical\ninterpretability."
                },
                "authors": [
                    {
                        "name": "Junrong Lai"
                    },
                    {
                        "name": "Changhong Li"
                    }
                ],
                "author_detail": {
                    "name": "Changhong Li"
                },
                "author": "Changhong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04211v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04211v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09742v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09742v1",
                "updated": "2025-06-11T13:48:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    48,
                    25,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T13:48:25Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    48,
                    25,
                    2,
                    162,
                    0
                ],
                "title": "Feature Engineering for Agents: An Adaptive Cognitive Architecture for\n  Interpretable ML Monitoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature Engineering for Agents: An Adaptive Cognitive Architecture for\n  Interpretable ML Monitoring"
                },
                "summary": "Monitoring Machine Learning (ML) models in production environments is\ncrucial, yet traditional approaches often yield verbose, low-interpretability\noutputs that hinder effective decision-making. We propose a cognitive\narchitecture for ML monitoring that applies feature engineering principles to\nagents based on Large Language Models (LLMs), significantly enhancing the\ninterpretability of monitoring outputs. Central to our approach is a Decision\nProcedure module that simulates feature engineering through three key steps:\nRefactor, Break Down, and Compile. The Refactor step improves data\nrepresentation to better capture feature semantics, allowing the LLM to focus\non salient aspects of the monitoring data while reducing noise and irrelevant\ninformation. Break Down decomposes complex information for detailed analysis,\nand Compile integrates sub-insights into clear, interpretable outputs. This\nprocess leads to a more deterministic planning approach, reducing dependence on\nLLM-generated planning, which can sometimes be inconsistent and overly general.\nThe combination of feature engineering-driven planning and selective LLM\nutilization results in a robust decision support system, capable of providing\nhighly interpretable and actionable insights. Experiments using multiple LLMs\ndemonstrate the efficacy of our approach, achieving significantly higher\naccuracy compared to various baselines across several domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monitoring Machine Learning (ML) models in production environments is\ncrucial, yet traditional approaches often yield verbose, low-interpretability\noutputs that hinder effective decision-making. We propose a cognitive\narchitecture for ML monitoring that applies feature engineering principles to\nagents based on Large Language Models (LLMs), significantly enhancing the\ninterpretability of monitoring outputs. Central to our approach is a Decision\nProcedure module that simulates feature engineering through three key steps:\nRefactor, Break Down, and Compile. The Refactor step improves data\nrepresentation to better capture feature semantics, allowing the LLM to focus\non salient aspects of the monitoring data while reducing noise and irrelevant\ninformation. Break Down decomposes complex information for detailed analysis,\nand Compile integrates sub-insights into clear, interpretable outputs. This\nprocess leads to a more deterministic planning approach, reducing dependence on\nLLM-generated planning, which can sometimes be inconsistent and overly general.\nThe combination of feature engineering-driven planning and selective LLM\nutilization results in a robust decision support system, capable of providing\nhighly interpretable and actionable insights. Experiments using multiple LLMs\ndemonstrate the efficacy of our approach, achieving significantly higher\naccuracy compared to various baselines across several domains."
                },
                "authors": [
                    {
                        "name": "Gusseppe Bravo-Rocca"
                    },
                    {
                        "name": "Peini Liu"
                    },
                    {
                        "name": "Jordi Guitart"
                    },
                    {
                        "name": "Rodrigo M Carrillo-Larco"
                    },
                    {
                        "name": "Ajay Dholakia"
                    },
                    {
                        "name": "David Ellison"
                    }
                ],
                "author_detail": {
                    "name": "David Ellison"
                },
                "author": "David Ellison",
                "arxiv_comment": "Accepted at AAMAS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09742v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09742v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07943v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07943v2",
                "updated": "2025-06-11T13:48:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    48,
                    23,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-09T17:05:02Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    17,
                    5,
                    2,
                    0,
                    160,
                    0
                ],
                "title": "Decoupling the Image Perception and Multimodal Reasoning for Reasoning\n  Segmentation with Digital Twin Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoupling the Image Perception and Multimodal Reasoning for Reasoning\n  Segmentation with Digital Twin Representations"
                },
                "summary": "Reasoning Segmentation (RS) is a multimodal vision-text task that requires\nsegmenting objects based on implicit text queries, demanding both precise\nvisual perception and vision-text reasoning capabilities. Current RS approaches\nrely on fine-tuning vision-language models (VLMs) for both perception and\nreasoning, but their tokenization of images fundamentally disrupts continuous\nspatial relationships between objects. We introduce DTwinSeger, a novel RS\napproach that leverages Digital Twin (DT) representation as an intermediate\nlayer to decouple perception from reasoning. Innovatively, DTwinSeger\nreformulates RS as a two-stage process, where the first transforms the image\ninto a structured DT representation that preserves spatial relationships and\nsemantic properties and then employs a Large Language Model (LLM) to perform\nexplicit reasoning over this representation to identify target objects. We\npropose a supervised fine-tuning method specifically for LLM with DT\nrepresentation, together with a corresponding fine-tuning dataset Seg-DT, to\nenhance the LLM's reasoning capabilities with DT representations. Experiments\nshow that our method can achieve state-of-the-art performance on two image RS\nbenchmarks and three image referring segmentation benchmarks. It yields that DT\nrepresentation functions as an effective bridge between vision and text,\nenabling complex multimodal reasoning tasks to be accomplished solely with an\nLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Segmentation (RS) is a multimodal vision-text task that requires\nsegmenting objects based on implicit text queries, demanding both precise\nvisual perception and vision-text reasoning capabilities. Current RS approaches\nrely on fine-tuning vision-language models (VLMs) for both perception and\nreasoning, but their tokenization of images fundamentally disrupts continuous\nspatial relationships between objects. We introduce DTwinSeger, a novel RS\napproach that leverages Digital Twin (DT) representation as an intermediate\nlayer to decouple perception from reasoning. Innovatively, DTwinSeger\nreformulates RS as a two-stage process, where the first transforms the image\ninto a structured DT representation that preserves spatial relationships and\nsemantic properties and then employs a Large Language Model (LLM) to perform\nexplicit reasoning over this representation to identify target objects. We\npropose a supervised fine-tuning method specifically for LLM with DT\nrepresentation, together with a corresponding fine-tuning dataset Seg-DT, to\nenhance the LLM's reasoning capabilities with DT representations. Experiments\nshow that our method can achieve state-of-the-art performance on two image RS\nbenchmarks and three image referring segmentation benchmarks. It yields that DT\nrepresentation functions as an effective bridge between vision and text,\nenabling complex multimodal reasoning tasks to be accomplished solely with an\nLLM."
                },
                "authors": [
                    {
                        "name": "Yizhen Li"
                    },
                    {
                        "name": "Dell Zhang"
                    },
                    {
                        "name": "Xuelong Li"
                    },
                    {
                        "name": "Yiqing Shen"
                    }
                ],
                "author_detail": {
                    "name": "Yiqing Shen"
                },
                "author": "Yiqing Shen",
                "arxiv_comment": "This work was submitted without the consent of all co-authors. We\n  request withdrawal until all parties agree",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07943v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07943v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09738v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09738v1",
                "updated": "2025-06-11T13:41:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    41,
                    29,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T13:41:29Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    41,
                    29,
                    2,
                    162,
                    0
                ],
                "title": "Towards Multi-modal Graph Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Multi-modal Graph Large Language Model"
                },
                "summary": "Multi-modal graphs, which integrate diverse multi-modal features and\nrelations, are ubiquitous in real-world applications. However, existing\nmulti-modal graph learning methods are typically trained from scratch for\nspecific graph data and tasks, failing to generalize across various multi-modal\ngraph data and tasks. To bridge this gap, we explore the potential of\nMulti-modal Graph Large Language Models (MG-LLM) to unify and generalize across\ndiverse multi-modal graph data and tasks. We propose a unified framework of\nmulti-modal graph data, task, and model, discovering the inherent\nmulti-granularity and multi-scale characteristics in multi-modal graphs.\nSpecifically, we present five key desired characteristics for MG-LLM: 1)\nunified space for multi-modal structures and attributes, 2) capability of\nhandling diverse multi-modal graph tasks, 3) multi-modal graph in-context\nlearning, 4) multi-modal graph interaction with natural language, and 5)\nmulti-modal graph reasoning. We then elaborate on the key challenges, review\nrelated works, and highlight promising future research directions towards\nrealizing these ambitious characteristics. Finally, we summarize existing\nmulti-modal graph datasets pertinent for model training. We believe this paper\ncan contribute to the ongoing advancement of the research towards MG-LLM for\ngeneralization across multi-modal graph data and tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal graphs, which integrate diverse multi-modal features and\nrelations, are ubiquitous in real-world applications. However, existing\nmulti-modal graph learning methods are typically trained from scratch for\nspecific graph data and tasks, failing to generalize across various multi-modal\ngraph data and tasks. To bridge this gap, we explore the potential of\nMulti-modal Graph Large Language Models (MG-LLM) to unify and generalize across\ndiverse multi-modal graph data and tasks. We propose a unified framework of\nmulti-modal graph data, task, and model, discovering the inherent\nmulti-granularity and multi-scale characteristics in multi-modal graphs.\nSpecifically, we present five key desired characteristics for MG-LLM: 1)\nunified space for multi-modal structures and attributes, 2) capability of\nhandling diverse multi-modal graph tasks, 3) multi-modal graph in-context\nlearning, 4) multi-modal graph interaction with natural language, and 5)\nmulti-modal graph reasoning. We then elaborate on the key challenges, review\nrelated works, and highlight promising future research directions towards\nrealizing these ambitious characteristics. Finally, we summarize existing\nmulti-modal graph datasets pertinent for model training. We believe this paper\ncan contribute to the ongoing advancement of the research towards MG-LLM for\ngeneralization across multi-modal graph data and tasks."
                },
                "authors": [
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Zeyang Zhang"
                    },
                    {
                        "name": "Linxin Xiao"
                    },
                    {
                        "name": "Haibo Chen"
                    },
                    {
                        "name": "Chendi Ge"
                    },
                    {
                        "name": "Wenwu Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Wenwu Zhu"
                },
                "author": "Wenwu Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09738v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09738v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09713v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09713v1",
                "updated": "2025-06-11T13:25:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    25,
                    36,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T13:25:36Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    25,
                    36,
                    2,
                    162,
                    0
                ],
                "title": "A First Look at Bugs in LLM Inference Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A First Look at Bugs in LLM Inference Engines"
                },
                "summary": "Large language model-specific inference engines (in short as \\emph{LLM\ninference engines}) have become a fundamental component of modern AI\ninfrastructure, enabling the deployment of LLM-powered applications (LLM apps)\nacross cloud and local devices. Despite their critical role, LLM inference\nengines are prone to bugs due to the immense resource demands of LLMs and the\ncomplexities of cross-platform compatibility. However, a systematic\nunderstanding of these bugs remains lacking. To bridge this gap, we present the\nfirst empirical study on bugs in LLM inference engines. We mine official\nrepositories of 5 widely adopted LLM inference engines, constructing a\ncomprehensive dataset of 929 real-world bugs. Through a rigorous open coding\nprocess, we analyze these bugs to uncover their symptoms, root causes, and\ncommonality. Our findings reveal six major bug symptoms and a taxonomy of 28\nroot causes, shedding light on the key challenges in bug detection and location\nwithin LLM inference engines. Based on these insights, we propose a series of\nactionable implications for researchers, inference engine vendors, and LLM app\ndevelopers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model-specific inference engines (in short as \\emph{LLM\ninference engines}) have become a fundamental component of modern AI\ninfrastructure, enabling the deployment of LLM-powered applications (LLM apps)\nacross cloud and local devices. Despite their critical role, LLM inference\nengines are prone to bugs due to the immense resource demands of LLMs and the\ncomplexities of cross-platform compatibility. However, a systematic\nunderstanding of these bugs remains lacking. To bridge this gap, we present the\nfirst empirical study on bugs in LLM inference engines. We mine official\nrepositories of 5 widely adopted LLM inference engines, constructing a\ncomprehensive dataset of 929 real-world bugs. Through a rigorous open coding\nprocess, we analyze these bugs to uncover their symptoms, root causes, and\ncommonality. Our findings reveal six major bug symptoms and a taxonomy of 28\nroot causes, shedding light on the key challenges in bug detection and location\nwithin LLM inference engines. Based on these insights, we propose a series of\nactionable implications for researchers, inference engine vendors, and LLM app\ndevelopers."
                },
                "authors": [
                    {
                        "name": "Mugeng Liu"
                    },
                    {
                        "name": "Siqi Zhong"
                    },
                    {
                        "name": "Weichen Bi"
                    },
                    {
                        "name": "Yixuan Zhang"
                    },
                    {
                        "name": "Zhiyang Chen"
                    },
                    {
                        "name": "Zhenpeng Chen"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Yun Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yun Ma"
                },
                "author": "Yun Ma",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09713v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09713v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09707v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09707v1",
                "updated": "2025-06-11T13:21:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    21,
                    6,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T13:21:06Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    21,
                    6,
                    2,
                    162,
                    0
                ],
                "title": "Fine-Tuning Large Audio-Language Models with LoRA for Precise Temporal\n  Localization of Prolonged Exposure Therapy Elements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Tuning Large Audio-Language Models with LoRA for Precise Temporal\n  Localization of Prolonged Exposure Therapy Elements"
                },
                "summary": "Prolonged Exposure (PE) therapy is an effective treatment for post-traumatic\nstress disorder (PTSD), but evaluating therapist fidelity remains\nlabor-intensive due to the need for manual review of session recordings. We\npresent a method for the automatic temporal localization of key PE fidelity\nelements -- identifying their start and stop times -- directly from session\naudio and transcripts. Our approach fine-tunes a large pre-trained\naudio-language model, Qwen2-Audio, using Low-Rank Adaptation (LoRA) to process\nfocused 30-second windows of audio-transcript input. Fidelity labels for three\ncore protocol phases -- therapist orientation (P1), imaginal exposure (P2), and\npost-imaginal processing (P3) -- are generated via LLM-based prompting and\nverified by trained raters. The model is trained to predict normalized boundary\noffsets using soft supervision guided by task-specific prompts. On a dataset of\n313 real PE sessions, our best configuration (LoRA rank 8, 30s windows)\nachieves a mean absolute error (MAE) of 5.3 seconds across tasks. We further\nanalyze the effects of window size and LoRA rank, highlighting the importance\nof context granularity and model adaptation. This work introduces a scalable\nframework for fidelity tracking in PE therapy, with potential to support\nclinician training, supervision, and quality assurance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prolonged Exposure (PE) therapy is an effective treatment for post-traumatic\nstress disorder (PTSD), but evaluating therapist fidelity remains\nlabor-intensive due to the need for manual review of session recordings. We\npresent a method for the automatic temporal localization of key PE fidelity\nelements -- identifying their start and stop times -- directly from session\naudio and transcripts. Our approach fine-tunes a large pre-trained\naudio-language model, Qwen2-Audio, using Low-Rank Adaptation (LoRA) to process\nfocused 30-second windows of audio-transcript input. Fidelity labels for three\ncore protocol phases -- therapist orientation (P1), imaginal exposure (P2), and\npost-imaginal processing (P3) -- are generated via LLM-based prompting and\nverified by trained raters. The model is trained to predict normalized boundary\noffsets using soft supervision guided by task-specific prompts. On a dataset of\n313 real PE sessions, our best configuration (LoRA rank 8, 30s windows)\nachieves a mean absolute error (MAE) of 5.3 seconds across tasks. We further\nanalyze the effects of window size and LoRA rank, highlighting the importance\nof context granularity and model adaptation. This work introduces a scalable\nframework for fidelity tracking in PE therapy, with potential to support\nclinician training, supervision, and quality assurance."
                },
                "authors": [
                    {
                        "name": "Suhas BN"
                    },
                    {
                        "name": "Andrew M. Sherrill"
                    },
                    {
                        "name": "Jyoti Alaparthi"
                    },
                    {
                        "name": "Dominik Mattioli"
                    },
                    {
                        "name": "Rosa I. Arriaga"
                    },
                    {
                        "name": "Chris W. Wiese"
                    },
                    {
                        "name": "Saeed Abdullah"
                    }
                ],
                "author_detail": {
                    "name": "Saeed Abdullah"
                },
                "author": "Saeed Abdullah",
                "arxiv_comment": "5 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09707v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09707v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.5.4; H.5.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14917v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14917v2",
                "updated": "2025-06-11T13:19:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    19,
                    51,
                    2,
                    162,
                    0
                ],
                "published": "2024-06-21T07:20:51Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    7,
                    20,
                    51,
                    4,
                    173,
                    0
                ],
                "title": "LLM2TEA: Agentic AI Designer Finds Innovative Objects with Generative\n  Evolutionary Multitasking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM2TEA: Agentic AI Designer Finds Innovative Objects with Generative\n  Evolutionary Multitasking"
                },
                "summary": "In this paper, we introduce LLM-driven MultiTask Evolutionary Algorithm\n(LLM2TEA), the first agentic AI designer within a generative evolutionary\nmultitasking (GEM) framework that promotes the crossover and synergy of designs\nfrom multiple domains, leading to innovative solutions that transcend\nindividual disciplines. Of particular interest is the discovery of objects that\nare not only innovative but also conform to the physical specifications of the\nreal world in science and engineering. LLM2TEA comprises a large language model\nto initialize a population of genotypes (defined by text prompts) describing\nthe objects of interest, a text-to-3D generative model to produce phenotypes\nfrom these prompts, a classifier to interpret the semantic representations of\nthe objects, and a physics simulation model to assess their physical\nproperties. We propose several novel LLM-based multitask evolutionary operators\nto guide the search toward the discovery of high-performing practical objects.\nExperimental results in conceptual design optimization validate the\neffectiveness of LLM2TEA, revealing from 97\\% to 174\\% improvement in the\ndiversity of innovative objects compared to the present text-to-3D generative\nmodel baseline. In addition, more than 73\\% of the generated designs have\nbetter physical performance than the top 1\\% percentile of the designs\ngenerated in the baseline. Moreover, LLM2TEA generates designs that are not\nonly aesthetically creative but also functional in real-world applications.\nSeveral of these designs have been successfully 3D-printed, emphasizing the\nproposed approach's capacity to transform AI-generated outputs into tangible\nphysical objects. The designs produced by LLM2TEA meets practical requirements\nwhile showcasing creative and innovative features, underscoring its potential\napplications in complex design optimization and discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce LLM-driven MultiTask Evolutionary Algorithm\n(LLM2TEA), the first agentic AI designer within a generative evolutionary\nmultitasking (GEM) framework that promotes the crossover and synergy of designs\nfrom multiple domains, leading to innovative solutions that transcend\nindividual disciplines. Of particular interest is the discovery of objects that\nare not only innovative but also conform to the physical specifications of the\nreal world in science and engineering. LLM2TEA comprises a large language model\nto initialize a population of genotypes (defined by text prompts) describing\nthe objects of interest, a text-to-3D generative model to produce phenotypes\nfrom these prompts, a classifier to interpret the semantic representations of\nthe objects, and a physics simulation model to assess their physical\nproperties. We propose several novel LLM-based multitask evolutionary operators\nto guide the search toward the discovery of high-performing practical objects.\nExperimental results in conceptual design optimization validate the\neffectiveness of LLM2TEA, revealing from 97\\% to 174\\% improvement in the\ndiversity of innovative objects compared to the present text-to-3D generative\nmodel baseline. In addition, more than 73\\% of the generated designs have\nbetter physical performance than the top 1\\% percentile of the designs\ngenerated in the baseline. Moreover, LLM2TEA generates designs that are not\nonly aesthetically creative but also functional in real-world applications.\nSeveral of these designs have been successfully 3D-printed, emphasizing the\nproposed approach's capacity to transform AI-generated outputs into tangible\nphysical objects. The designs produced by LLM2TEA meets practical requirements\nwhile showcasing creative and innovative features, underscoring its potential\napplications in complex design optimization and discovery."
                },
                "authors": [
                    {
                        "name": "Melvin Wong"
                    },
                    {
                        "name": "Jiao Liu"
                    },
                    {
                        "name": "Thiago Rios"
                    },
                    {
                        "name": "Stefan Menzel"
                    },
                    {
                        "name": "Yew Soon Ong"
                    }
                ],
                "author_detail": {
                    "name": "Yew Soon Ong"
                },
                "author": "Yew Soon Ong",
                "arxiv_comment": "This work has been submitted to the IEEE for review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14917v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14917v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11223v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11223v4",
                "updated": "2025-06-11T13:19:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    19,
                    22,
                    2,
                    162,
                    0
                ],
                "published": "2025-01-20T02:16:19Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    2,
                    16,
                    19,
                    0,
                    20,
                    0
                ],
                "title": "Reasoning Language Models: A Blueprint",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Language Models: A Blueprint"
                },
                "summary": "Reasoning language models (RLMs), also known as Large Reasoning Models\n(LRMs), such as OpenAI's o1 and o3, DeepSeek-R1, and Alibaba's QwQ, have\nredefined AI's problem-solving capabilities by extending LLMs with advanced\nreasoning mechanisms. Yet, their high costs, proprietary nature, and complex\narchitectures - uniquely combining reinforcement learning (RL), search\nheuristics, and LLMs - present accessibility and scalability challenges. To\naddress these, we propose a comprehensive blueprint that organizes RLM\ncomponents into a modular framework, based on a survey and analysis of all RLM\nworks. This blueprint incorporates diverse reasoning structures (chains, trees,\ngraphs, and nested forms), reasoning strategies (e.g., Monte Carlo Tree Search,\nBeam Search), RL concepts (policy, value models and others), supervision\nschemes (Outcome-Based and Process-Based Supervision), and other related\nconcepts (e.g., Test-Time Compute, Retrieval-Augmented Generation, agent\ntools). We also provide detailed mathematical formulations and algorithmic\nspecifications to simplify RLM implementation. By showing how schemes like\nLLaMA-Berry, QwQ, Journey Learning, and Graph of Thoughts fit as special cases,\nwe demonstrate the blueprint's versatility and unifying potential. To\nillustrate its utility, we introduce x1, a modular implementation for rapid RLM\nprototyping and experimentation. Using x1 and a literature review, we provide\nkey insights, such as multi-phase training for policy and value models, and the\nimportance of familiar training distributions. Finally, we discuss scalable RLM\ncloud deployments and we outline how RLMs can integrate with a broader LLM\necosystem. Our work demystifies RLM construction, democratizes advanced\nreasoning capabilities, and fosters innovation, aiming to mitigate the gap\nbetween \"rich AI\" and \"poor AI\" by lowering barriers to RLM design and\nexperimentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning language models (RLMs), also known as Large Reasoning Models\n(LRMs), such as OpenAI's o1 and o3, DeepSeek-R1, and Alibaba's QwQ, have\nredefined AI's problem-solving capabilities by extending LLMs with advanced\nreasoning mechanisms. Yet, their high costs, proprietary nature, and complex\narchitectures - uniquely combining reinforcement learning (RL), search\nheuristics, and LLMs - present accessibility and scalability challenges. To\naddress these, we propose a comprehensive blueprint that organizes RLM\ncomponents into a modular framework, based on a survey and analysis of all RLM\nworks. This blueprint incorporates diverse reasoning structures (chains, trees,\ngraphs, and nested forms), reasoning strategies (e.g., Monte Carlo Tree Search,\nBeam Search), RL concepts (policy, value models and others), supervision\nschemes (Outcome-Based and Process-Based Supervision), and other related\nconcepts (e.g., Test-Time Compute, Retrieval-Augmented Generation, agent\ntools). We also provide detailed mathematical formulations and algorithmic\nspecifications to simplify RLM implementation. By showing how schemes like\nLLaMA-Berry, QwQ, Journey Learning, and Graph of Thoughts fit as special cases,\nwe demonstrate the blueprint's versatility and unifying potential. To\nillustrate its utility, we introduce x1, a modular implementation for rapid RLM\nprototyping and experimentation. Using x1 and a literature review, we provide\nkey insights, such as multi-phase training for policy and value models, and the\nimportance of familiar training distributions. Finally, we discuss scalable RLM\ncloud deployments and we outline how RLMs can integrate with a broader LLM\necosystem. Our work demystifies RLM construction, democratizes advanced\nreasoning capabilities, and fosters innovation, aiming to mitigate the gap\nbetween \"rich AI\" and \"poor AI\" by lowering barriers to RLM design and\nexperimentation."
                },
                "authors": [
                    {
                        "name": "Maciej Besta"
                    },
                    {
                        "name": "Julia Barth"
                    },
                    {
                        "name": "Eric Schreiber"
                    },
                    {
                        "name": "Ales Kubicek"
                    },
                    {
                        "name": "Afonso Catarino"
                    },
                    {
                        "name": "Robert Gerstenberger"
                    },
                    {
                        "name": "Piotr Nyczyk"
                    },
                    {
                        "name": "Patrick Iff"
                    },
                    {
                        "name": "Yueling Li"
                    },
                    {
                        "name": "Sam Houliston"
                    },
                    {
                        "name": "Tomasz Sternal"
                    },
                    {
                        "name": "Marcin Copik"
                    },
                    {
                        "name": "Grzegorz Kwaniewski"
                    },
                    {
                        "name": "Jrgen Mller"
                    },
                    {
                        "name": "ukasz Flis"
                    },
                    {
                        "name": "Hannes Eberhard"
                    },
                    {
                        "name": "Zixuan Chen"
                    },
                    {
                        "name": "Hubert Niewiadomski"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11223v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11223v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09701v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09701v1",
                "updated": "2025-06-11T13:14:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    14,
                    1,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T13:14:01Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    14,
                    1,
                    2,
                    162,
                    0
                ],
                "title": "TRIDENT: Temporally Restricted Inference via DFA-Enhanced Neural\n  Traversal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRIDENT: Temporally Restricted Inference via DFA-Enhanced Neural\n  Traversal"
                },
                "summary": "Large Language Models (LLMs) and other neural architectures have achieved\nimpressive results across a variety of generative and classification tasks.\nHowever, they remain fundamentally ill-equipped to ensure that their outputs\nsatisfy temporal constraints, such as those expressible in Linear Temporal\nLogic over finite traces (LTLf). In this paper, we introduce TRIDENT: a general\nand model-agnostic inference-time algorithm that guarantees compliance with\nsuch constraints without requiring any retraining. TRIDENT compiles LTLf\nformulas into a Deterministic Finite Automaton (DFA), which is used to guide a\nconstrained variant of beam search. At each decoding step, transitions that\nwould lead to constraint violations are masked, while remaining paths are\ndynamically re-ranked based on both the model's probabilities and the DFA's\nacceptance structure. We formally prove that the resulting sequences are\nguaranteed to satisfy the given LTLf constraints, and we empirically\ndemonstrate that TRIDENT also improves output quality. We validate our approach\non two distinct tasks: temporally constrained image-stream classification and\ncontrolled text generation. In both settings, TRIDENT achieves perfect\nconstraint satisfaction, while comparison with the state of the art shows\nimproved efficiency and high standard quality metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) and other neural architectures have achieved\nimpressive results across a variety of generative and classification tasks.\nHowever, they remain fundamentally ill-equipped to ensure that their outputs\nsatisfy temporal constraints, such as those expressible in Linear Temporal\nLogic over finite traces (LTLf). In this paper, we introduce TRIDENT: a general\nand model-agnostic inference-time algorithm that guarantees compliance with\nsuch constraints without requiring any retraining. TRIDENT compiles LTLf\nformulas into a Deterministic Finite Automaton (DFA), which is used to guide a\nconstrained variant of beam search. At each decoding step, transitions that\nwould lead to constraint violations are masked, while remaining paths are\ndynamically re-ranked based on both the model's probabilities and the DFA's\nacceptance structure. We formally prove that the resulting sequences are\nguaranteed to satisfy the given LTLf constraints, and we empirically\ndemonstrate that TRIDENT also improves output quality. We validate our approach\non two distinct tasks: temporally constrained image-stream classification and\ncontrolled text generation. In both settings, TRIDENT achieves perfect\nconstraint satisfaction, while comparison with the state of the art shows\nimproved efficiency and high standard quality metrics."
                },
                "authors": [
                    {
                        "name": "Vincenzo Collura"
                    },
                    {
                        "name": "Karim Tit"
                    },
                    {
                        "name": "Laura Bussi"
                    },
                    {
                        "name": "Eleonora Giunchiglia"
                    },
                    {
                        "name": "Maxime Cordy"
                    }
                ],
                "author_detail": {
                    "name": "Maxime Cordy"
                },
                "author": "Maxime Cordy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09701v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09701v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08570v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08570v2",
                "updated": "2025-06-11T13:09:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    9,
                    5,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-10T08:37:45Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    8,
                    37,
                    45,
                    1,
                    161,
                    0
                ],
                "title": "Auto-Regressive vs Flow-Matching: a Comparative Study of Modeling\n  Paradigms for Text-to-Music Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-Regressive vs Flow-Matching: a Comparative Study of Modeling\n  Paradigms for Text-to-Music Generation"
                },
                "summary": "Recent progress in text-to-music generation has enabled models to synthesize\nhigh-quality musical segments, full compositions, and even respond to\nfine-grained control signals, e.g. chord progressions. State-of-the-art (SOTA)\nsystems differ significantly across many dimensions, such as training datasets,\nmodeling paradigms, and architectural choices. This diversity complicates\nefforts to evaluate models fairly and pinpoint which design choices most\ninfluence performance. While factors like data and architecture are important,\nin this study we focus exclusively on the modeling paradigm. We conduct a\nsystematic empirical analysis to isolate its effects, offering insights into\nassociated trade-offs and emergent behaviors that can guide future\ntext-to-music generation systems. Specifically, we compare the two arguably\nmost common modeling paradigms: Auto-Regressive decoding and Conditional\nFlow-Matching. We conduct a controlled comparison by training all models from\nscratch using identical datasets, training configurations, and similar backbone\narchitectures. Performance is evaluated across multiple axes, including\ngeneration quality, robustness to inference configurations, scalability,\nadherence to both textual and temporally aligned conditioning, and editing\ncapabilities in the form of audio inpainting. This comparative study sheds\nlight on distinct strengths and limitations of each paradigm, providing\nactionable insights that can inform future architectural and training decisions\nin the evolving landscape of text-to-music generation. Audio sampled examples\nare available at: https://huggingface.co/spaces/ortal1602/ARvsFM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in text-to-music generation has enabled models to synthesize\nhigh-quality musical segments, full compositions, and even respond to\nfine-grained control signals, e.g. chord progressions. State-of-the-art (SOTA)\nsystems differ significantly across many dimensions, such as training datasets,\nmodeling paradigms, and architectural choices. This diversity complicates\nefforts to evaluate models fairly and pinpoint which design choices most\ninfluence performance. While factors like data and architecture are important,\nin this study we focus exclusively on the modeling paradigm. We conduct a\nsystematic empirical analysis to isolate its effects, offering insights into\nassociated trade-offs and emergent behaviors that can guide future\ntext-to-music generation systems. Specifically, we compare the two arguably\nmost common modeling paradigms: Auto-Regressive decoding and Conditional\nFlow-Matching. We conduct a controlled comparison by training all models from\nscratch using identical datasets, training configurations, and similar backbone\narchitectures. Performance is evaluated across multiple axes, including\ngeneration quality, robustness to inference configurations, scalability,\nadherence to both textual and temporally aligned conditioning, and editing\ncapabilities in the form of audio inpainting. This comparative study sheds\nlight on distinct strengths and limitations of each paradigm, providing\nactionable insights that can inform future architectural and training decisions\nin the evolving landscape of text-to-music generation. Audio sampled examples\nare available at: https://huggingface.co/spaces/ortal1602/ARvsFM"
                },
                "authors": [
                    {
                        "name": "Or Tal"
                    },
                    {
                        "name": "Felix Kreuk"
                    },
                    {
                        "name": "Yossi Adi"
                    }
                ],
                "author_detail": {
                    "name": "Yossi Adi"
                },
                "author": "Yossi Adi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08570v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08570v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20761v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20761v2",
                "updated": "2025-06-11T13:08:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    8,
                    58,
                    2,
                    162,
                    0
                ],
                "published": "2024-05-31T12:27:38Z",
                "published_parsed": [
                    2024,
                    5,
                    31,
                    12,
                    27,
                    38,
                    4,
                    152,
                    0
                ],
                "title": "Share Secrets for Privacy: Confidential Forecasting with Vertical\n  Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Share Secrets for Privacy: Confidential Forecasting with Vertical\n  Federated Learning"
                },
                "summary": "Vertical federated learning (VFL) is a promising area for time series\nforecasting in many applications, such as healthcare and manufacturing.\nCritical challenges to address include data privacy and over-fitting on small\nand noisy datasets during both training and inference. Additionally, such\nforecasting models must scale well with the number of parties while ensuring\nstrong convergence and low-tuning complexity. We address these challenges and\npropose ``Secret-shared Time Series Forecasting with VFL'' (STV), a novel\nframework with the following key features: i) a privacy-preserving algorithm\nfor forecasting with SARIMAX and autoregressive trees on vertically-partitioned\ndata; ii) decentralised forecasting using secret sharing and multi-party\ncomputation; and iii) novel N-party algorithms for matrix multiplication and\ninverse operations for exact parameter optimization, giving strong convergence\nwith minimal tuning complexity. We evaluate on six representative datasets from\npublic and industry-specific contexts. Results demonstrate that STV's\nforecasting accuracy is comparable to those of centralized approaches. Our\nexact optimization outperforms centralized methods, including state-of-the-art\ndiffusion models and long-short-term memory, by 23.81% on forecasting accuracy.\nWe also evaluate scalability by examining the communication costs of exact and\niterative optimization to navigate the choice between the two. STV's code and\nsupplementary material is available online: https://github.com/adis98/STV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vertical federated learning (VFL) is a promising area for time series\nforecasting in many applications, such as healthcare and manufacturing.\nCritical challenges to address include data privacy and over-fitting on small\nand noisy datasets during both training and inference. Additionally, such\nforecasting models must scale well with the number of parties while ensuring\nstrong convergence and low-tuning complexity. We address these challenges and\npropose ``Secret-shared Time Series Forecasting with VFL'' (STV), a novel\nframework with the following key features: i) a privacy-preserving algorithm\nfor forecasting with SARIMAX and autoregressive trees on vertically-partitioned\ndata; ii) decentralised forecasting using secret sharing and multi-party\ncomputation; and iii) novel N-party algorithms for matrix multiplication and\ninverse operations for exact parameter optimization, giving strong convergence\nwith minimal tuning complexity. We evaluate on six representative datasets from\npublic and industry-specific contexts. Results demonstrate that STV's\nforecasting accuracy is comparable to those of centralized approaches. Our\nexact optimization outperforms centralized methods, including state-of-the-art\ndiffusion models and long-short-term memory, by 23.81% on forecasting accuracy.\nWe also evaluate scalability by examining the communication costs of exact and\niterative optimization to navigate the choice between the two. STV's code and\nsupplementary material is available online: https://github.com/adis98/STV."
                },
                "authors": [
                    {
                        "name": "Aditya Shankar"
                    },
                    {
                        "name": "Jrmie Decouchant"
                    },
                    {
                        "name": "Dimitra Gkorou"
                    },
                    {
                        "name": "Rihan Hai"
                    },
                    {
                        "name": "Lydia Y. Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lydia Y. Chen"
                },
                "author": "Lydia Y. Chen",
                "arxiv_comment": "Accepted at the 20th International Conference on Availability,\n  Reliability and Security (ARES 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20761v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20761v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18553v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18553v3",
                "updated": "2025-06-11T13:08:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    8,
                    24,
                    2,
                    162,
                    0
                ],
                "published": "2024-11-27T17:51:58Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    17,
                    51,
                    58,
                    2,
                    332,
                    0
                ],
                "title": "Retrofitting Large Language Models with Dynamic Tokenization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrofitting Large Language Models with Dynamic Tokenization"
                },
                "summary": "Current language models (LMs) use a fixed, static subword tokenizer. This\ndefault choice typically results in degraded efficiency and language\ncapabilities, especially in languages other than English. To address this\nissue, we challenge the static design and propose retrofitting LMs with dynamic\ntokenization: a way to dynamically decide on token boundaries based on the\ninput text via a subword-merging algorithm inspired by byte-pair encoding. We\nmerge frequent subword sequences in a batch, then apply a pre-trained\nembedding-prediction hypernetwork to compute the token embeddings on-the-fly.\nFor encoder-style models (e.g., XLM-R), this on average reduces token sequence\nlengths by >20% across 14 languages while degrading performance by less than\n2%. The same method applied to pre-filling and scoring in decoder-style models\n(e.g., Mistral-7B) results in minimal performance degradation at up to 17%\nreduction in sequence length. Overall, we find that dynamic tokenization can\nmitigate the limitations of static tokenization by substantially improving\ninference speed and promoting fairness across languages, enabling more\nequitable and adaptable LMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current language models (LMs) use a fixed, static subword tokenizer. This\ndefault choice typically results in degraded efficiency and language\ncapabilities, especially in languages other than English. To address this\nissue, we challenge the static design and propose retrofitting LMs with dynamic\ntokenization: a way to dynamically decide on token boundaries based on the\ninput text via a subword-merging algorithm inspired by byte-pair encoding. We\nmerge frequent subword sequences in a batch, then apply a pre-trained\nembedding-prediction hypernetwork to compute the token embeddings on-the-fly.\nFor encoder-style models (e.g., XLM-R), this on average reduces token sequence\nlengths by >20% across 14 languages while degrading performance by less than\n2%. The same method applied to pre-filling and scoring in decoder-style models\n(e.g., Mistral-7B) results in minimal performance degradation at up to 17%\nreduction in sequence length. Overall, we find that dynamic tokenization can\nmitigate the limitations of static tokenization by substantially improving\ninference speed and promoting fairness across languages, enabling more\nequitable and adaptable LMs."
                },
                "authors": [
                    {
                        "name": "Darius Feher"
                    },
                    {
                        "name": "Ivan Vuli"
                    },
                    {
                        "name": "Benjamin Minixhofer"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Minixhofer"
                },
                "author": "Benjamin Minixhofer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18553v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18553v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09691v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09691v1",
                "updated": "2025-06-11T13:06:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    6,
                    25,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T13:06:25Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    6,
                    25,
                    2,
                    162,
                    0
                ],
                "title": "Adding simple structure at inference improves Vision-Language\n  Compositionality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adding simple structure at inference improves Vision-Language\n  Compositionality"
                },
                "summary": "Dual encoder Vision-Language Models (VLM) such as CLIP are widely used for\nimage-text retrieval tasks. However, those models struggle with\ncompositionality, showing a bag-of-words-like behavior that limits their\nretrieval performance. Many different training approaches have been proposed to\nimprove the vision-language compositionality capabilities of those models. In\ncomparison, inference-time techniques have received little attention. In this\npaper, we propose to add simple structure at inference, where, given an image\nand a caption: i) we divide the image into different smaller crops, ii) we\nextract text segments, capturing objects, attributes and relations, iii) using\na VLM, we find the image crops that better align with text segments obtaining\nmatches, and iv) we compute the final image-text similarity aggregating the\nindividual similarities of the matches. Based on various popular dual encoder\nVLMs, we evaluate our approach in controlled and natural datasets for VL\ncompositionality. We find that our approach consistently improves the\nperformance of evaluated VLMs without any training, which shows the potential\nof inference-time techniques. The results are especially good for\nattribute-object binding as shown in the controlled dataset. As a result of an\nextensive analysis: i) we show that processing image crops is actually\nessential for the observed gains in performance, and ii) we identify specific\nareas to further improve inference-time approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dual encoder Vision-Language Models (VLM) such as CLIP are widely used for\nimage-text retrieval tasks. However, those models struggle with\ncompositionality, showing a bag-of-words-like behavior that limits their\nretrieval performance. Many different training approaches have been proposed to\nimprove the vision-language compositionality capabilities of those models. In\ncomparison, inference-time techniques have received little attention. In this\npaper, we propose to add simple structure at inference, where, given an image\nand a caption: i) we divide the image into different smaller crops, ii) we\nextract text segments, capturing objects, attributes and relations, iii) using\na VLM, we find the image crops that better align with text segments obtaining\nmatches, and iv) we compute the final image-text similarity aggregating the\nindividual similarities of the matches. Based on various popular dual encoder\nVLMs, we evaluate our approach in controlled and natural datasets for VL\ncompositionality. We find that our approach consistently improves the\nperformance of evaluated VLMs without any training, which shows the potential\nof inference-time techniques. The results are especially good for\nattribute-object binding as shown in the controlled dataset. As a result of an\nextensive analysis: i) we show that processing image crops is actually\nessential for the observed gains in performance, and ii) we identify specific\nareas to further improve inference-time approaches."
                },
                "authors": [
                    {
                        "name": "Imanol Miranda"
                    },
                    {
                        "name": "Ander Salaberria"
                    },
                    {
                        "name": "Eneko Agirre"
                    },
                    {
                        "name": "Gorka Azkune"
                    }
                ],
                "author_detail": {
                    "name": "Gorka Azkune"
                },
                "author": "Gorka Azkune",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09691v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09691v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09690v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09690v1",
                "updated": "2025-06-11T13:06:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    6,
                    21,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T13:06:21Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    6,
                    21,
                    2,
                    162,
                    0
                ],
                "title": "Knockoffs Inference under Privacy Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knockoffs Inference under Privacy Constraints"
                },
                "summary": "Model-X knockoff framework offers a model-free variable selection method that\nensures finite sample false discovery rate (FDR) control. However, the\ncomplexity of generating knockoff variables, coupled with the model-free\nassumption, presents significant challenges for protecting data privacy in this\ncontext. In this paper, we propose a comprehensive framework for knockoff\ninference within the differential privacy paradigm. Our proposed method\nguarantees robust privacy protection while preserving the exact FDR control\nentailed by the original model-X knockoff procedure. We further conduct power\nanalysis and establish sufficient conditions under which the noise added for\nprivacy preservation does not asymptotically compromise power. Through various\napplications, we demonstrate that the differential privacy knockoff\n(DP-knockoff) method can be effectively utilized to safeguard privacy during\nvariable selection with FDR control in both low and high dimensional settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-X knockoff framework offers a model-free variable selection method that\nensures finite sample false discovery rate (FDR) control. However, the\ncomplexity of generating knockoff variables, coupled with the model-free\nassumption, presents significant challenges for protecting data privacy in this\ncontext. In this paper, we propose a comprehensive framework for knockoff\ninference within the differential privacy paradigm. Our proposed method\nguarantees robust privacy protection while preserving the exact FDR control\nentailed by the original model-X knockoff procedure. We further conduct power\nanalysis and establish sufficient conditions under which the noise added for\nprivacy preservation does not asymptotically compromise power. Through various\napplications, we demonstrate that the differential privacy knockoff\n(DP-knockoff) method can be effectively utilized to safeguard privacy during\nvariable selection with FDR control in both low and high dimensional settings."
                },
                "authors": [
                    {
                        "name": "Zhanrui Cai"
                    },
                    {
                        "name": "Yingying Fan"
                    },
                    {
                        "name": "Lan Gao"
                    }
                ],
                "author_detail": {
                    "name": "Lan Gao"
                },
                "author": "Lan Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09690v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09690v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09684v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09684v1",
                "updated": "2025-06-11T13:02:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    2,
                    17,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T13:02:17Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    2,
                    17,
                    2,
                    162,
                    0
                ],
                "title": "Inv-Entropy: A Fully Probabilistic Framework for Uncertainty\n  Quantification in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inv-Entropy: A Fully Probabilistic Framework for Uncertainty\n  Quantification in Language Models"
                },
                "summary": "Large language models (LLMs) have transformed natural language processing,\nbut their reliable deployment requires effective uncertainty quantification\n(UQ). Existing UQ methods are often heuristic and lack a probabilistic\nfoundation. This paper begins by providing a theoretical justification for the\nrole of perturbations in UQ for LLMs. We then introduce a dual random walk\nperspective, modeling input-output pairs as two Markov chains with transition\nprobabilities defined by semantic similarity. Building on this, we propose a\nfully probabilistic framework based on an inverse model, which quantifies\nuncertainty by evaluating the diversity of the input space conditioned on a\ngiven output through systematic perturbations. Within this framework, we define\na new uncertainty measure, Inv-Entropy. A key strength of our framework is its\nflexibility: it supports various definitions of uncertainty measures,\nembeddings, perturbation strategies, and similarity metrics. We also propose\nGAAP, a perturbation algorithm based on genetic algorithms, which enhances the\ndiversity of sampled inputs. In addition, we introduce a new evaluation metric,\nTemperature Sensitivity of Uncertainty (TSU), which directly assesses\nuncertainty without relying on correctness as a proxy. Extensive experiments\ndemonstrate that Inv-Entropy outperforms existing semantic UQ methods. The code\nto reproduce the results can be found at\nhttps://github.com/UMDataScienceLab/Uncertainty-Quantification-for-LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have transformed natural language processing,\nbut their reliable deployment requires effective uncertainty quantification\n(UQ). Existing UQ methods are often heuristic and lack a probabilistic\nfoundation. This paper begins by providing a theoretical justification for the\nrole of perturbations in UQ for LLMs. We then introduce a dual random walk\nperspective, modeling input-output pairs as two Markov chains with transition\nprobabilities defined by semantic similarity. Building on this, we propose a\nfully probabilistic framework based on an inverse model, which quantifies\nuncertainty by evaluating the diversity of the input space conditioned on a\ngiven output through systematic perturbations. Within this framework, we define\na new uncertainty measure, Inv-Entropy. A key strength of our framework is its\nflexibility: it supports various definitions of uncertainty measures,\nembeddings, perturbation strategies, and similarity metrics. We also propose\nGAAP, a perturbation algorithm based on genetic algorithms, which enhances the\ndiversity of sampled inputs. In addition, we introduce a new evaluation metric,\nTemperature Sensitivity of Uncertainty (TSU), which directly assesses\nuncertainty without relying on correctness as a proxy. Extensive experiments\ndemonstrate that Inv-Entropy outperforms existing semantic UQ methods. The code\nto reproduce the results can be found at\nhttps://github.com/UMDataScienceLab/Uncertainty-Quantification-for-LLMs."
                },
                "authors": [
                    {
                        "name": "Haoyi Song"
                    },
                    {
                        "name": "Ruihan Ji"
                    },
                    {
                        "name": "Naichen Shi"
                    },
                    {
                        "name": "Fan Lai"
                    },
                    {
                        "name": "Raed Al Kontar"
                    }
                ],
                "author_detail": {
                    "name": "Raed Al Kontar"
                },
                "author": "Raed Al Kontar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09684v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09684v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09679v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09679v1",
                "updated": "2025-06-11T12:53:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    12,
                    53,
                    9,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T12:53:09Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    12,
                    53,
                    9,
                    2,
                    162,
                    0
                ],
                "title": "Geometric flow regularization in latent spaces for smooth dynamics with\n  the efficient variations of curvature",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geometric flow regularization in latent spaces for smooth dynamics with\n  the efficient variations of curvature"
                },
                "summary": "We design strategies in nonlinear geometric analysis to temper the effects of\nadversarial learning for sufficiently smooth data of numerical method-type\ndynamics in encoder-decoder methods, variational and deterministic, through the\nuse of geometric flow regularization. We augment latent spaces with geometric\nflows to control structure. Our techniques rely on adaptations of curvature and\nRicci flow. We invent new geometric flows or discover them neurally and\nnon-parametrically. All of our flows are solved using physics-informed\nlearning. Traditional geometric meaning is traded for computing ability, but we\nmaintain key geometric invariants, the primary of which are maintained,\nintrinsically-low structure, canonicity or a lack of irregularity,\nnontriviality due to sufficient lower bounds on curvature, and distortion of\nvolume element, that develop quality in the inference stage. Our primary\ncontributions are fourfold. We develop a loss based on Gaussian curvature using\nclosed path circulation integration for surfaces, bypassing automatic\ndifferentiation of the Christoffel symbols through use of Stokes' theorem. We\ninvent a new parametric flow derived from a linear version of the Gauss\nequation and a Riemannian decomposition for a custom tensor defined with a\nnormal Hessian and Weyl tensor proxies. We develop two strategies based on time\ndifferentiation of functionals, one with a special case of scalar curvature for\nconformally-changed metrics, and another with harmonic maps, their energy, and\ninduced metrics. Our methods, while diminished analytically, maintain overall\nintegral latent structure. We showcase that curvature flows and the formulation\nof geometric structure in intermediary encoded settings enhance learning and\noverall zero-shot and adversarial fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We design strategies in nonlinear geometric analysis to temper the effects of\nadversarial learning for sufficiently smooth data of numerical method-type\ndynamics in encoder-decoder methods, variational and deterministic, through the\nuse of geometric flow regularization. We augment latent spaces with geometric\nflows to control structure. Our techniques rely on adaptations of curvature and\nRicci flow. We invent new geometric flows or discover them neurally and\nnon-parametrically. All of our flows are solved using physics-informed\nlearning. Traditional geometric meaning is traded for computing ability, but we\nmaintain key geometric invariants, the primary of which are maintained,\nintrinsically-low structure, canonicity or a lack of irregularity,\nnontriviality due to sufficient lower bounds on curvature, and distortion of\nvolume element, that develop quality in the inference stage. Our primary\ncontributions are fourfold. We develop a loss based on Gaussian curvature using\nclosed path circulation integration for surfaces, bypassing automatic\ndifferentiation of the Christoffel symbols through use of Stokes' theorem. We\ninvent a new parametric flow derived from a linear version of the Gauss\nequation and a Riemannian decomposition for a custom tensor defined with a\nnormal Hessian and Weyl tensor proxies. We develop two strategies based on time\ndifferentiation of functionals, one with a special case of scalar curvature for\nconformally-changed metrics, and another with harmonic maps, their energy, and\ninduced metrics. Our methods, while diminished analytically, maintain overall\nintegral latent structure. We showcase that curvature flows and the formulation\nof geometric structure in intermediary encoded settings enhance learning and\noverall zero-shot and adversarial fidelity."
                },
                "authors": [
                    {
                        "name": "Andrew Gracyk"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Gracyk"
                },
                "author": "Andrew Gracyk",
                "arxiv_comment": "First version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09679v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09679v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09672v1",
                "updated": "2025-06-11T12:43:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    12,
                    43,
                    10,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T12:43:10Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    12,
                    43,
                    10,
                    2,
                    162,
                    0
                ],
                "title": "Is Fine-Tuning an Effective Solution? Reassessing Knowledge Editing for\n  Unstructured Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Fine-Tuning an Effective Solution? Reassessing Knowledge Editing for\n  Unstructured Data"
                },
                "summary": "Unstructured Knowledge Editing (UKE) is crucial for updating the relevant\nknowledge of large language models (LLMs). It focuses on unstructured inputs,\nsuch as long or free-form texts, which are common forms of real-world\nknowledge. Although previous studies have proposed effective methods and tested\nthem, some issues exist: (1) Lack of Locality evaluation for UKE, and (2)\nAbnormal failure of fine-tuning (FT) based methods for UKE. To address these\nissues, we first construct two datasets, UnKEBench-Loc and AKEW-Loc (CF), by\nextending two existing UKE datasets with locality test data from the\nunstructured and structured views. This enables a systematic evaluation of the\nLocality of post-edited models. Furthermore, we identify four factors that may\naffect the performance of FT-based methods. Based on these factors, we conduct\nexperiments to determine how the well-performing FT-based methods should be\ntrained for the UKE task, providing a training recipe for future research. Our\nexperimental results indicate that the FT-based method with the optimal setting\n(FT-UKE) is surprisingly strong, outperforming the existing state-of-the-art\n(SOTA). In batch editing scenarios, FT-UKE shows strong performance as well,\nwith its advantage over SOTA methods increasing as the batch size grows,\nexpanding the average metric lead from +6.78% to +10.80%",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unstructured Knowledge Editing (UKE) is crucial for updating the relevant\nknowledge of large language models (LLMs). It focuses on unstructured inputs,\nsuch as long or free-form texts, which are common forms of real-world\nknowledge. Although previous studies have proposed effective methods and tested\nthem, some issues exist: (1) Lack of Locality evaluation for UKE, and (2)\nAbnormal failure of fine-tuning (FT) based methods for UKE. To address these\nissues, we first construct two datasets, UnKEBench-Loc and AKEW-Loc (CF), by\nextending two existing UKE datasets with locality test data from the\nunstructured and structured views. This enables a systematic evaluation of the\nLocality of post-edited models. Furthermore, we identify four factors that may\naffect the performance of FT-based methods. Based on these factors, we conduct\nexperiments to determine how the well-performing FT-based methods should be\ntrained for the UKE task, providing a training recipe for future research. Our\nexperimental results indicate that the FT-based method with the optimal setting\n(FT-UKE) is surprisingly strong, outperforming the existing state-of-the-art\n(SOTA). In batch editing scenarios, FT-UKE shows strong performance as well,\nwith its advantage over SOTA methods increasing as the batch size grows,\nexpanding the average metric lead from +6.78% to +10.80%"
                },
                "authors": [
                    {
                        "name": "Hao Xiong"
                    },
                    {
                        "name": "Chuanyuan Tan"
                    },
                    {
                        "name": "Wenliang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenliang Chen"
                },
                "author": "Wenliang Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12768v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12768v2",
                "updated": "2025-06-11T12:40:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    12,
                    40,
                    14,
                    2,
                    162,
                    0
                ],
                "published": "2024-11-18T07:52:12Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    7,
                    52,
                    12,
                    0,
                    323,
                    0
                ],
                "title": "CROW: Eliminating Backdoors from Large Language Models via Internal\n  Consistency Regularization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CROW: Eliminating Backdoors from Large Language Models via Internal\n  Consistency Regularization"
                },
                "summary": "Large Language Models (LLMs) are vulnerable to backdoor attacks that\nmanipulate outputs via hidden triggers. Existing defense methods--designed for\nvision/text classification tasks--fail for text generation. We propose Internal\nConsistency Regularization (CROW), a defense leveraging the observation that\nbackdoored models exhibit unstable layer-wise hidden representations when\ntriggered, while clean models show smooth transitions. CROW enforces\nconsistency across layers via adversarial perturbations and regularization\nduring finetuning, neutralizing backdoors without requiring clean reference\nmodels or trigger knowledge--only a small clean dataset. Experiments across\nLlama-2 (7B, 13B), CodeLlama (7B, 13B), and Mistral-7B demonstrate CROW's\neffectiveness: it achieves significant reductions in attack success rates\nacross diverse backdoor strategies (sentiment steering, targeted refusal, code\ninjection) while preserving generative performance. CROW's\narchitecture-agnostic design enables practical deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are vulnerable to backdoor attacks that\nmanipulate outputs via hidden triggers. Existing defense methods--designed for\nvision/text classification tasks--fail for text generation. We propose Internal\nConsistency Regularization (CROW), a defense leveraging the observation that\nbackdoored models exhibit unstable layer-wise hidden representations when\ntriggered, while clean models show smooth transitions. CROW enforces\nconsistency across layers via adversarial perturbations and regularization\nduring finetuning, neutralizing backdoors without requiring clean reference\nmodels or trigger knowledge--only a small clean dataset. Experiments across\nLlama-2 (7B, 13B), CodeLlama (7B, 13B), and Mistral-7B demonstrate CROW's\neffectiveness: it achieves significant reductions in attack success rates\nacross diverse backdoor strategies (sentiment steering, targeted refusal, code\ninjection) while preserving generative performance. CROW's\narchitecture-agnostic design enables practical deployment."
                },
                "authors": [
                    {
                        "name": "Nay Myat Min"
                    },
                    {
                        "name": "Long H. Pham"
                    },
                    {
                        "name": "Yige Li"
                    },
                    {
                        "name": "Jun Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jun Sun"
                },
                "author": "Jun Sun",
                "arxiv_comment": "Accepted at ICML 2025, 20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12768v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12768v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09669v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09669v1",
                "updated": "2025-06-11T12:39:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    12,
                    39,
                    48,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T12:39:48Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    12,
                    39,
                    48,
                    2,
                    162,
                    0
                ],
                "title": "Query-Level Uncertainty in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query-Level Uncertainty in Large Language Models"
                },
                "summary": "It is important for Large Language Models to be aware of the boundary of\ntheir knowledge, the mechanism of identifying known and unknown queries. This\ntype of awareness can help models perform adaptive inference, such as invoking\nRAG, engaging in slow and deep thinking, or adopting the abstention mechanism,\nwhich is beneficial to the development of efficient and trustworthy AI. In this\nwork, we propose a method to detect knowledge boundaries via Query-Level\nUncertainty, which aims to determine if the model is able to address a given\nquery without generating any tokens. To this end, we introduce a novel and\ntraining-free method called \\emph{Internal Confidence}, which leverages\nself-evaluations across layers and tokens. Empirical results on both factual QA\nand mathematical reasoning tasks demonstrate that our internal confidence can\noutperform several baselines. Furthermore, we showcase that our proposed method\ncan be used for efficient RAG and model cascading, which is able to reduce\ninference costs while maintaining performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is important for Large Language Models to be aware of the boundary of\ntheir knowledge, the mechanism of identifying known and unknown queries. This\ntype of awareness can help models perform adaptive inference, such as invoking\nRAG, engaging in slow and deep thinking, or adopting the abstention mechanism,\nwhich is beneficial to the development of efficient and trustworthy AI. In this\nwork, we propose a method to detect knowledge boundaries via Query-Level\nUncertainty, which aims to determine if the model is able to address a given\nquery without generating any tokens. To this end, we introduce a novel and\ntraining-free method called \\emph{Internal Confidence}, which leverages\nself-evaluations across layers and tokens. Empirical results on both factual QA\nand mathematical reasoning tasks demonstrate that our internal confidence can\noutperform several baselines. Furthermore, we showcase that our proposed method\ncan be used for efficient RAG and model cascading, which is able to reduce\ninference costs while maintaining performance."
                },
                "authors": [
                    {
                        "name": "Lihu Chen"
                    },
                    {
                        "name": "Gal Varoquaux"
                    }
                ],
                "author_detail": {
                    "name": "Gal Varoquaux"
                },
                "author": "Gal Varoquaux",
                "arxiv_comment": "In Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09669v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09669v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00397v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00397v5",
                "updated": "2025-06-11T12:36:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    12,
                    36,
                    31,
                    2,
                    162,
                    0
                ],
                "published": "2024-06-29T10:50:23Z",
                "published_parsed": [
                    2024,
                    6,
                    29,
                    10,
                    50,
                    23,
                    5,
                    181,
                    0
                ],
                "title": "Learning Time-Varying Multi-Region Brain Communications via Scalable\n  Markovian Gaussian Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Time-Varying Multi-Region Brain Communications via Scalable\n  Markovian Gaussian Processes"
                },
                "summary": "Understanding and constructing brain communications that capture dynamic\ncommunications across multiple regions is fundamental to modern system\nneuroscience, yet current methods struggle to find time-varying region-level\ncommunications or scale to large neural datasets with long recording durations.\nWe present a novel framework using Markovian Gaussian Processes to learn brain\ncommunications with time-varying temporal delays from multi-region neural\nrecordings, named Adaptive Delay Model (ADM). Our method combines Gaussian\nProcesses with State Space Models and employs parallel scan inference\nalgorithms, enabling efficient scaling to large datasets while identifying\nconcurrent communication patterns that evolve over time. This time-varying\napproach captures how brain region interactions shift dynamically during\ncognitive processes. Validated on synthetic and multi-region neural recordings\ndatasets, our approach discovers both the directionality and temporal dynamics\nof neural communication. This work advances our understanding of distributed\nneural computation and provides a scalable tool for analyzing dynamic brain\nnetworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and constructing brain communications that capture dynamic\ncommunications across multiple regions is fundamental to modern system\nneuroscience, yet current methods struggle to find time-varying region-level\ncommunications or scale to large neural datasets with long recording durations.\nWe present a novel framework using Markovian Gaussian Processes to learn brain\ncommunications with time-varying temporal delays from multi-region neural\nrecordings, named Adaptive Delay Model (ADM). Our method combines Gaussian\nProcesses with State Space Models and employs parallel scan inference\nalgorithms, enabling efficient scaling to large datasets while identifying\nconcurrent communication patterns that evolve over time. This time-varying\napproach captures how brain region interactions shift dynamically during\ncognitive processes. Validated on synthetic and multi-region neural recordings\ndatasets, our approach discovers both the directionality and temporal dynamics\nof neural communication. This work advances our understanding of distributed\nneural computation and provides a scalable tool for analyzing dynamic brain\nnetworks."
                },
                "authors": [
                    {
                        "name": "Weihan Li"
                    },
                    {
                        "name": "Yule Wang"
                    },
                    {
                        "name": "Chengrui Li"
                    },
                    {
                        "name": "Anqi Wu"
                    }
                ],
                "author_detail": {
                    "name": "Anqi Wu"
                },
                "author": "Anqi Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00397v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00397v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09662v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09662v1",
                "updated": "2025-06-11T12:32:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    12,
                    32,
                    6,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T12:32:06Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    12,
                    32,
                    6,
                    2,
                    162,
                    0
                ],
                "title": "Empirical Quantification of Spurious Correlations in Malware Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empirical Quantification of Spurious Correlations in Malware Detection"
                },
                "summary": "End-to-end deep learning exhibits unmatched performance for detecting\nmalware, but such an achievement is reached by exploiting spurious correlations\n-- features with high relevance at inference time, but known to be useless\nthrough domain knowledge. While previous work highlighted that deep networks\nmainly focus on metadata, none investigated the phenomenon further, without\nquantifying their impact on the decision. In this work, we deepen our\nunderstanding of how spurious correlation affects deep learning for malware\ndetection by highlighting how much models rely on empty spaces left by the\ncompiler, which diminishes the relevance of the compiled code. Through our\nseminal analysis on a small-scale balanced dataset, we introduce a ranking of\ntwo end-to-end models to better understand which is more suitable to be put in\nproduction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-end deep learning exhibits unmatched performance for detecting\nmalware, but such an achievement is reached by exploiting spurious correlations\n-- features with high relevance at inference time, but known to be useless\nthrough domain knowledge. While previous work highlighted that deep networks\nmainly focus on metadata, none investigated the phenomenon further, without\nquantifying their impact on the decision. In this work, we deepen our\nunderstanding of how spurious correlation affects deep learning for malware\ndetection by highlighting how much models rely on empty spaces left by the\ncompiler, which diminishes the relevance of the compiled code. Through our\nseminal analysis on a small-scale balanced dataset, we introduce a ranking of\ntwo end-to-end models to better understand which is more suitable to be put in\nproduction."
                },
                "authors": [
                    {
                        "name": "Bianca Perasso"
                    },
                    {
                        "name": "Ludovico Lozza"
                    },
                    {
                        "name": "Andrea Ponte"
                    },
                    {
                        "name": "Luca Demetrio"
                    },
                    {
                        "name": "Luca Oneto"
                    },
                    {
                        "name": "Fabio Roli"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Roli"
                },
                "author": "Fabio Roli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09662v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09662v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05794v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05794v2",
                "updated": "2025-06-11T12:31:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    12,
                    31,
                    22,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-06T06:39:46Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    6,
                    39,
                    46,
                    4,
                    157,
                    0
                ],
                "title": "Markov Blanket Density and Free Energy Minimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Markov Blanket Density and Free Energy Minimization"
                },
                "summary": "This paper presents a continuous, information-theoretic extension of the Free\nEnergy Principle through the concept of Markov blanket density, i.e., a scalar\nfield that quantifies the degree of conditional independence between internal\nand external states at each point in space (ranging from 0 for full coupling to\n1 for full separation). It demonstrates that active inference dynamics\n(including the minimization of variational and expected free energy) naturally\nemerge from spatial gradients in this density, making Markov blanket density a\nnecessary foundation for the definability and coherence of the Free Energy\nPrinciple. These ideas are developed through a mathematically framework that\nlinks density gradients to precise and testable dynamics, offering a foundation\nfor novel predictions and simulation paradigms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a continuous, information-theoretic extension of the Free\nEnergy Principle through the concept of Markov blanket density, i.e., a scalar\nfield that quantifies the degree of conditional independence between internal\nand external states at each point in space (ranging from 0 for full coupling to\n1 for full separation). It demonstrates that active inference dynamics\n(including the minimization of variational and expected free energy) naturally\nemerge from spatial gradients in this density, making Markov blanket density a\nnecessary foundation for the definability and coherence of the Free Energy\nPrinciple. These ideas are developed through a mathematically framework that\nlinks density gradients to precise and testable dynamics, offering a foundation\nfor novel predictions and simulation paradigms."
                },
                "authors": [
                    {
                        "name": "Luca M. Possati"
                    }
                ],
                "author_detail": {
                    "name": "Luca M. Possati"
                },
                "author": "Luca M. Possati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05794v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05794v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09659v1",
                "updated": "2025-06-11T12:26:45Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    12,
                    26,
                    45,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T12:26:45Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    12,
                    26,
                    45,
                    2,
                    162,
                    0
                ],
                "title": "Intent Factored Generation: Unleashing the Diversity in Your Language\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intent Factored Generation: Unleashing the Diversity in Your Language\n  Model"
                },
                "summary": "Obtaining multiple meaningfully diverse, high quality samples from Large\nLanguage Models for a fixed prompt remains an open challenge. Current methods\nfor increasing diversity often only operate at the token-level, paraphrasing\nthe same response. This is problematic because it leads to poor exploration on\nreasoning problems and to unengaging, repetitive conversational agents. To\naddress this we propose Intent Factored Generation (IFG), factorising the\nsampling process into two stages. First, we sample a semantically dense intent,\ne.g., a summary or keywords. Second, we sample the final response conditioning\non both the original prompt and the intent from the first stage. This allows us\nto use a higher temperature during the intent step to promote conceptual\ndiversity, and a lower temperature during the final generation to ensure the\noutputs are coherent and self-consistent. Additionally, we find that prompting\nthe model to explicitly state its intent for each step of the chain-of-thought\nbefore generating the step is beneficial for reasoning tasks. We demonstrate\nour method's effectiveness across a diverse set of tasks. We show this method\nimproves both pass@k and Reinforcement Learning from Verifier Feedback on maths\nand code tasks. For instruction-tuning, we combine IFG with Direct Preference\nOptimisation to increase conversational diversity without sacrificing reward.\nFinally, we achieve higher diversity while maintaining the quality of\ngenerations on a general language modelling task, using a new dataset of reader\ncomments and news articles that we collect and open-source. In summary, we\npresent a simple method of increasing the sample diversity of LLMs while\nmaintaining performance. This method can be implemented by changing the prompt\nand varying the temperature during generation, making it easy to integrate into\nmany algorithms for gains across various applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Obtaining multiple meaningfully diverse, high quality samples from Large\nLanguage Models for a fixed prompt remains an open challenge. Current methods\nfor increasing diversity often only operate at the token-level, paraphrasing\nthe same response. This is problematic because it leads to poor exploration on\nreasoning problems and to unengaging, repetitive conversational agents. To\naddress this we propose Intent Factored Generation (IFG), factorising the\nsampling process into two stages. First, we sample a semantically dense intent,\ne.g., a summary or keywords. Second, we sample the final response conditioning\non both the original prompt and the intent from the first stage. This allows us\nto use a higher temperature during the intent step to promote conceptual\ndiversity, and a lower temperature during the final generation to ensure the\noutputs are coherent and self-consistent. Additionally, we find that prompting\nthe model to explicitly state its intent for each step of the chain-of-thought\nbefore generating the step is beneficial for reasoning tasks. We demonstrate\nour method's effectiveness across a diverse set of tasks. We show this method\nimproves both pass@k and Reinforcement Learning from Verifier Feedback on maths\nand code tasks. For instruction-tuning, we combine IFG with Direct Preference\nOptimisation to increase conversational diversity without sacrificing reward.\nFinally, we achieve higher diversity while maintaining the quality of\ngenerations on a general language modelling task, using a new dataset of reader\ncomments and news articles that we collect and open-source. In summary, we\npresent a simple method of increasing the sample diversity of LLMs while\nmaintaining performance. This method can be implemented by changing the prompt\nand varying the temperature during generation, making it easy to integrate into\nmany algorithms for gains across various applications."
                },
                "authors": [
                    {
                        "name": "Eltayeb Ahmed"
                    },
                    {
                        "name": "Uljad Berdica"
                    },
                    {
                        "name": "Martha Elliott"
                    },
                    {
                        "name": "Danijela Horak"
                    },
                    {
                        "name": "Jakob N. Foerster"
                    }
                ],
                "author_detail": {
                    "name": "Jakob N. Foerster"
                },
                "author": "Jakob N. Foerster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09657v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09657v1",
                "updated": "2025-06-11T12:26:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    12,
                    26,
                    8,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T12:26:08Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    12,
                    26,
                    8,
                    2,
                    162,
                    0
                ],
                "title": "Bridging the Gap Between Open-Source and Proprietary LLMs in Table QA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the Gap Between Open-Source and Proprietary LLMs in Table QA"
                },
                "summary": "This paper presents a system developed for SemEval 2025 Task 8: Question\nAnswering (QA) over tabular data. Our approach integrates several key\ncomponents: text-to-SQL and text-to-code generation modules, a self-correction\nmechanism, and a retrieval-augmented generation (RAG). Additionally, it\nincludes an end-to-end (E2E) module, all orchestrated by a large language model\n(LLM). Through ablation studies, we analyzed the effects of different parts of\nour pipeline and identified the challenges that are still present in this\nfield. During the evaluation phase of the competition, our solution achieved an\naccuracy of 80%, resulting in a top-13 ranking among the 38 participating\nteams. Our pipeline demonstrates a significant improvement in accuracy for\nopen-source models and achieves a performance comparable to proprietary LLMs in\nQA tasks over tables. The code is available at GitHub repository.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a system developed for SemEval 2025 Task 8: Question\nAnswering (QA) over tabular data. Our approach integrates several key\ncomponents: text-to-SQL and text-to-code generation modules, a self-correction\nmechanism, and a retrieval-augmented generation (RAG). Additionally, it\nincludes an end-to-end (E2E) module, all orchestrated by a large language model\n(LLM). Through ablation studies, we analyzed the effects of different parts of\nour pipeline and identified the challenges that are still present in this\nfield. During the evaluation phase of the competition, our solution achieved an\naccuracy of 80%, resulting in a top-13 ranking among the 38 participating\nteams. Our pipeline demonstrates a significant improvement in accuracy for\nopen-source models and achieves a performance comparable to proprietary LLMs in\nQA tasks over tables. The code is available at GitHub repository."
                },
                "authors": [
                    {
                        "name": "Nikolas Evkarpidi"
                    },
                    {
                        "name": "Elena Tutubalina"
                    }
                ],
                "author_detail": {
                    "name": "Elena Tutubalina"
                },
                "author": "Elena Tutubalina",
                "arxiv_comment": "Accepted for publication at the 19th International Workshop on\n  Semantic Evaluation (SemEval-2025), to be held in conjunction with ACL 2025.\n  15 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09657v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09657v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09656v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09656v1",
                "updated": "2025-06-11T12:25:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    12,
                    25,
                    38,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T12:25:38Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    12,
                    25,
                    38,
                    2,
                    162,
                    0
                ],
                "title": "Application-Driven Value Alignment in Agentic AI Systems: Survey and\n  Perspectives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application-Driven Value Alignment in Agentic AI Systems: Survey and\n  Perspectives"
                },
                "summary": "The ongoing evolution of AI paradigms has propelled AI research into the\nAgentic AI stage. Consequently, the focus of research has shifted from single\nagents and simple applications towards multi-agent autonomous decision-making\nand task collaboration in complex environments. As Large Language Models (LLMs)\nadvance, their applications become more diverse and complex, leading to\nincreasingly situational and systemic risks. This has brought significant\nattention to value alignment for AI agents, which aims to ensure that an\nagent's goals, preferences, and behaviors align with human values and societal\nnorms. This paper reviews value alignment in agent systems within specific\napplication scenarios. It integrates the advancements in AI driven by large\nmodels with the demands of social governance. Our review covers value\nprinciples, agent system application scenarios, and agent value alignment\nevaluation. Specifically, value principles are organized hierarchically from a\ntop-down perspective, encompassing macro, meso, and micro levels. Agent system\napplication scenarios are categorized and reviewed from a general-to-specific\nviewpoint. Agent value alignment evaluation systematically examines datasets\nfor value alignment assessment and relevant value alignment methods.\nAdditionally, we delve into value coordination among multiple agents within\nagent systems. Finally, we propose several potential research directions in\nthis field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ongoing evolution of AI paradigms has propelled AI research into the\nAgentic AI stage. Consequently, the focus of research has shifted from single\nagents and simple applications towards multi-agent autonomous decision-making\nand task collaboration in complex environments. As Large Language Models (LLMs)\nadvance, their applications become more diverse and complex, leading to\nincreasingly situational and systemic risks. This has brought significant\nattention to value alignment for AI agents, which aims to ensure that an\nagent's goals, preferences, and behaviors align with human values and societal\nnorms. This paper reviews value alignment in agent systems within specific\napplication scenarios. It integrates the advancements in AI driven by large\nmodels with the demands of social governance. Our review covers value\nprinciples, agent system application scenarios, and agent value alignment\nevaluation. Specifically, value principles are organized hierarchically from a\ntop-down perspective, encompassing macro, meso, and micro levels. Agent system\napplication scenarios are categorized and reviewed from a general-to-specific\nviewpoint. Agent value alignment evaluation systematically examines datasets\nfor value alignment assessment and relevant value alignment methods.\nAdditionally, we delve into value coordination among multiple agents within\nagent systems. Finally, we propose several potential research directions in\nthis field."
                },
                "authors": [
                    {
                        "name": "Wei Zeng"
                    },
                    {
                        "name": "Hengshu Zhu"
                    },
                    {
                        "name": "Chuan Qin"
                    },
                    {
                        "name": "Han Wu"
                    },
                    {
                        "name": "Yihang Cheng"
                    },
                    {
                        "name": "Sirui Zhang"
                    },
                    {
                        "name": "Xiaowei Jin"
                    },
                    {
                        "name": "Yinuo Shen"
                    },
                    {
                        "name": "Zhenxing Wang"
                    },
                    {
                        "name": "Feimin Zhong"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09656v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09656v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09655v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09655v1",
                "updated": "2025-06-11T12:25:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    12,
                    25,
                    32,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T12:25:32Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    12,
                    25,
                    32,
                    2,
                    162,
                    0
                ],
                "title": "DipLLM: Fine-Tuning LLM for Strategic Decision-making in Diplomacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DipLLM: Fine-Tuning LLM for Strategic Decision-making in Diplomacy"
                },
                "summary": "Diplomacy is a complex multiplayer game that requires both cooperation and\ncompetition, posing significant challenges for AI systems. Traditional methods\nrely on equilibrium search to generate extensive game data for training, which\ndemands substantial computational resources. Large Language Models (LLMs) offer\na promising alternative, leveraging pre-trained knowledge to achieve strong\nperformance with relatively small-scale fine-tuning. However, applying LLMs to\nDiplomacy remains challenging due to the exponential growth of possible action\ncombinations and the intricate strategic interactions among players. To address\nthis challenge, we propose DipLLM, a fine-tuned LLM-based agent that learns\nequilibrium policies for Diplomacy. DipLLM employs an autoregressive\nfactorization framework to simplify the complex task of multi-unit action\nassignment into a sequence of unit-level decisions. By defining an equilibrium\npolicy within this framework as the learning objective, we fine-tune the model\nusing only 1.5% of the data required by the state-of-the-art Cicero model,\nsurpassing its performance. Our results demonstrate the potential of fine-tuned\nLLMs for tackling complex strategic decision-making in multiplayer games.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diplomacy is a complex multiplayer game that requires both cooperation and\ncompetition, posing significant challenges for AI systems. Traditional methods\nrely on equilibrium search to generate extensive game data for training, which\ndemands substantial computational resources. Large Language Models (LLMs) offer\na promising alternative, leveraging pre-trained knowledge to achieve strong\nperformance with relatively small-scale fine-tuning. However, applying LLMs to\nDiplomacy remains challenging due to the exponential growth of possible action\ncombinations and the intricate strategic interactions among players. To address\nthis challenge, we propose DipLLM, a fine-tuned LLM-based agent that learns\nequilibrium policies for Diplomacy. DipLLM employs an autoregressive\nfactorization framework to simplify the complex task of multi-unit action\nassignment into a sequence of unit-level decisions. By defining an equilibrium\npolicy within this framework as the learning objective, we fine-tune the model\nusing only 1.5% of the data required by the state-of-the-art Cicero model,\nsurpassing its performance. Our results demonstrate the potential of fine-tuned\nLLMs for tackling complex strategic decision-making in multiplayer games."
                },
                "authors": [
                    {
                        "name": "Kaixuan Xu"
                    },
                    {
                        "name": "Jiajun Chai"
                    },
                    {
                        "name": "Sicheng Li"
                    },
                    {
                        "name": "Yuqian Fu"
                    },
                    {
                        "name": "Yuanheng Zhu"
                    },
                    {
                        "name": "Dongbin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongbin Zhao"
                },
                "author": "Dongbin Zhao",
                "arxiv_comment": "Accepted to the 42nd International Conference on Machine Learning\n  (ICML 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09655v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09655v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14186v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14186v3",
                "updated": "2025-06-11T12:21:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    12,
                    21,
                    9,
                    2,
                    162,
                    0
                ],
                "published": "2024-02-22T00:24:03Z",
                "published_parsed": [
                    2024,
                    2,
                    22,
                    0,
                    24,
                    3,
                    3,
                    53,
                    0
                ],
                "title": "On the Arrow of Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Arrow of Inference"
                },
                "summary": "Just as the arrow of time structures physics, the arrow of inference\norganizes cognition, directing the flow of information in perception, action,\nand memory. The Context-Content Uncertainty Principle (CCUP) formalizes this\nasymmetry, between high-entropy context and low-entropy content, and frames\ninference as a cycle that aligns the two through selective, bidirectional\ninteraction. Cycle formation resolves the Information Bottleneck (IB) in\nOptimal Transport (OT) by coordinating bottom-up contextual disambiguation with\ntop-down content reconstruction, a process neurobiologically mirrored in the\ncyclical interplay between dorsal (context) and ventral (content) streams.\nLocal inference cycles extend into memory chains that simulate goals, support\ncounterfactual reasoning, and scaffold internal model refinement across time.\nBy operating on delta-seeded goal manifolds, each level of the hierarchy\ncircumvents the curse of dimensionality through structured diffusion guided by\npriors and context. This mechanism generalizes across timescales, from\nperception-action loops to the sleep-wake cycle-and scales socially through\nlanguage, which externalizes inference by transmitting latent content across\nminds. Thus, CCUP provides a unifying framework for understanding cognition as\ncycle-consistent inference, anchoring both individual thought and collective\nintelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Just as the arrow of time structures physics, the arrow of inference\norganizes cognition, directing the flow of information in perception, action,\nand memory. The Context-Content Uncertainty Principle (CCUP) formalizes this\nasymmetry, between high-entropy context and low-entropy content, and frames\ninference as a cycle that aligns the two through selective, bidirectional\ninteraction. Cycle formation resolves the Information Bottleneck (IB) in\nOptimal Transport (OT) by coordinating bottom-up contextual disambiguation with\ntop-down content reconstruction, a process neurobiologically mirrored in the\ncyclical interplay between dorsal (context) and ventral (content) streams.\nLocal inference cycles extend into memory chains that simulate goals, support\ncounterfactual reasoning, and scaffold internal model refinement across time.\nBy operating on delta-seeded goal manifolds, each level of the hierarchy\ncircumvents the curse of dimensionality through structured diffusion guided by\npriors and context. This mechanism generalizes across timescales, from\nperception-action loops to the sleep-wake cycle-and scales socially through\nlanguage, which externalizes inference by transmitting latent content across\nminds. Thus, CCUP provides a unifying framework for understanding cognition as\ncycle-consistent inference, anchoring both individual thought and collective\nintelligence."
                },
                "authors": [
                    {
                        "name": "Xin Li"
                    }
                ],
                "author_detail": {
                    "name": "Xin Li"
                },
                "author": "Xin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14186v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14186v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08356v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08356v2",
                "updated": "2025-06-11T12:15:03Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    12,
                    15,
                    3,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-10T02:14:15Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    2,
                    14,
                    15,
                    1,
                    161,
                    0
                ],
                "title": "MedMoE: Modality-Specialized Mixture of Experts for Medical\n  Vision-Language Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedMoE: Modality-Specialized Mixture of Experts for Medical\n  Vision-Language Understanding"
                },
                "summary": "Different medical imaging modalities capture diagnostic information at\nvarying spatial resolutions, from coarse global patterns to fine-grained\nlocalized structures. However, most existing vision-language frameworks in the\nmedical domain apply a uniform strategy for local feature extraction,\noverlooking the modality-specific demands. In this work, we present MedMoE, a\nmodular and extensible vision-language processing framework that dynamically\nadapts visual representation based on the diagnostic context. MedMoE\nincorporates a Mixture-of-Experts (MoE) module conditioned on the report type,\nwhich routes multi-scale image features through specialized expert branches\ntrained to capture modality-specific visual semantics. These experts operate\nover feature pyramids derived from a Swin Transformer backbone, enabling\nspatially adaptive attention to clinically relevant regions. This framework\nproduces localized visual representations aligned with textual descriptions,\nwithout requiring modality-specific supervision at inference. Empirical results\non diverse medical benchmarks demonstrate that MedMoE improves alignment and\nretrieval performance across imaging modalities, underscoring the value of\nmodality-specialized visual representations in clinical vision-language\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Different medical imaging modalities capture diagnostic information at\nvarying spatial resolutions, from coarse global patterns to fine-grained\nlocalized structures. However, most existing vision-language frameworks in the\nmedical domain apply a uniform strategy for local feature extraction,\noverlooking the modality-specific demands. In this work, we present MedMoE, a\nmodular and extensible vision-language processing framework that dynamically\nadapts visual representation based on the diagnostic context. MedMoE\nincorporates a Mixture-of-Experts (MoE) module conditioned on the report type,\nwhich routes multi-scale image features through specialized expert branches\ntrained to capture modality-specific visual semantics. These experts operate\nover feature pyramids derived from a Swin Transformer backbone, enabling\nspatially adaptive attention to clinically relevant regions. This framework\nproduces localized visual representations aligned with textual descriptions,\nwithout requiring modality-specific supervision at inference. Empirical results\non diverse medical benchmarks demonstrate that MedMoE improves alignment and\nretrieval performance across imaging modalities, underscoring the value of\nmodality-specialized visual representations in clinical vision-language\nsystems."
                },
                "authors": [
                    {
                        "name": "Shivang Chopra"
                    },
                    {
                        "name": "Gabriela Sanchez-Rodriguez"
                    },
                    {
                        "name": "Lingchao Mao"
                    },
                    {
                        "name": "Andrew J Feola"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Zsolt Kira"
                    }
                ],
                "author_detail": {
                    "name": "Zsolt Kira"
                },
                "author": "Zsolt Kira",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08356v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08356v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08903v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08903v3",
                "updated": "2025-06-11T12:11:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    12,
                    11,
                    10,
                    2,
                    162,
                    0
                ],
                "published": "2025-05-13T18:45:10Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    18,
                    45,
                    10,
                    1,
                    133,
                    0
                ],
                "title": "Assessing and Advancing Benchmarks for Evaluating Large Language Models\n  in Software Engineering Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing and Advancing Benchmarks for Evaluating Large Language Models\n  in Software Engineering Tasks"
                },
                "summary": "Large language models (LLMs) are gaining increasing popularity in software\nengineering (SE) due to their unprecedented performance across various\napplications. These models are increasingly being utilized for a range of SE\ntasks, including requirements engineering and design, code analysis and\ngeneration, software maintenance, and quality assurance. As LLMs become more\nintegral to SE, evaluating their effectiveness is crucial for understanding\ntheir potential in this field. In recent years, substantial efforts have been\nmade to assess LLM performance in various SE tasks, resulting in the creation\nof several benchmarks tailored to this purpose. This paper offers a thorough\nreview of 291 benchmarks, addressing three main aspects: what benchmarks are\navailable, how benchmarks are constructed, and the future outlook for these\nbenchmarks. We begin by examining SE tasks such as requirements engineering and\ndesign, coding assistant, software testing, AIOPs, software maintenance, and\nquality management. We then analyze the benchmarks and their development\nprocesses, highlighting the limitations of existing benchmarks. Additionally,\nwe discuss the successes and failures of LLMs in different software tasks and\nexplore future opportunities and challenges for SE-related benchmarks. We aim\nto provide a comprehensive overview of benchmark research in SE and offer\ninsights to support the creation of more effective evaluation tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are gaining increasing popularity in software\nengineering (SE) due to their unprecedented performance across various\napplications. These models are increasingly being utilized for a range of SE\ntasks, including requirements engineering and design, code analysis and\ngeneration, software maintenance, and quality assurance. As LLMs become more\nintegral to SE, evaluating their effectiveness is crucial for understanding\ntheir potential in this field. In recent years, substantial efforts have been\nmade to assess LLM performance in various SE tasks, resulting in the creation\nof several benchmarks tailored to this purpose. This paper offers a thorough\nreview of 291 benchmarks, addressing three main aspects: what benchmarks are\navailable, how benchmarks are constructed, and the future outlook for these\nbenchmarks. We begin by examining SE tasks such as requirements engineering and\ndesign, coding assistant, software testing, AIOPs, software maintenance, and\nquality management. We then analyze the benchmarks and their development\nprocesses, highlighting the limitations of existing benchmarks. Additionally,\nwe discuss the successes and failures of LLMs in different software tasks and\nexplore future opportunities and challenges for SE-related benchmarks. We aim\nto provide a comprehensive overview of benchmark research in SE and offer\ninsights to support the creation of more effective evaluation tools."
                },
                "authors": [
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Feifei Niu"
                    },
                    {
                        "name": "Junkai Chen"
                    },
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Junwei Zhang"
                    },
                    {
                        "name": "Junda He"
                    },
                    {
                        "name": "Xin Xia"
                    },
                    {
                        "name": "David Lo"
                    }
                ],
                "author_detail": {
                    "name": "David Lo"
                },
                "author": "David Lo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08903v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08903v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09648v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09648v1",
                "updated": "2025-06-11T12:09:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    12,
                    9,
                    5,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T12:09:05Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    12,
                    9,
                    5,
                    2,
                    162,
                    0
                ],
                "title": "Scaling Laws for Uncertainty in Deep Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Laws for Uncertainty in Deep Learning"
                },
                "summary": "Deep learning has recently revealed the existence of scaling laws,\ndemonstrating that model performance follows predictable trends based on\ndataset and model sizes. Inspired by these findings and fascinating phenomena\nemerging in the over-parameterized regime, we examine a parallel direction: do\nsimilar scaling laws govern predictive uncertainties in deep learning? In\nidentifiable parametric models, such scaling laws can be derived in a\nstraightforward manner by treating model parameters in a Bayesian way. In this\ncase, for example, we obtain $O(1/N)$ contraction rates for epistemic\nuncertainty with respect to the number of data $N$. However, in\nover-parameterized models, these guarantees do not hold, leading to largely\nunexplored behaviors. In this work, we empirically show the existence of\nscaling laws associated with various measures of predictive uncertainty with\nrespect to dataset and model sizes. Through experiments on vision and language\ntasks, we observe such scaling laws for in- and out-of-distribution predictive\nuncertainty estimated through popular approximate Bayesian inference and\nensemble methods. Besides the elegance of scaling laws and the practical\nutility of extrapolating uncertainties to larger data or models, this work\nprovides strong evidence to dispel recurring skepticism against Bayesian\napproaches: \"In many applications of deep learning we have so much data\navailable: what do we need Bayes for?\". Our findings show that \"so much data\"\nis typically not enough to make epistemic uncertainty negligible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning has recently revealed the existence of scaling laws,\ndemonstrating that model performance follows predictable trends based on\ndataset and model sizes. Inspired by these findings and fascinating phenomena\nemerging in the over-parameterized regime, we examine a parallel direction: do\nsimilar scaling laws govern predictive uncertainties in deep learning? In\nidentifiable parametric models, such scaling laws can be derived in a\nstraightforward manner by treating model parameters in a Bayesian way. In this\ncase, for example, we obtain $O(1/N)$ contraction rates for epistemic\nuncertainty with respect to the number of data $N$. However, in\nover-parameterized models, these guarantees do not hold, leading to largely\nunexplored behaviors. In this work, we empirically show the existence of\nscaling laws associated with various measures of predictive uncertainty with\nrespect to dataset and model sizes. Through experiments on vision and language\ntasks, we observe such scaling laws for in- and out-of-distribution predictive\nuncertainty estimated through popular approximate Bayesian inference and\nensemble methods. Besides the elegance of scaling laws and the practical\nutility of extrapolating uncertainties to larger data or models, this work\nprovides strong evidence to dispel recurring skepticism against Bayesian\napproaches: \"In many applications of deep learning we have so much data\navailable: what do we need Bayes for?\". Our findings show that \"so much data\"\nis typically not enough to make epistemic uncertainty negligible."
                },
                "authors": [
                    {
                        "name": "Mattia Rosso"
                    },
                    {
                        "name": "Simone Rossi"
                    },
                    {
                        "name": "Giulio Franzese"
                    },
                    {
                        "name": "Markus Heinonen"
                    },
                    {
                        "name": "Maurizio Filippone"
                    }
                ],
                "author_detail": {
                    "name": "Maurizio Filippone"
                },
                "author": "Maurizio Filippone",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09648v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09648v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01940v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01940v2",
                "updated": "2025-06-11T12:06:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    12,
                    6,
                    37,
                    2,
                    162,
                    0
                ],
                "published": "2025-03-03T12:55:49Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    12,
                    55,
                    49,
                    0,
                    62,
                    0
                ],
                "title": "AskToAct: Enhancing LLMs Tool Use via Self-Correcting Clarification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AskToAct: Enhancing LLMs Tool Use via Self-Correcting Clarification"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\ntool learning. In real-world scenarios, user queries are often ambiguous and\nincomplete, requiring effective clarification. However, existing interactive\nclarification approaches face two critical limitations: reliance on manually\nconstructed datasets, which inherently constrains training data scale and\ndiversity, and lack of error correction mechanisms during multi-turn\nclarification, leading to error accumulation that compromises both accuracy and\nefficiency. We present AskToAct, which addresses these challenges by exploiting\nthe structural mapping between queries and their tool invocation solutions. Our\nkey insight is that tool parameters naturally represent explicit user intents.\nBy systematically removing key parameters from queries while retaining them as\nground truth, we enable automated construction of high-quality training data.\nWe further enhance model robustness through error-correction pairs and\nselective masking, enabling dynamic error detection during clarification\ninteractions. Comprehensive experiments demonstrate that AskToAct significantly\noutperforms existing approaches, achieving above 57% accuracy in recovering\ncritical unspecified intents and enhancing clarification efficiency by an\naverage of 10.46% while maintaining high accuracy in tool invocation. Our\nframework exhibits robust performance across different model architectures and\nsuccessfully generalizes to entirely unseen APIs without additional training,\nachieving performance comparable to GPT-4o with substantially fewer\ncomputational resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities in\ntool learning. In real-world scenarios, user queries are often ambiguous and\nincomplete, requiring effective clarification. However, existing interactive\nclarification approaches face two critical limitations: reliance on manually\nconstructed datasets, which inherently constrains training data scale and\ndiversity, and lack of error correction mechanisms during multi-turn\nclarification, leading to error accumulation that compromises both accuracy and\nefficiency. We present AskToAct, which addresses these challenges by exploiting\nthe structural mapping between queries and their tool invocation solutions. Our\nkey insight is that tool parameters naturally represent explicit user intents.\nBy systematically removing key parameters from queries while retaining them as\nground truth, we enable automated construction of high-quality training data.\nWe further enhance model robustness through error-correction pairs and\nselective masking, enabling dynamic error detection during clarification\ninteractions. Comprehensive experiments demonstrate that AskToAct significantly\noutperforms existing approaches, achieving above 57% accuracy in recovering\ncritical unspecified intents and enhancing clarification efficiency by an\naverage of 10.46% while maintaining high accuracy in tool invocation. Our\nframework exhibits robust performance across different model architectures and\nsuccessfully generalizes to entirely unseen APIs without additional training,\nachieving performance comparable to GPT-4o with substantially fewer\ncomputational resources."
                },
                "authors": [
                    {
                        "name": "Xuan Zhang"
                    },
                    {
                        "name": "Yongliang Shen"
                    },
                    {
                        "name": "Zhe Zheng"
                    },
                    {
                        "name": "Linjuan Wu"
                    },
                    {
                        "name": "Wenqi Zhang"
                    },
                    {
                        "name": "Yuchen Yan"
                    },
                    {
                        "name": "Qiuying Peng"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Weiming Lu"
                    }
                ],
                "author_detail": {
                    "name": "Weiming Lu"
                },
                "author": "Weiming Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01940v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01940v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09645v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09645v1",
                "updated": "2025-06-11T12:03:52Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    12,
                    3,
                    52,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T12:03:52Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    12,
                    3,
                    52,
                    2,
                    162,
                    0
                ],
                "title": "Learning Efficient and Generalizable Graph Retriever for Knowledge-Graph\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Efficient and Generalizable Graph Retriever for Knowledge-Graph\n  Question Answering"
                },
                "summary": "Large Language Models (LLMs) have shown strong inductive reasoning ability\nacross various domains, but their reliability is hindered by the outdated\nknowledge and hallucinations. Retrieval-Augmented Generation mitigates these\nissues by grounding LLMs with external knowledge; however, most existing RAG\npipelines rely on unstructured text, limiting interpretability and structured\nreasoning. Knowledge graphs, which represent facts as relational triples, offer\na more structured and compact alternative. Recent studies have explored\nintegrating knowledge graphs with LLMs for knowledge graph question answering\n(KGQA), with a significant proportion adopting the retrieve-then-reasoning\nparadigm. In this framework, graph-based retrievers have demonstrated strong\nempirical performance, yet they still face challenges in generalization\nability. In this work, we propose RAPL, a novel framework for efficient and\neffective graph retrieval in KGQA. RAPL addresses these limitations through\nthree aspects: (1) a two-stage labeling strategy that combines heuristic\nsignals with parametric models to provide causally grounded supervision; (2) a\nmodel-agnostic graph transformation approach to capture both intra- and\ninter-triple interactions, thereby enhancing representational capacity; and (3)\na path-based reasoning strategy that facilitates learning from the injected\nrational knowledge, and supports downstream reasoner through structured inputs.\nEmpirically, RAPL outperforms state-of-the-art methods by $2.66\\%-20.34\\%$, and\nsignificantly reduces the performance gap between smaller and more powerful\nLLM-based reasoners, as well as the gap under cross-dataset settings,\nhighlighting its superior retrieval capability and generalizability. Codes are\navailable at: https://github.com/tianyao-aka/RAPL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown strong inductive reasoning ability\nacross various domains, but their reliability is hindered by the outdated\nknowledge and hallucinations. Retrieval-Augmented Generation mitigates these\nissues by grounding LLMs with external knowledge; however, most existing RAG\npipelines rely on unstructured text, limiting interpretability and structured\nreasoning. Knowledge graphs, which represent facts as relational triples, offer\na more structured and compact alternative. Recent studies have explored\nintegrating knowledge graphs with LLMs for knowledge graph question answering\n(KGQA), with a significant proportion adopting the retrieve-then-reasoning\nparadigm. In this framework, graph-based retrievers have demonstrated strong\nempirical performance, yet they still face challenges in generalization\nability. In this work, we propose RAPL, a novel framework for efficient and\neffective graph retrieval in KGQA. RAPL addresses these limitations through\nthree aspects: (1) a two-stage labeling strategy that combines heuristic\nsignals with parametric models to provide causally grounded supervision; (2) a\nmodel-agnostic graph transformation approach to capture both intra- and\ninter-triple interactions, thereby enhancing representational capacity; and (3)\na path-based reasoning strategy that facilitates learning from the injected\nrational knowledge, and supports downstream reasoner through structured inputs.\nEmpirically, RAPL outperforms state-of-the-art methods by $2.66\\%-20.34\\%$, and\nsignificantly reduces the performance gap between smaller and more powerful\nLLM-based reasoners, as well as the gap under cross-dataset settings,\nhighlighting its superior retrieval capability and generalizability. Codes are\navailable at: https://github.com/tianyao-aka/RAPL."
                },
                "authors": [
                    {
                        "name": "Tianjun Yao"
                    },
                    {
                        "name": "Haoxuan Li"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    },
                    {
                        "name": "Pan Li"
                    },
                    {
                        "name": "Tongliang Liu"
                    },
                    {
                        "name": "Kun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kun Zhang"
                },
                "author": "Kun Zhang",
                "arxiv_comment": "32 pages, 28 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09645v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09645v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04118v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04118v3",
                "updated": "2025-06-11T12:00:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    12,
                    0,
                    16,
                    2,
                    162,
                    0
                ],
                "published": "2025-04-05T09:19:22Z",
                "published_parsed": [
                    2025,
                    4,
                    5,
                    9,
                    19,
                    22,
                    5,
                    95,
                    0
                ],
                "title": "Joint Analysis of Constraints on f(R) Parametrization from Recent\n  Cosmological Observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Analysis of Constraints on f(R) Parametrization from Recent\n  Cosmological Observations"
                },
                "summary": "In this study, we present constraints on the parameters of three well-known\n$f(R)$ gravity models, viz. (i) Hu-Sawicki, (ii) Starobinsky, and (iii) ArcTanh\nby using a joint analysis of recent cosmological observations. We perform\nanalytical approximations for the Hubble parameter, $H(z)$, and cosmological\ndistances in terms of the Hubble constant $(H_0)$, matter density\n$(\\Omega_{m0})$, and a deviation parameter $b$ for each model. {Our analysis\ncombines early and late-universe cosmological data from five cosmological\nobservations:} (a) Hubble parameter measurements (Cosmic Chronometers), (b)\nType Ia Supernovae (Union 3.0), (c) Baryon Acoustic Oscillations (DESI-2025),\n(d) Gamma-Ray Bursts (GRBs) and (e) Cosmic Microwave Background (CMB). We first\noptimize the models using each dataset independently, and subsequently, we\nperform a comprehensive joint analysis combining all four datasets. Our results\nshow that the Hu-Sawicki and ArcTanh models do not deviate significantly from\nthe $\\Lambda$CDM model at 95% confidence level for individual datasets and\nremain consistent at 99% confidence level in the joint analysis. In contrast,\nthe Starobinsky model shows a strong deviation and appears as a viable\nalternative to $\\Lambda$CDM. We also constrain the transition redshift\nparameter ($z_t$), and check that the obtained value agrees with the values\ninferred from both early-time measurement (Planck) and late-time data from Type\nIa Supernovae. These results support the potential support of $f(R)$ gravity to\nexplain the late-time cosmic acceleration effectively. Finally, a statistical\nmodel comparison using $\\chi^2_{\\text{min}}$, AIC, and BIC indicates that all\nthree $f(R)$ models are favored over $\\Lambda$CDM, with the Starobinsky model\nreceiving very strong support.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we present constraints on the parameters of three well-known\n$f(R)$ gravity models, viz. (i) Hu-Sawicki, (ii) Starobinsky, and (iii) ArcTanh\nby using a joint analysis of recent cosmological observations. We perform\nanalytical approximations for the Hubble parameter, $H(z)$, and cosmological\ndistances in terms of the Hubble constant $(H_0)$, matter density\n$(\\Omega_{m0})$, and a deviation parameter $b$ for each model. {Our analysis\ncombines early and late-universe cosmological data from five cosmological\nobservations:} (a) Hubble parameter measurements (Cosmic Chronometers), (b)\nType Ia Supernovae (Union 3.0), (c) Baryon Acoustic Oscillations (DESI-2025),\n(d) Gamma-Ray Bursts (GRBs) and (e) Cosmic Microwave Background (CMB). We first\noptimize the models using each dataset independently, and subsequently, we\nperform a comprehensive joint analysis combining all four datasets. Our results\nshow that the Hu-Sawicki and ArcTanh models do not deviate significantly from\nthe $\\Lambda$CDM model at 95% confidence level for individual datasets and\nremain consistent at 99% confidence level in the joint analysis. In contrast,\nthe Starobinsky model shows a strong deviation and appears as a viable\nalternative to $\\Lambda$CDM. We also constrain the transition redshift\nparameter ($z_t$), and check that the obtained value agrees with the values\ninferred from both early-time measurement (Planck) and late-time data from Type\nIa Supernovae. These results support the potential support of $f(R)$ gravity to\nexplain the late-time cosmic acceleration effectively. Finally, a statistical\nmodel comparison using $\\chi^2_{\\text{min}}$, AIC, and BIC indicates that all\nthree $f(R)$ models are favored over $\\Lambda$CDM, with the Starobinsky model\nreceiving very strong support."
                },
                "authors": [
                    {
                        "name": "Darshan Kumar"
                    },
                    {
                        "name": "Praveen Kumar Dhankar"
                    },
                    {
                        "name": "Saibal Ray"
                    },
                    {
                        "name": "Fengge Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Fengge Zhang"
                },
                "arxiv_affiliation": "HNAS",
                "author": "Fengge Zhang",
                "arxiv_comment": "15 pages, 6 figures, and 4 tables. Accepted for publication in Phys.\n  Dark Univ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04118v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04118v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2506.09996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09996v1",
                "updated": "2025-06-11T17:59:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    59,
                    58,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T17:59:58Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    59,
                    58,
                    2,
                    162,
                    0
                ],
                "title": "From Judgment to Interference: Early Stopping LLM Harmful Outputs via\n  Streaming Content Monitoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Judgment to Interference: Early Stopping LLM Harmful Outputs via\n  Streaming Content Monitoring"
                },
                "summary": "Though safety alignment has been applied to most large language models\n(LLMs), LLM service providers generally deploy a subsequent moderation as the\nexternal safety guardrail in real-world products. Existing moderators mainly\npractice a conventional full detection, which determines the harmfulness based\non the complete LLM output, causing high service latency. Recent works pay more\nattention to partial detection where moderators oversee the generation midway\nand early stop the output if harmfulness is detected, but they directly apply\nmoderators trained with the full detection paradigm to incomplete outputs,\nintroducing a training-inference gap that lowers the performance. In this\npaper, we explore how to form a data-and-model solution that natively supports\npartial detection. For the data, we construct FineHarm, a dataset consisting of\n29K prompt-response pairs with fine-grained annotations to provide reasonable\nsupervision for token-level training. Then, we propose the streaming content\nmonitor, which is trained with dual supervision of response- and token-level\nlabels and can follow the output stream of LLM to make a timely judgment of\nharmfulness. Experiments show that SCM gains 0.95+ in macro F1 score that is\ncomparable to full detection, by only seeing the first 18% of tokens in\nresponses on average. Moreover, the SCM can serve as a pseudo-harmfulness\nannotator for improving safety alignment and lead to a higher harmlessness\nscore than DPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Though safety alignment has been applied to most large language models\n(LLMs), LLM service providers generally deploy a subsequent moderation as the\nexternal safety guardrail in real-world products. Existing moderators mainly\npractice a conventional full detection, which determines the harmfulness based\non the complete LLM output, causing high service latency. Recent works pay more\nattention to partial detection where moderators oversee the generation midway\nand early stop the output if harmfulness is detected, but they directly apply\nmoderators trained with the full detection paradigm to incomplete outputs,\nintroducing a training-inference gap that lowers the performance. In this\npaper, we explore how to form a data-and-model solution that natively supports\npartial detection. For the data, we construct FineHarm, a dataset consisting of\n29K prompt-response pairs with fine-grained annotations to provide reasonable\nsupervision for token-level training. Then, we propose the streaming content\nmonitor, which is trained with dual supervision of response- and token-level\nlabels and can follow the output stream of LLM to make a timely judgment of\nharmfulness. Experiments show that SCM gains 0.95+ in macro F1 score that is\ncomparable to full detection, by only seeing the first 18% of tokens in\nresponses on average. Moreover, the SCM can serve as a pseudo-harmfulness\nannotator for improving safety alignment and lead to a higher harmlessness\nscore than DPO."
                },
                "authors": [
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Qiang Sheng"
                    },
                    {
                        "name": "Yehan Yang"
                    },
                    {
                        "name": "Xueyao Zhang"
                    },
                    {
                        "name": "Juan Cao"
                    }
                ],
                "author_detail": {
                    "name": "Juan Cao"
                },
                "author": "Juan Cao",
                "arxiv_comment": "22 pages, 7 figures, and 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09998v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09998v1",
                "updated": "2025-06-11T17:59:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    59,
                    58,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T17:59:58Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    59,
                    58,
                    2,
                    162,
                    0
                ],
                "title": "Flipping Against All Odds: Reducing LLM Coin Flip Bias via Verbalized\n  Rejection Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flipping Against All Odds: Reducing LLM Coin Flip Bias via Verbalized\n  Rejection Sampling"
                },
                "summary": "Large language models (LLMs) can often accurately describe probability\ndistributions using natural language, yet they still struggle to generate\nfaithful samples from them. This mismatch limits their use in tasks requiring\nreliable stochasticity, such as Monte Carlo methods, agent-based simulations,\nand randomized decision-making. We investigate this gap between knowledge and\nsampling in the context of Bernoulli distributions. We introduce Verbalized\nRejection Sampling (VRS), a natural-language adaptation of classical rejection\nsampling that prompts the LLM to reason about and accept or reject proposed\nsamples. Despite relying on the same Bernoulli mechanism internally, VRS\nsubstantially reduces sampling bias across models. We provide theoretical\nanalysis showing that, under mild assumptions, VRS improves over direct\nsampling, with gains attributable to both the algorithm and prompt design. More\nbroadly, our results show how classical probabilistic tools can be verbalized\nand embedded into LLM workflows to improve reliability, without requiring\naccess to model internals or heavy prompt engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can often accurately describe probability\ndistributions using natural language, yet they still struggle to generate\nfaithful samples from them. This mismatch limits their use in tasks requiring\nreliable stochasticity, such as Monte Carlo methods, agent-based simulations,\nand randomized decision-making. We investigate this gap between knowledge and\nsampling in the context of Bernoulli distributions. We introduce Verbalized\nRejection Sampling (VRS), a natural-language adaptation of classical rejection\nsampling that prompts the LLM to reason about and accept or reject proposed\nsamples. Despite relying on the same Bernoulli mechanism internally, VRS\nsubstantially reduces sampling bias across models. We provide theoretical\nanalysis showing that, under mild assumptions, VRS improves over direct\nsampling, with gains attributable to both the algorithm and prompt design. More\nbroadly, our results show how classical probabilistic tools can be verbalized\nand embedded into LLM workflows to improve reliability, without requiring\naccess to model internals or heavy prompt engineering."
                },
                "authors": [
                    {
                        "name": "Tim Z. Xiao"
                    },
                    {
                        "name": "Johannes Zenn"
                    },
                    {
                        "name": "Zhen Liu"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Robert Bamler"
                    },
                    {
                        "name": "Bernhard Schlkopf"
                    }
                ],
                "author_detail": {
                    "name": "Bernhard Schlkopf"
                },
                "author": "Bernhard Schlkopf",
                "arxiv_comment": "Technical Report v1 (21 pages, 14 figures)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09998v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09998v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09991v1",
                "updated": "2025-06-11T17:59:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    59,
                    23,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T17:59:23Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    59,
                    23,
                    2,
                    162,
                    0
                ],
                "title": "Multiverse: Your Language Models Secretly Decide How to Parallelize and\n  Merge Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiverse: Your Language Models Secretly Decide How to Parallelize and\n  Merge Generation"
                },
                "summary": "Autoregressive Large Language Models (AR-LLMs) frequently exhibit implicit\nparallelism in sequential generation. Inspired by this, we introduce\nMultiverse, a new generative model that enables natively parallel generation.\nMultiverse internalizes a MapReduce paradigm, generating automatically through\nthree stages: (i) a Map stage for adaptive task decomposition, (ii) a Process\nstage for parallel subtask execution, and (iii) a Reduce stage for lossless\nresult synthesis. Next, we build a real-world Multiverse reasoning model with\nco-design of data, algorithm, and system, enabling rapid and seamless transfer\nfrom frontier AR-LLMs. Starting from sequential reasoning chains, we create\nMultiverse 1K by converting them into structured training data using an\nautomated LLM-assisted pipeline, avoiding costly human annotations.\nAlgorithmically, we design Multiverse Attention to separate parallel reasoning\nsteps while keeping compatibility with causal attention for efficient training.\nSystematically, we implement Multiverse Engine to enable parallel inference. It\nfeatures a dedicated scheduler that dynamically switches between sequential and\nparallel generation, triggered directly by the model. After a 3-hour\nfine-tuning with 1K examples, our Multiverse-32B stands as the only\nopen-sourced non-AR model achieving performance on par with leading AR-LLMs of\nthe same scale, evidenced by AIME24 & 25 scores of 54% and 46%, respectively.\nMoreover, our budget control experiments show that Multiverse-32B exhibits\nsuperior scaling, outperforming AR-LLMs by 1.87% on average using the same\ncontext length. Such scaling further leads to practical efficiency gain,\nachieving up to 2x speedup across varying batch sizes. We have open-sourced the\nentire Multiverse ecosystem, including data, model weights, engine, supporting\ntools, as well as complete data curation prompts and detailed training and\nevaluation recipes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Large Language Models (AR-LLMs) frequently exhibit implicit\nparallelism in sequential generation. Inspired by this, we introduce\nMultiverse, a new generative model that enables natively parallel generation.\nMultiverse internalizes a MapReduce paradigm, generating automatically through\nthree stages: (i) a Map stage for adaptive task decomposition, (ii) a Process\nstage for parallel subtask execution, and (iii) a Reduce stage for lossless\nresult synthesis. Next, we build a real-world Multiverse reasoning model with\nco-design of data, algorithm, and system, enabling rapid and seamless transfer\nfrom frontier AR-LLMs. Starting from sequential reasoning chains, we create\nMultiverse 1K by converting them into structured training data using an\nautomated LLM-assisted pipeline, avoiding costly human annotations.\nAlgorithmically, we design Multiverse Attention to separate parallel reasoning\nsteps while keeping compatibility with causal attention for efficient training.\nSystematically, we implement Multiverse Engine to enable parallel inference. It\nfeatures a dedicated scheduler that dynamically switches between sequential and\nparallel generation, triggered directly by the model. After a 3-hour\nfine-tuning with 1K examples, our Multiverse-32B stands as the only\nopen-sourced non-AR model achieving performance on par with leading AR-LLMs of\nthe same scale, evidenced by AIME24 & 25 scores of 54% and 46%, respectively.\nMoreover, our budget control experiments show that Multiverse-32B exhibits\nsuperior scaling, outperforming AR-LLMs by 1.87% on average using the same\ncontext length. Such scaling further leads to practical efficiency gain,\nachieving up to 2x speedup across varying batch sizes. We have open-sourced the\nentire Multiverse ecosystem, including data, model weights, engine, supporting\ntools, as well as complete data curation prompts and detailed training and\nevaluation recipes."
                },
                "authors": [
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Yuwei An"
                    },
                    {
                        "name": "Hongyi Liu"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09983v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09983v1",
                "updated": "2025-06-11T17:56:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    56,
                    10,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T17:56:10Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    56,
                    10,
                    2,
                    162,
                    0
                ],
                "title": "Step-by-step Instructions and a Simple Tabular Output Format Improve the\n  Dependency Parsing Accuracy of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Step-by-step Instructions and a Simple Tabular Output Format Improve the\n  Dependency Parsing Accuracy of LLMs"
                },
                "summary": "Recent advances in large language models (LLMs) have enabled impressive\nperformance in various tasks. However, standard prompting often struggles to\nproduce structurally valid and accurate outputs, especially in dependency\nparsing. We propose a novel step-by-step instruction strategy, where universal\npart-of-speech tagging precedes the prediction of syntactic heads and\ndependency labels, and a simplified CoNLL-U like output format, our method\nachieves state-of-the-art accuracy on Universal Dependencies datasets across 17\nlanguages without hallucination or contamination. We further show that\nmultilingual fine-tuning simultaneously improves cross-language generalization\nperformance. Our results highlight the effectiveness of explicit reasoning\nsteps in LLM-based parsing and offer a scalable, format-consistent alternative\nto bracket-based approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have enabled impressive\nperformance in various tasks. However, standard prompting often struggles to\nproduce structurally valid and accurate outputs, especially in dependency\nparsing. We propose a novel step-by-step instruction strategy, where universal\npart-of-speech tagging precedes the prediction of syntactic heads and\ndependency labels, and a simplified CoNLL-U like output format, our method\nachieves state-of-the-art accuracy on Universal Dependencies datasets across 17\nlanguages without hallucination or contamination. We further show that\nmultilingual fine-tuning simultaneously improves cross-language generalization\nperformance. Our results highlight the effectiveness of explicit reasoning\nsteps in LLM-based parsing and offer a scalable, format-consistent alternative\nto bracket-based approaches."
                },
                "authors": [
                    {
                        "name": "Hiroshi Matsuda"
                    },
                    {
                        "name": "Chunpeng Ma"
                    },
                    {
                        "name": "Masayuki Asahara"
                    }
                ],
                "author_detail": {
                    "name": "Masayuki Asahara"
                },
                "author": "Masayuki Asahara",
                "arxiv_comment": "9 pages, 2 figures, accepted for SyntaxFest 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09983v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09983v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09975v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09975v1",
                "updated": "2025-06-11T17:51:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    51,
                    28,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T17:51:28Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    51,
                    28,
                    2,
                    162,
                    0
                ],
                "title": "When Detection Fails: The Power of Fine-Tuned Models to Generate\n  Human-Like Social Media Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Detection Fails: The Power of Fine-Tuned Models to Generate\n  Human-Like Social Media Text"
                },
                "summary": "Detecting AI-generated text is a difficult problem to begin with; detecting\nAI-generated text on social media is made even more difficult due to the short\ntext length and informal, idiosyncratic language of the internet. It is\nnonetheless important to tackle this problem, as social media represents a\nsignificant attack vector in online influence campaigns, which may be bolstered\nthrough the use of mass-produced AI-generated posts supporting (or opposing)\nparticular policies, decisions, or events. We approach this problem with the\nmindset and resources of a reasonably sophisticated threat actor, and create a\ndataset of 505,159 AI-generated social media posts from a combination of\nopen-source, closed-source, and fine-tuned LLMs, covering 11 different\ncontroversial topics. We show that while the posts can be detected under\ntypical research assumptions about knowledge of and access to the generating\nmodels, under the more realistic assumption that an attacker will not release\ntheir fine-tuned model to the public, detectability drops dramatically. This\nresult is confirmed with a human study. Ablation experiments highlight the\nvulnerability of various detection algorithms to fine-tuned LLMs. This result\nhas implications across all detection domains, since fine-tuning is a generally\napplicable and realistic LLM use case.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting AI-generated text is a difficult problem to begin with; detecting\nAI-generated text on social media is made even more difficult due to the short\ntext length and informal, idiosyncratic language of the internet. It is\nnonetheless important to tackle this problem, as social media represents a\nsignificant attack vector in online influence campaigns, which may be bolstered\nthrough the use of mass-produced AI-generated posts supporting (or opposing)\nparticular policies, decisions, or events. We approach this problem with the\nmindset and resources of a reasonably sophisticated threat actor, and create a\ndataset of 505,159 AI-generated social media posts from a combination of\nopen-source, closed-source, and fine-tuned LLMs, covering 11 different\ncontroversial topics. We show that while the posts can be detected under\ntypical research assumptions about knowledge of and access to the generating\nmodels, under the more realistic assumption that an attacker will not release\ntheir fine-tuned model to the public, detectability drops dramatically. This\nresult is confirmed with a human study. Ablation experiments highlight the\nvulnerability of various detection algorithms to fine-tuned LLMs. This result\nhas implications across all detection domains, since fine-tuning is a generally\napplicable and realistic LLM use case."
                },
                "authors": [
                    {
                        "name": "Hillary Dawkins"
                    },
                    {
                        "name": "Kathleen C. Fraser"
                    },
                    {
                        "name": "Svetlana Kiritchenko"
                    }
                ],
                "author_detail": {
                    "name": "Svetlana Kiritchenko"
                },
                "author": "Svetlana Kiritchenko",
                "arxiv_comment": "to appear in ACL Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09975v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09975v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12372v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12372v6",
                "updated": "2025-06-11T17:49:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    49,
                    37,
                    2,
                    162,
                    0
                ],
                "published": "2025-01-21T18:52:15Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    18,
                    52,
                    15,
                    1,
                    21,
                    0
                ],
                "title": "Is Long Context All You Need? Leveraging LLM's Extended Context for\n  NL2SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Long Context All You Need? Leveraging LLM's Extended Context for\n  NL2SQL"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities across\na range of natural language processing tasks. In particular, improvements in\nreasoning abilities and the expansion of context windows have opened new\navenues for leveraging these powerful models. NL2SQL is challenging in that the\nnatural language question is inherently ambiguous, while the SQL generation\nrequires a precise understanding of complex data schema and semantics. One\napproach to this semantic ambiguous problem is to provide more and sufficient\ncontextual information.\n  In this work, we explore the performance and the latency trade-offs of the\nextended context window (a.k.a., long context) offered by Google's\nstate-of-the-art LLM (\\textit{gemini-1.5-pro}). We study the impact of various\ncontextual information, including column example values, question and SQL query\npairs, user-provided hints, SQL documentation, and schema. To the best of our\nknowledge, this is the first work to study how the extended context window and\nextra contextual information can help NL2SQL generation with respect to both\naccuracy and latency cost. We show that long context LLMs are robust and do not\nget lost in the extended contextual information. Additionally, our long-context\nNL2SQL pipeline based on Google's \\textit{gemini-pro-1.5} achieve strong\nperformances on various benchmark datasets without finetuning and expensive\nself-consistency based techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities across\na range of natural language processing tasks. In particular, improvements in\nreasoning abilities and the expansion of context windows have opened new\navenues for leveraging these powerful models. NL2SQL is challenging in that the\nnatural language question is inherently ambiguous, while the SQL generation\nrequires a precise understanding of complex data schema and semantics. One\napproach to this semantic ambiguous problem is to provide more and sufficient\ncontextual information.\n  In this work, we explore the performance and the latency trade-offs of the\nextended context window (a.k.a., long context) offered by Google's\nstate-of-the-art LLM (\\textit{gemini-1.5-pro}). We study the impact of various\ncontextual information, including column example values, question and SQL query\npairs, user-provided hints, SQL documentation, and schema. To the best of our\nknowledge, this is the first work to study how the extended context window and\nextra contextual information can help NL2SQL generation with respect to both\naccuracy and latency cost. We show that long context LLMs are robust and do not\nget lost in the extended contextual information. Additionally, our long-context\nNL2SQL pipeline based on Google's \\textit{gemini-pro-1.5} achieve strong\nperformances on various benchmark datasets without finetuning and expensive\nself-consistency based techniques."
                },
                "authors": [
                    {
                        "name": "Yeounoh Chung"
                    },
                    {
                        "name": "Gaurav T. Kakkar"
                    },
                    {
                        "name": "Yu Gan"
                    },
                    {
                        "name": "Brenton Milne"
                    },
                    {
                        "name": "Fatma Ozcan"
                    }
                ],
                "author_detail": {
                    "name": "Fatma Ozcan"
                },
                "author": "Fatma Ozcan",
                "arxiv_doi": "10.14778/3742728.3742761",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.14778/3742728.3742761",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.12372v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12372v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "13 pages, 6 figures, VLDB 2025",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.16998v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.16998v5",
                "updated": "2025-06-11T17:46:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    46,
                    56,
                    2,
                    162,
                    0
                ],
                "published": "2024-03-25T17:59:09Z",
                "published_parsed": [
                    2024,
                    3,
                    25,
                    17,
                    59,
                    9,
                    0,
                    85,
                    0
                ],
                "title": "Understanding Long Videos with Multimodal Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Long Videos with Multimodal Language Models"
                },
                "summary": "Large Language Models (LLMs) have allowed recent LLM-based approaches to\nachieve excellent performance on long-video understanding benchmarks. We\ninvestigate how extensive world knowledge and strong reasoning skills of\nunderlying LLMs influence this strong performance. Surprisingly, we discover\nthat LLM-based approaches can yield surprisingly good accuracy on long-video\ntasks with limited video information, sometimes even with no video specific\ninformation. Building on this, we explore injecting video-specific information\ninto an LLM-based framework. We utilize off-the-shelf vision tools to extract\nthree object-centric information modalities from videos, and then leverage\nnatural language as a medium for fusing this information. Our resulting\nMultimodal Video Understanding (MVU) framework demonstrates state-of-the-art\nperformance across multiple video understanding benchmarks. Strong performance\nalso on robotics domain tasks establish its strong generality. Code:\nhttps://github.com/kahnchana/mvu",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have allowed recent LLM-based approaches to\nachieve excellent performance on long-video understanding benchmarks. We\ninvestigate how extensive world knowledge and strong reasoning skills of\nunderlying LLMs influence this strong performance. Surprisingly, we discover\nthat LLM-based approaches can yield surprisingly good accuracy on long-video\ntasks with limited video information, sometimes even with no video specific\ninformation. Building on this, we explore injecting video-specific information\ninto an LLM-based framework. We utilize off-the-shelf vision tools to extract\nthree object-centric information modalities from videos, and then leverage\nnatural language as a medium for fusing this information. Our resulting\nMultimodal Video Understanding (MVU) framework demonstrates state-of-the-art\nperformance across multiple video understanding benchmarks. Strong performance\nalso on robotics domain tasks establish its strong generality. Code:\nhttps://github.com/kahnchana/mvu"
                },
                "authors": [
                    {
                        "name": "Kanchana Ranasinghe"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Kumara Kahatapitiya"
                    },
                    {
                        "name": "Michael S. Ryoo"
                    }
                ],
                "author_detail": {
                    "name": "Michael S. Ryoo"
                },
                "author": "Michael S. Ryoo",
                "arxiv_comment": "17 pages (main paper), 7 pages appendix. ICLR 2025 conference paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.16998v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.16998v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09968v1",
                "updated": "2025-06-11T17:45:03Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    45,
                    3,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T17:45:03Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    45,
                    3,
                    2,
                    162,
                    0
                ],
                "title": "SRLAgent: Enhancing Self-Regulated Learning Skills through Gamification\n  and LLM Assistance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SRLAgent: Enhancing Self-Regulated Learning Skills through Gamification\n  and LLM Assistance"
                },
                "summary": "Self-regulated learning (SRL) is crucial for college students navigating\nincreased academic demands and independence. Insufficient SRL skills can lead\nto disorganized study habits, low motivation, and poor time management,\nundermining learners ability to thrive in challenging environments. Through a\nformative study involving 59 college students, we identified key challenges\nstudents face in developing SRL skills, including difficulties with\ngoal-setting, time management, and reflective learning. To address these\nchallenges, we introduce SRLAgent, an LLM-assisted system that fosters SRL\nskills through gamification and adaptive support from large language models\n(LLMs). Grounded in Zimmermans three-phase SRL framework, SRLAgent enables\nstudents to engage in goal-setting, strategy execution, and self-reflection\nwithin an interactive game-based environment. The system offers real-time\nfeedback and scaffolding powered by LLMs to support students independent study\nefforts. We evaluated SRLAgent using a between-subjects design, comparing it to\na baseline system (SRL without Agent features) and a traditional multimedia\nlearning condition. Results showed significant improvements in SRL skills\nwithin the SRLAgent group (p < .001, Cohens d = 0.234) and higher engagement\ncompared to the baselines. This work highlights the value of embedding SRL\nscaffolding and real-time AI support within gamified environments, offering\ndesign implications for educational technologies that aim to promote deeper\nlearning and metacognitive skill development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-regulated learning (SRL) is crucial for college students navigating\nincreased academic demands and independence. Insufficient SRL skills can lead\nto disorganized study habits, low motivation, and poor time management,\nundermining learners ability to thrive in challenging environments. Through a\nformative study involving 59 college students, we identified key challenges\nstudents face in developing SRL skills, including difficulties with\ngoal-setting, time management, and reflective learning. To address these\nchallenges, we introduce SRLAgent, an LLM-assisted system that fosters SRL\nskills through gamification and adaptive support from large language models\n(LLMs). Grounded in Zimmermans three-phase SRL framework, SRLAgent enables\nstudents to engage in goal-setting, strategy execution, and self-reflection\nwithin an interactive game-based environment. The system offers real-time\nfeedback and scaffolding powered by LLMs to support students independent study\nefforts. We evaluated SRLAgent using a between-subjects design, comparing it to\na baseline system (SRL without Agent features) and a traditional multimedia\nlearning condition. Results showed significant improvements in SRL skills\nwithin the SRLAgent group (p < .001, Cohens d = 0.234) and higher engagement\ncompared to the baselines. This work highlights the value of embedding SRL\nscaffolding and real-time AI support within gamified environments, offering\ndesign implications for educational technologies that aim to promote deeper\nlearning and metacognitive skill development."
                },
                "authors": [
                    {
                        "name": "Wentao Ge"
                    },
                    {
                        "name": "Yuqing Sun"
                    },
                    {
                        "name": "Ziyan Wang"
                    },
                    {
                        "name": "Haoyue Zheng"
                    },
                    {
                        "name": "Weiyang He"
                    },
                    {
                        "name": "Piaohong Wang"
                    },
                    {
                        "name": "Qianyu Zhu"
                    },
                    {
                        "name": "Benyou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Benyou Wang"
                },
                "author": "Benyou Wang",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.1; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09965v1",
                "updated": "2025-06-11T17:41:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    41,
                    50,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T17:41:50Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    41,
                    50,
                    2,
                    162,
                    0
                ],
                "title": "Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven\n  Thinking and Visual Drawing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven\n  Thinking and Visual Drawing"
                },
                "summary": "As textual reasoning with large language models (LLMs) has advanced\nsignificantly, there has been growing interest in enhancing the multimodal\nreasoning capabilities of large vision-language models (LVLMs). However,\nexisting methods primarily approach multimodal reasoning in a straightforward,\ntext-centric manner, where both reasoning and answer derivation are conducted\npurely through text, with the only difference being the presence of multimodal\ninput. As a result, these methods often encounter fundamental limitations in\nspatial reasoning tasks that demand precise geometric understanding and\ncontinuous spatial tracking-capabilities that humans achieve through mental\nvisualization and manipulation. To address the limitations, we propose drawing\nto reason in space, a novel paradigm that enables LVLMs to reason through\nelementary drawing operations in the visual space. By equipping models with\nbasic drawing operations, including annotating bounding boxes and drawing\nauxiliary lines, we empower them to express and analyze spatial relationships\nthrough direct visual manipulation, meanwhile avoiding the performance ceiling\nimposed by specialized perception tools in previous tool-integrated reasoning\napproaches. To cultivate this capability, we develop a three-stage training\nframework: cold-start training with synthetic data to establish basic drawing\nabilities, reflective rejection sampling to enhance self-reflection behaviors,\nand reinforcement learning to directly optimize for target rewards. Extensive\nexperiments demonstrate that our model, named VILASR, consistently outperforms\nexisting methods across diverse spatial reasoning benchmarks, involving maze\nnavigation, static spatial reasoning, video-based reasoning, and\nmulti-view-based reasoning tasks, with an average improvement of 18.4%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As textual reasoning with large language models (LLMs) has advanced\nsignificantly, there has been growing interest in enhancing the multimodal\nreasoning capabilities of large vision-language models (LVLMs). However,\nexisting methods primarily approach multimodal reasoning in a straightforward,\ntext-centric manner, where both reasoning and answer derivation are conducted\npurely through text, with the only difference being the presence of multimodal\ninput. As a result, these methods often encounter fundamental limitations in\nspatial reasoning tasks that demand precise geometric understanding and\ncontinuous spatial tracking-capabilities that humans achieve through mental\nvisualization and manipulation. To address the limitations, we propose drawing\nto reason in space, a novel paradigm that enables LVLMs to reason through\nelementary drawing operations in the visual space. By equipping models with\nbasic drawing operations, including annotating bounding boxes and drawing\nauxiliary lines, we empower them to express and analyze spatial relationships\nthrough direct visual manipulation, meanwhile avoiding the performance ceiling\nimposed by specialized perception tools in previous tool-integrated reasoning\napproaches. To cultivate this capability, we develop a three-stage training\nframework: cold-start training with synthetic data to establish basic drawing\nabilities, reflective rejection sampling to enhance self-reflection behaviors,\nand reinforcement learning to directly optimize for target rewards. Extensive\nexperiments demonstrate that our model, named VILASR, consistently outperforms\nexisting methods across diverse spatial reasoning benchmarks, involving maze\nnavigation, static spatial reasoning, video-based reasoning, and\nmulti-view-based reasoning tasks, with an average improvement of 18.4%."
                },
                "authors": [
                    {
                        "name": "Junfei Wu"
                    },
                    {
                        "name": "Jian Guan"
                    },
                    {
                        "name": "Kaituo Feng"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Shu Wu"
                    },
                    {
                        "name": "Liang Wang"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Tieniu Tan"
                    }
                ],
                "author_detail": {
                    "name": "Tieniu Tan"
                },
                "author": "Tieniu Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13909v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13909v4",
                "updated": "2025-06-11T17:41:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    41,
                    16,
                    2,
                    162,
                    0
                ],
                "published": "2025-02-19T17:41:09Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    41,
                    9,
                    2,
                    50,
                    0
                ],
                "title": "Lost in Sequence: Do Large Language Models Understand Sequential\n  Recommendation?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lost in Sequence: Do Large Language Models Understand Sequential\n  Recommendation?"
                },
                "summary": "Large Language Models (LLMs) have recently emerged as promising tools for\nrecommendation thanks to their advanced textual understanding ability and\ncontext-awareness. Despite the current practice of training and evaluating\nLLM-based recommendation (LLM4Rec) models under a sequential recommendation\nscenario, we found that whether these models understand the sequential\ninformation inherent in users' item interaction sequences has been largely\noverlooked. In this paper, we first demonstrate through a series of experiments\nthat existing LLM4Rec models do not fully capture sequential information both\nduring training and inference. Then, we propose a simple yet effective\nLLM-based sequential recommender, called LLM-SRec, a method that enhances the\nintegration of sequential information into LLMs by distilling the user\nrepresentations extracted from a pre-trained CF-SRec model into LLMs. Our\nextensive experiments show that LLM-SRec enhances LLMs' ability to understand\nusers' item interaction sequences, ultimately leading to improved\nrecommendation performance. Furthermore, unlike existing LLM4Rec models that\nrequire fine-tuning of LLMs, LLM-SRec achieves state-of-the-art performance by\ntraining only a few lightweight MLPs, highlighting its practicality in\nreal-world applications. Our code is available at\nhttps://github.com/Sein-Kim/LLM-SRec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently emerged as promising tools for\nrecommendation thanks to their advanced textual understanding ability and\ncontext-awareness. Despite the current practice of training and evaluating\nLLM-based recommendation (LLM4Rec) models under a sequential recommendation\nscenario, we found that whether these models understand the sequential\ninformation inherent in users' item interaction sequences has been largely\noverlooked. In this paper, we first demonstrate through a series of experiments\nthat existing LLM4Rec models do not fully capture sequential information both\nduring training and inference. Then, we propose a simple yet effective\nLLM-based sequential recommender, called LLM-SRec, a method that enhances the\nintegration of sequential information into LLMs by distilling the user\nrepresentations extracted from a pre-trained CF-SRec model into LLMs. Our\nextensive experiments show that LLM-SRec enhances LLMs' ability to understand\nusers' item interaction sequences, ultimately leading to improved\nrecommendation performance. Furthermore, unlike existing LLM4Rec models that\nrequire fine-tuning of LLMs, LLM-SRec achieves state-of-the-art performance by\ntraining only a few lightweight MLPs, highlighting its practicality in\nreal-world applications. Our code is available at\nhttps://github.com/Sein-Kim/LLM-SRec."
                },
                "authors": [
                    {
                        "name": "Sein Kim"
                    },
                    {
                        "name": "Hongseok Kang"
                    },
                    {
                        "name": "Kibum Kim"
                    },
                    {
                        "name": "Jiwan Kim"
                    },
                    {
                        "name": "Donghyun Kim"
                    },
                    {
                        "name": "Minchul Yang"
                    },
                    {
                        "name": "Kwangjin Oh"
                    },
                    {
                        "name": "Julian McAuley"
                    },
                    {
                        "name": "Chanyoung Park"
                    }
                ],
                "author_detail": {
                    "name": "Chanyoung Park"
                },
                "author": "Chanyoung Park",
                "arxiv_comment": "KDD 2025 Research Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13909v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13909v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09956v1",
                "updated": "2025-06-11T17:30:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    30,
                    7,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T17:30:07Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    30,
                    7,
                    2,
                    162,
                    0
                ],
                "title": "LLMail-Inject: A Dataset from a Realistic Adaptive Prompt Injection\n  Challenge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMail-Inject: A Dataset from a Realistic Adaptive Prompt Injection\n  Challenge"
                },
                "summary": "Indirect Prompt Injection attacks exploit the inherent limitation of Large\nLanguage Models (LLMs) to distinguish between instructions and data in their\ninputs. Despite numerous defense proposals, the systematic evaluation against\nadaptive adversaries remains limited, even when successful attacks can have\nwide security and privacy implications, and many real-world LLM-based\napplications remain vulnerable. We present the results of LLMail-Inject, a\npublic challenge simulating a realistic scenario in which participants\nadaptively attempted to inject malicious instructions into emails in order to\ntrigger unauthorized tool calls in an LLM-based email assistant. The challenge\nspanned multiple defense strategies, LLM architectures, and retrieval\nconfigurations, resulting in a dataset of 208,095 unique attack submissions\nfrom 839 participants. We release the challenge code, the full dataset of\nsubmissions, and our analysis demonstrating how this data can provide new\ninsights into the instruction-data separation problem. We hope this will serve\nas a foundation for future research towards practical structural solutions to\nprompt injection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Indirect Prompt Injection attacks exploit the inherent limitation of Large\nLanguage Models (LLMs) to distinguish between instructions and data in their\ninputs. Despite numerous defense proposals, the systematic evaluation against\nadaptive adversaries remains limited, even when successful attacks can have\nwide security and privacy implications, and many real-world LLM-based\napplications remain vulnerable. We present the results of LLMail-Inject, a\npublic challenge simulating a realistic scenario in which participants\nadaptively attempted to inject malicious instructions into emails in order to\ntrigger unauthorized tool calls in an LLM-based email assistant. The challenge\nspanned multiple defense strategies, LLM architectures, and retrieval\nconfigurations, resulting in a dataset of 208,095 unique attack submissions\nfrom 839 participants. We release the challenge code, the full dataset of\nsubmissions, and our analysis demonstrating how this data can provide new\ninsights into the instruction-data separation problem. We hope this will serve\nas a foundation for future research towards practical structural solutions to\nprompt injection."
                },
                "authors": [
                    {
                        "name": "Sahar Abdelnabi"
                    },
                    {
                        "name": "Aideen Fay"
                    },
                    {
                        "name": "Ahmed Salem"
                    },
                    {
                        "name": "Egor Zverev"
                    },
                    {
                        "name": "Kai-Chieh Liao"
                    },
                    {
                        "name": "Chi-Huang Liu"
                    },
                    {
                        "name": "Chun-Chih Kuo"
                    },
                    {
                        "name": "Jannis Weigend"
                    },
                    {
                        "name": "Danyael Manlangit"
                    },
                    {
                        "name": "Alex Apostolov"
                    },
                    {
                        "name": "Haris Umair"
                    },
                    {
                        "name": "Joo Donato"
                    },
                    {
                        "name": "Masayuki Kawakita"
                    },
                    {
                        "name": "Athar Mahboob"
                    },
                    {
                        "name": "Tran Huu Bach"
                    },
                    {
                        "name": "Tsun-Han Chiang"
                    },
                    {
                        "name": "Myeongjin Cho"
                    },
                    {
                        "name": "Hajin Choi"
                    },
                    {
                        "name": "Byeonghyeon Kim"
                    },
                    {
                        "name": "Hyeonjin Lee"
                    },
                    {
                        "name": "Benjamin Pannell"
                    },
                    {
                        "name": "Conor McCauley"
                    },
                    {
                        "name": "Mark Russinovich"
                    },
                    {
                        "name": "Andrew Paverd"
                    },
                    {
                        "name": "Giovanni Cherubin"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Cherubin"
                },
                "author": "Giovanni Cherubin",
                "arxiv_comment": "Dataset at:\n  https://huggingface.co/datasets/microsoft/llmail-inject-challenge",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06144v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06144v4",
                "updated": "2025-06-11T17:23:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    23,
                    47,
                    2,
                    162,
                    0
                ],
                "published": "2024-06-10T10:03:16Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    10,
                    3,
                    16,
                    0,
                    162,
                    0
                ],
                "title": "Language Models Resist Alignment: Evidence From Data Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models Resist Alignment: Evidence From Data Compression"
                },
                "summary": "Large language models (LLMs) may exhibit unintended or undesirable behaviors.\nRecent works have concentrated on aligning LLMs to mitigate harmful outputs.\nDespite these efforts, some anomalies indicate that even a well-conducted\nalignment process can be easily circumvented, whether intentionally or\naccidentally. Does alignment fine-tuning yield have robust effects on models,\nor are its impacts merely superficial? In this work, we make the first\nexploration of this phenomenon from both theoretical and empirical\nperspectives. Empirically, we demonstrate the $\\mathbf{elasticity}$ of\npost-alignment models, i.e., the tendency to revert to the behavior\ndistribution formed during the pre-training phase upon further fine-tuning.\nLeveraging compression theory, we formally deduce that fine-tuning\ndisproportionately undermines alignment relative to pre-training, potentially\nby orders of magnitude. We validate the presence of elasticity through\nexperiments on models of varying types and scales. Specifically, we find that\nmodel performance declines rapidly before reverting to the pre-training\ndistribution, after which the rate of decline drops significantly. Furthermore,\nwe further reveal that elasticity positively correlates with the increased\nmodel size and the expansion of pre-training data. Our findings underscore the\nneed to address the inherent elasticity of LLMs to mitigate their resistance to\nalignment. The model weight and code are available at\npku-lm-resist-alignment.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) may exhibit unintended or undesirable behaviors.\nRecent works have concentrated on aligning LLMs to mitigate harmful outputs.\nDespite these efforts, some anomalies indicate that even a well-conducted\nalignment process can be easily circumvented, whether intentionally or\naccidentally. Does alignment fine-tuning yield have robust effects on models,\nor are its impacts merely superficial? In this work, we make the first\nexploration of this phenomenon from both theoretical and empirical\nperspectives. Empirically, we demonstrate the $\\mathbf{elasticity}$ of\npost-alignment models, i.e., the tendency to revert to the behavior\ndistribution formed during the pre-training phase upon further fine-tuning.\nLeveraging compression theory, we formally deduce that fine-tuning\ndisproportionately undermines alignment relative to pre-training, potentially\nby orders of magnitude. We validate the presence of elasticity through\nexperiments on models of varying types and scales. Specifically, we find that\nmodel performance declines rapidly before reverting to the pre-training\ndistribution, after which the rate of decline drops significantly. Furthermore,\nwe further reveal that elasticity positively correlates with the increased\nmodel size and the expansion of pre-training data. Our findings underscore the\nneed to address the inherent elasticity of LLMs to mitigate their resistance to\nalignment. The model weight and code are available at\npku-lm-resist-alignment.github.io."
                },
                "authors": [
                    {
                        "name": "Jiaming Ji"
                    },
                    {
                        "name": "Kaile Wang"
                    },
                    {
                        "name": "Tianyi Qiu"
                    },
                    {
                        "name": "Boyuan Chen"
                    },
                    {
                        "name": "Jiayi Zhou"
                    },
                    {
                        "name": "Changye Li"
                    },
                    {
                        "name": "Hantao Lou"
                    },
                    {
                        "name": "Juntao Dai"
                    },
                    {
                        "name": "Yunhuai Liu"
                    },
                    {
                        "name": "Yaodong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yaodong Yang"
                },
                "author": "Yaodong Yang",
                "arxiv_comment": "Accepted by ACL2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06144v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06144v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09944v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09944v1",
                "updated": "2025-06-11T17:12:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    12,
                    6,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T17:12:06Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    12,
                    6,
                    2,
                    162,
                    0
                ],
                "title": "Query-Focused Retrieval Heads Improve Long-Context Reasoning and\n  Re-ranking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query-Focused Retrieval Heads Improve Long-Context Reasoning and\n  Re-ranking"
                },
                "summary": "Recent work has identified retrieval heads (Wu et al., 2025b), a subset of\nattention heads responsible for retrieving salient information in long-context\nlanguage models (LMs), as measured by their copy-paste behavior in\nNeedle-in-a-Haystack tasks. In this paper, we introduce QRHEAD (Query-Focused\nRetrieval Head), an improved set of attention heads that enhance retrieval from\nlong context. We identify QRHEAD by aggregating attention scores with respect\nto the input query, using a handful of examples from real-world tasks (e.g.,\nlong-context QA). We further introduce QR- RETRIEVER, an efficient and\neffective retriever that uses the accumulated attention mass of QRHEAD as\nretrieval scores. We use QR- RETRIEVER for long-context reasoning by selecting\nthe most relevant parts with the highest retrieval scores. On multi-hop\nreasoning tasks LongMemEval and CLIPPER, this yields over 10% performance gains\nover full context and outperforms strong dense retrievers. We also evaluate\nQRRETRIEVER as a re-ranker on the BEIR benchmark and find that it achieves\nstrong zero-shot performance, outperforming other LLM-based re-rankers such as\nRankGPT. Further analysis shows that both the querycontext attention scoring\nand task selection are crucial for identifying QRHEAD with strong downstream\nutility. Overall, our work contributes a general-purpose retriever and offers\ninterpretability insights into the long-context capabilities of LMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work has identified retrieval heads (Wu et al., 2025b), a subset of\nattention heads responsible for retrieving salient information in long-context\nlanguage models (LMs), as measured by their copy-paste behavior in\nNeedle-in-a-Haystack tasks. In this paper, we introduce QRHEAD (Query-Focused\nRetrieval Head), an improved set of attention heads that enhance retrieval from\nlong context. We identify QRHEAD by aggregating attention scores with respect\nto the input query, using a handful of examples from real-world tasks (e.g.,\nlong-context QA). We further introduce QR- RETRIEVER, an efficient and\neffective retriever that uses the accumulated attention mass of QRHEAD as\nretrieval scores. We use QR- RETRIEVER for long-context reasoning by selecting\nthe most relevant parts with the highest retrieval scores. On multi-hop\nreasoning tasks LongMemEval and CLIPPER, this yields over 10% performance gains\nover full context and outperforms strong dense retrievers. We also evaluate\nQRRETRIEVER as a re-ranker on the BEIR benchmark and find that it achieves\nstrong zero-shot performance, outperforming other LLM-based re-rankers such as\nRankGPT. Further analysis shows that both the querycontext attention scoring\nand task selection are crucial for identifying QRHEAD with strong downstream\nutility. Overall, our work contributes a general-purpose retriever and offers\ninterpretability insights into the long-context capabilities of LMs."
                },
                "authors": [
                    {
                        "name": "Wuwei Zhang"
                    },
                    {
                        "name": "Fangcong Yin"
                    },
                    {
                        "name": "Howard Yen"
                    },
                    {
                        "name": "Danqi Chen"
                    },
                    {
                        "name": "Xi Ye"
                    }
                ],
                "author_detail": {
                    "name": "Xi Ye"
                },
                "author": "Xi Ye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09944v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09944v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06845v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06845v5",
                "updated": "2025-06-11T17:10:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    10,
                    59,
                    2,
                    162,
                    0
                ],
                "published": "2024-12-08T02:01:46Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    2,
                    1,
                    46,
                    6,
                    343,
                    0
                ],
                "title": "7B Fully Open Source Moxin-LLM/VLM -- From Pretraining to GRPO-based\n  Reinforcement Learning Enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "7B Fully Open Source Moxin-LLM/VLM -- From Pretraining to GRPO-based\n  Reinforcement Learning Enhancement"
                },
                "summary": "Recently, Large Language Models (LLMs) have undergone a significant\ntransformation, marked by a rapid rise in both their popularity and\ncapabilities. Leading this evolution are proprietary LLMs like GPT-4 and\nGPT-o1, which have captured widespread attention in the AI community due to\ntheir remarkable performance and versatility. Simultaneously, open-source LLMs,\nsuch as LLaMA, have made great contributions to the ever-increasing popularity\nof LLMs due to the ease to customize and deploy the models across diverse\napplications. Although open-source LLMs present unprecedented opportunities for\ninnovation and research, the commercialization of LLMs has raised concerns\nabout transparency, reproducibility, and safety. Many open-source LLMs fail to\nmeet fundamental transparency requirements by withholding essential components\nlike training code and data, which may hinder further innovations on LLMs. To\nmitigate this issue, we introduce Moxin 7B, a fully open-source LLM developed,\nadhering to principles of open science, open source, open data, and open\naccess. We release the pre-training code and configurations, training and\nfine-tuning datasets, and intermediate and final checkpoints, aiming to make\ncontinuous commitments to fully open-source LLMs. After pre-training the base\nmodel, we finetune the Moxin Base model with SOTA post-training framework and\ninstruction data to obtain Moxin Instruct model. To improve the reasoning\ncapability, we further finetune our Instruct model with chain-of-thought data\ndistilled from DeepSeek R1, and then use Group Relative Policy Optimization\n(GRPO) following DeepSeek R1 to finetune our model, leading to the Moxin\nReasoning model. Moreover, we develop our vision language model based on our\nMoxin model. Experiments show that our models achieve superior performance in\nvarious evaluations such as zero-shot evaluation, few-shot evaluation, and CoT\nevaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Large Language Models (LLMs) have undergone a significant\ntransformation, marked by a rapid rise in both their popularity and\ncapabilities. Leading this evolution are proprietary LLMs like GPT-4 and\nGPT-o1, which have captured widespread attention in the AI community due to\ntheir remarkable performance and versatility. Simultaneously, open-source LLMs,\nsuch as LLaMA, have made great contributions to the ever-increasing popularity\nof LLMs due to the ease to customize and deploy the models across diverse\napplications. Although open-source LLMs present unprecedented opportunities for\ninnovation and research, the commercialization of LLMs has raised concerns\nabout transparency, reproducibility, and safety. Many open-source LLMs fail to\nmeet fundamental transparency requirements by withholding essential components\nlike training code and data, which may hinder further innovations on LLMs. To\nmitigate this issue, we introduce Moxin 7B, a fully open-source LLM developed,\nadhering to principles of open science, open source, open data, and open\naccess. We release the pre-training code and configurations, training and\nfine-tuning datasets, and intermediate and final checkpoints, aiming to make\ncontinuous commitments to fully open-source LLMs. After pre-training the base\nmodel, we finetune the Moxin Base model with SOTA post-training framework and\ninstruction data to obtain Moxin Instruct model. To improve the reasoning\ncapability, we further finetune our Instruct model with chain-of-thought data\ndistilled from DeepSeek R1, and then use Group Relative Policy Optimization\n(GRPO) following DeepSeek R1 to finetune our model, leading to the Moxin\nReasoning model. Moreover, we develop our vision language model based on our\nMoxin model. Experiments show that our models achieve superior performance in\nvarious evaluations such as zero-shot evaluation, few-shot evaluation, and CoT\nevaluation."
                },
                "authors": [
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Xuan Shen"
                    },
                    {
                        "name": "Zhenglun Kong"
                    },
                    {
                        "name": "Yixin Shen"
                    },
                    {
                        "name": "Sung-En Chang"
                    },
                    {
                        "name": "Timothy Rupprecht"
                    },
                    {
                        "name": "Lei Lu"
                    },
                    {
                        "name": "Enfu Nan"
                    },
                    {
                        "name": "Changdi Yang"
                    },
                    {
                        "name": "Yumei He"
                    },
                    {
                        "name": "Weiyan Shi"
                    },
                    {
                        "name": "Xingchen Xu"
                    },
                    {
                        "name": "Yu Huang"
                    },
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Yue Chen"
                    },
                    {
                        "name": "Yong He"
                    },
                    {
                        "name": "Yanzhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yanzhi Wang"
                },
                "author": "Yanzhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06845v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06845v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09942v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09942v1",
                "updated": "2025-06-11T17:10:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    10,
                    36,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T17:10:36Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    10,
                    36,
                    2,
                    162,
                    0
                ],
                "title": "VerIF: Verification Engineering for Reinforcement Learning in\n  Instruction Following",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VerIF: Verification Engineering for Reinforcement Learning in\n  Instruction Following"
                },
                "summary": "Reinforcement learning with verifiable rewards (RLVR) has become a key\ntechnique for enhancing large language models (LLMs), with verification\nengineering playing a central role. However, best practices for RL in\ninstruction following remain underexplored. In this work, we explore the\nverification challenge in RL for instruction following and propose VerIF, a\nverification method that combines rule-based code verification with LLM-based\nverification from a large reasoning model (e.g., QwQ-32B). To support this\napproach, we construct a high-quality instruction-following dataset,\nVerInstruct, containing approximately 22,000 instances with associated\nverification signals. We apply RL training with VerIF to two models, achieving\nsignificant improvements across several representative instruction-following\nbenchmarks. The trained models reach state-of-the-art performance among models\nof comparable size and generalize well to unseen constraints. We further\nobserve that their general capabilities remain unaffected, suggesting that RL\nwith VerIF can be integrated into existing RL recipes to enhance overall model\nperformance. We have released our datasets, codes, and models to facilitate\nfuture research at https://github.com/THU-KEG/VerIF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning with verifiable rewards (RLVR) has become a key\ntechnique for enhancing large language models (LLMs), with verification\nengineering playing a central role. However, best practices for RL in\ninstruction following remain underexplored. In this work, we explore the\nverification challenge in RL for instruction following and propose VerIF, a\nverification method that combines rule-based code verification with LLM-based\nverification from a large reasoning model (e.g., QwQ-32B). To support this\napproach, we construct a high-quality instruction-following dataset,\nVerInstruct, containing approximately 22,000 instances with associated\nverification signals. We apply RL training with VerIF to two models, achieving\nsignificant improvements across several representative instruction-following\nbenchmarks. The trained models reach state-of-the-art performance among models\nof comparable size and generalize well to unseen constraints. We further\nobserve that their general capabilities remain unaffected, suggesting that RL\nwith VerIF can be integrated into existing RL recipes to enhance overall model\nperformance. We have released our datasets, codes, and models to facilitate\nfuture research at https://github.com/THU-KEG/VerIF."
                },
                "authors": [
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Yunjia Qi"
                    },
                    {
                        "name": "Xiaozhi Wang"
                    },
                    {
                        "name": "Bin Xu"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "arxiv_comment": "16 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09942v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09942v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14259v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14259v4",
                "updated": "2025-06-11T17:09:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    9,
                    58,
                    2,
                    162,
                    0
                ],
                "published": "2024-05-23T07:39:42Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    7,
                    39,
                    42,
                    3,
                    144,
                    0
                ],
                "title": "Let's Fuse Step by Step: A Generative Fusion Decoding Algorithm with\n  LLMs for Robust and Instruction-Aware ASR and OCR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let's Fuse Step by Step: A Generative Fusion Decoding Algorithm with\n  LLMs for Robust and Instruction-Aware ASR and OCR"
                },
                "summary": "We propose \"Generative Fusion Decoding\" (GFD), a novel shallow fusion\nframework designed to integrate large language models (LLMs) into cross-modal\ntext recognition systems for automatic speech recognition (ASR) and optical\ncharacter recognition (OCR). We derive the necessary formulations to enable GFD\nto operate across mismatched token spaces of different models by calculating\nlikelihood at the byte level, thereby enabling seamless fusion and synchronous\nprogression during the decoding process. GFD is plug-and-play by design, making\nit readily compatible with various auto-regressive models without the need for\nany re-training. GFD proves effective for general ASR and OCR tasks through\nintermediate and frequent interactions with LLMs, surpassing cascaded methods\nin English and Mandarin benchmarks. In addition, GFD transfers in-context\nlearning abilities of LLMs and allows for adaptive ASR in instruction-aware and\nlong-context settings, yielding significant WER reductions of up to 17.7\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose \"Generative Fusion Decoding\" (GFD), a novel shallow fusion\nframework designed to integrate large language models (LLMs) into cross-modal\ntext recognition systems for automatic speech recognition (ASR) and optical\ncharacter recognition (OCR). We derive the necessary formulations to enable GFD\nto operate across mismatched token spaces of different models by calculating\nlikelihood at the byte level, thereby enabling seamless fusion and synchronous\nprogression during the decoding process. GFD is plug-and-play by design, making\nit readily compatible with various auto-regressive models without the need for\nany re-training. GFD proves effective for general ASR and OCR tasks through\nintermediate and frequent interactions with LLMs, surpassing cascaded methods\nin English and Mandarin benchmarks. In addition, GFD transfers in-context\nlearning abilities of LLMs and allows for adaptive ASR in instruction-aware and\nlong-context settings, yielding significant WER reductions of up to 17.7\\%."
                },
                "authors": [
                    {
                        "name": "Chan-Jan Hsu"
                    },
                    {
                        "name": "Yi-Chang Chen"
                    },
                    {
                        "name": "Feng-Ting Liao"
                    },
                    {
                        "name": "Pei-Chen Ho"
                    },
                    {
                        "name": "Yu-Hsiang Wang"
                    },
                    {
                        "name": "Po-Chun Hsu"
                    },
                    {
                        "name": "Da-shan Shiu"
                    }
                ],
                "author_detail": {
                    "name": "Da-shan Shiu"
                },
                "author": "Da-shan Shiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14259v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14259v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04430v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04430v2",
                "updated": "2025-06-11T17:05:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    5,
                    40,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-04T20:27:17Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    20,
                    27,
                    17,
                    2,
                    155,
                    0
                ],
                "title": "Leveraging Coordinate Momentum in SignSGD and Muon: Memory-Optimized\n  Zero-Order",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Coordinate Momentum in SignSGD and Muon: Memory-Optimized\n  Zero-Order"
                },
                "summary": "Fine-tuning Large Language Models (LLMs) is essential for adapting\npre-trained models to downstream tasks. Yet traditional first-order optimizers\nsuch as Stochastic Gradient Descent (SGD) and Adam incur prohibitive memory and\ncomputational costs that scale poorly with model size. In this paper, we\ninvestigate zero-order (ZO) optimization methods as a memory- and\ncompute-efficient alternative, particularly in the context of\nparameter-efficient fine-tuning techniques like LoRA. We propose\n$\\texttt{JAGUAR SignSGD}$, a ZO momentum-based algorithm that extends ZO\nSignSGD, requiring the same number of parameters as the standard ZO SGD and\nonly $\\mathcal{O}(1)$ function evaluations per iteration. To the best of our\nknowledge, this is the first study to establish rigorous convergence guarantees\nfor SignSGD in the stochastic ZO case. We further propose $\\texttt{JAGUAR\nMuon}$, a novel ZO extension of the Muon optimizer that leverages the matrix\nstructure of model parameters, and we provide its convergence rate under\narbitrary stochastic noise. Through extensive experiments on challenging LLM\nfine-tuning benchmarks, we demonstrate that the proposed algorithms meet or\nexceed the convergence quality of standard first-order methods, achieving\nsignificant memory reduction. Our theoretical and empirical results establish\nnew ZO optimization methods as a practical and theoretically grounded approach\nfor resource-constrained LLM adaptation. Our code is available at\nhttps://github.com/brain-mmo-lab/ZO_LLM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning Large Language Models (LLMs) is essential for adapting\npre-trained models to downstream tasks. Yet traditional first-order optimizers\nsuch as Stochastic Gradient Descent (SGD) and Adam incur prohibitive memory and\ncomputational costs that scale poorly with model size. In this paper, we\ninvestigate zero-order (ZO) optimization methods as a memory- and\ncompute-efficient alternative, particularly in the context of\nparameter-efficient fine-tuning techniques like LoRA. We propose\n$\\texttt{JAGUAR SignSGD}$, a ZO momentum-based algorithm that extends ZO\nSignSGD, requiring the same number of parameters as the standard ZO SGD and\nonly $\\mathcal{O}(1)$ function evaluations per iteration. To the best of our\nknowledge, this is the first study to establish rigorous convergence guarantees\nfor SignSGD in the stochastic ZO case. We further propose $\\texttt{JAGUAR\nMuon}$, a novel ZO extension of the Muon optimizer that leverages the matrix\nstructure of model parameters, and we provide its convergence rate under\narbitrary stochastic noise. Through extensive experiments on challenging LLM\nfine-tuning benchmarks, we demonstrate that the proposed algorithms meet or\nexceed the convergence quality of standard first-order methods, achieving\nsignificant memory reduction. Our theoretical and empirical results establish\nnew ZO optimization methods as a practical and theoretically grounded approach\nfor resource-constrained LLM adaptation. Our code is available at\nhttps://github.com/brain-mmo-lab/ZO_LLM"
                },
                "authors": [
                    {
                        "name": "Egor Petrov"
                    },
                    {
                        "name": "Grigoriy Evseev"
                    },
                    {
                        "name": "Aleksey Antonov"
                    },
                    {
                        "name": "Andrey Veprikov"
                    },
                    {
                        "name": "Pavel Plyusnin"
                    },
                    {
                        "name": "Nikolay Bushkov"
                    },
                    {
                        "name": "Stanislav Moiseev"
                    },
                    {
                        "name": "Aleksandr Beznosikov"
                    }
                ],
                "author_detail": {
                    "name": "Aleksandr Beznosikov"
                },
                "author": "Aleksandr Beznosikov",
                "arxiv_comment": "26 pages, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04430v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04430v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08174v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08174v2",
                "updated": "2025-06-11T17:04:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    4,
                    39,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-09T19:39:09Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    19,
                    39,
                    9,
                    0,
                    160,
                    0
                ],
                "title": "LLM-BT-Terms: Back-Translation as a Framework for Terminology\n  Standardization and Dynamic Semantic Embedding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-BT-Terms: Back-Translation as a Framework for Terminology\n  Standardization and Dynamic Semantic Embedding"
                },
                "summary": "The rapid expansion of English technical terminology presents a significant\nchallenge to traditional expert-based standardization, particularly in rapidly\ndeveloping areas such as artificial intelligence and quantum computing. Manual\napproaches face difficulties in maintaining consistent multilingual\nterminology. To address this, we introduce LLM-BT, a back-translation framework\npowered by large language models (LLMs) designed to automate terminology\nverification and standardization through cross-lingual semantic alignment. Our\nkey contributions include: (1) term-level consistency validation: by performing\nEnglish -> intermediate language -> English back-translation, LLM-BT achieves\nhigh term consistency across different models (such as GPT-4, DeepSeek, and\nGrok). Case studies demonstrate over 90 percent of terms are preserved either\nexactly or semantically; (2) multi-path verification workflow: we develop a\nnovel pipeline described as Retrieve -> Generate -> Verify -> Optimize, which\nsupports both serial paths (e.g., English -> Simplified Chinese -> Traditional\nChinese -> English) and parallel paths (e.g., English -> Chinese / Portuguese\n-> English). BLEU scores and term-level accuracy indicate strong cross-lingual\nrobustness, with BLEU scores exceeding 0.45 and Portuguese term accuracy\nreaching 100 percent; (3) back-translation as semantic embedding: we\nreinterpret back-translation as a form of dynamic semantic embedding that\nuncovers latent trajectories of meaning. In contrast to static embeddings,\nLLM-BT offers transparent, path-based embeddings shaped by the evolution of the\nmodels. This reframing positions back-translation as an active mechanism for\nmultilingual terminology standardization, fostering collaboration between\nmachines and humans - machines preserve semantic integrity, while humans\nprovide cultural interpretation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid expansion of English technical terminology presents a significant\nchallenge to traditional expert-based standardization, particularly in rapidly\ndeveloping areas such as artificial intelligence and quantum computing. Manual\napproaches face difficulties in maintaining consistent multilingual\nterminology. To address this, we introduce LLM-BT, a back-translation framework\npowered by large language models (LLMs) designed to automate terminology\nverification and standardization through cross-lingual semantic alignment. Our\nkey contributions include: (1) term-level consistency validation: by performing\nEnglish -> intermediate language -> English back-translation, LLM-BT achieves\nhigh term consistency across different models (such as GPT-4, DeepSeek, and\nGrok). Case studies demonstrate over 90 percent of terms are preserved either\nexactly or semantically; (2) multi-path verification workflow: we develop a\nnovel pipeline described as Retrieve -> Generate -> Verify -> Optimize, which\nsupports both serial paths (e.g., English -> Simplified Chinese -> Traditional\nChinese -> English) and parallel paths (e.g., English -> Chinese / Portuguese\n-> English). BLEU scores and term-level accuracy indicate strong cross-lingual\nrobustness, with BLEU scores exceeding 0.45 and Portuguese term accuracy\nreaching 100 percent; (3) back-translation as semantic embedding: we\nreinterpret back-translation as a form of dynamic semantic embedding that\nuncovers latent trajectories of meaning. In contrast to static embeddings,\nLLM-BT offers transparent, path-based embeddings shaped by the evolution of the\nmodels. This reframing positions back-translation as an active mechanism for\nmultilingual terminology standardization, fostering collaboration between\nmachines and humans - machines preserve semantic integrity, while humans\nprovide cultural interpretation."
                },
                "authors": [
                    {
                        "name": "Li Weigang"
                    },
                    {
                        "name": "Pedro Carvalho Brom"
                    }
                ],
                "author_detail": {
                    "name": "Pedro Carvalho Brom"
                },
                "author": "Pedro Carvalho Brom",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08174v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08174v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09938v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09938v1",
                "updated": "2025-06-11T17:02:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    2,
                    12,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T17:02:12Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    2,
                    12,
                    2,
                    162,
                    0
                ],
                "title": "Microservices and Real-Time Processing in Retail IT: A Review of\n  Open-Source Toolchains and Deployment Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microservices and Real-Time Processing in Retail IT: A Review of\n  Open-Source Toolchains and Deployment Strategies"
                },
                "summary": "With the rapid pace of digital transformation, the retail industry is\nincreasingly depending on real-time, scalable, and resilient systems to manage\nfinancial transactions, analyze customer behavior, and streamline order\nprocessing. This literature review explores how modern event-driven and\nmicroservices-based architectures, particularly those leveraging Apache Kafka,\nSpring Boot, MongoDB, and Kubernetes are transforming retail and financial\nsystems. By systematically reviewing academic publications, technical white\npapers, and industry reports from recent years, this study synthesizes key\nthemes and implementation strategies. The analysis reveals that technologies\nlike Kafka and Spring Boot are instrumental in building low-latency,\nevent-driven applications that support real-time analytics and fraud detection,\nwhile MongoDB, when deployed on Kubernetes, ensures fault tolerance and high\navailability in inventory and transaction systems. Kubernetes itself plays a\ncrucial role in automating deployment and scaling of microservices. These\nfindings provide valuable insights for industry practitioners aiming to design\nscalable infrastructures, identify research opportunities in hybrid deployment\nmodels, and offer educators a foundation to integrate modern system\narchitectures into professional and technical communication training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid pace of digital transformation, the retail industry is\nincreasingly depending on real-time, scalable, and resilient systems to manage\nfinancial transactions, analyze customer behavior, and streamline order\nprocessing. This literature review explores how modern event-driven and\nmicroservices-based architectures, particularly those leveraging Apache Kafka,\nSpring Boot, MongoDB, and Kubernetes are transforming retail and financial\nsystems. By systematically reviewing academic publications, technical white\npapers, and industry reports from recent years, this study synthesizes key\nthemes and implementation strategies. The analysis reveals that technologies\nlike Kafka and Spring Boot are instrumental in building low-latency,\nevent-driven applications that support real-time analytics and fraud detection,\nwhile MongoDB, when deployed on Kubernetes, ensures fault tolerance and high\navailability in inventory and transaction systems. Kubernetes itself plays a\ncrucial role in automating deployment and scaling of microservices. These\nfindings provide valuable insights for industry practitioners aiming to design\nscalable infrastructures, identify research opportunities in hybrid deployment\nmodels, and offer educators a foundation to integrate modern system\narchitectures into professional and technical communication training."
                },
                "authors": [
                    {
                        "name": "Aaditaa Vashisht"
                    },
                    {
                        "name": "Rekha B S"
                    }
                ],
                "author_detail": {
                    "name": "Rekha B S"
                },
                "arxiv_affiliation": "Department of Information Science and Engineering, RV College of Engineering, India",
                "author": "Rekha B S",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09938v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09938v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17761v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17761v3",
                "updated": "2025-06-11T16:56:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    16,
                    56,
                    58,
                    2,
                    162,
                    0
                ],
                "published": "2024-06-25T17:45:26Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    17,
                    45,
                    26,
                    1,
                    177,
                    0
                ],
                "title": "CaLMQA: Exploring culturally specific long-form question answering\n  across 23 languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaLMQA: Exploring culturally specific long-form question answering\n  across 23 languages"
                },
                "summary": "Despite rising global usage of large language models (LLMs), their ability to\ngenerate long-form answers to culturally specific questions remains unexplored\nin many languages. To fill this gap, we perform the first study of textual\nmultilingual long-form QA by creating CaLMQA, a dataset of 51.7K culturally\nspecific questions across 23 different languages. We define culturally specific\nquestions as those that refer to concepts unique to one or a few cultures, or\nhave different answers depending on the cultural or regional context. We obtain\nthese questions by crawling naturally-occurring questions from community web\nforums in high-resource languages, and by hiring native speakers to write\nquestions in under-resourced, rarely-studied languages such as Fijian and\nKirundi. Our data collection methodologies are translation-free, enabling the\ncollection of culturally unique questions like \"Kuber iki umwami wa mbere\nw'uburundi yitwa Ntare?\" (Kirundi; English translation: \"Why was the first king\nof Burundi called Ntare (Lion)?\"). We evaluate factuality, relevance and\nsurface-level quality of LLM-generated long-form answers, finding that (1) for\nmany languages, even the best models make critical surface-level errors (e.g.,\nanswering in the wrong language, repetition), especially for low-resource\nlanguages; and (2) answers to culturally specific questions contain more\nfactual errors than answers to culturally agnostic questions -- questions that\nhave consistent meaning and answer across many cultures. We release CaLMQA to\nfacilitate future research in cultural and multilingual long-form QA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite rising global usage of large language models (LLMs), their ability to\ngenerate long-form answers to culturally specific questions remains unexplored\nin many languages. To fill this gap, we perform the first study of textual\nmultilingual long-form QA by creating CaLMQA, a dataset of 51.7K culturally\nspecific questions across 23 different languages. We define culturally specific\nquestions as those that refer to concepts unique to one or a few cultures, or\nhave different answers depending on the cultural or regional context. We obtain\nthese questions by crawling naturally-occurring questions from community web\nforums in high-resource languages, and by hiring native speakers to write\nquestions in under-resourced, rarely-studied languages such as Fijian and\nKirundi. Our data collection methodologies are translation-free, enabling the\ncollection of culturally unique questions like \"Kuber iki umwami wa mbere\nw'uburundi yitwa Ntare?\" (Kirundi; English translation: \"Why was the first king\nof Burundi called Ntare (Lion)?\"). We evaluate factuality, relevance and\nsurface-level quality of LLM-generated long-form answers, finding that (1) for\nmany languages, even the best models make critical surface-level errors (e.g.,\nanswering in the wrong language, repetition), especially for low-resource\nlanguages; and (2) answers to culturally specific questions contain more\nfactual errors than answers to culturally agnostic questions -- questions that\nhave consistent meaning and answer across many cultures. We release CaLMQA to\nfacilitate future research in cultural and multilingual long-form QA."
                },
                "authors": [
                    {
                        "name": "Shane Arora"
                    },
                    {
                        "name": "Marzena Karpinska"
                    },
                    {
                        "name": "Hung-Ting Chen"
                    },
                    {
                        "name": "Ipsita Bhattacharjee"
                    },
                    {
                        "name": "Mohit Iyyer"
                    },
                    {
                        "name": "Eunsol Choi"
                    }
                ],
                "author_detail": {
                    "name": "Eunsol Choi"
                },
                "author": "Eunsol Choi",
                "arxiv_comment": "46 pages, 26 figures. Accepted as a main conference paper at ACL\n  2025. Code and data available at https://github.com/2015aroras/CaLMQA .\n  Dataset expanded to 51.7K questions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17761v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17761v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.08726v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.08726v2",
                "updated": "2025-06-11T16:54:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    16,
                    54,
                    54,
                    2,
                    162,
                    0
                ],
                "published": "2024-06-13T01:08:40Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    1,
                    8,
                    40,
                    3,
                    165,
                    0
                ],
                "title": "Standard Language Ideology in AI-Generated Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Standard Language Ideology in AI-Generated Language"
                },
                "summary": "Standard language ideology is reflected and reinforced in language generated\nby large language models (LLMs). We present a faceted taxonomy of open problems\nthat illustrate how standard language ideology manifests in AI-generated\nlanguage, alongside implications for minoritized language communities and\nsociety more broadly. We introduce the concept of standard AI-generated\nlanguage ideology, a process through which LLMs position \"standard\"\nlanguages--particularly Standard American English (SAE)--as the linguistic\ndefault, reinforcing the perception that SAE is the most \"appropriate\"\nlanguage. We then discuss ongoing tensions around what constitutes desirable\nsystem behavior, as well as advantages and drawbacks of generative AI tools\nattempting, or refusing, to imitate different English language varieties.\nRather than prescribing narrow technical fixes, we offer three recommendations\nfor researchers, practitioners, and funders that focus on shifting structural\nconditions and supporting more emancipatory outcomes for diverse language\ncommunities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Standard language ideology is reflected and reinforced in language generated\nby large language models (LLMs). We present a faceted taxonomy of open problems\nthat illustrate how standard language ideology manifests in AI-generated\nlanguage, alongside implications for minoritized language communities and\nsociety more broadly. We introduce the concept of standard AI-generated\nlanguage ideology, a process through which LLMs position \"standard\"\nlanguages--particularly Standard American English (SAE)--as the linguistic\ndefault, reinforcing the perception that SAE is the most \"appropriate\"\nlanguage. We then discuss ongoing tensions around what constitutes desirable\nsystem behavior, as well as advantages and drawbacks of generative AI tools\nattempting, or refusing, to imitate different English language varieties.\nRather than prescribing narrow technical fixes, we offer three recommendations\nfor researchers, practitioners, and funders that focus on shifting structural\nconditions and supporting more emancipatory outcomes for diverse language\ncommunities."
                },
                "authors": [
                    {
                        "name": "Genevieve Smith"
                    },
                    {
                        "name": "Eve Fleisig"
                    },
                    {
                        "name": "Madeline Bossi"
                    },
                    {
                        "name": "Ishita Rustagi"
                    },
                    {
                        "name": "Xavier Yin"
                    }
                ],
                "author_detail": {
                    "name": "Xavier Yin"
                },
                "author": "Xavier Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.08726v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.08726v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09932v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09932v1",
                "updated": "2025-06-11T16:54:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    16,
                    54,
                    34,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T16:54:34Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    16,
                    54,
                    34,
                    2,
                    162,
                    0
                ],
                "title": "HadaNorm: Diffusion Transformer Quantization through Mean-Centered\n  Transformations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HadaNorm: Diffusion Transformer Quantization through Mean-Centered\n  Transformations"
                },
                "summary": "Diffusion models represent the cutting edge in image generation, but their\nhigh memory and computational demands hinder deployment on resource-constrained\ndevices. Post-Training Quantization (PTQ) offers a promising solution by\nreducing the bitwidth of matrix operations. However, standard PTQ methods\nstruggle with outliers, and achieving higher compression often requires\ntransforming model weights and activations before quantization. In this work,\nwe propose HadaNorm, a novel linear transformation that extends existing\napproaches and effectively mitigates outliers by normalizing activations\nfeature channels before applying Hadamard transformations, enabling more\naggressive activation quantization. We demonstrate that HadaNorm consistently\nreduces quantization error across the various components of transformer blocks,\nachieving superior efficiency-performance trade-offs when compared to\nstate-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models represent the cutting edge in image generation, but their\nhigh memory and computational demands hinder deployment on resource-constrained\ndevices. Post-Training Quantization (PTQ) offers a promising solution by\nreducing the bitwidth of matrix operations. However, standard PTQ methods\nstruggle with outliers, and achieving higher compression often requires\ntransforming model weights and activations before quantization. In this work,\nwe propose HadaNorm, a novel linear transformation that extends existing\napproaches and effectively mitigates outliers by normalizing activations\nfeature channels before applying Hadamard transformations, enabling more\naggressive activation quantization. We demonstrate that HadaNorm consistently\nreduces quantization error across the various components of transformer blocks,\nachieving superior efficiency-performance trade-offs when compared to\nstate-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Marco Federici"
                    },
                    {
                        "name": "Riccardo Del Chiaro"
                    },
                    {
                        "name": "Boris van Breugel"
                    },
                    {
                        "name": "Paul Whatmough"
                    },
                    {
                        "name": "Markus Nagel"
                    }
                ],
                "author_detail": {
                    "name": "Markus Nagel"
                },
                "author": "Markus Nagel",
                "arxiv_comment": "4 Pages, 5 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09932v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09932v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06366v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06366v3",
                "updated": "2025-06-12T10:22:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    10,
                    22,
                    1,
                    3,
                    163,
                    0
                ],
                "published": "2025-06-04T08:12:32Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    8,
                    12,
                    32,
                    2,
                    155,
                    0
                ],
                "title": "AI Agent Behavioral Science",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Agent Behavioral Science"
                },
                "summary": "Recent advances in large language models (LLMs) have enabled the development\nof AI agents that exhibit increasingly human-like behaviors, including\nplanning, adaptation, and social dynamics across diverse, interactive, and\nopen-ended scenarios. These behaviors are not solely the product of the\ninternal architectures of the underlying models, but emerge from their\nintegration into agentic systems operating within specific contexts, where\nenvironmental factors, social cues, and interaction feedbacks shape behavior\nover time. This evolution necessitates a new scientific perspective: AI Agent\nBehavioral Science. Rather than focusing only on internal mechanisms, this\nperspective emphasizes the systematic observation of behavior, design of\ninterventions to test hypotheses, and theory-guided interpretation of how AI\nagents act, adapt, and interact over time. We systematize a growing body of\nresearch across individual agent, multi-agent, and human-agent interaction\nsettings, and further demonstrate how this perspective informs responsible AI\nby treating fairness, safety, interpretability, accountability, and privacy as\nbehavioral properties. By unifying recent findings and laying out future\ndirections, we position AI Agent Behavioral Science as a necessary complement\nto traditional model-centric approaches, providing essential tools for\nunderstanding, evaluating, and governing the real-world behavior of\nincreasingly autonomous AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have enabled the development\nof AI agents that exhibit increasingly human-like behaviors, including\nplanning, adaptation, and social dynamics across diverse, interactive, and\nopen-ended scenarios. These behaviors are not solely the product of the\ninternal architectures of the underlying models, but emerge from their\nintegration into agentic systems operating within specific contexts, where\nenvironmental factors, social cues, and interaction feedbacks shape behavior\nover time. This evolution necessitates a new scientific perspective: AI Agent\nBehavioral Science. Rather than focusing only on internal mechanisms, this\nperspective emphasizes the systematic observation of behavior, design of\ninterventions to test hypotheses, and theory-guided interpretation of how AI\nagents act, adapt, and interact over time. We systematize a growing body of\nresearch across individual agent, multi-agent, and human-agent interaction\nsettings, and further demonstrate how this perspective informs responsible AI\nby treating fairness, safety, interpretability, accountability, and privacy as\nbehavioral properties. By unifying recent findings and laying out future\ndirections, we position AI Agent Behavioral Science as a necessary complement\nto traditional model-centric approaches, providing essential tools for\nunderstanding, evaluating, and governing the real-world behavior of\nincreasingly autonomous AI systems."
                },
                "authors": [
                    {
                        "name": "Lin Chen"
                    },
                    {
                        "name": "Yunke Zhang"
                    },
                    {
                        "name": "Jie Feng"
                    },
                    {
                        "name": "Haoye Chai"
                    },
                    {
                        "name": "Honglin Zhang"
                    },
                    {
                        "name": "Bingbing Fan"
                    },
                    {
                        "name": "Yibo Ma"
                    },
                    {
                        "name": "Shiyuan Zhang"
                    },
                    {
                        "name": "Nian Li"
                    },
                    {
                        "name": "Tianhui Liu"
                    },
                    {
                        "name": "Nicholas Sukiennik"
                    },
                    {
                        "name": "Keyu Zhao"
                    },
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Ziyi Liu"
                    },
                    {
                        "name": "Fengli Xu"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06366v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06366v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24461v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24461v2",
                "updated": "2025-06-11T16:40:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    16,
                    40,
                    39,
                    2,
                    162,
                    0
                ],
                "published": "2025-05-30T10:57:09Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    10,
                    57,
                    9,
                    4,
                    150,
                    0
                ],
                "title": "Logits-Based Finetuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logits-Based Finetuning"
                },
                "summary": "In recent years, developing compact and efficient large language models\n(LLMs) has emerged as a thriving area of research. Traditional Supervised\nFine-Tuning (SFT), which relies on singular ground truth labels, often fails to\ncapture token-level dependencies and linguistic diversity. To address these\nlimitations, we propose a logits-based fine-tuning framework that integrates\nthe strengths of supervised learning and knowledge distillation. Our approach\nconstructs enriched training targets by combining teacher logits with ground\ntruth labels, preserving both correctness and linguistic diversity. This\nensures more reliable and effective training. We constructed a large-scale 1.2M\nlogits dataset and trained a series of science-focused models. Experimental\nresults demonstrate that our method achieves significant improvements, with\naccuracy gains of 18% on Mawps and 22.7% on TabMWP. Across nine widely used\nmathematical benchmarks, our method consistently outperforms prior SFT models,\nachieving an average improvement of 7.28%. Codes are available at\nhttps://github.com/dvlab-research/Logits-Based-Finetuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, developing compact and efficient large language models\n(LLMs) has emerged as a thriving area of research. Traditional Supervised\nFine-Tuning (SFT), which relies on singular ground truth labels, often fails to\ncapture token-level dependencies and linguistic diversity. To address these\nlimitations, we propose a logits-based fine-tuning framework that integrates\nthe strengths of supervised learning and knowledge distillation. Our approach\nconstructs enriched training targets by combining teacher logits with ground\ntruth labels, preserving both correctness and linguistic diversity. This\nensures more reliable and effective training. We constructed a large-scale 1.2M\nlogits dataset and trained a series of science-focused models. Experimental\nresults demonstrate that our method achieves significant improvements, with\naccuracy gains of 18% on Mawps and 22.7% on TabMWP. Across nine widely used\nmathematical benchmarks, our method consistently outperforms prior SFT models,\nachieving an average improvement of 7.28%. Codes are available at\nhttps://github.com/dvlab-research/Logits-Based-Finetuning."
                },
                "authors": [
                    {
                        "name": "Jingyao Li"
                    },
                    {
                        "name": "Senqiao Yang"
                    },
                    {
                        "name": "Sitong Wu"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Chuanyang Zheng"
                    },
                    {
                        "name": "Hong Xu"
                    },
                    {
                        "name": "Jiaya Jia"
                    }
                ],
                "author_detail": {
                    "name": "Jiaya Jia"
                },
                "author": "Jiaya Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24461v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24461v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09902v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09902v1",
                "updated": "2025-06-11T16:16:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    16,
                    16,
                    7,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T16:16:07Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    16,
                    16,
                    7,
                    2,
                    162,
                    0
                ],
                "title": "PersonaLens: A Benchmark for Personalization Evaluation in\n  Conversational AI Assistants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PersonaLens: A Benchmark for Personalization Evaluation in\n  Conversational AI Assistants"
                },
                "summary": "Large language models (LLMs) have advanced conversational AI assistants.\nHowever, systematically evaluating how well these assistants apply\npersonalization--adapting to individual user preferences while completing\ntasks--remains challenging. Existing personalization benchmarks focus on\nchit-chat, non-conversational tasks, or narrow domains, failing to capture the\ncomplexities of personalized task-oriented assistance. To address this, we\nintroduce PersonaLens, a comprehensive benchmark for evaluating personalization\nin task-oriented AI assistants. Our benchmark features diverse user profiles\nequipped with rich preferences and interaction histories, along with two\nspecialized LLM-based agents: a user agent that engages in realistic\ntask-oriented dialogues with AI assistants, and a judge agent that employs the\nLLM-as-a-Judge paradigm to assess personalization, response quality, and task\nsuccess. Through extensive experiments with current LLM assistants across\ndiverse tasks, we reveal significant variability in their personalization\ncapabilities, providing crucial insights for advancing conversational AI\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have advanced conversational AI assistants.\nHowever, systematically evaluating how well these assistants apply\npersonalization--adapting to individual user preferences while completing\ntasks--remains challenging. Existing personalization benchmarks focus on\nchit-chat, non-conversational tasks, or narrow domains, failing to capture the\ncomplexities of personalized task-oriented assistance. To address this, we\nintroduce PersonaLens, a comprehensive benchmark for evaluating personalization\nin task-oriented AI assistants. Our benchmark features diverse user profiles\nequipped with rich preferences and interaction histories, along with two\nspecialized LLM-based agents: a user agent that engages in realistic\ntask-oriented dialogues with AI assistants, and a judge agent that employs the\nLLM-as-a-Judge paradigm to assess personalization, response quality, and task\nsuccess. Through extensive experiments with current LLM assistants across\ndiverse tasks, we reveal significant variability in their personalization\ncapabilities, providing crucial insights for advancing conversational AI\nsystems."
                },
                "authors": [
                    {
                        "name": "Zheng Zhao"
                    },
                    {
                        "name": "Clara Vania"
                    },
                    {
                        "name": "Subhradeep Kayal"
                    },
                    {
                        "name": "Naila Khan"
                    },
                    {
                        "name": "Shay B. Cohen"
                    },
                    {
                        "name": "Emine Yilmaz"
                    }
                ],
                "author_detail": {
                    "name": "Emine Yilmaz"
                },
                "author": "Emine Yilmaz",
                "arxiv_comment": "Accepted to ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09902v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.19384v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19384v2",
                "updated": "2025-06-11T16:12:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    16,
                    12,
                    50,
                    2,
                    162,
                    0
                ],
                "published": "2024-06-27T17:57:03Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    17,
                    57,
                    3,
                    3,
                    179,
                    0
                ],
                "title": "The Remarkable Robustness of LLMs: Stages of Inference?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Remarkable Robustness of LLMs: Stages of Inference?"
                },
                "summary": "We investigate the robustness of Large Language Models (LLMs) to structural\ninterventions by deleting and swapping adjacent layers during inference.\nSurprisingly, models retain 72-95% of their original top-1 prediction accuracy\nwithout any fine-tuning. We find that performance degradation is not uniform\nacross layers: interventions to the early and final layers cause the most\ndegradation, while the model is remarkably robust to dropping middle layers.\nThis pattern of localized sensitivity motivates our hypothesis of four stages\nof inference, observed across diverse model families and sizes: (1)\ndetokenization, where local context is integrated to lift raw token embeddings\ninto higher-level representations; (2) feature engineering, where task- and\nentity-specific features are iteratively refined; (3) prediction ensembling,\nwhere hidden states are aggregated into plausible next-token predictions; and\n(4) residual sharpening, where irrelevant features are suppressed to finalize\nthe output distribution. Synthesizing behavioral and mechanistic evidence, we\nprovide a framework for interpreting depth-dependent computations in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the robustness of Large Language Models (LLMs) to structural\ninterventions by deleting and swapping adjacent layers during inference.\nSurprisingly, models retain 72-95% of their original top-1 prediction accuracy\nwithout any fine-tuning. We find that performance degradation is not uniform\nacross layers: interventions to the early and final layers cause the most\ndegradation, while the model is remarkably robust to dropping middle layers.\nThis pattern of localized sensitivity motivates our hypothesis of four stages\nof inference, observed across diverse model families and sizes: (1)\ndetokenization, where local context is integrated to lift raw token embeddings\ninto higher-level representations; (2) feature engineering, where task- and\nentity-specific features are iteratively refined; (3) prediction ensembling,\nwhere hidden states are aggregated into plausible next-token predictions; and\n(4) residual sharpening, where irrelevant features are suppressed to finalize\nthe output distribution. Synthesizing behavioral and mechanistic evidence, we\nprovide a framework for interpreting depth-dependent computations in LLMs."
                },
                "authors": [
                    {
                        "name": "Vedang Lad"
                    },
                    {
                        "name": "Wes Gurnee"
                    },
                    {
                        "name": "Max Tegmark"
                    }
                ],
                "author_detail": {
                    "name": "Max Tegmark"
                },
                "author": "Max Tegmark",
                "arxiv_comment": "For Github code see\n  https://github.com/vdlad/Remarkable-Robustness-of-LLMs. Send all\n  correspondence to the first author",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.19384v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19384v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05387v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05387v2",
                "updated": "2025-06-11T16:08:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    16,
                    8,
                    29,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-03T14:25:23Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    25,
                    23,
                    1,
                    154,
                    0
                ],
                "title": "Advancing Decoding Strategies: Enhancements in Locally Typical Sampling\n  for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Decoding Strategies: Enhancements in Locally Typical Sampling\n  for LLMs"
                },
                "summary": "This chapter explores advancements in decoding strategies for large language\nmodels (LLMs), focusing on enhancing the Locally Typical Sampling (LTS)\nalgorithm. Traditional decoding methods, such as top-k and nucleus sampling,\noften struggle to balance fluency, diversity, and coherence in text generation.\nTo address these challenges, Adaptive Semantic-Aware Typicality Sampling (ASTS)\nis proposed as an improved version of LTS, incorporating dynamic entropy\nthresholding, multi-objective scoring, and reward-penalty adjustments. ASTS\nensures contextually coherent and diverse text generation while maintaining\ncomputational efficiency. Its performance is evaluated across multiple\nbenchmarks, including story generation and abstractive summarization, using\nmetrics such as perplexity, MAUVE, and diversity scores. Experimental results\ndemonstrate that ASTS outperforms existing sampling techniques by reducing\nrepetition, enhancing semantic alignment, and improving fluency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This chapter explores advancements in decoding strategies for large language\nmodels (LLMs), focusing on enhancing the Locally Typical Sampling (LTS)\nalgorithm. Traditional decoding methods, such as top-k and nucleus sampling,\noften struggle to balance fluency, diversity, and coherence in text generation.\nTo address these challenges, Adaptive Semantic-Aware Typicality Sampling (ASTS)\nis proposed as an improved version of LTS, incorporating dynamic entropy\nthresholding, multi-objective scoring, and reward-penalty adjustments. ASTS\nensures contextually coherent and diverse text generation while maintaining\ncomputational efficiency. Its performance is evaluated across multiple\nbenchmarks, including story generation and abstractive summarization, using\nmetrics such as perplexity, MAUVE, and diversity scores. Experimental results\ndemonstrate that ASTS outperforms existing sampling techniques by reducing\nrepetition, enhancing semantic alignment, and improving fluency."
                },
                "authors": [
                    {
                        "name": "Jaydip Sen"
                    },
                    {
                        "name": "Saptarshi Sengupta"
                    },
                    {
                        "name": "Subhasis Dasgupta"
                    }
                ],
                "author_detail": {
                    "name": "Subhasis Dasgupta"
                },
                "author": "Subhasis Dasgupta",
                "arxiv_comment": "This is the accepted but pre-reviewed version of the chapter that has\n  been accepted for publication in the Springer volume 'Decision-Making in\n  Computational Intelligence-Based Systems,' edited by Witold Pedrycz, Gilberto\n  Rivera, Rose Ma Rodriguez, and Salvador Ibarra Martinez. The chapter is 39\n  pages long, and it contains 2 figures and 6 tables. This is NOT the final\n  camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05387v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05387v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09890v1",
                "updated": "2025-06-11T16:00:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    16,
                    0,
                    54,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T16:00:54Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    16,
                    0,
                    54,
                    2,
                    162,
                    0
                ],
                "title": "The Emergence of Abstract Thought in Large Language Models Beyond Any\n  Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Emergence of Abstract Thought in Large Language Models Beyond Any\n  Language"
                },
                "summary": "As large language models (LLMs) continue to advance, their capacity to\nfunction effectively across a diverse range of languages has shown marked\nimprovement. Preliminary studies observe that the hidden activations of LLMs\noften resemble English, even when responding to non-English prompts. This has\nled to the widespread assumption that LLMs may \"think\" in English. However,\nmore recent results showing strong multilingual performance, even surpassing\nEnglish performance on specific tasks in other languages, challenge this view.\nIn this work, we find that LLMs progressively develop a core language-agnostic\nparameter space-a remarkably small subset of parameters whose deactivation\nresults in significant performance degradation across all languages. This\ncompact yet critical set of parameters underlies the model's ability to\ngeneralize beyond individual languages, supporting the emergence of abstract\nthought that is not tied to any specific linguistic system. Specifically, we\nidentify language-related neurons-those are consistently activated during the\nprocessing of particular languages, and categorize them as either shared\n(active across multiple languages) or exclusive (specific to one). As LLMs\nundergo continued development over time, we observe a marked increase in both\nthe proportion and functional importance of shared neurons, while exclusive\nneurons progressively diminish in influence. These shared neurons constitute\nthe backbone of the core language-agnostic parameter space, supporting the\nemergence of abstract thought. Motivated by these insights, we propose\nneuron-specific training strategies tailored to LLMs' language-agnostic levels\nat different development stages. Experiments across diverse LLM families\nsupport our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to advance, their capacity to\nfunction effectively across a diverse range of languages has shown marked\nimprovement. Preliminary studies observe that the hidden activations of LLMs\noften resemble English, even when responding to non-English prompts. This has\nled to the widespread assumption that LLMs may \"think\" in English. However,\nmore recent results showing strong multilingual performance, even surpassing\nEnglish performance on specific tasks in other languages, challenge this view.\nIn this work, we find that LLMs progressively develop a core language-agnostic\nparameter space-a remarkably small subset of parameters whose deactivation\nresults in significant performance degradation across all languages. This\ncompact yet critical set of parameters underlies the model's ability to\ngeneralize beyond individual languages, supporting the emergence of abstract\nthought that is not tied to any specific linguistic system. Specifically, we\nidentify language-related neurons-those are consistently activated during the\nprocessing of particular languages, and categorize them as either shared\n(active across multiple languages) or exclusive (specific to one). As LLMs\nundergo continued development over time, we observe a marked increase in both\nthe proportion and functional importance of shared neurons, while exclusive\nneurons progressively diminish in influence. These shared neurons constitute\nthe backbone of the core language-agnostic parameter space, supporting the\nemergence of abstract thought. Motivated by these insights, we propose\nneuron-specific training strategies tailored to LLMs' language-agnostic levels\nat different development stages. Experiments across diverse LLM families\nsupport our approach."
                },
                "authors": [
                    {
                        "name": "Yuxin Chen"
                    },
                    {
                        "name": "Yiran Zhao"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "An Zhang"
                    },
                    {
                        "name": "Kenji Kawaguchi"
                    },
                    {
                        "name": "Shafiq Joty"
                    },
                    {
                        "name": "Junnan Li"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    },
                    {
                        "name": "Michael Qizhe Shieh"
                    },
                    {
                        "name": "Wenxuan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wenxuan Zhang"
                },
                "author": "Wenxuan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09886v1",
                "updated": "2025-06-11T15:59:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    15,
                    59,
                    15,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T15:59:15Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    15,
                    59,
                    15,
                    2,
                    162,
                    0
                ],
                "title": "Attention Head Embeddings with Trainable Deep Kernels for Hallucination\n  Detection in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Head Embeddings with Trainable Deep Kernels for Hallucination\n  Detection in LLMs"
                },
                "summary": "We present a novel approach for detecting hallucinations in large language\nmodels (LLMs) by analyzing the probabilistic divergence between prompt and\nresponse hidden-state distributions. Counterintuitively, we find that\nhallucinated responses exhibit smaller deviations from their prompts compared\nto grounded responses, suggesting that hallucinations often arise from\nsuperficial rephrasing rather than substantive reasoning. Leveraging this\ninsight, we propose a model-intrinsic detection method that uses distributional\ndistances as principled hallucination scores, eliminating the need for external\nknowledge or auxiliary models. To enhance sensitivity, we employ deep learnable\nkernels that automatically adapt to capture nuanced geometric differences\nbetween distributions. Our approach outperforms existing baselines,\ndemonstrating state-of-the-art performance on several benchmarks. The method\nremains competitive even without kernel training, offering a robust, scalable\nsolution for hallucination detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel approach for detecting hallucinations in large language\nmodels (LLMs) by analyzing the probabilistic divergence between prompt and\nresponse hidden-state distributions. Counterintuitively, we find that\nhallucinated responses exhibit smaller deviations from their prompts compared\nto grounded responses, suggesting that hallucinations often arise from\nsuperficial rephrasing rather than substantive reasoning. Leveraging this\ninsight, we propose a model-intrinsic detection method that uses distributional\ndistances as principled hallucination scores, eliminating the need for external\nknowledge or auxiliary models. To enhance sensitivity, we employ deep learnable\nkernels that automatically adapt to capture nuanced geometric differences\nbetween distributions. Our approach outperforms existing baselines,\ndemonstrating state-of-the-art performance on several benchmarks. The method\nremains competitive even without kernel training, offering a robust, scalable\nsolution for hallucination detection."
                },
                "authors": [
                    {
                        "name": "Rodion Oblovatny"
                    },
                    {
                        "name": "Alexandra Bazarova"
                    },
                    {
                        "name": "Alexey Zaytsev"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Zaytsev"
                },
                "author": "Alexey Zaytsev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08403v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08403v2",
                "updated": "2025-06-11T15:57:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    15,
                    57,
                    34,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-10T03:22:30Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    3,
                    22,
                    30,
                    1,
                    161,
                    0
                ],
                "title": "TACTIC: Translation Agents with Cognitive-Theoretic Interactive\n  Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TACTIC: Translation Agents with Cognitive-Theoretic Interactive\n  Collaboration"
                },
                "summary": "Machine translation has long been a central task in natural language\nprocessing. With the rapid advancement of large language models (LLMs), there\nhas been remarkable progress in translation quality. However, fully realizing\nthe translation potential of LLMs remains an open challenge. Recent studies\nhave explored multi-agent systems to decompose complex translation tasks into\ncollaborative subtasks, showing initial promise in enhancing translation\nquality through agent cooperation and specialization. Nevertheless, existing\nmulti-agent translation frameworks largely neglect foundational insights from\ncognitive translation studies. These insights emphasize how human translators\nemploy different cognitive strategies, such as balancing literal and free\ntranslation, refining expressions based on context, and iteratively evaluating\noutputs. To address this limitation, we propose a cognitively informed\nmulti-agent framework called TACTIC, which stands for T ranslation A gents with\nCognitive- T heoretic Interactive Collaboration. The framework comprises six\nfunctionally distinct agents that mirror key cognitive processes observed in\nhuman translation behavior. These include agents for drafting, refinement,\nevaluation, scoring, context reasoning, and external knowledge gathering. By\nsimulating an interactive and theory-grounded translation workflow, TACTIC\neffectively leverages the full capacity of LLMs for high-quality translation.\nExperimental results on diverse language pairs from the FLORES-200 and WMT24\nbenchmarks show that our method consistently achieves state-of-the-art\nperformance. Using DeepSeek-V3 as the base model, TACTIC surpasses GPT-4.1 by\nan average of +0.6 XCOMET and +1.18 COMETKIWI-23. Compared to DeepSeek-R1, it\nfurther improves by +0.84 XCOMET and +2.99 COMETKIWI-23. Code is available at\nhttps://github.com/weiyali126/TACTIC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine translation has long been a central task in natural language\nprocessing. With the rapid advancement of large language models (LLMs), there\nhas been remarkable progress in translation quality. However, fully realizing\nthe translation potential of LLMs remains an open challenge. Recent studies\nhave explored multi-agent systems to decompose complex translation tasks into\ncollaborative subtasks, showing initial promise in enhancing translation\nquality through agent cooperation and specialization. Nevertheless, existing\nmulti-agent translation frameworks largely neglect foundational insights from\ncognitive translation studies. These insights emphasize how human translators\nemploy different cognitive strategies, such as balancing literal and free\ntranslation, refining expressions based on context, and iteratively evaluating\noutputs. To address this limitation, we propose a cognitively informed\nmulti-agent framework called TACTIC, which stands for T ranslation A gents with\nCognitive- T heoretic Interactive Collaboration. The framework comprises six\nfunctionally distinct agents that mirror key cognitive processes observed in\nhuman translation behavior. These include agents for drafting, refinement,\nevaluation, scoring, context reasoning, and external knowledge gathering. By\nsimulating an interactive and theory-grounded translation workflow, TACTIC\neffectively leverages the full capacity of LLMs for high-quality translation.\nExperimental results on diverse language pairs from the FLORES-200 and WMT24\nbenchmarks show that our method consistently achieves state-of-the-art\nperformance. Using DeepSeek-V3 as the base model, TACTIC surpasses GPT-4.1 by\nan average of +0.6 XCOMET and +1.18 COMETKIWI-23. Compared to DeepSeek-R1, it\nfurther improves by +0.84 XCOMET and +2.99 COMETKIWI-23. Code is available at\nhttps://github.com/weiyali126/TACTIC."
                },
                "authors": [
                    {
                        "name": "Weiya Li"
                    },
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Bei Li"
                    },
                    {
                        "name": "Boyang Liu"
                    },
                    {
                        "name": "Zichen Wen"
                    },
                    {
                        "name": "Nuanqiao Shan"
                    },
                    {
                        "name": "Xiaoqian Liu"
                    },
                    {
                        "name": "Anping Liu"
                    },
                    {
                        "name": "Huajie Liu"
                    },
                    {
                        "name": "Hu Song"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "20 pages, 4 figures, Under review. Code:\n  https://github.com/weiyali126/TACTIC",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08403v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08403v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09878v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09878v1",
                "updated": "2025-06-11T15:48:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    15,
                    48,
                    29,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T15:48:29Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    15,
                    48,
                    29,
                    2,
                    162,
                    0
                ],
                "title": "Virtualizing RAN: Science, Strategy, and Architecture of\n  Software-Defined Mobile Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtualizing RAN: Science, Strategy, and Architecture of\n  Software-Defined Mobile Networks"
                },
                "summary": "Virtualising the Radio Access Network (RAN) is widely touted as the\ncorner-stone of affordable 5G and a prerequisite for AI-native 6G. Yet current\ndiscourse often isolates spectrum policy, cloud engineering and organisational\nreadiness into silos. This paper delivers an integrated analysis that spans\nscience, technology, business strategy and culture. I first review\nspectrum-auction economics and show-via a comparative study of T-Mobile US and\nVerizon-that mid-band contiguity leveraged through software-defined carrier\naggregation outperforms mmWave-centric deployments in both coverage and churn\nmetrics. I then formalise the technical foundations of virtualised and open\nRAN, deriving capacity limits from contiguous and dis-contiguous spectrum maths\nand quantifying hardware ceilings for 400 MHz mmWave channels. Edge compute\nplatforms (NVIDIA EGX, Samsung vRAN 3.0) and SDN-controlled RAN Intelligent\nControllers are examined alongside AI ML pipelines that enable\ndigital-twin-driven optimisation. A security cost model extends recent O-RAN\nmeasurements to show how 256-bit cipher enforcement adds 35-60 us latency\nunless mitigated by inline crypto off-load. Finally, a national automation case\nstudy of live vRAN sites -- demonstrates an 81 to 13 day cycle-time reduction\nonce cultural change errors are corrected. I conclude with open research\nchallenges for sub-THz 6G, energy-neutral AI accelerators and zero-trust\norchestration, offering actionable recommendations for operators, vendors and\nresearchers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtualising the Radio Access Network (RAN) is widely touted as the\ncorner-stone of affordable 5G and a prerequisite for AI-native 6G. Yet current\ndiscourse often isolates spectrum policy, cloud engineering and organisational\nreadiness into silos. This paper delivers an integrated analysis that spans\nscience, technology, business strategy and culture. I first review\nspectrum-auction economics and show-via a comparative study of T-Mobile US and\nVerizon-that mid-band contiguity leveraged through software-defined carrier\naggregation outperforms mmWave-centric deployments in both coverage and churn\nmetrics. I then formalise the technical foundations of virtualised and open\nRAN, deriving capacity limits from contiguous and dis-contiguous spectrum maths\nand quantifying hardware ceilings for 400 MHz mmWave channels. Edge compute\nplatforms (NVIDIA EGX, Samsung vRAN 3.0) and SDN-controlled RAN Intelligent\nControllers are examined alongside AI ML pipelines that enable\ndigital-twin-driven optimisation. A security cost model extends recent O-RAN\nmeasurements to show how 256-bit cipher enforcement adds 35-60 us latency\nunless mitigated by inline crypto off-load. Finally, a national automation case\nstudy of live vRAN sites -- demonstrates an 81 to 13 day cycle-time reduction\nonce cultural change errors are corrected. I conclude with open research\nchallenges for sub-THz 6G, energy-neutral AI accelerators and zero-trust\norchestration, offering actionable recommendations for operators, vendors and\nresearchers."
                },
                "authors": [
                    {
                        "name": "Ryan Barker"
                    }
                ],
                "author_detail": {
                    "name": "Ryan Barker"
                },
                "author": "Ryan Barker",
                "arxiv_comment": "12 pages, 4 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09878v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09878v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01271v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01271v5",
                "updated": "2025-06-11T15:41:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    15,
                    41,
                    56,
                    2,
                    162,
                    0
                ],
                "published": "2025-01-02T14:11:44Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    14,
                    11,
                    44,
                    3,
                    2,
                    0
                ],
                "title": "Energy-and Spectral-Efficiency Trade-off in Distributed Massive-MIMO\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy-and Spectral-Efficiency Trade-off in Distributed Massive-MIMO\n  Networks"
                },
                "summary": "This paper investigates a fundamental yet under-explored trade-off between\nenergy efficiency (EE) and spectral efficiency (SE) in distributed massive MIMO\n(D-mMIMO) systems. Unlike conventional EE-SE trade-off studies that primarily\nfocus on transmission power, D-mMIMO systems introduce new energy consumption\nfactors including fronthaul signaling and distributed signal processing, which\nare heavily influenced by AP-UE association. This work highlights the critical\nneed for a system-level EE-SE trade-off framework that accounts for these\nunique aspects of D-mMIMO. We formulate a joint optimization problem that\nmaximizes EE while satisfying uplink sum-SE constraints, through the\ncoordinated design of power allocation and AP-UE association strategies. By\nexplicitly considering both transmission and infrastructure-related energy\ncosts, our approach enables energy-aware network design without compromising\nthroughput. Numerical simulations demonstrate the substantial impact of dynamic\nAP-UE association and power control on the EE-SE trade-off, providing\nactionable insights for an efficient deployment of large-scale distributed MIMO\nnetworks in next-generation wireless systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates a fundamental yet under-explored trade-off between\nenergy efficiency (EE) and spectral efficiency (SE) in distributed massive MIMO\n(D-mMIMO) systems. Unlike conventional EE-SE trade-off studies that primarily\nfocus on transmission power, D-mMIMO systems introduce new energy consumption\nfactors including fronthaul signaling and distributed signal processing, which\nare heavily influenced by AP-UE association. This work highlights the critical\nneed for a system-level EE-SE trade-off framework that accounts for these\nunique aspects of D-mMIMO. We formulate a joint optimization problem that\nmaximizes EE while satisfying uplink sum-SE constraints, through the\ncoordinated design of power allocation and AP-UE association strategies. By\nexplicitly considering both transmission and infrastructure-related energy\ncosts, our approach enables energy-aware network design without compromising\nthroughput. Numerical simulations demonstrate the substantial impact of dynamic\nAP-UE association and power control on the EE-SE trade-off, providing\nactionable insights for an efficient deployment of large-scale distributed MIMO\nnetworks in next-generation wireless systems."
                },
                "authors": [
                    {
                        "name": "Mohd Saif Ali Khan"
                    },
                    {
                        "name": "Karthik RM"
                    },
                    {
                        "name": "Samar Agnihotri"
                    }
                ],
                "author_detail": {
                    "name": "Samar Agnihotri"
                },
                "author": "Samar Agnihotri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01271v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01271v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13228v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13228v2",
                "updated": "2025-06-11T15:39:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    15,
                    39,
                    13,
                    2,
                    162,
                    0
                ],
                "published": "2025-02-18T19:06:21Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    19,
                    6,
                    21,
                    1,
                    49,
                    0
                ],
                "title": "Conformal Prediction as Bayesian Quadrature",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conformal Prediction as Bayesian Quadrature"
                },
                "summary": "As machine learning-based prediction systems are increasingly used in\nhigh-stakes situations, it is important to understand how such predictive\nmodels will perform upon deployment. Distribution-free uncertainty\nquantification techniques such as conformal prediction provide guarantees about\nthe loss black-box models will incur even when the details of the models are\nhidden. However, such methods are based on frequentist probability, which\nunduly limits their applicability. We revisit the central aspects of conformal\nprediction from a Bayesian perspective and thereby illuminate the shortcomings\nof frequentist guarantees. We propose a practical alternative based on Bayesian\nquadrature that provides interpretable guarantees and offers a richer\nrepresentation of the likely range of losses to be observed at test time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As machine learning-based prediction systems are increasingly used in\nhigh-stakes situations, it is important to understand how such predictive\nmodels will perform upon deployment. Distribution-free uncertainty\nquantification techniques such as conformal prediction provide guarantees about\nthe loss black-box models will incur even when the details of the models are\nhidden. However, such methods are based on frequentist probability, which\nunduly limits their applicability. We revisit the central aspects of conformal\nprediction from a Bayesian perspective and thereby illuminate the shortcomings\nof frequentist guarantees. We propose a practical alternative based on Bayesian\nquadrature that provides interpretable guarantees and offers a richer\nrepresentation of the likely range of losses to be observed at test time."
                },
                "authors": [
                    {
                        "name": "Jake C. Snell"
                    },
                    {
                        "name": "Thomas L. Griffiths"
                    }
                ],
                "author_detail": {
                    "name": "Thomas L. Griffiths"
                },
                "author": "Thomas L. Griffiths",
                "arxiv_comment": "ICML 2025 camera-ready version (accepted as an oral presentation). 16\n  pages, 4 figures. Code available at\n  https://github.com/jakesnell/conformal-as-bayes-quad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13228v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13228v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03949v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03949v2",
                "updated": "2025-06-11T15:37:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    15,
                    37,
                    7,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-04T13:39:01Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    13,
                    39,
                    1,
                    2,
                    155,
                    0
                ],
                "title": "TableEval: A Real-World Benchmark for Complex, Multilingual, and\n  Multi-Structured Table Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TableEval: A Real-World Benchmark for Complex, Multilingual, and\n  Multi-Structured Table Question Answering"
                },
                "summary": "LLMs have shown impressive progress in natural language processing. However,\nthey still face significant challenges in TableQA, where real-world\ncomplexities such as diverse table structures, multilingual data, and\ndomain-specific reasoning are crucial. Existing TableQA benchmarks are often\nlimited by their focus on simple flat tables and suffer from data leakage.\nFurthermore, most benchmarks are monolingual and fail to capture the\ncross-lingual and cross-domain variability in practical applications. To\naddress these limitations, we introduce TableEval, a new benchmark designed to\nevaluate LLMs on realistic TableQA tasks. Specifically, TableEval includes\ntables with various structures (such as concise, hierarchical, and nested\ntables) collected from four domains (including government, finance, academia,\nand industry reports). Besides, TableEval features cross-lingual scenarios with\ntables in Simplified Chinese, Traditional Chinese, and English. To minimize the\nrisk of data leakage, we collect all data from recent real-world documents.\nConsidering that existing TableQA metrics fail to capture semantic accuracy, we\nfurther propose SEAT, a new evaluation framework that assesses the alignment\nbetween model responses and reference answers at the sub-question level.\nExperimental results have shown that SEAT achieves high agreement with human\njudgment. Extensive experiments on TableEval reveal critical gaps in the\nability of state-of-the-art LLMs to handle these complex, real-world TableQA\ntasks, offering insights for future improvements. We make our dataset available\nhere: https://github.com/wenge-research/TableEval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have shown impressive progress in natural language processing. However,\nthey still face significant challenges in TableQA, where real-world\ncomplexities such as diverse table structures, multilingual data, and\ndomain-specific reasoning are crucial. Existing TableQA benchmarks are often\nlimited by their focus on simple flat tables and suffer from data leakage.\nFurthermore, most benchmarks are monolingual and fail to capture the\ncross-lingual and cross-domain variability in practical applications. To\naddress these limitations, we introduce TableEval, a new benchmark designed to\nevaluate LLMs on realistic TableQA tasks. Specifically, TableEval includes\ntables with various structures (such as concise, hierarchical, and nested\ntables) collected from four domains (including government, finance, academia,\nand industry reports). Besides, TableEval features cross-lingual scenarios with\ntables in Simplified Chinese, Traditional Chinese, and English. To minimize the\nrisk of data leakage, we collect all data from recent real-world documents.\nConsidering that existing TableQA metrics fail to capture semantic accuracy, we\nfurther propose SEAT, a new evaluation framework that assesses the alignment\nbetween model responses and reference answers at the sub-question level.\nExperimental results have shown that SEAT achieves high agreement with human\njudgment. Extensive experiments on TableEval reveal critical gaps in the\nability of state-of-the-art LLMs to handle these complex, real-world TableQA\ntasks, offering insights for future improvements. We make our dataset available\nhere: https://github.com/wenge-research/TableEval."
                },
                "authors": [
                    {
                        "name": "Junnan Zhu"
                    },
                    {
                        "name": "Jingyi Wang"
                    },
                    {
                        "name": "Bohan Yu"
                    },
                    {
                        "name": "Xiaoyu Wu"
                    },
                    {
                        "name": "Junbo Li"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Nan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Nan Xu"
                },
                "author": "Nan Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03949v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03949v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09855v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09855v1",
                "updated": "2025-06-11T15:27:03Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    15,
                    27,
                    3,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T15:27:03Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    15,
                    27,
                    3,
                    2,
                    162,
                    0
                ],
                "title": "Foundation Model-Aided Deep Reinforcement Learning for RIS-Assisted\n  Wireless Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation Model-Aided Deep Reinforcement Learning for RIS-Assisted\n  Wireless Communication"
                },
                "summary": "Reconfigurable intelligent surfaces (RIS) have emerged as a promising\ntechnology for enhancing wireless communication by dynamically controlling\nsignal propagation in the environment. However, their efficient deployment\nrelies on accurate channel state information (CSI), which leads to high channel\nestimation overhead due to their passive nature and the large number of\nreflective elements. In this work, we solve this challenge by proposing a novel\nframework that leverages a pre-trained open-source foundation model (FM) named\nlarge wireless model (LWM) to process wireless channels and generate versatile\nand contextualized channel embeddings. These embeddings are then used for the\njoint optimization of the BS beamforming and RIS configurations. To be more\nspecific, for joint optimization, we design a deep reinforcement learning (DRL)\nmodel to automatically select the BS beamforming vector and RIS phase-shift\nmatrix, aiming to maximize the spectral efficiency (SE). This work shows that a\npre-trained FM for radio signal understanding can be fine-tuned and integrated\nwith DRL for effective decision-making in wireless networks. It highlights the\npotential of modality-specific FMs in real-world network optimization.\nAccording to the simulation results, the proposed method outperforms the\nDRL-based approach and beam sweeping-based approach, achieving 9.89% and 43.66%\nhigher SE, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconfigurable intelligent surfaces (RIS) have emerged as a promising\ntechnology for enhancing wireless communication by dynamically controlling\nsignal propagation in the environment. However, their efficient deployment\nrelies on accurate channel state information (CSI), which leads to high channel\nestimation overhead due to their passive nature and the large number of\nreflective elements. In this work, we solve this challenge by proposing a novel\nframework that leverages a pre-trained open-source foundation model (FM) named\nlarge wireless model (LWM) to process wireless channels and generate versatile\nand contextualized channel embeddings. These embeddings are then used for the\njoint optimization of the BS beamforming and RIS configurations. To be more\nspecific, for joint optimization, we design a deep reinforcement learning (DRL)\nmodel to automatically select the BS beamforming vector and RIS phase-shift\nmatrix, aiming to maximize the spectral efficiency (SE). This work shows that a\npre-trained FM for radio signal understanding can be fine-tuned and integrated\nwith DRL for effective decision-making in wireless networks. It highlights the\npotential of modality-specific FMs in real-world network optimization.\nAccording to the simulation results, the proposed method outperforms the\nDRL-based approach and beam sweeping-based approach, achieving 9.89% and 43.66%\nhigher SE, respectively."
                },
                "authors": [
                    {
                        "name": "Mohammad Ghassemi"
                    },
                    {
                        "name": "Sara Farrag Mobarak"
                    },
                    {
                        "name": "Han Zhang"
                    },
                    {
                        "name": "Ali Afana"
                    },
                    {
                        "name": "Akram Bin Sediq"
                    },
                    {
                        "name": "Melike Erol-Kantarci"
                    }
                ],
                "author_detail": {
                    "name": "Melike Erol-Kantarci"
                },
                "author": "Melike Erol-Kantarci",
                "arxiv_comment": "6 pages, 5 figures, PIMRC conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09855v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09855v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09853v1",
                "updated": "2025-06-11T15:22:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    15,
                    22,
                    9,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T15:22:09Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    15,
                    22,
                    9,
                    2,
                    162,
                    0
                ],
                "title": "Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning"
                },
                "summary": "Chain-of-Thought (CoT) prompting plays an indispensable role in endowing\nlarge language models (LLMs) with complex reasoning capabilities. However, CoT\ncurrently faces two fundamental challenges: (1) Sufficiency, which ensures that\nthe generated intermediate inference steps comprehensively cover and\nsubstantiate the final conclusion; and (2) Necessity, which identifies the\ninference steps that are truly indispensable for the soundness of the resulting\nanswer. We propose a causal framework that characterizes CoT reasoning through\nthe dual lenses of sufficiency and necessity. Incorporating causal Probability\nof Sufficiency and Necessity allows us not only to determine which steps are\nlogically sufficient or necessary to the prediction outcome, but also to\nquantify their actual influence on the final reasoning outcome under different\nintervention scenarios, thereby enabling the automated addition of missing\nsteps and the pruning of redundant ones. Extensive experimental results on\nvarious mathematical and commonsense reasoning benchmarks confirm substantial\nimprovements in reasoning efficiency and reduced token usage without\nsacrificing accuracy. Our work provides a promising direction for improving LLM\nreasoning performance and cost-effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) prompting plays an indispensable role in endowing\nlarge language models (LLMs) with complex reasoning capabilities. However, CoT\ncurrently faces two fundamental challenges: (1) Sufficiency, which ensures that\nthe generated intermediate inference steps comprehensively cover and\nsubstantiate the final conclusion; and (2) Necessity, which identifies the\ninference steps that are truly indispensable for the soundness of the resulting\nanswer. We propose a causal framework that characterizes CoT reasoning through\nthe dual lenses of sufficiency and necessity. Incorporating causal Probability\nof Sufficiency and Necessity allows us not only to determine which steps are\nlogically sufficient or necessary to the prediction outcome, but also to\nquantify their actual influence on the final reasoning outcome under different\nintervention scenarios, thereby enabling the automated addition of missing\nsteps and the pruning of redundant ones. Extensive experimental results on\nvarious mathematical and commonsense reasoning benchmarks confirm substantial\nimprovements in reasoning efficiency and reduced token usage without\nsacrificing accuracy. Our work provides a promising direction for improving LLM\nreasoning performance and cost-effectiveness."
                },
                "authors": [
                    {
                        "name": "Xiangning Yu"
                    },
                    {
                        "name": "Zhuohan Wang"
                    },
                    {
                        "name": "Linyi Yang"
                    },
                    {
                        "name": "Haoxuan Li"
                    },
                    {
                        "name": "Anjie Liu"
                    },
                    {
                        "name": "Xiao Xue"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Mengyue Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mengyue Yang"
                },
                "author": "Mengyue Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09847v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09847v1",
                "updated": "2025-06-11T15:21:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    15,
                    21,
                    5,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T15:21:05Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    15,
                    21,
                    5,
                    2,
                    162,
                    0
                ],
                "title": "Dataset of News Articles with Provenance Metadata for Media Relevance\n  Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dataset of News Articles with Provenance Metadata for Media Relevance\n  Assessment"
                },
                "summary": "Out-of-context and misattributed imagery is the leading form of media\nmanipulation in today's misinformation and disinformation landscape. The\nexisting methods attempting to detect this practice often only consider whether\nthe semantics of the imagery corresponds to the text narrative, missing\nmanipulation so long as the depicted objects or scenes somewhat correspond to\nthe narrative at hand. To tackle this, we introduce News Media Provenance\nDataset, a dataset of news articles with provenance-tagged images. We formulate\ntwo tasks on this dataset, location of origin relevance (LOR) and date and time\nof origin relevance (DTOR), and present baseline results on six large language\nmodels (LLMs). We identify that, while the zero-shot performance on LOR is\npromising, the performance on DTOR hinders, leaving room for specialized\narchitectures and future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-context and misattributed imagery is the leading form of media\nmanipulation in today's misinformation and disinformation landscape. The\nexisting methods attempting to detect this practice often only consider whether\nthe semantics of the imagery corresponds to the text narrative, missing\nmanipulation so long as the depicted objects or scenes somewhat correspond to\nthe narrative at hand. To tackle this, we introduce News Media Provenance\nDataset, a dataset of news articles with provenance-tagged images. We formulate\ntwo tasks on this dataset, location of origin relevance (LOR) and date and time\nof origin relevance (DTOR), and present baseline results on six large language\nmodels (LLMs). We identify that, while the zero-shot performance on LOR is\npromising, the performance on DTOR hinders, leaving room for specialized\narchitectures and future work."
                },
                "authors": [
                    {
                        "name": "Tomas Peterka"
                    },
                    {
                        "name": "Matyas Bohacek"
                    }
                ],
                "author_detail": {
                    "name": "Matyas Bohacek"
                },
                "author": "Matyas Bohacek",
                "arxiv_journal_ref": "Workshop on NLP for Positive Impact @ ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09847v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09847v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07859v2",
                "updated": "2025-06-11T15:19:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    15,
                    19,
                    33,
                    2,
                    162,
                    0
                ],
                "published": "2025-05-08T11:17:10Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    11,
                    17,
                    10,
                    3,
                    128,
                    0
                ],
                "title": "Product of Experts with LLMs: Boosting Performance on ARC Is a Matter of\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Product of Experts with LLMs: Boosting Performance on ARC Is a Matter of\n  Perspective"
                },
                "summary": "The Abstraction and Reasoning Corpus (ARC-AGI) poses a significant challenge\nfor large language models (LLMs), exposing limitations in their abstract\nreasoning abilities. In this work, we leverage task-specific data augmentations\nthroughout the training, generation, and scoring phases, and employ a\ndepth-first search algorithm to generate diverse, high-probability candidate\nsolutions. Furthermore, we utilize the LLM not only as a generator but also as\na scorer, using its output probabilities to select the most promising\nsolutions. Our method achieves a score of 71.6% (286.5/400 solved tasks) on the\npublic ARC-AGI evaluation set, demonstrating state-of-the-art performance among\npublicly available approaches. While concurrent closed-source work has reported\nhigher scores, our method distinguishes itself through its transparency,\nreproducibility, and remarkably low inference cost, averaging only around 2ct\nper task on readily available hardware (we assume a price of 36ct/hour for a\nNvidia 4090 GPU).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Abstraction and Reasoning Corpus (ARC-AGI) poses a significant challenge\nfor large language models (LLMs), exposing limitations in their abstract\nreasoning abilities. In this work, we leverage task-specific data augmentations\nthroughout the training, generation, and scoring phases, and employ a\ndepth-first search algorithm to generate diverse, high-probability candidate\nsolutions. Furthermore, we utilize the LLM not only as a generator but also as\na scorer, using its output probabilities to select the most promising\nsolutions. Our method achieves a score of 71.6% (286.5/400 solved tasks) on the\npublic ARC-AGI evaluation set, demonstrating state-of-the-art performance among\npublicly available approaches. While concurrent closed-source work has reported\nhigher scores, our method distinguishes itself through its transparency,\nreproducibility, and remarkably low inference cost, averaging only around 2ct\nper task on readily available hardware (we assume a price of 36ct/hour for a\nNvidia 4090 GPU)."
                },
                "authors": [
                    {
                        "name": "Daniel Franzen"
                    },
                    {
                        "name": "Jan Disselhoff"
                    },
                    {
                        "name": "David Hartmann"
                    }
                ],
                "author_detail": {
                    "name": "David Hartmann"
                },
                "author": "David Hartmann",
                "arxiv_comment": "ICML 2025 camera-ready; 15 pages, 6 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17962v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17962v6",
                "updated": "2025-06-11T15:18:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    15,
                    18,
                    44,
                    2,
                    162,
                    0
                ],
                "published": "2024-06-25T22:44:17Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    22,
                    44,
                    17,
                    1,
                    177,
                    0
                ],
                "title": "Crafting Customisable Characters with LLMs: Introducing SimsChat, a\n  Persona-Driven Role-Playing Agent Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crafting Customisable Characters with LLMs: Introducing SimsChat, a\n  Persona-Driven Role-Playing Agent Framework"
                },
                "summary": "Large Language Models (LLMs) demonstrate remarkable ability to comprehend\ninstructions and generate human-like text, enabling sophisticated agent\nsimulation beyond basic behavior replication. However, the potential for\ncreating freely customisable characters remains underexplored. We introduce the\nCustomisable Conversation Agent Framework, which employs LLMs to simulate\nreal-world characters through personalised characteristic feature injection,\nenabling diverse character creation according to user preferences. We propose\nthe SimsConv dataset, comprising 68 customised characters and 13,971 multi-turn\nrole-playing dialogues across 1,360 real-world scenes. Characters are initially\ncustomised using pre-defined elements (career, aspiration, traits, skills),\nthen expanded through personal and social profiles. Building on this, we\npresent SimsChat, a freely customisable role-playing agent incorporating\nvarious realistic settings and topic-specified character interactions.\nExperimental results on both SimsConv and WikiRoleEval datasets demonstrate\nSimsChat's superior performance in maintaining character consistency, knowledge\naccuracy, and appropriate question rejection compared to existing models. Our\nframework provides valuable insights for developing more accurate and\ncustomisable human simulacra. Our data and code are publicly available at\nhttps://github.com/Bernard-Yang/SimsChat.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate remarkable ability to comprehend\ninstructions and generate human-like text, enabling sophisticated agent\nsimulation beyond basic behavior replication. However, the potential for\ncreating freely customisable characters remains underexplored. We introduce the\nCustomisable Conversation Agent Framework, which employs LLMs to simulate\nreal-world characters through personalised characteristic feature injection,\nenabling diverse character creation according to user preferences. We propose\nthe SimsConv dataset, comprising 68 customised characters and 13,971 multi-turn\nrole-playing dialogues across 1,360 real-world scenes. Characters are initially\ncustomised using pre-defined elements (career, aspiration, traits, skills),\nthen expanded through personal and social profiles. Building on this, we\npresent SimsChat, a freely customisable role-playing agent incorporating\nvarious realistic settings and topic-specified character interactions.\nExperimental results on both SimsConv and WikiRoleEval datasets demonstrate\nSimsChat's superior performance in maintaining character consistency, knowledge\naccuracy, and appropriate question rejection compared to existing models. Our\nframework provides valuable insights for developing more accurate and\ncustomisable human simulacra. Our data and code are publicly available at\nhttps://github.com/Bernard-Yang/SimsChat."
                },
                "authors": [
                    {
                        "name": "Bohao Yang"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Chenghao Xiao"
                    },
                    {
                        "name": "Kun Zhao"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Lin Yuan"
                    },
                    {
                        "name": "Guang Yang"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17962v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17962v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01129v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01129v4",
                "updated": "2025-06-11T15:02:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    15,
                    2,
                    28,
                    2,
                    162,
                    0
                ],
                "published": "2024-04-01T14:11:45Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    14,
                    11,
                    45,
                    0,
                    92,
                    0
                ],
                "title": "Emphasising Structured Information: Integrating Abstract Meaning\n  Representation into LLMs for Enhanced Open-Domain Dialogue Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emphasising Structured Information: Integrating Abstract Meaning\n  Representation into LLMs for Enhanced Open-Domain Dialogue Evaluation"
                },
                "summary": "Automatic open-domain dialogue evaluation has attracted increasing attention,\nyet remains challenging due to the complexity of assessing response\nappropriateness. Traditional evaluation metrics, typically trained with true\npositive and randomly selected negative responses, tend to assign higher scores\nto responses that share greater content similarity with contexts. However,\nadversarial negative responses, despite possessing high lexical overlap with\ncontexts, can be semantically incongruous. Consequently, existing metrics\nstruggle to effectively evaluate such responses, resulting in low correlations\nwith human judgments. While recent studies have demonstrated the effectiveness\nof Large Language Models (LLMs) for open-domain dialogue evaluation, they still\nface challenges in handling adversarial negative examples. We propose a novel\nevaluation framework that integrates Abstract Meaning Representation (AMR)\nenhanced domain-specific language models (SLMs) with LLMs. Our SLMs explicitly\nincorporate AMR graph information through a gating mechanism for enhanced\nsemantic representation learning, while both SLM predictions and AMR knowledge\nare integrated into LLM prompts for robust evaluation. Extensive experiments on\nopen-domain dialogue evaluation tasks demonstrate the superiority of our method\ncompared to state-of-the-art baselines. Our comprehensive ablation studies\nreveal that AMR graph information contributes substantially more to performance\nimprovements. Our framework achieves strong correlations with human judgments\nacross multiple datasets, establishing a new benchmark for dialogue evaluation.\nOur code and data are publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic open-domain dialogue evaluation has attracted increasing attention,\nyet remains challenging due to the complexity of assessing response\nappropriateness. Traditional evaluation metrics, typically trained with true\npositive and randomly selected negative responses, tend to assign higher scores\nto responses that share greater content similarity with contexts. However,\nadversarial negative responses, despite possessing high lexical overlap with\ncontexts, can be semantically incongruous. Consequently, existing metrics\nstruggle to effectively evaluate such responses, resulting in low correlations\nwith human judgments. While recent studies have demonstrated the effectiveness\nof Large Language Models (LLMs) for open-domain dialogue evaluation, they still\nface challenges in handling adversarial negative examples. We propose a novel\nevaluation framework that integrates Abstract Meaning Representation (AMR)\nenhanced domain-specific language models (SLMs) with LLMs. Our SLMs explicitly\nincorporate AMR graph information through a gating mechanism for enhanced\nsemantic representation learning, while both SLM predictions and AMR knowledge\nare integrated into LLM prompts for robust evaluation. Extensive experiments on\nopen-domain dialogue evaluation tasks demonstrate the superiority of our method\ncompared to state-of-the-art baselines. Our comprehensive ablation studies\nreveal that AMR graph information contributes substantially more to performance\nimprovements. Our framework achieves strong correlations with human judgments\nacross multiple datasets, establishing a new benchmark for dialogue evaluation.\nOur code and data are publicly available."
                },
                "authors": [
                    {
                        "name": "Bohao Yang"
                    },
                    {
                        "name": "Kun Zhao"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Liang Zhan"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01129v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01129v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09817v1",
                "updated": "2025-06-11T14:56:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    56,
                    24,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T14:56:24Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    56,
                    24,
                    2,
                    162,
                    0
                ],
                "title": "Enhanced V2X Communication Using Game-Theory Based Adaptive MAC\n  Protocols",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced V2X Communication Using Game-Theory Based Adaptive MAC\n  Protocols"
                },
                "summary": "This paper presents an enhanced Vehicle-to-Everything (V2X) communication\nsystem featuring adaptive Medium Access Control (MAC) using game theory. Our\napproach integrates dynamic transmission power control, dynamic beacon rates,\ncontention window adaptation, and implicit acknowledgment mechanisms within a\nManhattan-like grid-based mobility scenario. Simulations are conducted in a\ncircular coverage area, incorporating refined signal propagation models and\nprobabilistic vehicle mobility with boundary reflection. The results\ndemonstrate effective beacon delivery with average delays under 0.35 s and\npacket loss rates less than 1% in high-density conditions specifically, with up\nto 80 vehicles operating within a 250 m radius. Key innovations include game\ntheory-based environment-aware transmission parameter adaptation and a scalable\ndesign suited for interference-prone V2X deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an enhanced Vehicle-to-Everything (V2X) communication\nsystem featuring adaptive Medium Access Control (MAC) using game theory. Our\napproach integrates dynamic transmission power control, dynamic beacon rates,\ncontention window adaptation, and implicit acknowledgment mechanisms within a\nManhattan-like grid-based mobility scenario. Simulations are conducted in a\ncircular coverage area, incorporating refined signal propagation models and\nprobabilistic vehicle mobility with boundary reflection. The results\ndemonstrate effective beacon delivery with average delays under 0.35 s and\npacket loss rates less than 1% in high-density conditions specifically, with up\nto 80 vehicles operating within a 250 m radius. Key innovations include game\ntheory-based environment-aware transmission parameter adaptation and a scalable\ndesign suited for interference-prone V2X deployments."
                },
                "authors": [
                    {
                        "name": "Dhrumil Bhatt"
                    },
                    {
                        "name": "Nirbhay Singhal"
                    }
                ],
                "author_detail": {
                    "name": "Nirbhay Singhal"
                },
                "author": "Nirbhay Singhal",
                "arxiv_comment": "Accepted at the 16th ICCCNT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00103v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00103v2",
                "updated": "2025-06-11T14:56:19Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    56,
                    19,
                    2,
                    162,
                    0
                ],
                "published": "2025-05-30T14:34:57Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    34,
                    57,
                    4,
                    150,
                    0
                ],
                "title": "Writing-Zero: Bridge the Gap Between Non-verifiable Tasks and Verifiable\n  Rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Writing-Zero: Bridge the Gap Between Non-verifiable Tasks and Verifiable\n  Rewards"
                },
                "summary": "Reinforcement learning with verifiable rewards (RLVR) has enabled large\nlanguage models (LLMs) to achieve remarkable breakthroughs in reasoning tasks\nwith objective ground-truth answers, such as mathematics and code generation.\nHowever, a significant gap remains for non-verifiable tasks, like creative\nwriting and open-ended dialogue, where quality assessment is inherently\nsubjective and lacks definitive references. Existing approaches for these\ndomains often rely on scalar reward models trained with human preferences,\nwhich suffer from limited generalization and are prone to reward hacking, such\nas over-explanation and length bias. In this work, we propose a unified\nRLVR-based training paradigm that bridges the gap between non-verifiable tasks\nand verifiable rewards. We introduce a writing-principle-based pairwise\nGenerative Reward Model (GenRM) and a novel Bootstrapped Relative Policy\nOptimization (BRPO) algorithm. The pairwise writing GenRM leverages\nself-principled critique to transform subjective assessments into reliable,\nverifiable rewards, while BRPO enables dynamic, reference-free pairwise\ncomparison by leveraging a bootstrapped response as temporary reference from\nwithin group rollouts during RL training. Our approach empowers LLMs to develop\nrobust writing capabilities without supervised fine-tuning, as demonstrated by\nWriting-Zero, which shows consistent improvement and strong resistance to\nreward hacking compared to scalar reward baselines. Furthermore, our method\nachieves competitive results on both in-house and open-source writing\nbenchmarks. Our findings suggest the potential to unify rule-based,\nreference-based, and reference-free reward modeling under the RLVR framework,\nthus paving the way for a comprehensive and scalable RL training paradigm\napplicable across all language tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning with verifiable rewards (RLVR) has enabled large\nlanguage models (LLMs) to achieve remarkable breakthroughs in reasoning tasks\nwith objective ground-truth answers, such as mathematics and code generation.\nHowever, a significant gap remains for non-verifiable tasks, like creative\nwriting and open-ended dialogue, where quality assessment is inherently\nsubjective and lacks definitive references. Existing approaches for these\ndomains often rely on scalar reward models trained with human preferences,\nwhich suffer from limited generalization and are prone to reward hacking, such\nas over-explanation and length bias. In this work, we propose a unified\nRLVR-based training paradigm that bridges the gap between non-verifiable tasks\nand verifiable rewards. We introduce a writing-principle-based pairwise\nGenerative Reward Model (GenRM) and a novel Bootstrapped Relative Policy\nOptimization (BRPO) algorithm. The pairwise writing GenRM leverages\nself-principled critique to transform subjective assessments into reliable,\nverifiable rewards, while BRPO enables dynamic, reference-free pairwise\ncomparison by leveraging a bootstrapped response as temporary reference from\nwithin group rollouts during RL training. Our approach empowers LLMs to develop\nrobust writing capabilities without supervised fine-tuning, as demonstrated by\nWriting-Zero, which shows consistent improvement and strong resistance to\nreward hacking compared to scalar reward baselines. Furthermore, our method\nachieves competitive results on both in-house and open-source writing\nbenchmarks. Our findings suggest the potential to unify rule-based,\nreference-based, and reference-free reward modeling under the RLVR framework,\nthus paving the way for a comprehensive and scalable RL training paradigm\napplicable across all language tasks."
                },
                "authors": [
                    {
                        "name": "Ruipeng Jia"
                    },
                    {
                        "name": "Yunyi Yang"
                    },
                    {
                        "name": "Yongbo Gai"
                    },
                    {
                        "name": "Kai Luo"
                    },
                    {
                        "name": "Shihao Huang"
                    },
                    {
                        "name": "Jianhe Lin"
                    },
                    {
                        "name": "Xiaoxi Jiang"
                    },
                    {
                        "name": "Guanjun Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Guanjun Jiang"
                },
                "author": "Guanjun Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00103v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00103v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09813v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09813v1",
                "updated": "2025-06-11T14:53:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    53,
                    47,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T14:53:47Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    53,
                    47,
                    2,
                    162,
                    0
                ],
                "title": "Metritocracy: Representative Metrics for Lite Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metritocracy: Representative Metrics for Lite Benchmarks"
                },
                "summary": "A common problem in LLM evaluation is how to choose a subset of metrics from\na full suite of possible metrics. Subset selection is usually done for\nefficiency or interpretability reasons, and the goal is often to select a\n``representative'' subset of metrics. However, ``representative'' is rarely\nclearly defined. In this work, we use ideas from social choice theory to\nformalize two notions of representation for the selection of a subset of\nevaluation metrics. We first introduce positional representation, which\nguarantees every alternative is sufficiently represented at every position\ncutoff. We then introduce positional proportionality, which guarantees no\nalternative is proportionally over- or under-represented by more than a small\nerror at any position. We prove upper and lower bounds on the smallest number\nof metrics needed to guarantee either of these properties in the worst case. We\nalso study a generalized form of each property that allows for additional input\non groups of metrics that must be represented. Finally, we tie theory to\npractice through real-world case studies on both LLM evaluation and hospital\nquality evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A common problem in LLM evaluation is how to choose a subset of metrics from\na full suite of possible metrics. Subset selection is usually done for\nefficiency or interpretability reasons, and the goal is often to select a\n``representative'' subset of metrics. However, ``representative'' is rarely\nclearly defined. In this work, we use ideas from social choice theory to\nformalize two notions of representation for the selection of a subset of\nevaluation metrics. We first introduce positional representation, which\nguarantees every alternative is sufficiently represented at every position\ncutoff. We then introduce positional proportionality, which guarantees no\nalternative is proportionally over- or under-represented by more than a small\nerror at any position. We prove upper and lower bounds on the smallest number\nof metrics needed to guarantee either of these properties in the worst case. We\nalso study a generalized form of each property that allows for additional input\non groups of metrics that must be represented. Finally, we tie theory to\npractice through real-world case studies on both LLM evaluation and hospital\nquality evaluation."
                },
                "authors": [
                    {
                        "name": "Ariel Procaccia"
                    },
                    {
                        "name": "Benjamin Schiffer"
                    },
                    {
                        "name": "Serena Wang"
                    },
                    {
                        "name": "Shirley Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shirley Zhang"
                },
                "author": "Shirley Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09813v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09813v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05023v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05023v3",
                "updated": "2025-06-11T14:47:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    47,
                    23,
                    2,
                    162,
                    0
                ],
                "published": "2024-12-06T13:20:57Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    13,
                    20,
                    57,
                    4,
                    341,
                    0
                ],
                "title": "Steps are all you need: Rethinking STEM Education with Prompt\n  Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steps are all you need: Rethinking STEM Education with Prompt\n  Engineering"
                },
                "summary": "Few shot and Chain-of-Thought prompting have shown promise when applied to\nPhysics Question Answering Tasks, but are limited by the lack of mathematical\nability inherent to LLMs, and are prone to hallucination. By utilizing a\nMixture of Experts (MoE) Model, along with analogical prompting, we are able to\nshow improved model performance when compared to the baseline on standard LLMs.\nWe also survey the limits of these prompting techniques and the effects they\nhave on model performance. Additionally, we propose Analogical CoT prompting, a\nprompting technique designed to allow smaller, open source models to leverage\nAnalogical prompting, something they have struggled with, possibly due to a\nlack of specialist training data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few shot and Chain-of-Thought prompting have shown promise when applied to\nPhysics Question Answering Tasks, but are limited by the lack of mathematical\nability inherent to LLMs, and are prone to hallucination. By utilizing a\nMixture of Experts (MoE) Model, along with analogical prompting, we are able to\nshow improved model performance when compared to the baseline on standard LLMs.\nWe also survey the limits of these prompting techniques and the effects they\nhave on model performance. Additionally, we propose Analogical CoT prompting, a\nprompting technique designed to allow smaller, open source models to leverage\nAnalogical prompting, something they have struggled with, possibly due to a\nlack of specialist training data."
                },
                "authors": [
                    {
                        "name": "Krishnasai Addala"
                    },
                    {
                        "name": "Kabir Dev Paul Baghel"
                    },
                    {
                        "name": "Navya Gupta"
                    },
                    {
                        "name": "Rishitej Reddy Vyalla"
                    },
                    {
                        "name": "Chhavi Kirtani"
                    },
                    {
                        "name": "Avinash Anand"
                    },
                    {
                        "name": "Rajiv Ratn Shah"
                    }
                ],
                "author_detail": {
                    "name": "Rajiv Ratn Shah"
                },
                "author": "Rajiv Ratn Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05023v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05023v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05453v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05453v3",
                "updated": "2025-06-11T14:43:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    43,
                    44,
                    2,
                    162,
                    0
                ],
                "published": "2024-12-06T22:25:23Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    22,
                    25,
                    23,
                    4,
                    341,
                    0
                ],
                "title": "Knowledge Graphs are all you need: Leveraging KGs in Physics Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Graphs are all you need: Leveraging KGs in Physics Question\n  Answering"
                },
                "summary": "This study explores the effectiveness of using knowledge graphs generated by\nlarge language models to decompose high school-level physics questions into\nsub-questions. We introduce a pipeline aimed at enhancing model response\nquality for Question Answering tasks. By employing LLMs to construct knowledge\ngraphs that capture the internal logic of the questions, these graphs then\nguide the generation of subquestions. We hypothesize that this method yields\nsub-questions that are more logically consistent with the original questions\ncompared to traditional decomposition techniques. Our results show that\nsub-questions derived from knowledge graphs exhibit significantly improved\nfidelity to the original question's logic. This approach not only enhances the\nlearning experience by providing clearer and more contextually appropriate\nsub-questions but also highlights the potential of LLMs to transform\neducational methodologies. The findings indicate a promising direction for\napplying AI to improve the quality and effectiveness of educational content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the effectiveness of using knowledge graphs generated by\nlarge language models to decompose high school-level physics questions into\nsub-questions. We introduce a pipeline aimed at enhancing model response\nquality for Question Answering tasks. By employing LLMs to construct knowledge\ngraphs that capture the internal logic of the questions, these graphs then\nguide the generation of subquestions. We hypothesize that this method yields\nsub-questions that are more logically consistent with the original questions\ncompared to traditional decomposition techniques. Our results show that\nsub-questions derived from knowledge graphs exhibit significantly improved\nfidelity to the original question's logic. This approach not only enhances the\nlearning experience by providing clearer and more contextually appropriate\nsub-questions but also highlights the potential of LLMs to transform\neducational methodologies. The findings indicate a promising direction for\napplying AI to improve the quality and effectiveness of educational content."
                },
                "authors": [
                    {
                        "name": "Krishnasai Addala"
                    },
                    {
                        "name": "Kabir Dev Paul Baghel"
                    },
                    {
                        "name": "Dhruv Jain"
                    },
                    {
                        "name": "Navya Gupta"
                    },
                    {
                        "name": "Rishitej Reddy Vyalla"
                    },
                    {
                        "name": "Chhavi Kirtani"
                    },
                    {
                        "name": "Avinash Anand"
                    },
                    {
                        "name": "Rajiv Ratn Shah"
                    }
                ],
                "author_detail": {
                    "name": "Rajiv Ratn Shah"
                },
                "author": "Rajiv Ratn Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05453v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05453v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14352v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14352v2",
                "updated": "2025-06-11T14:42:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    42,
                    54,
                    2,
                    162,
                    0
                ],
                "published": "2024-08-26T15:29:34Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    15,
                    29,
                    34,
                    0,
                    239,
                    0
                ],
                "title": "LogProber: Disentangling confidence from contamination in LLM responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogProber: Disentangling confidence from contamination in LLM responses"
                },
                "summary": "In machine learning, contamination refers to situations where testing data\nleak into the training set. The issue is particularly relevant for the\nevaluation of the performance of Large Language Models (LLMs), which are\ngenerally trained on gargantuan, and generally opaque, corpora of text scraped\nfrom the world wide web. Developing tools to detect contamination is therefore\ncrucial to be able to fairly and properly track the evolution of the\nperformance of LLMs. To date, only a few recent studies have attempted to\naddress the issue of quantifying and detecting contamination in short text\nsequences, such as those commonly found in benchmarks. However, these methods\nhave limitations that can sometimes render them impractical.In the present\npaper, we introduce LogProber, a novel, efficient algorithm that we show to be\nable to detect contamination in a black box setting that tries to tackle some\nof these drawbacks by focusing on the familiarity with the question rather than\nthe answer. Here, we explore the properties of the proposed method in\ncomparison with concurrent approaches, identify its advantages and limitations,\nand illustrate how different forms of contamination can go undetected depending\non the design of the detection algorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In machine learning, contamination refers to situations where testing data\nleak into the training set. The issue is particularly relevant for the\nevaluation of the performance of Large Language Models (LLMs), which are\ngenerally trained on gargantuan, and generally opaque, corpora of text scraped\nfrom the world wide web. Developing tools to detect contamination is therefore\ncrucial to be able to fairly and properly track the evolution of the\nperformance of LLMs. To date, only a few recent studies have attempted to\naddress the issue of quantifying and detecting contamination in short text\nsequences, such as those commonly found in benchmarks. However, these methods\nhave limitations that can sometimes render them impractical.In the present\npaper, we introduce LogProber, a novel, efficient algorithm that we show to be\nable to detect contamination in a black box setting that tries to tackle some\nof these drawbacks by focusing on the familiarity with the question rather than\nthe answer. Here, we explore the properties of the proposed method in\ncomparison with concurrent approaches, identify its advantages and limitations,\nand illustrate how different forms of contamination can go undetected depending\non the design of the detection algorithm."
                },
                "authors": [
                    {
                        "name": "Nicolas Yax"
                    },
                    {
                        "name": "Pierre-Yves Oudeyer"
                    },
                    {
                        "name": "Stefano Palminteri"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Palminteri"
                },
                "author": "Stefano Palminteri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14352v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14352v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09800v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09800v1",
                "updated": "2025-06-11T14:42:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    42,
                    11,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T14:42:11Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    42,
                    11,
                    2,
                    162,
                    0
                ],
                "title": "Reinforced Refinement with Self-Aware Expansion for End-to-End\n  Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforced Refinement with Self-Aware Expansion for End-to-End\n  Autonomous Driving"
                },
                "summary": "End-to-end autonomous driving has emerged as a promising paradigm for\ndirectly mapping sensor inputs to planning maneuvers using learning-based\nmodular integrations. However, existing imitation learning (IL)-based models\nsuffer from generalization to hard cases, and a lack of corrective feedback\nloop under post-deployment. While reinforcement learning (RL) offers a\npotential solution to tackle hard cases with optimality, it is often hindered\nby overfitting to specific driving cases, resulting in catastrophic forgetting\nof generalizable knowledge and sample inefficiency. To overcome these\nchallenges, we propose Reinforced Refinement with Self-aware Expansion (R2SE),\na novel learning pipeline that constantly refines hard domain while keeping\ngeneralizable driving policy for model-agnostic end-to-end driving systems.\nThrough reinforcement fine-tuning and policy expansion that facilitates\ncontinuous improvement, R2SE features three key components: 1) Generalist\nPretraining with hard-case allocation trains a generalist imitation learning\n(IL) driving system while dynamically identifying failure-prone cases for\ntargeted refinement; 2) Residual Reinforced Specialist Fine-tuning optimizes\nresidual corrections using reinforcement learning (RL) to improve performance\nin hard case domain while preserving global driving knowledge; 3) Self-aware\nAdapter Expansion dynamically integrates specialist policies back into the\ngeneralist model, enhancing continuous performance improvement. Experimental\nresults in closed-loop simulation and real-world datasets demonstrate\nimprovements in generalization, safety, and long-horizon policy robustness over\nstate-of-the-art E2E systems, highlighting the effectiveness of reinforce\nrefinement for scalable autonomous driving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-end autonomous driving has emerged as a promising paradigm for\ndirectly mapping sensor inputs to planning maneuvers using learning-based\nmodular integrations. However, existing imitation learning (IL)-based models\nsuffer from generalization to hard cases, and a lack of corrective feedback\nloop under post-deployment. While reinforcement learning (RL) offers a\npotential solution to tackle hard cases with optimality, it is often hindered\nby overfitting to specific driving cases, resulting in catastrophic forgetting\nof generalizable knowledge and sample inefficiency. To overcome these\nchallenges, we propose Reinforced Refinement with Self-aware Expansion (R2SE),\na novel learning pipeline that constantly refines hard domain while keeping\ngeneralizable driving policy for model-agnostic end-to-end driving systems.\nThrough reinforcement fine-tuning and policy expansion that facilitates\ncontinuous improvement, R2SE features three key components: 1) Generalist\nPretraining with hard-case allocation trains a generalist imitation learning\n(IL) driving system while dynamically identifying failure-prone cases for\ntargeted refinement; 2) Residual Reinforced Specialist Fine-tuning optimizes\nresidual corrections using reinforcement learning (RL) to improve performance\nin hard case domain while preserving global driving knowledge; 3) Self-aware\nAdapter Expansion dynamically integrates specialist policies back into the\ngeneralist model, enhancing continuous performance improvement. Experimental\nresults in closed-loop simulation and real-world datasets demonstrate\nimprovements in generalization, safety, and long-horizon policy robustness over\nstate-of-the-art E2E systems, highlighting the effectiveness of reinforce\nrefinement for scalable autonomous driving."
                },
                "authors": [
                    {
                        "name": "Haochen Liu"
                    },
                    {
                        "name": "Tianyu Li"
                    },
                    {
                        "name": "Haohan Yang"
                    },
                    {
                        "name": "Li Chen"
                    },
                    {
                        "name": "Caojun Wang"
                    },
                    {
                        "name": "Ke Guo"
                    },
                    {
                        "name": "Haochen Tian"
                    },
                    {
                        "name": "Hongchen Li"
                    },
                    {
                        "name": "Hongyang Li"
                    },
                    {
                        "name": "Chen Lv"
                    }
                ],
                "author_detail": {
                    "name": "Chen Lv"
                },
                "author": "Chen Lv",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09800v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09800v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09796v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09796v1",
                "updated": "2025-06-11T14:41:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    41,
                    10,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T14:41:10Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    41,
                    10,
                    2,
                    162,
                    0
                ],
                "title": "Do LLMs Give Psychometrically Plausible Responses in Educational\n  Assessments?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Give Psychometrically Plausible Responses in Educational\n  Assessments?"
                },
                "summary": "Knowing how test takers answer items in educational assessments is essential\nfor test development, to evaluate item quality, and to improve test validity.\nHowever, this process usually requires extensive pilot studies with human\nparticipants. If large language models (LLMs) exhibit human-like response\nbehavior to test items, this could open up the possibility of using them as\npilot participants to accelerate test development. In this paper, we evaluate\nthe human-likeness or psychometric plausibility of responses from 18\ninstruction-tuned LLMs with two publicly available datasets of multiple-choice\ntest items across three subjects: reading, U.S. history, and economics. Our\nmethodology builds on two theoretical frameworks from psychometrics which are\ncommonly used in educational assessment, classical test theory and item\nresponse theory. The results show that while larger models are excessively\nconfident, their response distributions can be more human-like when calibrated\nwith temperature scaling. In addition, we find that LLMs tend to correlate\nbetter with humans in reading comprehension items compared to other subjects.\nHowever, the correlations are not very strong overall, indicating that LLMs\nshould not be used for piloting educational assessments in a zero-shot setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowing how test takers answer items in educational assessments is essential\nfor test development, to evaluate item quality, and to improve test validity.\nHowever, this process usually requires extensive pilot studies with human\nparticipants. If large language models (LLMs) exhibit human-like response\nbehavior to test items, this could open up the possibility of using them as\npilot participants to accelerate test development. In this paper, we evaluate\nthe human-likeness or psychometric plausibility of responses from 18\ninstruction-tuned LLMs with two publicly available datasets of multiple-choice\ntest items across three subjects: reading, U.S. history, and economics. Our\nmethodology builds on two theoretical frameworks from psychometrics which are\ncommonly used in educational assessment, classical test theory and item\nresponse theory. The results show that while larger models are excessively\nconfident, their response distributions can be more human-like when calibrated\nwith temperature scaling. In addition, we find that LLMs tend to correlate\nbetter with humans in reading comprehension items compared to other subjects.\nHowever, the correlations are not very strong overall, indicating that LLMs\nshould not be used for piloting educational assessments in a zero-shot setting."
                },
                "authors": [
                    {
                        "name": "Andreas Suberli"
                    },
                    {
                        "name": "Diego Frassinelli"
                    },
                    {
                        "name": "Barbara Plank"
                    }
                ],
                "author_detail": {
                    "name": "Barbara Plank"
                },
                "author": "Barbara Plank",
                "arxiv_comment": "Accepted for publication at the 20th Workshop on Innovative Use of\n  NLP for Building Educational Applications (BEA) at ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09796v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09796v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09771v1",
                "updated": "2025-06-11T14:10:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    10,
                    39,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T14:10:39Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    10,
                    39,
                    2,
                    162,
                    0
                ],
                "title": "Where Journalism Silenced Voices: Exploring Discrimination in the\n  Representation of Indigenous Communities in Bangladesh",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Where Journalism Silenced Voices: Exploring Discrimination in the\n  Representation of Indigenous Communities in Bangladesh"
                },
                "summary": "In this paper, we examine the intersections of indigeneity and media\nrepresentation in shaping perceptions of indigenous communities in Bangladesh.\nUsing a mixed-methods approach, we combine quantitative analysis of media data\nwith qualitative insights from focus group discussions (FGD). First, we\nidentify a total of 4,893 indigenous-related articles from our initial dataset\nof 2.2 million newspaper articles, using a combination of keyword-based\nfiltering and LLM, achieving 77% accuracy and an F1-score of 81.9\\%. From\nmanually inspecting 3 prominent Bangla newspapers, we identify 15 genres that\nwe use as our topics for semi-supervised topic modeling using CorEx. Results\nshow indigenous news articles have higher representation of culture and\nentertainment (19%, 10% higher than general news articles), and a\ndisproportionate focus on conflict and protest (9%, 7% higher than general\nnews). On the other hand, sentiment analysis reveals that 57% of articles on\nindigenous topics carry a negative tone, compared to 27% for non-indigenous\nrelated news. Drawing from communication studies, we further analyze framing,\npriming, and agenda-setting (frequency of themes) to support the case for\ndiscrimination in representation of indigenous news coverage. For the\nqualitative part of our analysis, we facilitated FGD, where participants\nfurther validated these findings. Participants unanimously expressed their\nfeeling of being under-represented, and that critical issues affecting their\ncommunities (such as education, healthcare, and land rights) are systematically\nmarginalized in news media coverage. By highlighting 8 cases of discrimination\nand media misrepresentation that were frequently mentioned by participants in\nthe FGD, this study emphasizes the urgent need for more equitable media\npractices that accurately reflect the experiences and struggles of marginalized\ncommunities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we examine the intersections of indigeneity and media\nrepresentation in shaping perceptions of indigenous communities in Bangladesh.\nUsing a mixed-methods approach, we combine quantitative analysis of media data\nwith qualitative insights from focus group discussions (FGD). First, we\nidentify a total of 4,893 indigenous-related articles from our initial dataset\nof 2.2 million newspaper articles, using a combination of keyword-based\nfiltering and LLM, achieving 77% accuracy and an F1-score of 81.9\\%. From\nmanually inspecting 3 prominent Bangla newspapers, we identify 15 genres that\nwe use as our topics for semi-supervised topic modeling using CorEx. Results\nshow indigenous news articles have higher representation of culture and\nentertainment (19%, 10% higher than general news articles), and a\ndisproportionate focus on conflict and protest (9%, 7% higher than general\nnews). On the other hand, sentiment analysis reveals that 57% of articles on\nindigenous topics carry a negative tone, compared to 27% for non-indigenous\nrelated news. Drawing from communication studies, we further analyze framing,\npriming, and agenda-setting (frequency of themes) to support the case for\ndiscrimination in representation of indigenous news coverage. For the\nqualitative part of our analysis, we facilitated FGD, where participants\nfurther validated these findings. Participants unanimously expressed their\nfeeling of being under-represented, and that critical issues affecting their\ncommunities (such as education, healthcare, and land rights) are systematically\nmarginalized in news media coverage. By highlighting 8 cases of discrimination\nand media misrepresentation that were frequently mentioned by participants in\nthe FGD, this study emphasizes the urgent need for more equitable media\npractices that accurately reflect the experiences and struggles of marginalized\ncommunities."
                },
                "authors": [
                    {
                        "name": "Abhijit Paul"
                    },
                    {
                        "name": "Adity Khisa"
                    },
                    {
                        "name": "Zarif Masud"
                    },
                    {
                        "name": "Sharif Md. Abdullah"
                    },
                    {
                        "name": "Ahmedul Kabir"
                    },
                    {
                        "name": "Shebuti Rayana"
                    }
                ],
                "author_detail": {
                    "name": "Shebuti Rayana"
                },
                "author": "Shebuti Rayana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12977v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12977v4",
                "updated": "2025-06-11T14:09:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    9,
                    4,
                    2,
                    162,
                    0
                ],
                "published": "2024-11-20T02:10:44Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    2,
                    10,
                    44,
                    2,
                    325,
                    0
                ],
                "title": "MindForge: Empowering Embodied Agents with Theory of Mind for Lifelong\n  Cultural Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MindForge: Empowering Embodied Agents with Theory of Mind for Lifelong\n  Cultural Learning"
                },
                "summary": "Embodied agents powered by large language models (LLMs), such as Voyager,\npromise open-ended competence in worlds such as Minecraft. However, when\npowered by open-weight LLMs they still falter on elementary tasks after\ndomain-specific fine-tuning. We propose MindForge, a generative-agent framework\nfor cultural lifelong learning through explicit perspective taking. We\nintroduce three key innovations: (1) a structured theory of mind representation\nlinking percepts, beliefs, desires, and actions; (2) natural inter-agent\ncommunication; and (3) a multi-component memory system. Following the cultural\nlearning framework, we test MindForge in both instructive and collaborative\nsettings within Minecraft. In an instructive setting with GPT-4, MindForge\nagents powered by open-weight LLMs significantly outperform their Voyager\ncounterparts in basic tasks yielding $3\\times$ more tech-tree milestones and\ncollecting $2.3\\times$ more unique items than the Voyager baseline.\nFurthermore, in fully \\textit{collaborative} settings, we find that the\nperformance of two underachieving agents improves with more communication\nrounds, echoing the Condorcet Jury Theorem. MindForge agents demonstrate\nsophisticated behaviors, including expert-novice knowledge transfer,\ncollaborative problem solving, and adaptation to out-of-distribution tasks\nthrough accumulated cultural experiences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied agents powered by large language models (LLMs), such as Voyager,\npromise open-ended competence in worlds such as Minecraft. However, when\npowered by open-weight LLMs they still falter on elementary tasks after\ndomain-specific fine-tuning. We propose MindForge, a generative-agent framework\nfor cultural lifelong learning through explicit perspective taking. We\nintroduce three key innovations: (1) a structured theory of mind representation\nlinking percepts, beliefs, desires, and actions; (2) natural inter-agent\ncommunication; and (3) a multi-component memory system. Following the cultural\nlearning framework, we test MindForge in both instructive and collaborative\nsettings within Minecraft. In an instructive setting with GPT-4, MindForge\nagents powered by open-weight LLMs significantly outperform their Voyager\ncounterparts in basic tasks yielding $3\\times$ more tech-tree milestones and\ncollecting $2.3\\times$ more unique items than the Voyager baseline.\nFurthermore, in fully \\textit{collaborative} settings, we find that the\nperformance of two underachieving agents improves with more communication\nrounds, echoing the Condorcet Jury Theorem. MindForge agents demonstrate\nsophisticated behaviors, including expert-novice knowledge transfer,\ncollaborative problem solving, and adaptation to out-of-distribution tasks\nthrough accumulated cultural experiences."
                },
                "authors": [
                    {
                        "name": "Mircea Lic"
                    },
                    {
                        "name": "Ojas Shirekar"
                    },
                    {
                        "name": "Baptiste Colle"
                    },
                    {
                        "name": "Chirag Raman"
                    }
                ],
                "author_detail": {
                    "name": "Chirag Raman"
                },
                "author": "Chirag Raman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12977v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12977v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09758v1",
                "updated": "2025-06-11T14:03:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    3,
                    13,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T14:03:13Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    3,
                    13,
                    2,
                    162,
                    0
                ],
                "title": "Mainframe-style channel controllers for modern disaggregated memory\n  systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mainframe-style channel controllers for modern disaggregated memory\n  systems"
                },
                "summary": "Despite the promise of alleviating the main memory bottleneck, and the\nexistence of commercial hardware implementations, techniques for Near-Data\nProcessing have seen relatively little real-world deployment. The idea has\nreceived renewed interest with the appearance of disaggregated or \"far\" memory,\nfor example in the use of CXL memory pools.\n  However, we argue that the lack of a clear OS-centric abstraction of\nNear-Data Processing is a major barrier to adoption of the technology. Inspired\nby the channel controllers which interface the CPU to disk drives in mainframe\nsystems, we propose memory channel controllers as a convenient, portable, and\nvirtualizable abstraction of Near-Data Processing for modern disaggregated\nmemory systems.\n  In addition to providing a clean abstraction that enables OS integration\nwhile requiring no changes to CPU architecture, memory channel controllers\nincorporate another key innovation: they exploit the cache coherence provided\nby emerging interconnects to provide a much richer programming model, with more\nfine-grained interaction, than has been possible with existing designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the promise of alleviating the main memory bottleneck, and the\nexistence of commercial hardware implementations, techniques for Near-Data\nProcessing have seen relatively little real-world deployment. The idea has\nreceived renewed interest with the appearance of disaggregated or \"far\" memory,\nfor example in the use of CXL memory pools.\n  However, we argue that the lack of a clear OS-centric abstraction of\nNear-Data Processing is a major barrier to adoption of the technology. Inspired\nby the channel controllers which interface the CPU to disk drives in mainframe\nsystems, we propose memory channel controllers as a convenient, portable, and\nvirtualizable abstraction of Near-Data Processing for modern disaggregated\nmemory systems.\n  In addition to providing a clean abstraction that enables OS integration\nwhile requiring no changes to CPU architecture, memory channel controllers\nincorporate another key innovation: they exploit the cache coherence provided\nby emerging interconnects to provide a much richer programming model, with more\nfine-grained interaction, than has been possible with existing designs."
                },
                "authors": [
                    {
                        "name": "Zikai Liu"
                    },
                    {
                        "name": "Jasmin Schult"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09755v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09755v1",
                "updated": "2025-06-11T13:57:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    57,
                    26,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T13:57:26Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    57,
                    26,
                    2,
                    162,
                    0
                ],
                "title": "Intelligent Design 4.0: Paradigm Evolution Toward the Agentic AI Era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent Design 4.0: Paradigm Evolution Toward the Agentic AI Era"
                },
                "summary": "Research and practice in Intelligent Design (ID) have significantly enhanced\nengineering innovation, efficiency, quality, and productivity over recent\ndecades, fundamentally reshaping how engineering designers think, behave, and\ninteract with design processes. The recent emergence of Foundation Models\n(FMs), particularly Large Language Models (LLMs), has demonstrated general\nknowledge-based reasoning capabilities, and open new paths and avenues for\nfurther transformation in engineering design. In this context, this paper\nintroduces Intelligent Design 4.0 (ID 4.0) as an emerging paradigm empowered by\nagentic AI systems. We review the historical evolution of ID across four\ndistinct stages: rule-based expert systems, task-specific machine learning\nmodels, large-scale foundation AI models, and the recent emerging paradigm of\nmulti-agent collaboration. We propose a conceptual framework for ID 4.0 and\ndiscuss its potential to support end-to-end automation of engineering design\nprocesses through coordinated, autonomous multi-agent-based systems.\nFurthermore, we discuss future perspectives to enhance and fully realize ID\n4.0's potential, including more complex design scenarios, more practical design\nimplementations, novel agent coordination mechanisms, and autonomous design\ngoal-setting with better human value alignment. In sum, these insights lay a\nfoundation for advancing Intelligent Design toward greater adaptivity,\nautonomy, and effectiveness in addressing increasingly complex design\nchallenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research and practice in Intelligent Design (ID) have significantly enhanced\nengineering innovation, efficiency, quality, and productivity over recent\ndecades, fundamentally reshaping how engineering designers think, behave, and\ninteract with design processes. The recent emergence of Foundation Models\n(FMs), particularly Large Language Models (LLMs), has demonstrated general\nknowledge-based reasoning capabilities, and open new paths and avenues for\nfurther transformation in engineering design. In this context, this paper\nintroduces Intelligent Design 4.0 (ID 4.0) as an emerging paradigm empowered by\nagentic AI systems. We review the historical evolution of ID across four\ndistinct stages: rule-based expert systems, task-specific machine learning\nmodels, large-scale foundation AI models, and the recent emerging paradigm of\nmulti-agent collaboration. We propose a conceptual framework for ID 4.0 and\ndiscuss its potential to support end-to-end automation of engineering design\nprocesses through coordinated, autonomous multi-agent-based systems.\nFurthermore, we discuss future perspectives to enhance and fully realize ID\n4.0's potential, including more complex design scenarios, more practical design\nimplementations, novel agent coordination mechanisms, and autonomous design\ngoal-setting with better human value alignment. In sum, these insights lay a\nfoundation for advancing Intelligent Design toward greater adaptivity,\nautonomy, and effectiveness in addressing increasingly complex design\nchallenges."
                },
                "authors": [
                    {
                        "name": "Shuo Jiang"
                    },
                    {
                        "name": "Min Xie"
                    },
                    {
                        "name": "Frank Youhua Chen"
                    },
                    {
                        "name": "Jian Ma"
                    },
                    {
                        "name": "Jianxi Luo"
                    }
                ],
                "author_detail": {
                    "name": "Jianxi Luo"
                },
                "author": "Jianxi Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09755v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09755v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09749v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09749v1",
                "updated": "2025-06-11T13:53:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    53,
                    35,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T13:53:35Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    53,
                    35,
                    2,
                    162,
                    0
                ],
                "title": "Large Language Models for Design Structure Matrix Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Design Structure Matrix Optimization"
                },
                "summary": "In complex engineering systems, the interdependencies among components or\ndevelopment activities are often modeled and analyzed using Design Structure\nMatrix (DSM). Reorganizing elements within a DSM to minimize feedback loops and\nenhance modularity or process efficiency constitutes a challenging\ncombinatorial optimization (CO) problem in engineering design and operations.\nAs problem sizes increase and dependency networks become more intricate,\ntraditional optimization methods that solely use mathematical heuristics often\nfail to capture the contextual nuances and struggle to deliver effective\nsolutions. In this study, we explore the potential of Large Language Models\n(LLMs) for helping solve such CO problems by leveraging their capabilities for\nadvanced reasoning and contextual understanding. We propose a novel LLM-based\nframework that integrates network topology with contextual domain knowledge for\niterative optimization of DSM element sequencing - a common CO problem.\nExperiments on various DSM cases show that our method consistently achieves\nfaster convergence and superior solution quality compared to both stochastic\nand deterministic baselines. Notably, we find that incorporating contextual\ndomain knowledge significantly enhances optimization performance regardless of\nthe chosen LLM backbone. These findings highlight the potential of LLMs to\nsolve complex engineering CO problems by combining semantic and mathematical\nreasoning. This approach paves the way towards a new paradigm in LLM-based\nengineering design optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In complex engineering systems, the interdependencies among components or\ndevelopment activities are often modeled and analyzed using Design Structure\nMatrix (DSM). Reorganizing elements within a DSM to minimize feedback loops and\nenhance modularity or process efficiency constitutes a challenging\ncombinatorial optimization (CO) problem in engineering design and operations.\nAs problem sizes increase and dependency networks become more intricate,\ntraditional optimization methods that solely use mathematical heuristics often\nfail to capture the contextual nuances and struggle to deliver effective\nsolutions. In this study, we explore the potential of Large Language Models\n(LLMs) for helping solve such CO problems by leveraging their capabilities for\nadvanced reasoning and contextual understanding. We propose a novel LLM-based\nframework that integrates network topology with contextual domain knowledge for\niterative optimization of DSM element sequencing - a common CO problem.\nExperiments on various DSM cases show that our method consistently achieves\nfaster convergence and superior solution quality compared to both stochastic\nand deterministic baselines. Notably, we find that incorporating contextual\ndomain knowledge significantly enhances optimization performance regardless of\nthe chosen LLM backbone. These findings highlight the potential of LLMs to\nsolve complex engineering CO problems by combining semantic and mathematical\nreasoning. This approach paves the way towards a new paradigm in LLM-based\nengineering design optimization."
                },
                "authors": [
                    {
                        "name": "Shuo Jiang"
                    },
                    {
                        "name": "Min Xie"
                    },
                    {
                        "name": "Jianxi Luo"
                    }
                ],
                "author_detail": {
                    "name": "Jianxi Luo"
                },
                "author": "Jianxi Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09749v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09749v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09742v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09742v1",
                "updated": "2025-06-11T13:48:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    48,
                    25,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T13:48:25Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    48,
                    25,
                    2,
                    162,
                    0
                ],
                "title": "Feature Engineering for Agents: An Adaptive Cognitive Architecture for\n  Interpretable ML Monitoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature Engineering for Agents: An Adaptive Cognitive Architecture for\n  Interpretable ML Monitoring"
                },
                "summary": "Monitoring Machine Learning (ML) models in production environments is\ncrucial, yet traditional approaches often yield verbose, low-interpretability\noutputs that hinder effective decision-making. We propose a cognitive\narchitecture for ML monitoring that applies feature engineering principles to\nagents based on Large Language Models (LLMs), significantly enhancing the\ninterpretability of monitoring outputs. Central to our approach is a Decision\nProcedure module that simulates feature engineering through three key steps:\nRefactor, Break Down, and Compile. The Refactor step improves data\nrepresentation to better capture feature semantics, allowing the LLM to focus\non salient aspects of the monitoring data while reducing noise and irrelevant\ninformation. Break Down decomposes complex information for detailed analysis,\nand Compile integrates sub-insights into clear, interpretable outputs. This\nprocess leads to a more deterministic planning approach, reducing dependence on\nLLM-generated planning, which can sometimes be inconsistent and overly general.\nThe combination of feature engineering-driven planning and selective LLM\nutilization results in a robust decision support system, capable of providing\nhighly interpretable and actionable insights. Experiments using multiple LLMs\ndemonstrate the efficacy of our approach, achieving significantly higher\naccuracy compared to various baselines across several domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monitoring Machine Learning (ML) models in production environments is\ncrucial, yet traditional approaches often yield verbose, low-interpretability\noutputs that hinder effective decision-making. We propose a cognitive\narchitecture for ML monitoring that applies feature engineering principles to\nagents based on Large Language Models (LLMs), significantly enhancing the\ninterpretability of monitoring outputs. Central to our approach is a Decision\nProcedure module that simulates feature engineering through three key steps:\nRefactor, Break Down, and Compile. The Refactor step improves data\nrepresentation to better capture feature semantics, allowing the LLM to focus\non salient aspects of the monitoring data while reducing noise and irrelevant\ninformation. Break Down decomposes complex information for detailed analysis,\nand Compile integrates sub-insights into clear, interpretable outputs. This\nprocess leads to a more deterministic planning approach, reducing dependence on\nLLM-generated planning, which can sometimes be inconsistent and overly general.\nThe combination of feature engineering-driven planning and selective LLM\nutilization results in a robust decision support system, capable of providing\nhighly interpretable and actionable insights. Experiments using multiple LLMs\ndemonstrate the efficacy of our approach, achieving significantly higher\naccuracy compared to various baselines across several domains."
                },
                "authors": [
                    {
                        "name": "Gusseppe Bravo-Rocca"
                    },
                    {
                        "name": "Peini Liu"
                    },
                    {
                        "name": "Jordi Guitart"
                    },
                    {
                        "name": "Rodrigo M Carrillo-Larco"
                    },
                    {
                        "name": "Ajay Dholakia"
                    },
                    {
                        "name": "David Ellison"
                    }
                ],
                "author_detail": {
                    "name": "David Ellison"
                },
                "author": "David Ellison",
                "arxiv_comment": "Accepted at AAMAS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09742v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09742v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07943v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07943v2",
                "updated": "2025-06-11T13:48:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    48,
                    23,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-09T17:05:02Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    17,
                    5,
                    2,
                    0,
                    160,
                    0
                ],
                "title": "Decoupling the Image Perception and Multimodal Reasoning for Reasoning\n  Segmentation with Digital Twin Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoupling the Image Perception and Multimodal Reasoning for Reasoning\n  Segmentation with Digital Twin Representations"
                },
                "summary": "Reasoning Segmentation (RS) is a multimodal vision-text task that requires\nsegmenting objects based on implicit text queries, demanding both precise\nvisual perception and vision-text reasoning capabilities. Current RS approaches\nrely on fine-tuning vision-language models (VLMs) for both perception and\nreasoning, but their tokenization of images fundamentally disrupts continuous\nspatial relationships between objects. We introduce DTwinSeger, a novel RS\napproach that leverages Digital Twin (DT) representation as an intermediate\nlayer to decouple perception from reasoning. Innovatively, DTwinSeger\nreformulates RS as a two-stage process, where the first transforms the image\ninto a structured DT representation that preserves spatial relationships and\nsemantic properties and then employs a Large Language Model (LLM) to perform\nexplicit reasoning over this representation to identify target objects. We\npropose a supervised fine-tuning method specifically for LLM with DT\nrepresentation, together with a corresponding fine-tuning dataset Seg-DT, to\nenhance the LLM's reasoning capabilities with DT representations. Experiments\nshow that our method can achieve state-of-the-art performance on two image RS\nbenchmarks and three image referring segmentation benchmarks. It yields that DT\nrepresentation functions as an effective bridge between vision and text,\nenabling complex multimodal reasoning tasks to be accomplished solely with an\nLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Segmentation (RS) is a multimodal vision-text task that requires\nsegmenting objects based on implicit text queries, demanding both precise\nvisual perception and vision-text reasoning capabilities. Current RS approaches\nrely on fine-tuning vision-language models (VLMs) for both perception and\nreasoning, but their tokenization of images fundamentally disrupts continuous\nspatial relationships between objects. We introduce DTwinSeger, a novel RS\napproach that leverages Digital Twin (DT) representation as an intermediate\nlayer to decouple perception from reasoning. Innovatively, DTwinSeger\nreformulates RS as a two-stage process, where the first transforms the image\ninto a structured DT representation that preserves spatial relationships and\nsemantic properties and then employs a Large Language Model (LLM) to perform\nexplicit reasoning over this representation to identify target objects. We\npropose a supervised fine-tuning method specifically for LLM with DT\nrepresentation, together with a corresponding fine-tuning dataset Seg-DT, to\nenhance the LLM's reasoning capabilities with DT representations. Experiments\nshow that our method can achieve state-of-the-art performance on two image RS\nbenchmarks and three image referring segmentation benchmarks. It yields that DT\nrepresentation functions as an effective bridge between vision and text,\nenabling complex multimodal reasoning tasks to be accomplished solely with an\nLLM."
                },
                "authors": [
                    {
                        "name": "Yizhen Li"
                    },
                    {
                        "name": "Dell Zhang"
                    },
                    {
                        "name": "Xuelong Li"
                    },
                    {
                        "name": "Yiqing Shen"
                    }
                ],
                "author_detail": {
                    "name": "Yiqing Shen"
                },
                "author": "Yiqing Shen",
                "arxiv_comment": "This work was submitted without the consent of all co-authors. We\n  request withdrawal until all parties agree",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07943v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07943v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09738v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09738v1",
                "updated": "2025-06-11T13:41:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    41,
                    29,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T13:41:29Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    41,
                    29,
                    2,
                    162,
                    0
                ],
                "title": "Towards Multi-modal Graph Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Multi-modal Graph Large Language Model"
                },
                "summary": "Multi-modal graphs, which integrate diverse multi-modal features and\nrelations, are ubiquitous in real-world applications. However, existing\nmulti-modal graph learning methods are typically trained from scratch for\nspecific graph data and tasks, failing to generalize across various multi-modal\ngraph data and tasks. To bridge this gap, we explore the potential of\nMulti-modal Graph Large Language Models (MG-LLM) to unify and generalize across\ndiverse multi-modal graph data and tasks. We propose a unified framework of\nmulti-modal graph data, task, and model, discovering the inherent\nmulti-granularity and multi-scale characteristics in multi-modal graphs.\nSpecifically, we present five key desired characteristics for MG-LLM: 1)\nunified space for multi-modal structures and attributes, 2) capability of\nhandling diverse multi-modal graph tasks, 3) multi-modal graph in-context\nlearning, 4) multi-modal graph interaction with natural language, and 5)\nmulti-modal graph reasoning. We then elaborate on the key challenges, review\nrelated works, and highlight promising future research directions towards\nrealizing these ambitious characteristics. Finally, we summarize existing\nmulti-modal graph datasets pertinent for model training. We believe this paper\ncan contribute to the ongoing advancement of the research towards MG-LLM for\ngeneralization across multi-modal graph data and tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal graphs, which integrate diverse multi-modal features and\nrelations, are ubiquitous in real-world applications. However, existing\nmulti-modal graph learning methods are typically trained from scratch for\nspecific graph data and tasks, failing to generalize across various multi-modal\ngraph data and tasks. To bridge this gap, we explore the potential of\nMulti-modal Graph Large Language Models (MG-LLM) to unify and generalize across\ndiverse multi-modal graph data and tasks. We propose a unified framework of\nmulti-modal graph data, task, and model, discovering the inherent\nmulti-granularity and multi-scale characteristics in multi-modal graphs.\nSpecifically, we present five key desired characteristics for MG-LLM: 1)\nunified space for multi-modal structures and attributes, 2) capability of\nhandling diverse multi-modal graph tasks, 3) multi-modal graph in-context\nlearning, 4) multi-modal graph interaction with natural language, and 5)\nmulti-modal graph reasoning. We then elaborate on the key challenges, review\nrelated works, and highlight promising future research directions towards\nrealizing these ambitious characteristics. Finally, we summarize existing\nmulti-modal graph datasets pertinent for model training. We believe this paper\ncan contribute to the ongoing advancement of the research towards MG-LLM for\ngeneralization across multi-modal graph data and tasks."
                },
                "authors": [
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Zeyang Zhang"
                    },
                    {
                        "name": "Linxin Xiao"
                    },
                    {
                        "name": "Haibo Chen"
                    },
                    {
                        "name": "Chendi Ge"
                    },
                    {
                        "name": "Wenwu Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Wenwu Zhu"
                },
                "author": "Wenwu Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09738v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09738v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09713v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09713v1",
                "updated": "2025-06-11T13:25:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    25,
                    36,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T13:25:36Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    25,
                    36,
                    2,
                    162,
                    0
                ],
                "title": "A First Look at Bugs in LLM Inference Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A First Look at Bugs in LLM Inference Engines"
                },
                "summary": "Large language model-specific inference engines (in short as \\emph{LLM\ninference engines}) have become a fundamental component of modern AI\ninfrastructure, enabling the deployment of LLM-powered applications (LLM apps)\nacross cloud and local devices. Despite their critical role, LLM inference\nengines are prone to bugs due to the immense resource demands of LLMs and the\ncomplexities of cross-platform compatibility. However, a systematic\nunderstanding of these bugs remains lacking. To bridge this gap, we present the\nfirst empirical study on bugs in LLM inference engines. We mine official\nrepositories of 5 widely adopted LLM inference engines, constructing a\ncomprehensive dataset of 929 real-world bugs. Through a rigorous open coding\nprocess, we analyze these bugs to uncover their symptoms, root causes, and\ncommonality. Our findings reveal six major bug symptoms and a taxonomy of 28\nroot causes, shedding light on the key challenges in bug detection and location\nwithin LLM inference engines. Based on these insights, we propose a series of\nactionable implications for researchers, inference engine vendors, and LLM app\ndevelopers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model-specific inference engines (in short as \\emph{LLM\ninference engines}) have become a fundamental component of modern AI\ninfrastructure, enabling the deployment of LLM-powered applications (LLM apps)\nacross cloud and local devices. Despite their critical role, LLM inference\nengines are prone to bugs due to the immense resource demands of LLMs and the\ncomplexities of cross-platform compatibility. However, a systematic\nunderstanding of these bugs remains lacking. To bridge this gap, we present the\nfirst empirical study on bugs in LLM inference engines. We mine official\nrepositories of 5 widely adopted LLM inference engines, constructing a\ncomprehensive dataset of 929 real-world bugs. Through a rigorous open coding\nprocess, we analyze these bugs to uncover their symptoms, root causes, and\ncommonality. Our findings reveal six major bug symptoms and a taxonomy of 28\nroot causes, shedding light on the key challenges in bug detection and location\nwithin LLM inference engines. Based on these insights, we propose a series of\nactionable implications for researchers, inference engine vendors, and LLM app\ndevelopers."
                },
                "authors": [
                    {
                        "name": "Mugeng Liu"
                    },
                    {
                        "name": "Siqi Zhong"
                    },
                    {
                        "name": "Weichen Bi"
                    },
                    {
                        "name": "Yixuan Zhang"
                    },
                    {
                        "name": "Zhiyang Chen"
                    },
                    {
                        "name": "Zhenpeng Chen"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Yun Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yun Ma"
                },
                "author": "Yun Ma",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09713v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09713v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09707v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09707v1",
                "updated": "2025-06-11T13:21:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    21,
                    6,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T13:21:06Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    21,
                    6,
                    2,
                    162,
                    0
                ],
                "title": "Fine-Tuning Large Audio-Language Models with LoRA for Precise Temporal\n  Localization of Prolonged Exposure Therapy Elements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Tuning Large Audio-Language Models with LoRA for Precise Temporal\n  Localization of Prolonged Exposure Therapy Elements"
                },
                "summary": "Prolonged Exposure (PE) therapy is an effective treatment for post-traumatic\nstress disorder (PTSD), but evaluating therapist fidelity remains\nlabor-intensive due to the need for manual review of session recordings. We\npresent a method for the automatic temporal localization of key PE fidelity\nelements -- identifying their start and stop times -- directly from session\naudio and transcripts. Our approach fine-tunes a large pre-trained\naudio-language model, Qwen2-Audio, using Low-Rank Adaptation (LoRA) to process\nfocused 30-second windows of audio-transcript input. Fidelity labels for three\ncore protocol phases -- therapist orientation (P1), imaginal exposure (P2), and\npost-imaginal processing (P3) -- are generated via LLM-based prompting and\nverified by trained raters. The model is trained to predict normalized boundary\noffsets using soft supervision guided by task-specific prompts. On a dataset of\n313 real PE sessions, our best configuration (LoRA rank 8, 30s windows)\nachieves a mean absolute error (MAE) of 5.3 seconds across tasks. We further\nanalyze the effects of window size and LoRA rank, highlighting the importance\nof context granularity and model adaptation. This work introduces a scalable\nframework for fidelity tracking in PE therapy, with potential to support\nclinician training, supervision, and quality assurance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prolonged Exposure (PE) therapy is an effective treatment for post-traumatic\nstress disorder (PTSD), but evaluating therapist fidelity remains\nlabor-intensive due to the need for manual review of session recordings. We\npresent a method for the automatic temporal localization of key PE fidelity\nelements -- identifying their start and stop times -- directly from session\naudio and transcripts. Our approach fine-tunes a large pre-trained\naudio-language model, Qwen2-Audio, using Low-Rank Adaptation (LoRA) to process\nfocused 30-second windows of audio-transcript input. Fidelity labels for three\ncore protocol phases -- therapist orientation (P1), imaginal exposure (P2), and\npost-imaginal processing (P3) -- are generated via LLM-based prompting and\nverified by trained raters. The model is trained to predict normalized boundary\noffsets using soft supervision guided by task-specific prompts. On a dataset of\n313 real PE sessions, our best configuration (LoRA rank 8, 30s windows)\nachieves a mean absolute error (MAE) of 5.3 seconds across tasks. We further\nanalyze the effects of window size and LoRA rank, highlighting the importance\nof context granularity and model adaptation. This work introduces a scalable\nframework for fidelity tracking in PE therapy, with potential to support\nclinician training, supervision, and quality assurance."
                },
                "authors": [
                    {
                        "name": "Suhas BN"
                    },
                    {
                        "name": "Andrew M. Sherrill"
                    },
                    {
                        "name": "Jyoti Alaparthi"
                    },
                    {
                        "name": "Dominik Mattioli"
                    },
                    {
                        "name": "Rosa I. Arriaga"
                    },
                    {
                        "name": "Chris W. Wiese"
                    },
                    {
                        "name": "Saeed Abdullah"
                    }
                ],
                "author_detail": {
                    "name": "Saeed Abdullah"
                },
                "author": "Saeed Abdullah",
                "arxiv_comment": "5 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09707v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09707v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.5.4; H.5.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14917v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14917v2",
                "updated": "2025-06-11T13:19:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    19,
                    51,
                    2,
                    162,
                    0
                ],
                "published": "2024-06-21T07:20:51Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    7,
                    20,
                    51,
                    4,
                    173,
                    0
                ],
                "title": "LLM2TEA: Agentic AI Designer Finds Innovative Objects with Generative\n  Evolutionary Multitasking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM2TEA: Agentic AI Designer Finds Innovative Objects with Generative\n  Evolutionary Multitasking"
                },
                "summary": "In this paper, we introduce LLM-driven MultiTask Evolutionary Algorithm\n(LLM2TEA), the first agentic AI designer within a generative evolutionary\nmultitasking (GEM) framework that promotes the crossover and synergy of designs\nfrom multiple domains, leading to innovative solutions that transcend\nindividual disciplines. Of particular interest is the discovery of objects that\nare not only innovative but also conform to the physical specifications of the\nreal world in science and engineering. LLM2TEA comprises a large language model\nto initialize a population of genotypes (defined by text prompts) describing\nthe objects of interest, a text-to-3D generative model to produce phenotypes\nfrom these prompts, a classifier to interpret the semantic representations of\nthe objects, and a physics simulation model to assess their physical\nproperties. We propose several novel LLM-based multitask evolutionary operators\nto guide the search toward the discovery of high-performing practical objects.\nExperimental results in conceptual design optimization validate the\neffectiveness of LLM2TEA, revealing from 97\\% to 174\\% improvement in the\ndiversity of innovative objects compared to the present text-to-3D generative\nmodel baseline. In addition, more than 73\\% of the generated designs have\nbetter physical performance than the top 1\\% percentile of the designs\ngenerated in the baseline. Moreover, LLM2TEA generates designs that are not\nonly aesthetically creative but also functional in real-world applications.\nSeveral of these designs have been successfully 3D-printed, emphasizing the\nproposed approach's capacity to transform AI-generated outputs into tangible\nphysical objects. The designs produced by LLM2TEA meets practical requirements\nwhile showcasing creative and innovative features, underscoring its potential\napplications in complex design optimization and discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce LLM-driven MultiTask Evolutionary Algorithm\n(LLM2TEA), the first agentic AI designer within a generative evolutionary\nmultitasking (GEM) framework that promotes the crossover and synergy of designs\nfrom multiple domains, leading to innovative solutions that transcend\nindividual disciplines. Of particular interest is the discovery of objects that\nare not only innovative but also conform to the physical specifications of the\nreal world in science and engineering. LLM2TEA comprises a large language model\nto initialize a population of genotypes (defined by text prompts) describing\nthe objects of interest, a text-to-3D generative model to produce phenotypes\nfrom these prompts, a classifier to interpret the semantic representations of\nthe objects, and a physics simulation model to assess their physical\nproperties. We propose several novel LLM-based multitask evolutionary operators\nto guide the search toward the discovery of high-performing practical objects.\nExperimental results in conceptual design optimization validate the\neffectiveness of LLM2TEA, revealing from 97\\% to 174\\% improvement in the\ndiversity of innovative objects compared to the present text-to-3D generative\nmodel baseline. In addition, more than 73\\% of the generated designs have\nbetter physical performance than the top 1\\% percentile of the designs\ngenerated in the baseline. Moreover, LLM2TEA generates designs that are not\nonly aesthetically creative but also functional in real-world applications.\nSeveral of these designs have been successfully 3D-printed, emphasizing the\nproposed approach's capacity to transform AI-generated outputs into tangible\nphysical objects. The designs produced by LLM2TEA meets practical requirements\nwhile showcasing creative and innovative features, underscoring its potential\napplications in complex design optimization and discovery."
                },
                "authors": [
                    {
                        "name": "Melvin Wong"
                    },
                    {
                        "name": "Jiao Liu"
                    },
                    {
                        "name": "Thiago Rios"
                    },
                    {
                        "name": "Stefan Menzel"
                    },
                    {
                        "name": "Yew Soon Ong"
                    }
                ],
                "author_detail": {
                    "name": "Yew Soon Ong"
                },
                "author": "Yew Soon Ong",
                "arxiv_comment": "This work has been submitted to the IEEE for review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14917v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14917v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11223v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11223v4",
                "updated": "2025-06-11T13:19:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    19,
                    22,
                    2,
                    162,
                    0
                ],
                "published": "2025-01-20T02:16:19Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    2,
                    16,
                    19,
                    0,
                    20,
                    0
                ],
                "title": "Reasoning Language Models: A Blueprint",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Language Models: A Blueprint"
                },
                "summary": "Reasoning language models (RLMs), also known as Large Reasoning Models\n(LRMs), such as OpenAI's o1 and o3, DeepSeek-R1, and Alibaba's QwQ, have\nredefined AI's problem-solving capabilities by extending LLMs with advanced\nreasoning mechanisms. Yet, their high costs, proprietary nature, and complex\narchitectures - uniquely combining reinforcement learning (RL), search\nheuristics, and LLMs - present accessibility and scalability challenges. To\naddress these, we propose a comprehensive blueprint that organizes RLM\ncomponents into a modular framework, based on a survey and analysis of all RLM\nworks. This blueprint incorporates diverse reasoning structures (chains, trees,\ngraphs, and nested forms), reasoning strategies (e.g., Monte Carlo Tree Search,\nBeam Search), RL concepts (policy, value models and others), supervision\nschemes (Outcome-Based and Process-Based Supervision), and other related\nconcepts (e.g., Test-Time Compute, Retrieval-Augmented Generation, agent\ntools). We also provide detailed mathematical formulations and algorithmic\nspecifications to simplify RLM implementation. By showing how schemes like\nLLaMA-Berry, QwQ, Journey Learning, and Graph of Thoughts fit as special cases,\nwe demonstrate the blueprint's versatility and unifying potential. To\nillustrate its utility, we introduce x1, a modular implementation for rapid RLM\nprototyping and experimentation. Using x1 and a literature review, we provide\nkey insights, such as multi-phase training for policy and value models, and the\nimportance of familiar training distributions. Finally, we discuss scalable RLM\ncloud deployments and we outline how RLMs can integrate with a broader LLM\necosystem. Our work demystifies RLM construction, democratizes advanced\nreasoning capabilities, and fosters innovation, aiming to mitigate the gap\nbetween \"rich AI\" and \"poor AI\" by lowering barriers to RLM design and\nexperimentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning language models (RLMs), also known as Large Reasoning Models\n(LRMs), such as OpenAI's o1 and o3, DeepSeek-R1, and Alibaba's QwQ, have\nredefined AI's problem-solving capabilities by extending LLMs with advanced\nreasoning mechanisms. Yet, their high costs, proprietary nature, and complex\narchitectures - uniquely combining reinforcement learning (RL), search\nheuristics, and LLMs - present accessibility and scalability challenges. To\naddress these, we propose a comprehensive blueprint that organizes RLM\ncomponents into a modular framework, based on a survey and analysis of all RLM\nworks. This blueprint incorporates diverse reasoning structures (chains, trees,\ngraphs, and nested forms), reasoning strategies (e.g., Monte Carlo Tree Search,\nBeam Search), RL concepts (policy, value models and others), supervision\nschemes (Outcome-Based and Process-Based Supervision), and other related\nconcepts (e.g., Test-Time Compute, Retrieval-Augmented Generation, agent\ntools). We also provide detailed mathematical formulations and algorithmic\nspecifications to simplify RLM implementation. By showing how schemes like\nLLaMA-Berry, QwQ, Journey Learning, and Graph of Thoughts fit as special cases,\nwe demonstrate the blueprint's versatility and unifying potential. To\nillustrate its utility, we introduce x1, a modular implementation for rapid RLM\nprototyping and experimentation. Using x1 and a literature review, we provide\nkey insights, such as multi-phase training for policy and value models, and the\nimportance of familiar training distributions. Finally, we discuss scalable RLM\ncloud deployments and we outline how RLMs can integrate with a broader LLM\necosystem. Our work demystifies RLM construction, democratizes advanced\nreasoning capabilities, and fosters innovation, aiming to mitigate the gap\nbetween \"rich AI\" and \"poor AI\" by lowering barriers to RLM design and\nexperimentation."
                },
                "authors": [
                    {
                        "name": "Maciej Besta"
                    },
                    {
                        "name": "Julia Barth"
                    },
                    {
                        "name": "Eric Schreiber"
                    },
                    {
                        "name": "Ales Kubicek"
                    },
                    {
                        "name": "Afonso Catarino"
                    },
                    {
                        "name": "Robert Gerstenberger"
                    },
                    {
                        "name": "Piotr Nyczyk"
                    },
                    {
                        "name": "Patrick Iff"
                    },
                    {
                        "name": "Yueling Li"
                    },
                    {
                        "name": "Sam Houliston"
                    },
                    {
                        "name": "Tomasz Sternal"
                    },
                    {
                        "name": "Marcin Copik"
                    },
                    {
                        "name": "Grzegorz Kwaniewski"
                    },
                    {
                        "name": "Jrgen Mller"
                    },
                    {
                        "name": "ukasz Flis"
                    },
                    {
                        "name": "Hannes Eberhard"
                    },
                    {
                        "name": "Zixuan Chen"
                    },
                    {
                        "name": "Hubert Niewiadomski"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11223v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11223v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09701v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09701v1",
                "updated": "2025-06-11T13:14:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    14,
                    1,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T13:14:01Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    14,
                    1,
                    2,
                    162,
                    0
                ],
                "title": "TRIDENT: Temporally Restricted Inference via DFA-Enhanced Neural\n  Traversal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRIDENT: Temporally Restricted Inference via DFA-Enhanced Neural\n  Traversal"
                },
                "summary": "Large Language Models (LLMs) and other neural architectures have achieved\nimpressive results across a variety of generative and classification tasks.\nHowever, they remain fundamentally ill-equipped to ensure that their outputs\nsatisfy temporal constraints, such as those expressible in Linear Temporal\nLogic over finite traces (LTLf). In this paper, we introduce TRIDENT: a general\nand model-agnostic inference-time algorithm that guarantees compliance with\nsuch constraints without requiring any retraining. TRIDENT compiles LTLf\nformulas into a Deterministic Finite Automaton (DFA), which is used to guide a\nconstrained variant of beam search. At each decoding step, transitions that\nwould lead to constraint violations are masked, while remaining paths are\ndynamically re-ranked based on both the model's probabilities and the DFA's\nacceptance structure. We formally prove that the resulting sequences are\nguaranteed to satisfy the given LTLf constraints, and we empirically\ndemonstrate that TRIDENT also improves output quality. We validate our approach\non two distinct tasks: temporally constrained image-stream classification and\ncontrolled text generation. In both settings, TRIDENT achieves perfect\nconstraint satisfaction, while comparison with the state of the art shows\nimproved efficiency and high standard quality metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) and other neural architectures have achieved\nimpressive results across a variety of generative and classification tasks.\nHowever, they remain fundamentally ill-equipped to ensure that their outputs\nsatisfy temporal constraints, such as those expressible in Linear Temporal\nLogic over finite traces (LTLf). In this paper, we introduce TRIDENT: a general\nand model-agnostic inference-time algorithm that guarantees compliance with\nsuch constraints without requiring any retraining. TRIDENT compiles LTLf\nformulas into a Deterministic Finite Automaton (DFA), which is used to guide a\nconstrained variant of beam search. At each decoding step, transitions that\nwould lead to constraint violations are masked, while remaining paths are\ndynamically re-ranked based on both the model's probabilities and the DFA's\nacceptance structure. We formally prove that the resulting sequences are\nguaranteed to satisfy the given LTLf constraints, and we empirically\ndemonstrate that TRIDENT also improves output quality. We validate our approach\non two distinct tasks: temporally constrained image-stream classification and\ncontrolled text generation. In both settings, TRIDENT achieves perfect\nconstraint satisfaction, while comparison with the state of the art shows\nimproved efficiency and high standard quality metrics."
                },
                "authors": [
                    {
                        "name": "Vincenzo Collura"
                    },
                    {
                        "name": "Karim Tit"
                    },
                    {
                        "name": "Laura Bussi"
                    },
                    {
                        "name": "Eleonora Giunchiglia"
                    },
                    {
                        "name": "Maxime Cordy"
                    }
                ],
                "author_detail": {
                    "name": "Maxime Cordy"
                },
                "author": "Maxime Cordy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09701v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09701v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09695v1",
                "updated": "2025-06-11T13:10:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    10,
                    49,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T13:10:49Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    10,
                    49,
                    2,
                    162,
                    0
                ],
                "title": "Towards Practical Alzheimer's Disease Diagnosis: A Lightweight and\n  Interpretable Spiking Neural Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Practical Alzheimer's Disease Diagnosis: A Lightweight and\n  Interpretable Spiking Neural Model"
                },
                "summary": "Early diagnosis of Alzheimer's Disease (AD), especially at the mild cognitive\nimpairment (MCI) stage, is vital yet hindered by subjective assessments and the\nhigh cost of multimodal imaging modalities. Although deep learning methods\noffer automated alternatives, their energy inefficiency and computational\ndemands limit real-world deployment, particularly in resource-constrained\nsettings. As a brain-inspired paradigm, spiking neural networks (SNNs) are\ninherently well-suited for modeling the sparse, event-driven patterns of neural\ndegeneration in AD, offering a promising foundation for interpretable and\nlow-power medical diagnostics. However, existing SNNs often suffer from weak\nexpressiveness and unstable training, which restrict their effectiveness in\ncomplex medical tasks. To address these limitations, we propose FasterSNN, a\nhybrid neural architecture that integrates biologically inspired LIF neurons\nwith region-adaptive convolution and multi-scale spiking attention. This design\nenables sparse, efficient processing of 3D MRI while preserving diagnostic\naccuracy. Experiments on benchmark datasets demonstrate that FasterSNN achieves\ncompetitive performance with substantially improved efficiency and stability,\nsupporting its potential for practical AD screening. Our source code is\navailable at https://github.com/wuchangw/FasterSNN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early diagnosis of Alzheimer's Disease (AD), especially at the mild cognitive\nimpairment (MCI) stage, is vital yet hindered by subjective assessments and the\nhigh cost of multimodal imaging modalities. Although deep learning methods\noffer automated alternatives, their energy inefficiency and computational\ndemands limit real-world deployment, particularly in resource-constrained\nsettings. As a brain-inspired paradigm, spiking neural networks (SNNs) are\ninherently well-suited for modeling the sparse, event-driven patterns of neural\ndegeneration in AD, offering a promising foundation for interpretable and\nlow-power medical diagnostics. However, existing SNNs often suffer from weak\nexpressiveness and unstable training, which restrict their effectiveness in\ncomplex medical tasks. To address these limitations, we propose FasterSNN, a\nhybrid neural architecture that integrates biologically inspired LIF neurons\nwith region-adaptive convolution and multi-scale spiking attention. This design\nenables sparse, efficient processing of 3D MRI while preserving diagnostic\naccuracy. Experiments on benchmark datasets demonstrate that FasterSNN achieves\ncompetitive performance with substantially improved efficiency and stability,\nsupporting its potential for practical AD screening. Our source code is\navailable at https://github.com/wuchangw/FasterSNN."
                },
                "authors": [
                    {
                        "name": "Changwei Wu"
                    },
                    {
                        "name": "Yifei Chen"
                    },
                    {
                        "name": "Yuxin Du"
                    },
                    {
                        "name": "Jinying Zong"
                    },
                    {
                        "name": "Jie Dong"
                    },
                    {
                        "name": "Mingxuan Liu"
                    },
                    {
                        "name": "Yong Peng"
                    },
                    {
                        "name": "Jin Fan"
                    },
                    {
                        "name": "Feiwei Qin"
                    },
                    {
                        "name": "Changmiao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Changmiao Wang"
                },
                "author": "Changmiao Wang",
                "arxiv_comment": "11 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09684v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09684v1",
                "updated": "2025-06-11T13:02:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    2,
                    17,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T13:02:17Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    2,
                    17,
                    2,
                    162,
                    0
                ],
                "title": "Inv-Entropy: A Fully Probabilistic Framework for Uncertainty\n  Quantification in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inv-Entropy: A Fully Probabilistic Framework for Uncertainty\n  Quantification in Language Models"
                },
                "summary": "Large language models (LLMs) have transformed natural language processing,\nbut their reliable deployment requires effective uncertainty quantification\n(UQ). Existing UQ methods are often heuristic and lack a probabilistic\nfoundation. This paper begins by providing a theoretical justification for the\nrole of perturbations in UQ for LLMs. We then introduce a dual random walk\nperspective, modeling input-output pairs as two Markov chains with transition\nprobabilities defined by semantic similarity. Building on this, we propose a\nfully probabilistic framework based on an inverse model, which quantifies\nuncertainty by evaluating the diversity of the input space conditioned on a\ngiven output through systematic perturbations. Within this framework, we define\na new uncertainty measure, Inv-Entropy. A key strength of our framework is its\nflexibility: it supports various definitions of uncertainty measures,\nembeddings, perturbation strategies, and similarity metrics. We also propose\nGAAP, a perturbation algorithm based on genetic algorithms, which enhances the\ndiversity of sampled inputs. In addition, we introduce a new evaluation metric,\nTemperature Sensitivity of Uncertainty (TSU), which directly assesses\nuncertainty without relying on correctness as a proxy. Extensive experiments\ndemonstrate that Inv-Entropy outperforms existing semantic UQ methods. The code\nto reproduce the results can be found at\nhttps://github.com/UMDataScienceLab/Uncertainty-Quantification-for-LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have transformed natural language processing,\nbut their reliable deployment requires effective uncertainty quantification\n(UQ). Existing UQ methods are often heuristic and lack a probabilistic\nfoundation. This paper begins by providing a theoretical justification for the\nrole of perturbations in UQ for LLMs. We then introduce a dual random walk\nperspective, modeling input-output pairs as two Markov chains with transition\nprobabilities defined by semantic similarity. Building on this, we propose a\nfully probabilistic framework based on an inverse model, which quantifies\nuncertainty by evaluating the diversity of the input space conditioned on a\ngiven output through systematic perturbations. Within this framework, we define\na new uncertainty measure, Inv-Entropy. A key strength of our framework is its\nflexibility: it supports various definitions of uncertainty measures,\nembeddings, perturbation strategies, and similarity metrics. We also propose\nGAAP, a perturbation algorithm based on genetic algorithms, which enhances the\ndiversity of sampled inputs. In addition, we introduce a new evaluation metric,\nTemperature Sensitivity of Uncertainty (TSU), which directly assesses\nuncertainty without relying on correctness as a proxy. Extensive experiments\ndemonstrate that Inv-Entropy outperforms existing semantic UQ methods. The code\nto reproduce the results can be found at\nhttps://github.com/UMDataScienceLab/Uncertainty-Quantification-for-LLMs."
                },
                "authors": [
                    {
                        "name": "Haoyi Song"
                    },
                    {
                        "name": "Ruihan Ji"
                    },
                    {
                        "name": "Naichen Shi"
                    },
                    {
                        "name": "Fan Lai"
                    },
                    {
                        "name": "Raed Al Kontar"
                    }
                ],
                "author_detail": {
                    "name": "Raed Al Kontar"
                },
                "author": "Raed Al Kontar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09684v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09684v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03998v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03998v2",
                "updated": "2025-06-11T13:01:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    1,
                    4,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-04T14:27:50Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    27,
                    50,
                    2,
                    155,
                    0
                ],
                "title": "The QTF-Backbone: Proposal for a Nationwide Optical Fibre Backbone in\n  Germany for Quantum Technology and Time and Frequency Metrology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The QTF-Backbone: Proposal for a Nationwide Optical Fibre Backbone in\n  Germany for Quantum Technology and Time and Frequency Metrology"
                },
                "summary": "The recent breakthroughs in the distribution of quantum information and\nhigh-precision time and frequency (T&F) signals over long-haul optical fibre\nnetworks have transformative potential for physically secure communications,\nresilience of Global Navigation Satellite Systems (GNSS) and fundamental\nphysics. However, so far these capabilities remain confined to isolated\ntestbeds, with quantum and T&F signals accessible, for example in Germany, to\nonly a few institutions.\n  We propose the QTF-Backbone: a dedicated national fibre-optic infrastructure\nin Germany for the networked distribution of quantum and T&F signals using dark\nfibres and specialized hardware. The QTF-Backbone is planned as a four-phase\ndeployment over ten years to ensure scalable, sustainable access for research\ninstitutions and industry. The concept builds on successful demonstrations of\nhigh-TRL time and frequency distribution across Europe, including PTB-MPQ links\nin Germany, REFIMEVE in France, and the Italian LIFT network. The QTF-Backbone\nwill enable transformative R&D, support a nationwide QTF ecosystem, and ensure\nthe transition from innovation to deployment. As a national and European hub,\nit will position Germany and Europe at the forefront of quantum networking, as\nwell as time and frequency transfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent breakthroughs in the distribution of quantum information and\nhigh-precision time and frequency (T&F) signals over long-haul optical fibre\nnetworks have transformative potential for physically secure communications,\nresilience of Global Navigation Satellite Systems (GNSS) and fundamental\nphysics. However, so far these capabilities remain confined to isolated\ntestbeds, with quantum and T&F signals accessible, for example in Germany, to\nonly a few institutions.\n  We propose the QTF-Backbone: a dedicated national fibre-optic infrastructure\nin Germany for the networked distribution of quantum and T&F signals using dark\nfibres and specialized hardware. The QTF-Backbone is planned as a four-phase\ndeployment over ten years to ensure scalable, sustainable access for research\ninstitutions and industry. The concept builds on successful demonstrations of\nhigh-TRL time and frequency distribution across Europe, including PTB-MPQ links\nin Germany, REFIMEVE in France, and the Italian LIFT network. The QTF-Backbone\nwill enable transformative R&D, support a nationwide QTF ecosystem, and ensure\nthe transition from innovation to deployment. As a national and European hub,\nit will position Germany and Europe at the forefront of quantum networking, as\nwell as time and frequency transfer."
                },
                "authors": [
                    {
                        "name": "Klaus Blaum"
                    },
                    {
                        "name": "Peter Kaufmann"
                    },
                    {
                        "name": "Jochen Kronjger"
                    },
                    {
                        "name": "Stefan Kck"
                    },
                    {
                        "name": "Tara Cubel Liebisch"
                    },
                    {
                        "name": "Dieter Meschede"
                    },
                    {
                        "name": "Susanne Naegele-Jackson"
                    },
                    {
                        "name": "Stephan Schiller"
                    },
                    {
                        "name": "Harald Schnatz"
                    }
                ],
                "author_detail": {
                    "name": "Harald Schnatz"
                },
                "arxiv_affiliation": "Physikalisch-Technische Bundesanstalt",
                "author": "Harald Schnatz",
                "arxiv_comment": "52 pages, 7 figures, 9 tables, 73 contributors and 28 supporters in\n  addition to the 9 authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03998v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03998v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09672v1",
                "updated": "2025-06-11T12:43:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    12,
                    43,
                    10,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T12:43:10Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    12,
                    43,
                    10,
                    2,
                    162,
                    0
                ],
                "title": "Is Fine-Tuning an Effective Solution? Reassessing Knowledge Editing for\n  Unstructured Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Fine-Tuning an Effective Solution? Reassessing Knowledge Editing for\n  Unstructured Data"
                },
                "summary": "Unstructured Knowledge Editing (UKE) is crucial for updating the relevant\nknowledge of large language models (LLMs). It focuses on unstructured inputs,\nsuch as long or free-form texts, which are common forms of real-world\nknowledge. Although previous studies have proposed effective methods and tested\nthem, some issues exist: (1) Lack of Locality evaluation for UKE, and (2)\nAbnormal failure of fine-tuning (FT) based methods for UKE. To address these\nissues, we first construct two datasets, UnKEBench-Loc and AKEW-Loc (CF), by\nextending two existing UKE datasets with locality test data from the\nunstructured and structured views. This enables a systematic evaluation of the\nLocality of post-edited models. Furthermore, we identify four factors that may\naffect the performance of FT-based methods. Based on these factors, we conduct\nexperiments to determine how the well-performing FT-based methods should be\ntrained for the UKE task, providing a training recipe for future research. Our\nexperimental results indicate that the FT-based method with the optimal setting\n(FT-UKE) is surprisingly strong, outperforming the existing state-of-the-art\n(SOTA). In batch editing scenarios, FT-UKE shows strong performance as well,\nwith its advantage over SOTA methods increasing as the batch size grows,\nexpanding the average metric lead from +6.78% to +10.80%",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unstructured Knowledge Editing (UKE) is crucial for updating the relevant\nknowledge of large language models (LLMs). It focuses on unstructured inputs,\nsuch as long or free-form texts, which are common forms of real-world\nknowledge. Although previous studies have proposed effective methods and tested\nthem, some issues exist: (1) Lack of Locality evaluation for UKE, and (2)\nAbnormal failure of fine-tuning (FT) based methods for UKE. To address these\nissues, we first construct two datasets, UnKEBench-Loc and AKEW-Loc (CF), by\nextending two existing UKE datasets with locality test data from the\nunstructured and structured views. This enables a systematic evaluation of the\nLocality of post-edited models. Furthermore, we identify four factors that may\naffect the performance of FT-based methods. Based on these factors, we conduct\nexperiments to determine how the well-performing FT-based methods should be\ntrained for the UKE task, providing a training recipe for future research. Our\nexperimental results indicate that the FT-based method with the optimal setting\n(FT-UKE) is surprisingly strong, outperforming the existing state-of-the-art\n(SOTA). In batch editing scenarios, FT-UKE shows strong performance as well,\nwith its advantage over SOTA methods increasing as the batch size grows,\nexpanding the average metric lead from +6.78% to +10.80%"
                },
                "authors": [
                    {
                        "name": "Hao Xiong"
                    },
                    {
                        "name": "Chuanyuan Tan"
                    },
                    {
                        "name": "Wenliang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenliang Chen"
                },
                "author": "Wenliang Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12768v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12768v2",
                "updated": "2025-06-11T12:40:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    12,
                    40,
                    14,
                    2,
                    162,
                    0
                ],
                "published": "2024-11-18T07:52:12Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    7,
                    52,
                    12,
                    0,
                    323,
                    0
                ],
                "title": "CROW: Eliminating Backdoors from Large Language Models via Internal\n  Consistency Regularization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CROW: Eliminating Backdoors from Large Language Models via Internal\n  Consistency Regularization"
                },
                "summary": "Large Language Models (LLMs) are vulnerable to backdoor attacks that\nmanipulate outputs via hidden triggers. Existing defense methods--designed for\nvision/text classification tasks--fail for text generation. We propose Internal\nConsistency Regularization (CROW), a defense leveraging the observation that\nbackdoored models exhibit unstable layer-wise hidden representations when\ntriggered, while clean models show smooth transitions. CROW enforces\nconsistency across layers via adversarial perturbations and regularization\nduring finetuning, neutralizing backdoors without requiring clean reference\nmodels or trigger knowledge--only a small clean dataset. Experiments across\nLlama-2 (7B, 13B), CodeLlama (7B, 13B), and Mistral-7B demonstrate CROW's\neffectiveness: it achieves significant reductions in attack success rates\nacross diverse backdoor strategies (sentiment steering, targeted refusal, code\ninjection) while preserving generative performance. CROW's\narchitecture-agnostic design enables practical deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are vulnerable to backdoor attacks that\nmanipulate outputs via hidden triggers. Existing defense methods--designed for\nvision/text classification tasks--fail for text generation. We propose Internal\nConsistency Regularization (CROW), a defense leveraging the observation that\nbackdoored models exhibit unstable layer-wise hidden representations when\ntriggered, while clean models show smooth transitions. CROW enforces\nconsistency across layers via adversarial perturbations and regularization\nduring finetuning, neutralizing backdoors without requiring clean reference\nmodels or trigger knowledge--only a small clean dataset. Experiments across\nLlama-2 (7B, 13B), CodeLlama (7B, 13B), and Mistral-7B demonstrate CROW's\neffectiveness: it achieves significant reductions in attack success rates\nacross diverse backdoor strategies (sentiment steering, targeted refusal, code\ninjection) while preserving generative performance. CROW's\narchitecture-agnostic design enables practical deployment."
                },
                "authors": [
                    {
                        "name": "Nay Myat Min"
                    },
                    {
                        "name": "Long H. Pham"
                    },
                    {
                        "name": "Yige Li"
                    },
                    {
                        "name": "Jun Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jun Sun"
                },
                "author": "Jun Sun",
                "arxiv_comment": "Accepted at ICML 2025, 20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12768v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12768v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09660v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09660v1",
                "updated": "2025-06-11T12:29:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    12,
                    29,
                    20,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T12:29:20Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    12,
                    29,
                    20,
                    2,
                    162,
                    0
                ],
                "title": "SyncFed: Time-Aware Federated Learning through Explicit Timestamping and\n  Synchronization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SyncFed: Time-Aware Federated Learning through Explicit Timestamping and\n  Synchronization"
                },
                "summary": "As Federated Learning (FL) expands to larger and more distributed\nenvironments, consistency in training is challenged by network-induced delays,\nclock unsynchronicity, and variability in client updates. This combination of\nfactors may contribute to misaligned contributions that undermine model\nreliability and convergence. Existing methods like staleness-aware aggregation\nand model versioning address lagging updates heuristically, yet lack mechanisms\nto quantify staleness, especially in latency-sensitive and cross-regional\ndeployments. In light of these considerations, we introduce \\emph{SyncFed}, a\ntime-aware FL framework that employs explicit synchronization and timestamping\nto establish a common temporal reference across the system. Staleness is\nquantified numerically based on exchanged timestamps under the Network Time\nProtocol (NTP), enabling the server to reason about the relative freshness of\nclient updates and apply temporally informed weighting during aggregation. Our\nempirical evaluation on a geographically distributed testbed shows that, under\n\\emph{SyncFed}, the global model evolves within a stable temporal context,\nresulting in improved accuracy and information freshness compared to\nround-based baselines devoid of temporal semantics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Federated Learning (FL) expands to larger and more distributed\nenvironments, consistency in training is challenged by network-induced delays,\nclock unsynchronicity, and variability in client updates. This combination of\nfactors may contribute to misaligned contributions that undermine model\nreliability and convergence. Existing methods like staleness-aware aggregation\nand model versioning address lagging updates heuristically, yet lack mechanisms\nto quantify staleness, especially in latency-sensitive and cross-regional\ndeployments. In light of these considerations, we introduce \\emph{SyncFed}, a\ntime-aware FL framework that employs explicit synchronization and timestamping\nto establish a common temporal reference across the system. Staleness is\nquantified numerically based on exchanged timestamps under the Network Time\nProtocol (NTP), enabling the server to reason about the relative freshness of\nclient updates and apply temporally informed weighting during aggregation. Our\nempirical evaluation on a geographically distributed testbed shows that, under\n\\emph{SyncFed}, the global model evolves within a stable temporal context,\nresulting in improved accuracy and information freshness compared to\nround-based baselines devoid of temporal semantics."
                },
                "authors": [
                    {
                        "name": "Baran Can Gl"
                    },
                    {
                        "name": "Stefanos Tziampazis"
                    },
                    {
                        "name": "Nasser Jazdi"
                    },
                    {
                        "name": "Michael Weyrich"
                    }
                ],
                "author_detail": {
                    "name": "Michael Weyrich"
                },
                "author": "Michael Weyrich",
                "arxiv_comment": "Preprint version. Accepted for publication at IEEE ETFA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09660v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09660v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09659v1",
                "updated": "2025-06-11T12:26:45Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    12,
                    26,
                    45,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T12:26:45Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    12,
                    26,
                    45,
                    2,
                    162,
                    0
                ],
                "title": "Intent Factored Generation: Unleashing the Diversity in Your Language\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intent Factored Generation: Unleashing the Diversity in Your Language\n  Model"
                },
                "summary": "Obtaining multiple meaningfully diverse, high quality samples from Large\nLanguage Models for a fixed prompt remains an open challenge. Current methods\nfor increasing diversity often only operate at the token-level, paraphrasing\nthe same response. This is problematic because it leads to poor exploration on\nreasoning problems and to unengaging, repetitive conversational agents. To\naddress this we propose Intent Factored Generation (IFG), factorising the\nsampling process into two stages. First, we sample a semantically dense intent,\ne.g., a summary or keywords. Second, we sample the final response conditioning\non both the original prompt and the intent from the first stage. This allows us\nto use a higher temperature during the intent step to promote conceptual\ndiversity, and a lower temperature during the final generation to ensure the\noutputs are coherent and self-consistent. Additionally, we find that prompting\nthe model to explicitly state its intent for each step of the chain-of-thought\nbefore generating the step is beneficial for reasoning tasks. We demonstrate\nour method's effectiveness across a diverse set of tasks. We show this method\nimproves both pass@k and Reinforcement Learning from Verifier Feedback on maths\nand code tasks. For instruction-tuning, we combine IFG with Direct Preference\nOptimisation to increase conversational diversity without sacrificing reward.\nFinally, we achieve higher diversity while maintaining the quality of\ngenerations on a general language modelling task, using a new dataset of reader\ncomments and news articles that we collect and open-source. In summary, we\npresent a simple method of increasing the sample diversity of LLMs while\nmaintaining performance. This method can be implemented by changing the prompt\nand varying the temperature during generation, making it easy to integrate into\nmany algorithms for gains across various applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Obtaining multiple meaningfully diverse, high quality samples from Large\nLanguage Models for a fixed prompt remains an open challenge. Current methods\nfor increasing diversity often only operate at the token-level, paraphrasing\nthe same response. This is problematic because it leads to poor exploration on\nreasoning problems and to unengaging, repetitive conversational agents. To\naddress this we propose Intent Factored Generation (IFG), factorising the\nsampling process into two stages. First, we sample a semantically dense intent,\ne.g., a summary or keywords. Second, we sample the final response conditioning\non both the original prompt and the intent from the first stage. This allows us\nto use a higher temperature during the intent step to promote conceptual\ndiversity, and a lower temperature during the final generation to ensure the\noutputs are coherent and self-consistent. Additionally, we find that prompting\nthe model to explicitly state its intent for each step of the chain-of-thought\nbefore generating the step is beneficial for reasoning tasks. We demonstrate\nour method's effectiveness across a diverse set of tasks. We show this method\nimproves both pass@k and Reinforcement Learning from Verifier Feedback on maths\nand code tasks. For instruction-tuning, we combine IFG with Direct Preference\nOptimisation to increase conversational diversity without sacrificing reward.\nFinally, we achieve higher diversity while maintaining the quality of\ngenerations on a general language modelling task, using a new dataset of reader\ncomments and news articles that we collect and open-source. In summary, we\npresent a simple method of increasing the sample diversity of LLMs while\nmaintaining performance. This method can be implemented by changing the prompt\nand varying the temperature during generation, making it easy to integrate into\nmany algorithms for gains across various applications."
                },
                "authors": [
                    {
                        "name": "Eltayeb Ahmed"
                    },
                    {
                        "name": "Uljad Berdica"
                    },
                    {
                        "name": "Martha Elliott"
                    },
                    {
                        "name": "Danijela Horak"
                    },
                    {
                        "name": "Jakob N. Foerster"
                    }
                ],
                "author_detail": {
                    "name": "Jakob N. Foerster"
                },
                "author": "Jakob N. Foerster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09657v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09657v1",
                "updated": "2025-06-11T12:26:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    12,
                    26,
                    8,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T12:26:08Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    12,
                    26,
                    8,
                    2,
                    162,
                    0
                ],
                "title": "Bridging the Gap Between Open-Source and Proprietary LLMs in Table QA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the Gap Between Open-Source and Proprietary LLMs in Table QA"
                },
                "summary": "This paper presents a system developed for SemEval 2025 Task 8: Question\nAnswering (QA) over tabular data. Our approach integrates several key\ncomponents: text-to-SQL and text-to-code generation modules, a self-correction\nmechanism, and a retrieval-augmented generation (RAG). Additionally, it\nincludes an end-to-end (E2E) module, all orchestrated by a large language model\n(LLM). Through ablation studies, we analyzed the effects of different parts of\nour pipeline and identified the challenges that are still present in this\nfield. During the evaluation phase of the competition, our solution achieved an\naccuracy of 80%, resulting in a top-13 ranking among the 38 participating\nteams. Our pipeline demonstrates a significant improvement in accuracy for\nopen-source models and achieves a performance comparable to proprietary LLMs in\nQA tasks over tables. The code is available at GitHub repository.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a system developed for SemEval 2025 Task 8: Question\nAnswering (QA) over tabular data. Our approach integrates several key\ncomponents: text-to-SQL and text-to-code generation modules, a self-correction\nmechanism, and a retrieval-augmented generation (RAG). Additionally, it\nincludes an end-to-end (E2E) module, all orchestrated by a large language model\n(LLM). Through ablation studies, we analyzed the effects of different parts of\nour pipeline and identified the challenges that are still present in this\nfield. During the evaluation phase of the competition, our solution achieved an\naccuracy of 80%, resulting in a top-13 ranking among the 38 participating\nteams. Our pipeline demonstrates a significant improvement in accuracy for\nopen-source models and achieves a performance comparable to proprietary LLMs in\nQA tasks over tables. The code is available at GitHub repository."
                },
                "authors": [
                    {
                        "name": "Nikolas Evkarpidi"
                    },
                    {
                        "name": "Elena Tutubalina"
                    }
                ],
                "author_detail": {
                    "name": "Elena Tutubalina"
                },
                "author": "Elena Tutubalina",
                "arxiv_comment": "Accepted for publication at the 19th International Workshop on\n  Semantic Evaluation (SemEval-2025), to be held in conjunction with ACL 2025.\n  15 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09657v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09657v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09656v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09656v1",
                "updated": "2025-06-11T12:25:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    12,
                    25,
                    38,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T12:25:38Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    12,
                    25,
                    38,
                    2,
                    162,
                    0
                ],
                "title": "Application-Driven Value Alignment in Agentic AI Systems: Survey and\n  Perspectives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application-Driven Value Alignment in Agentic AI Systems: Survey and\n  Perspectives"
                },
                "summary": "The ongoing evolution of AI paradigms has propelled AI research into the\nAgentic AI stage. Consequently, the focus of research has shifted from single\nagents and simple applications towards multi-agent autonomous decision-making\nand task collaboration in complex environments. As Large Language Models (LLMs)\nadvance, their applications become more diverse and complex, leading to\nincreasingly situational and systemic risks. This has brought significant\nattention to value alignment for AI agents, which aims to ensure that an\nagent's goals, preferences, and behaviors align with human values and societal\nnorms. This paper reviews value alignment in agent systems within specific\napplication scenarios. It integrates the advancements in AI driven by large\nmodels with the demands of social governance. Our review covers value\nprinciples, agent system application scenarios, and agent value alignment\nevaluation. Specifically, value principles are organized hierarchically from a\ntop-down perspective, encompassing macro, meso, and micro levels. Agent system\napplication scenarios are categorized and reviewed from a general-to-specific\nviewpoint. Agent value alignment evaluation systematically examines datasets\nfor value alignment assessment and relevant value alignment methods.\nAdditionally, we delve into value coordination among multiple agents within\nagent systems. Finally, we propose several potential research directions in\nthis field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ongoing evolution of AI paradigms has propelled AI research into the\nAgentic AI stage. Consequently, the focus of research has shifted from single\nagents and simple applications towards multi-agent autonomous decision-making\nand task collaboration in complex environments. As Large Language Models (LLMs)\nadvance, their applications become more diverse and complex, leading to\nincreasingly situational and systemic risks. This has brought significant\nattention to value alignment for AI agents, which aims to ensure that an\nagent's goals, preferences, and behaviors align with human values and societal\nnorms. This paper reviews value alignment in agent systems within specific\napplication scenarios. It integrates the advancements in AI driven by large\nmodels with the demands of social governance. Our review covers value\nprinciples, agent system application scenarios, and agent value alignment\nevaluation. Specifically, value principles are organized hierarchically from a\ntop-down perspective, encompassing macro, meso, and micro levels. Agent system\napplication scenarios are categorized and reviewed from a general-to-specific\nviewpoint. Agent value alignment evaluation systematically examines datasets\nfor value alignment assessment and relevant value alignment methods.\nAdditionally, we delve into value coordination among multiple agents within\nagent systems. Finally, we propose several potential research directions in\nthis field."
                },
                "authors": [
                    {
                        "name": "Wei Zeng"
                    },
                    {
                        "name": "Hengshu Zhu"
                    },
                    {
                        "name": "Chuan Qin"
                    },
                    {
                        "name": "Han Wu"
                    },
                    {
                        "name": "Yihang Cheng"
                    },
                    {
                        "name": "Sirui Zhang"
                    },
                    {
                        "name": "Xiaowei Jin"
                    },
                    {
                        "name": "Yinuo Shen"
                    },
                    {
                        "name": "Zhenxing Wang"
                    },
                    {
                        "name": "Feimin Zhong"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09656v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09656v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09655v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09655v1",
                "updated": "2025-06-11T12:25:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    12,
                    25,
                    32,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T12:25:32Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    12,
                    25,
                    32,
                    2,
                    162,
                    0
                ],
                "title": "DipLLM: Fine-Tuning LLM for Strategic Decision-making in Diplomacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DipLLM: Fine-Tuning LLM for Strategic Decision-making in Diplomacy"
                },
                "summary": "Diplomacy is a complex multiplayer game that requires both cooperation and\ncompetition, posing significant challenges for AI systems. Traditional methods\nrely on equilibrium search to generate extensive game data for training, which\ndemands substantial computational resources. Large Language Models (LLMs) offer\na promising alternative, leveraging pre-trained knowledge to achieve strong\nperformance with relatively small-scale fine-tuning. However, applying LLMs to\nDiplomacy remains challenging due to the exponential growth of possible action\ncombinations and the intricate strategic interactions among players. To address\nthis challenge, we propose DipLLM, a fine-tuned LLM-based agent that learns\nequilibrium policies for Diplomacy. DipLLM employs an autoregressive\nfactorization framework to simplify the complex task of multi-unit action\nassignment into a sequence of unit-level decisions. By defining an equilibrium\npolicy within this framework as the learning objective, we fine-tune the model\nusing only 1.5% of the data required by the state-of-the-art Cicero model,\nsurpassing its performance. Our results demonstrate the potential of fine-tuned\nLLMs for tackling complex strategic decision-making in multiplayer games.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diplomacy is a complex multiplayer game that requires both cooperation and\ncompetition, posing significant challenges for AI systems. Traditional methods\nrely on equilibrium search to generate extensive game data for training, which\ndemands substantial computational resources. Large Language Models (LLMs) offer\na promising alternative, leveraging pre-trained knowledge to achieve strong\nperformance with relatively small-scale fine-tuning. However, applying LLMs to\nDiplomacy remains challenging due to the exponential growth of possible action\ncombinations and the intricate strategic interactions among players. To address\nthis challenge, we propose DipLLM, a fine-tuned LLM-based agent that learns\nequilibrium policies for Diplomacy. DipLLM employs an autoregressive\nfactorization framework to simplify the complex task of multi-unit action\nassignment into a sequence of unit-level decisions. By defining an equilibrium\npolicy within this framework as the learning objective, we fine-tune the model\nusing only 1.5% of the data required by the state-of-the-art Cicero model,\nsurpassing its performance. Our results demonstrate the potential of fine-tuned\nLLMs for tackling complex strategic decision-making in multiplayer games."
                },
                "authors": [
                    {
                        "name": "Kaixuan Xu"
                    },
                    {
                        "name": "Jiajun Chai"
                    },
                    {
                        "name": "Sicheng Li"
                    },
                    {
                        "name": "Yuqian Fu"
                    },
                    {
                        "name": "Yuanheng Zhu"
                    },
                    {
                        "name": "Dongbin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongbin Zhao"
                },
                "author": "Dongbin Zhao",
                "arxiv_comment": "Accepted to the 42nd International Conference on Machine Learning\n  (ICML 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09655v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09655v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08903v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08903v3",
                "updated": "2025-06-11T12:11:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    12,
                    11,
                    10,
                    2,
                    162,
                    0
                ],
                "published": "2025-05-13T18:45:10Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    18,
                    45,
                    10,
                    1,
                    133,
                    0
                ],
                "title": "Assessing and Advancing Benchmarks for Evaluating Large Language Models\n  in Software Engineering Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing and Advancing Benchmarks for Evaluating Large Language Models\n  in Software Engineering Tasks"
                },
                "summary": "Large language models (LLMs) are gaining increasing popularity in software\nengineering (SE) due to their unprecedented performance across various\napplications. These models are increasingly being utilized for a range of SE\ntasks, including requirements engineering and design, code analysis and\ngeneration, software maintenance, and quality assurance. As LLMs become more\nintegral to SE, evaluating their effectiveness is crucial for understanding\ntheir potential in this field. In recent years, substantial efforts have been\nmade to assess LLM performance in various SE tasks, resulting in the creation\nof several benchmarks tailored to this purpose. This paper offers a thorough\nreview of 291 benchmarks, addressing three main aspects: what benchmarks are\navailable, how benchmarks are constructed, and the future outlook for these\nbenchmarks. We begin by examining SE tasks such as requirements engineering and\ndesign, coding assistant, software testing, AIOPs, software maintenance, and\nquality management. We then analyze the benchmarks and their development\nprocesses, highlighting the limitations of existing benchmarks. Additionally,\nwe discuss the successes and failures of LLMs in different software tasks and\nexplore future opportunities and challenges for SE-related benchmarks. We aim\nto provide a comprehensive overview of benchmark research in SE and offer\ninsights to support the creation of more effective evaluation tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are gaining increasing popularity in software\nengineering (SE) due to their unprecedented performance across various\napplications. These models are increasingly being utilized for a range of SE\ntasks, including requirements engineering and design, code analysis and\ngeneration, software maintenance, and quality assurance. As LLMs become more\nintegral to SE, evaluating their effectiveness is crucial for understanding\ntheir potential in this field. In recent years, substantial efforts have been\nmade to assess LLM performance in various SE tasks, resulting in the creation\nof several benchmarks tailored to this purpose. This paper offers a thorough\nreview of 291 benchmarks, addressing three main aspects: what benchmarks are\navailable, how benchmarks are constructed, and the future outlook for these\nbenchmarks. We begin by examining SE tasks such as requirements engineering and\ndesign, coding assistant, software testing, AIOPs, software maintenance, and\nquality management. We then analyze the benchmarks and their development\nprocesses, highlighting the limitations of existing benchmarks. Additionally,\nwe discuss the successes and failures of LLMs in different software tasks and\nexplore future opportunities and challenges for SE-related benchmarks. We aim\nto provide a comprehensive overview of benchmark research in SE and offer\ninsights to support the creation of more effective evaluation tools."
                },
                "authors": [
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Feifei Niu"
                    },
                    {
                        "name": "Junkai Chen"
                    },
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Junwei Zhang"
                    },
                    {
                        "name": "Junda He"
                    },
                    {
                        "name": "Xin Xia"
                    },
                    {
                        "name": "David Lo"
                    }
                ],
                "author_detail": {
                    "name": "David Lo"
                },
                "author": "David Lo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08903v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08903v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01940v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01940v2",
                "updated": "2025-06-11T12:06:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    12,
                    6,
                    37,
                    2,
                    162,
                    0
                ],
                "published": "2025-03-03T12:55:49Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    12,
                    55,
                    49,
                    0,
                    62,
                    0
                ],
                "title": "AskToAct: Enhancing LLMs Tool Use via Self-Correcting Clarification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AskToAct: Enhancing LLMs Tool Use via Self-Correcting Clarification"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\ntool learning. In real-world scenarios, user queries are often ambiguous and\nincomplete, requiring effective clarification. However, existing interactive\nclarification approaches face two critical limitations: reliance on manually\nconstructed datasets, which inherently constrains training data scale and\ndiversity, and lack of error correction mechanisms during multi-turn\nclarification, leading to error accumulation that compromises both accuracy and\nefficiency. We present AskToAct, which addresses these challenges by exploiting\nthe structural mapping between queries and their tool invocation solutions. Our\nkey insight is that tool parameters naturally represent explicit user intents.\nBy systematically removing key parameters from queries while retaining them as\nground truth, we enable automated construction of high-quality training data.\nWe further enhance model robustness through error-correction pairs and\nselective masking, enabling dynamic error detection during clarification\ninteractions. Comprehensive experiments demonstrate that AskToAct significantly\noutperforms existing approaches, achieving above 57% accuracy in recovering\ncritical unspecified intents and enhancing clarification efficiency by an\naverage of 10.46% while maintaining high accuracy in tool invocation. Our\nframework exhibits robust performance across different model architectures and\nsuccessfully generalizes to entirely unseen APIs without additional training,\nachieving performance comparable to GPT-4o with substantially fewer\ncomputational resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities in\ntool learning. In real-world scenarios, user queries are often ambiguous and\nincomplete, requiring effective clarification. However, existing interactive\nclarification approaches face two critical limitations: reliance on manually\nconstructed datasets, which inherently constrains training data scale and\ndiversity, and lack of error correction mechanisms during multi-turn\nclarification, leading to error accumulation that compromises both accuracy and\nefficiency. We present AskToAct, which addresses these challenges by exploiting\nthe structural mapping between queries and their tool invocation solutions. Our\nkey insight is that tool parameters naturally represent explicit user intents.\nBy systematically removing key parameters from queries while retaining them as\nground truth, we enable automated construction of high-quality training data.\nWe further enhance model robustness through error-correction pairs and\nselective masking, enabling dynamic error detection during clarification\ninteractions. Comprehensive experiments demonstrate that AskToAct significantly\noutperforms existing approaches, achieving above 57% accuracy in recovering\ncritical unspecified intents and enhancing clarification efficiency by an\naverage of 10.46% while maintaining high accuracy in tool invocation. Our\nframework exhibits robust performance across different model architectures and\nsuccessfully generalizes to entirely unseen APIs without additional training,\nachieving performance comparable to GPT-4o with substantially fewer\ncomputational resources."
                },
                "authors": [
                    {
                        "name": "Xuan Zhang"
                    },
                    {
                        "name": "Yongliang Shen"
                    },
                    {
                        "name": "Zhe Zheng"
                    },
                    {
                        "name": "Linjuan Wu"
                    },
                    {
                        "name": "Wenqi Zhang"
                    },
                    {
                        "name": "Yuchen Yan"
                    },
                    {
                        "name": "Qiuying Peng"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Weiming Lu"
                    }
                ],
                "author_detail": {
                    "name": "Weiming Lu"
                },
                "author": "Weiming Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01940v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01940v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09645v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09645v1",
                "updated": "2025-06-11T12:03:52Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    12,
                    3,
                    52,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T12:03:52Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    12,
                    3,
                    52,
                    2,
                    162,
                    0
                ],
                "title": "Learning Efficient and Generalizable Graph Retriever for Knowledge-Graph\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Efficient and Generalizable Graph Retriever for Knowledge-Graph\n  Question Answering"
                },
                "summary": "Large Language Models (LLMs) have shown strong inductive reasoning ability\nacross various domains, but their reliability is hindered by the outdated\nknowledge and hallucinations. Retrieval-Augmented Generation mitigates these\nissues by grounding LLMs with external knowledge; however, most existing RAG\npipelines rely on unstructured text, limiting interpretability and structured\nreasoning. Knowledge graphs, which represent facts as relational triples, offer\na more structured and compact alternative. Recent studies have explored\nintegrating knowledge graphs with LLMs for knowledge graph question answering\n(KGQA), with a significant proportion adopting the retrieve-then-reasoning\nparadigm. In this framework, graph-based retrievers have demonstrated strong\nempirical performance, yet they still face challenges in generalization\nability. In this work, we propose RAPL, a novel framework for efficient and\neffective graph retrieval in KGQA. RAPL addresses these limitations through\nthree aspects: (1) a two-stage labeling strategy that combines heuristic\nsignals with parametric models to provide causally grounded supervision; (2) a\nmodel-agnostic graph transformation approach to capture both intra- and\ninter-triple interactions, thereby enhancing representational capacity; and (3)\na path-based reasoning strategy that facilitates learning from the injected\nrational knowledge, and supports downstream reasoner through structured inputs.\nEmpirically, RAPL outperforms state-of-the-art methods by $2.66\\%-20.34\\%$, and\nsignificantly reduces the performance gap between smaller and more powerful\nLLM-based reasoners, as well as the gap under cross-dataset settings,\nhighlighting its superior retrieval capability and generalizability. Codes are\navailable at: https://github.com/tianyao-aka/RAPL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown strong inductive reasoning ability\nacross various domains, but their reliability is hindered by the outdated\nknowledge and hallucinations. Retrieval-Augmented Generation mitigates these\nissues by grounding LLMs with external knowledge; however, most existing RAG\npipelines rely on unstructured text, limiting interpretability and structured\nreasoning. Knowledge graphs, which represent facts as relational triples, offer\na more structured and compact alternative. Recent studies have explored\nintegrating knowledge graphs with LLMs for knowledge graph question answering\n(KGQA), with a significant proportion adopting the retrieve-then-reasoning\nparadigm. In this framework, graph-based retrievers have demonstrated strong\nempirical performance, yet they still face challenges in generalization\nability. In this work, we propose RAPL, a novel framework for efficient and\neffective graph retrieval in KGQA. RAPL addresses these limitations through\nthree aspects: (1) a two-stage labeling strategy that combines heuristic\nsignals with parametric models to provide causally grounded supervision; (2) a\nmodel-agnostic graph transformation approach to capture both intra- and\ninter-triple interactions, thereby enhancing representational capacity; and (3)\na path-based reasoning strategy that facilitates learning from the injected\nrational knowledge, and supports downstream reasoner through structured inputs.\nEmpirically, RAPL outperforms state-of-the-art methods by $2.66\\%-20.34\\%$, and\nsignificantly reduces the performance gap between smaller and more powerful\nLLM-based reasoners, as well as the gap under cross-dataset settings,\nhighlighting its superior retrieval capability and generalizability. Codes are\navailable at: https://github.com/tianyao-aka/RAPL."
                },
                "authors": [
                    {
                        "name": "Tianjun Yao"
                    },
                    {
                        "name": "Haoxuan Li"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    },
                    {
                        "name": "Pan Li"
                    },
                    {
                        "name": "Tongliang Liu"
                    },
                    {
                        "name": "Kun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kun Zhang"
                },
                "author": "Kun Zhang",
                "arxiv_comment": "32 pages, 28 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09645v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09645v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09638v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09638v1",
                "updated": "2025-06-11T11:52:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    11,
                    52,
                    27,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T11:52:27Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    11,
                    52,
                    27,
                    2,
                    162,
                    0
                ],
                "title": "FedVLMBench: Benchmarking Federated Fine-Tuning of Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedVLMBench: Benchmarking Federated Fine-Tuning of Vision-Language\n  Models"
                },
                "summary": "Vision-Language Models (VLMs) have demonstrated remarkable capabilities in\ncross-modal understanding and generation by integrating visual and textual\ninformation. While instruction tuning and parameter-efficient fine-tuning\nmethods have substantially improved the generalization of VLMs, most existing\napproaches rely on centralized training, posing challenges for deployment in\ndomains with strict privacy requirements like healthcare. Recent efforts have\nintroduced Federated Learning (FL) into VLM fine-tuning to address these\nprivacy concerns, yet comprehensive benchmarks for evaluating federated\nfine-tuning strategies, model architectures, and task generalization remain\nlacking. In this work, we present \\textbf{FedVLMBench}, the first systematic\nbenchmark for federated fine-tuning of VLMs. FedVLMBench integrates two\nmainstream VLM architectures (encoder-based and encoder-free), four fine-tuning\nstrategies, five FL algorithms, six multimodal datasets spanning four\ncross-domain single-task scenarios and two cross-domain multitask settings,\ncovering four distinct downstream task categories. Through extensive\nexperiments, we uncover key insights into the interplay between VLM\narchitectures, fine-tuning strategies, data heterogeneity, and multi-task\nfederated optimization. Notably, we find that a 2-layer multilayer perceptron\n(MLP) connector with concurrent connector and LLM tuning emerges as the optimal\nconfiguration for encoder-based VLMs in FL. Furthermore, current FL methods\nexhibit significantly higher sensitivity to data heterogeneity in\nvision-centric tasks than text-centric ones, across both encoder-free and\nencoder-based VLM architectures. Our benchmark provides essential tools,\ndatasets, and empirical guidance for the research community, offering a\nstandardized platform to advance privacy-preserving, federated training of\nmultimodal foundation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) have demonstrated remarkable capabilities in\ncross-modal understanding and generation by integrating visual and textual\ninformation. While instruction tuning and parameter-efficient fine-tuning\nmethods have substantially improved the generalization of VLMs, most existing\napproaches rely on centralized training, posing challenges for deployment in\ndomains with strict privacy requirements like healthcare. Recent efforts have\nintroduced Federated Learning (FL) into VLM fine-tuning to address these\nprivacy concerns, yet comprehensive benchmarks for evaluating federated\nfine-tuning strategies, model architectures, and task generalization remain\nlacking. In this work, we present \\textbf{FedVLMBench}, the first systematic\nbenchmark for federated fine-tuning of VLMs. FedVLMBench integrates two\nmainstream VLM architectures (encoder-based and encoder-free), four fine-tuning\nstrategies, five FL algorithms, six multimodal datasets spanning four\ncross-domain single-task scenarios and two cross-domain multitask settings,\ncovering four distinct downstream task categories. Through extensive\nexperiments, we uncover key insights into the interplay between VLM\narchitectures, fine-tuning strategies, data heterogeneity, and multi-task\nfederated optimization. Notably, we find that a 2-layer multilayer perceptron\n(MLP) connector with concurrent connector and LLM tuning emerges as the optimal\nconfiguration for encoder-based VLMs in FL. Furthermore, current FL methods\nexhibit significantly higher sensitivity to data heterogeneity in\nvision-centric tasks than text-centric ones, across both encoder-free and\nencoder-based VLM architectures. Our benchmark provides essential tools,\ndatasets, and empirical guidance for the research community, offering a\nstandardized platform to advance privacy-preserving, federated training of\nmultimodal foundation models."
                },
                "authors": [
                    {
                        "name": "Weiying Zheng"
                    },
                    {
                        "name": "Ziyue Lin"
                    },
                    {
                        "name": "Pengxin Guo"
                    },
                    {
                        "name": "Yuyin Zhou"
                    },
                    {
                        "name": "Feifei Wang"
                    },
                    {
                        "name": "Liangqiong Qu"
                    }
                ],
                "author_detail": {
                    "name": "Liangqiong Qu"
                },
                "author": "Liangqiong Qu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09638v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09638v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09634v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09634v1",
                "updated": "2025-06-11T11:46:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    11,
                    46,
                    57,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T11:46:57Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    11,
                    46,
                    57,
                    2,
                    162,
                    0
                ],
                "title": "HSENet: Hybrid Spatial Encoding Network for 3D Medical Vision-Language\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HSENet: Hybrid Spatial Encoding Network for 3D Medical Vision-Language\n  Understanding"
                },
                "summary": "Automated 3D CT diagnosis empowers clinicians to make timely, evidence-based\ndecisions by enhancing diagnostic accuracy and workflow efficiency. While\nmultimodal large language models (MLLMs) exhibit promising performance in\nvisual-language understanding, existing methods mainly focus on 2D medical\nimages, which fundamentally limits their ability to capture complex 3D\nanatomical structures. This limitation often leads to misinterpretation of\nsubtle pathologies and causes diagnostic hallucinations. In this paper, we\npresent Hybrid Spatial Encoding Network (HSENet), a framework that exploits\nenriched 3D medical visual cues by effective visual perception and projection\nfor accurate and robust vision-language understanding. Specifically, HSENet\nemploys dual-3D vision encoders to perceive both global volumetric contexts and\nfine-grained anatomical details, which are pre-trained by dual-stage alignment\nwith diagnostic reports. Furthermore, we propose Spatial Packer, an efficient\nmultimodal projector that condenses high-resolution 3D spatial regions into a\ncompact set of informative visual tokens via centroid-based compression. By\nassigning spatial packers with dual-3D vision encoders, HSENet can seamlessly\nperceive and transfer hybrid visual representations to LLM's semantic space,\nfacilitating accurate diagnostic text generation. Experimental results\ndemonstrate that our method achieves state-of-the-art performance in 3D\nlanguage-visual retrieval (39.85% of R@100, +5.96% gain), 3D medical report\ngeneration (24.01% of BLEU-4, +8.01% gain), and 3D visual question answering\n(73.60% of Major Class Accuracy, +1.99% gain), confirming its effectiveness.\nOur code is available at https://github.com/YanzhaoShi/HSENet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated 3D CT diagnosis empowers clinicians to make timely, evidence-based\ndecisions by enhancing diagnostic accuracy and workflow efficiency. While\nmultimodal large language models (MLLMs) exhibit promising performance in\nvisual-language understanding, existing methods mainly focus on 2D medical\nimages, which fundamentally limits their ability to capture complex 3D\nanatomical structures. This limitation often leads to misinterpretation of\nsubtle pathologies and causes diagnostic hallucinations. In this paper, we\npresent Hybrid Spatial Encoding Network (HSENet), a framework that exploits\nenriched 3D medical visual cues by effective visual perception and projection\nfor accurate and robust vision-language understanding. Specifically, HSENet\nemploys dual-3D vision encoders to perceive both global volumetric contexts and\nfine-grained anatomical details, which are pre-trained by dual-stage alignment\nwith diagnostic reports. Furthermore, we propose Spatial Packer, an efficient\nmultimodal projector that condenses high-resolution 3D spatial regions into a\ncompact set of informative visual tokens via centroid-based compression. By\nassigning spatial packers with dual-3D vision encoders, HSENet can seamlessly\nperceive and transfer hybrid visual representations to LLM's semantic space,\nfacilitating accurate diagnostic text generation. Experimental results\ndemonstrate that our method achieves state-of-the-art performance in 3D\nlanguage-visual retrieval (39.85% of R@100, +5.96% gain), 3D medical report\ngeneration (24.01% of BLEU-4, +8.01% gain), and 3D visual question answering\n(73.60% of Major Class Accuracy, +1.99% gain), confirming its effectiveness.\nOur code is available at https://github.com/YanzhaoShi/HSENet."
                },
                "authors": [
                    {
                        "name": "Yanzhao Shi"
                    },
                    {
                        "name": "Xiaodan Zhang"
                    },
                    {
                        "name": "Junzhong Ji"
                    },
                    {
                        "name": "Haoning Jiang"
                    },
                    {
                        "name": "Chengxin Zheng"
                    },
                    {
                        "name": "Yinong Wang"
                    },
                    {
                        "name": "Liangqiong Qu"
                    }
                ],
                "author_detail": {
                    "name": "Liangqiong Qu"
                },
                "author": "Liangqiong Qu",
                "arxiv_comment": "27 pages, 9 figures. arXiv admin note: text overlap with\n  arXiv:2410.14200 by other authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09634v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09634v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09632v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09632v1",
                "updated": "2025-06-11T11:42:52Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    11,
                    42,
                    52,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T11:42:52Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    11,
                    42,
                    52,
                    2,
                    162,
                    0
                ],
                "title": "Ties of Trust: a bowtie model to uncover trustor-trustee relationships\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ties of Trust: a bowtie model to uncover trustor-trustee relationships\n  in LLMs"
                },
                "summary": "The rapid and unprecedented dominance of Artificial Intelligence (AI),\nparticularly through Large Language Models (LLMs), has raised critical trust\nchallenges in high-stakes domains like politics. Biased LLMs' decisions and\nmisinformation undermine democratic processes, and existing trust models fail\nto address the intricacies of trust in LLMs. Currently, oversimplified,\none-directional approaches have largely overlooked the many relationships\nbetween trustor (user) contextual factors (e.g. ideology, perceptions) and\ntrustee (LLMs) systemic elements (e.g. scientists, tool's features). In this\nwork, we introduce a bowtie model for holistically conceptualizing and\nformulating trust in LLMs, with a core component comprehensively exploring\ntrust by tying its two sides, namely the trustor and the trustee, as well as\ntheir intricate relationships. We uncover these relationships within the\nproposed bowtie model and beyond to its sociotechnical ecosystem, through a\nmixed-methods explanatory study, that exploits a political discourse analysis\ntool (integrating ChatGPT), by exploring and responding to the next critical\nquestions: 1) How do trustor's contextual factors influence trust-related\nactions? 2) How do these factors influence and interact with trustee systemic\nelements? 3) How does trust itself vary across trustee systemic elements? Our\nbowtie-based explanatory analysis reveals that past experiences and familiarity\nsignificantly shape trustor's trust-related actions; not all trustor contextual\nfactors equally influence trustee systemic elements; and trustee's\nhuman-in-the-loop features enhance trust, while lack of transparency decreases\nit. Finally, this solid evidence is exploited to deliver recommendations,\ninsights and pathways towards building robust trusting ecosystems in LLM-based\nsolutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid and unprecedented dominance of Artificial Intelligence (AI),\nparticularly through Large Language Models (LLMs), has raised critical trust\nchallenges in high-stakes domains like politics. Biased LLMs' decisions and\nmisinformation undermine democratic processes, and existing trust models fail\nto address the intricacies of trust in LLMs. Currently, oversimplified,\none-directional approaches have largely overlooked the many relationships\nbetween trustor (user) contextual factors (e.g. ideology, perceptions) and\ntrustee (LLMs) systemic elements (e.g. scientists, tool's features). In this\nwork, we introduce a bowtie model for holistically conceptualizing and\nformulating trust in LLMs, with a core component comprehensively exploring\ntrust by tying its two sides, namely the trustor and the trustee, as well as\ntheir intricate relationships. We uncover these relationships within the\nproposed bowtie model and beyond to its sociotechnical ecosystem, through a\nmixed-methods explanatory study, that exploits a political discourse analysis\ntool (integrating ChatGPT), by exploring and responding to the next critical\nquestions: 1) How do trustor's contextual factors influence trust-related\nactions? 2) How do these factors influence and interact with trustee systemic\nelements? 3) How does trust itself vary across trustee systemic elements? Our\nbowtie-based explanatory analysis reveals that past experiences and familiarity\nsignificantly shape trustor's trust-related actions; not all trustor contextual\nfactors equally influence trustee systemic elements; and trustee's\nhuman-in-the-loop features enhance trust, while lack of transparency decreases\nit. Finally, this solid evidence is exploited to deliver recommendations,\ninsights and pathways towards building robust trusting ecosystems in LLM-based\nsolutions."
                },
                "authors": [
                    {
                        "name": "Eva Paraschou"
                    },
                    {
                        "name": "Maria Michali"
                    },
                    {
                        "name": "Sofia Yfantidou"
                    },
                    {
                        "name": "Stelios Karamanidis"
                    },
                    {
                        "name": "Stefanos Rafail Kalogeros"
                    },
                    {
                        "name": "Athena Vakali"
                    }
                ],
                "author_detail": {
                    "name": "Athena Vakali"
                },
                "author": "Athena Vakali",
                "arxiv_comment": "Accepted for publication at The 2025 ACM Conference on Fairness,\n  Accountability, and Transparency (FAccT '25). This version corresponds to the\n  camera-ready manuscript submitted to the conference proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09632v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09632v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09630v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09630v1",
                "updated": "2025-06-11T11:39:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    11,
                    39,
                    29,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T11:39:29Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    11,
                    39,
                    29,
                    2,
                    162,
                    0
                ],
                "title": "In-Context Bias Propagation in LLM-Based Tabular Data Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Bias Propagation in LLM-Based Tabular Data Generation"
                },
                "summary": "Large Language Models (LLMs) are increasingly used for synthetic tabular data\ngeneration through in-context learning (ICL), offering a practical solution for\ndata augmentation in data scarce scenarios. While prior work has shown the\npotential of LLMs to improve downstream task performance through augmenting\nunderrepresented groups, these benefits often assume access to a subset of\nunbiased in-context examples, representative of the real dataset. In real-world\nsettings, however, data is frequently noisy and demographically skewed. In this\npaper, we systematically study how statistical biases within in-context\nexamples propagate to the distribution of synthetic tabular data, showing that\neven mild in-context biases lead to global statistical distortions. We further\nintroduce an adversarial scenario where a malicious contributor can inject bias\ninto the synthetic dataset via a subset of in-context examples, ultimately\ncompromising the fairness of downstream classifiers for a targeted and\nprotected subgroup. Our findings demonstrate a new vulnerability associated\nwith LLM-based data generation pipelines that rely on in-context prompts with\nin sensitive domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used for synthetic tabular data\ngeneration through in-context learning (ICL), offering a practical solution for\ndata augmentation in data scarce scenarios. While prior work has shown the\npotential of LLMs to improve downstream task performance through augmenting\nunderrepresented groups, these benefits often assume access to a subset of\nunbiased in-context examples, representative of the real dataset. In real-world\nsettings, however, data is frequently noisy and demographically skewed. In this\npaper, we systematically study how statistical biases within in-context\nexamples propagate to the distribution of synthetic tabular data, showing that\neven mild in-context biases lead to global statistical distortions. We further\nintroduce an adversarial scenario where a malicious contributor can inject bias\ninto the synthetic dataset via a subset of in-context examples, ultimately\ncompromising the fairness of downstream classifiers for a targeted and\nprotected subgroup. Our findings demonstrate a new vulnerability associated\nwith LLM-based data generation pipelines that rely on in-context prompts with\nin sensitive domains."
                },
                "authors": [
                    {
                        "name": "Pol G. Recasens"
                    },
                    {
                        "name": "Alberto Gutierrez"
                    },
                    {
                        "name": "Jordi Torres"
                    },
                    {
                        "name": "Josep. Ll Berral"
                    },
                    {
                        "name": "Anisa Halimi"
                    },
                    {
                        "name": "Kieran Fraser"
                    }
                ],
                "author_detail": {
                    "name": "Kieran Fraser"
                },
                "author": "Kieran Fraser",
                "arxiv_comment": "Paper accepted at ICML 2025 workshop DIG-BUG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09630v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09627v1",
                "updated": "2025-06-11T11:37:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    11,
                    37,
                    2,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T11:37:02Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    11,
                    37,
                    2,
                    2,
                    162,
                    0
                ],
                "title": "Benchmarking Debiasing Methods for LLM-based Parameter Estimates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Debiasing Methods for LLM-based Parameter Estimates"
                },
                "summary": "Large language models (LLMs) offer an inexpensive yet powerful way to\nannotate text, but are often inconsistent when compared with experts. These\nerrors can bias downstream estimates of population parameters such as\nregression coefficients and causal effects. To mitigate this bias, researchers\nhave developed debiasing methods such as Design-based Supervised Learning (DSL)\nand Prediction-Powered Inference (PPI), which promise valid estimation by\ncombining LLM annotations with a limited number of expensive expert\nannotations. Although these methods produce consistent estimates under\ntheoretical assumptions, it is unknown how they compare in finite samples of\nsizes encountered in applied research. We make two contributions: First, we\nstudy how each method's performance scales with the number of expert\nannotations, highlighting regimes where LLM bias or limited expert labels\nsignificantly affect results. Second, we compare DSL and PPI across a range of\ntasks, finding that although both achieve low bias with large datasets, DSL\noften outperforms PPI on bias reduction and empirical efficiency, but its\nperformance is less consistent across datasets. Our findings indicate that\nthere is a bias-variance tradeoff at the level of debiasing methods, calling\nfor more research on developing metrics for quantifying their efficiency in\nfinite samples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) offer an inexpensive yet powerful way to\nannotate text, but are often inconsistent when compared with experts. These\nerrors can bias downstream estimates of population parameters such as\nregression coefficients and causal effects. To mitigate this bias, researchers\nhave developed debiasing methods such as Design-based Supervised Learning (DSL)\nand Prediction-Powered Inference (PPI), which promise valid estimation by\ncombining LLM annotations with a limited number of expensive expert\nannotations. Although these methods produce consistent estimates under\ntheoretical assumptions, it is unknown how they compare in finite samples of\nsizes encountered in applied research. We make two contributions: First, we\nstudy how each method's performance scales with the number of expert\nannotations, highlighting regimes where LLM bias or limited expert labels\nsignificantly affect results. Second, we compare DSL and PPI across a range of\ntasks, finding that although both achieve low bias with large datasets, DSL\noften outperforms PPI on bias reduction and empirical efficiency, but its\nperformance is less consistent across datasets. Our findings indicate that\nthere is a bias-variance tradeoff at the level of debiasing methods, calling\nfor more research on developing metrics for quantifying their efficiency in\nfinite samples."
                },
                "authors": [
                    {
                        "name": "Nicolas Audinet de Pieuchon"
                    },
                    {
                        "name": "Adel Daoud"
                    },
                    {
                        "name": "Connor T. Jerzak"
                    },
                    {
                        "name": "Moa Johansson"
                    },
                    {
                        "name": "Richard Johansson"
                    }
                ],
                "author_detail": {
                    "name": "Richard Johansson"
                },
                "author": "Richard Johansson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09623v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09623v1",
                "updated": "2025-06-11T11:28:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    11,
                    28,
                    14,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T11:28:14Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    11,
                    28,
                    14,
                    2,
                    162,
                    0
                ],
                "title": "Analytic Task Scheduler: Recursive Least Squares Based Method for\n  Continual Learning in Embodied Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analytic Task Scheduler: Recursive Least Squares Based Method for\n  Continual Learning in Embodied Foundation Models"
                },
                "summary": "Embodied foundation models are crucial for Artificial Intelligence (AI)\ninteracting with the physical world by integrating multi-modal inputs, such as\nproprioception, vision and language, to understand human intentions and\ngenerate actions to control robots. While these models demonstrate strong\ngeneralization and few-shot learning capabilities, they face significant\nchallenges in continually acquiring new skills without forgetting previously\nlearned skills, a problem known as catastrophic forgetting. To address this\nissue, we propose the Analytic Task Scheduler (ATS), a novel framework for\ncontinual learning in embodied foundation models. ATS consists of a\ntask-specific model library, where each model is fine-tuned independently on a\nsingle task, and an analytic scheduler trained using recursive least squares\n(RLS) to learn the mapping between language instructions and task-specific\nmodels. This architecture enables accurate task recognition and dynamic model\nselection while fundamentally avoiding parameter interference across tasks. The\nscheduler updates its parameters incrementally using only statistics\n(autocorrelation and cross-correlation matrices), enabling forgetting-resistant\nlearning without the need to revisit historical data. We validate ATS on a\nreal-world robot platform (RM65B), demonstrating superior resistance to\nforgetting and strong adaptability to task variations. The results highlight\nATS as an effective, scalable, and deployable solution for continual learning\nin embodied foundation models operating in complex, dynamic environments. Our\ncode will be available at\nhttps://github.com/MIAA-Embodied-AI/AnalyticTaskScheduler",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied foundation models are crucial for Artificial Intelligence (AI)\ninteracting with the physical world by integrating multi-modal inputs, such as\nproprioception, vision and language, to understand human intentions and\ngenerate actions to control robots. While these models demonstrate strong\ngeneralization and few-shot learning capabilities, they face significant\nchallenges in continually acquiring new skills without forgetting previously\nlearned skills, a problem known as catastrophic forgetting. To address this\nissue, we propose the Analytic Task Scheduler (ATS), a novel framework for\ncontinual learning in embodied foundation models. ATS consists of a\ntask-specific model library, where each model is fine-tuned independently on a\nsingle task, and an analytic scheduler trained using recursive least squares\n(RLS) to learn the mapping between language instructions and task-specific\nmodels. This architecture enables accurate task recognition and dynamic model\nselection while fundamentally avoiding parameter interference across tasks. The\nscheduler updates its parameters incrementally using only statistics\n(autocorrelation and cross-correlation matrices), enabling forgetting-resistant\nlearning without the need to revisit historical data. We validate ATS on a\nreal-world robot platform (RM65B), demonstrating superior resistance to\nforgetting and strong adaptability to task variations. The results highlight\nATS as an effective, scalable, and deployable solution for continual learning\nin embodied foundation models operating in complex, dynamic environments. Our\ncode will be available at\nhttps://github.com/MIAA-Embodied-AI/AnalyticTaskScheduler"
                },
                "authors": [
                    {
                        "name": "Lipei Xie"
                    },
                    {
                        "name": "Yingxin Li"
                    },
                    {
                        "name": "Huiping Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Huiping Zhuang"
                },
                "author": "Huiping Zhuang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09623v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09623v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02550v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02550v2",
                "updated": "2025-06-11T11:16:41Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    11,
                    16,
                    41,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-03T07:36:52Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    7,
                    36,
                    52,
                    1,
                    154,
                    0
                ],
                "title": "Technical Report for Ego4D Long-Term Action Anticipation Challenge 2025",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Technical Report for Ego4D Long-Term Action Anticipation Challenge 2025"
                },
                "summary": "In this report, we present a novel three-stage framework developed for the\nEgo4D Long-Term Action Anticipation (LTA) task. Inspired by recent advances in\nfoundation models, our method consists of three stages: feature extraction,\naction recognition, and long-term action anticipation. First, visual features\nare extracted using a high-performance visual encoder. The features are then\nfed into a Transformer to predict verbs and nouns, with a verb-noun\nco-occurrence matrix incorporated to enhance recognition accuracy. Finally, the\npredicted verb-noun pairs are formatted as textual prompts and input into a\nfine-tuned large language model (LLM) to anticipate future action sequences.\nOur framework achieves first place in this challenge at CVPR 2025, establishing\na new state-of-the-art in long-term action prediction. Our code will be\nreleased at https://github.com/CorrineQiu/Ego4D-LTA-Challenge-2025.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this report, we present a novel three-stage framework developed for the\nEgo4D Long-Term Action Anticipation (LTA) task. Inspired by recent advances in\nfoundation models, our method consists of three stages: feature extraction,\naction recognition, and long-term action anticipation. First, visual features\nare extracted using a high-performance visual encoder. The features are then\nfed into a Transformer to predict verbs and nouns, with a verb-noun\nco-occurrence matrix incorporated to enhance recognition accuracy. Finally, the\npredicted verb-noun pairs are formatted as textual prompts and input into a\nfine-tuned large language model (LLM) to anticipate future action sequences.\nOur framework achieves first place in this challenge at CVPR 2025, establishing\na new state-of-the-art in long-term action prediction. Our code will be\nreleased at https://github.com/CorrineQiu/Ego4D-LTA-Challenge-2025."
                },
                "authors": [
                    {
                        "name": "Qiaohui Chu"
                    },
                    {
                        "name": "Haoyu Zhang"
                    },
                    {
                        "name": "Yisen Feng"
                    },
                    {
                        "name": "Meng Liu"
                    },
                    {
                        "name": "Weili Guan"
                    },
                    {
                        "name": "Yaowei Wang"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "The champion solution for the Ego4D Long-Term Action Anticipation\n  Challenge at the CVPR EgoVis Workshop 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02550v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02550v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09613v1",
                "updated": "2025-06-11T11:14:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    11,
                    14,
                    57,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T11:14:57Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    11,
                    14,
                    57,
                    2,
                    162,
                    0
                ],
                "title": "SparseSSM: Efficient Selective Structured State Space Models Can Be\n  Pruned in One-Shot",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseSSM: Efficient Selective Structured State Space Models Can Be\n  Pruned in One-Shot"
                },
                "summary": "State-space language models such as Mamba match Transformer quality while\npermitting linear complexity inference, yet still comprise billions of\nparameters that hinder deployment. Existing one-shot pruning methods are\ntailored to attention blocks and fail to account for the time-shared and\ndiscretized state-transition matrix at the heart of the selective state-space\nmodule (SSM). In this paper, we introduce SparseSSM, the first training-free\npruning framework that extends the classic optimal brain surgeon (OBS)\nframework to state space architectures. Our layer-wise algorithm (i) derives an\napproximate second-order saliency score that aggregates Hessian-trace\ninformation across time steps, (ii) incorporates a component sensitivity\nanalysis to guide feed-forward network (FFN) pruning, which also sheds light on\nwhere redundancy resides in mamba architecture, (iii) can be easily extended to\nsemi-structured and structured sparsity. Empirically, we prune 50% of SSM\nweights without fine-tuning and observe no zero-shot accuracy loss, achieving\nthe current state-of-the-art pruning algorithm for Mamba-based LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-space language models such as Mamba match Transformer quality while\npermitting linear complexity inference, yet still comprise billions of\nparameters that hinder deployment. Existing one-shot pruning methods are\ntailored to attention blocks and fail to account for the time-shared and\ndiscretized state-transition matrix at the heart of the selective state-space\nmodule (SSM). In this paper, we introduce SparseSSM, the first training-free\npruning framework that extends the classic optimal brain surgeon (OBS)\nframework to state space architectures. Our layer-wise algorithm (i) derives an\napproximate second-order saliency score that aggregates Hessian-trace\ninformation across time steps, (ii) incorporates a component sensitivity\nanalysis to guide feed-forward network (FFN) pruning, which also sheds light on\nwhere redundancy resides in mamba architecture, (iii) can be easily extended to\nsemi-structured and structured sparsity. Empirically, we prune 50% of SSM\nweights without fine-tuning and observe no zero-shot accuracy loss, achieving\nthe current state-of-the-art pruning algorithm for Mamba-based LLMs."
                },
                "authors": [
                    {
                        "name": "Kaiwen Tuo"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09601v1",
                "updated": "2025-06-11T10:59:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    10,
                    59,
                    57,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T10:59:57Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    10,
                    59,
                    57,
                    2,
                    162,
                    0
                ],
                "title": "ASTAGEN: Empirical Evaluation of Automated SATD Taxonomy Generation with\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASTAGEN: Empirical Evaluation of Automated SATD Taxonomy Generation with\n  LLMs"
                },
                "summary": "Technical debt refers to suboptimal code that degrades software quality. When\ndevelopers intentionally introduce such debt, it is called self-admitted\ntechnical debt (SATD). Since SATD hinders maintenance, identifying its\ncategories is key to uncovering quality issues. Traditionally, constructing\nsuch taxonomies requires manually inspecting SATD comments and surrounding\ncode, which is time-consuming, labor-intensive, and often inconsistent due to\nannotator subjectivity. This study presents ASTAGEN, an initial step toward\nautomating SATD taxonomy generation using large language models (LLMs). Given a\ncomment and its surrounding code, ASTAGEN first generates a concise explanation\nfor each SATD comment, then incrementally generates and updates categories to\nconstruct a taxonomy. We evaluate ASTAGEN on SATD datasets from three domains:\nquantum software, smart contracts, and machine learning. It successfully\nrecovers domain-specific categories reported in prior work, such as Layer\nConfiguration in machine learning. Compared to a naive use of an LLM, ASTAGEN\nproduces more consistent category assignments due to its explanation-driven,\niterative design. It also completes taxonomy generation in under two hours and\nfor less than one USD, even on the largest dataset. These results suggest that\nwhile full automation remains challenging, ASTAGEN is able to support\nsemi-automated taxonomy construction. Furthermore, our work opens up avenues\nfor future work, such as automatic taxonomy generation in other areas.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Technical debt refers to suboptimal code that degrades software quality. When\ndevelopers intentionally introduce such debt, it is called self-admitted\ntechnical debt (SATD). Since SATD hinders maintenance, identifying its\ncategories is key to uncovering quality issues. Traditionally, constructing\nsuch taxonomies requires manually inspecting SATD comments and surrounding\ncode, which is time-consuming, labor-intensive, and often inconsistent due to\nannotator subjectivity. This study presents ASTAGEN, an initial step toward\nautomating SATD taxonomy generation using large language models (LLMs). Given a\ncomment and its surrounding code, ASTAGEN first generates a concise explanation\nfor each SATD comment, then incrementally generates and updates categories to\nconstruct a taxonomy. We evaluate ASTAGEN on SATD datasets from three domains:\nquantum software, smart contracts, and machine learning. It successfully\nrecovers domain-specific categories reported in prior work, such as Layer\nConfiguration in machine learning. Compared to a naive use of an LLM, ASTAGEN\nproduces more consistent category assignments due to its explanation-driven,\niterative design. It also completes taxonomy generation in under two hours and\nfor less than one USD, even on the largest dataset. These results suggest that\nwhile full automation remains challenging, ASTAGEN is able to support\nsemi-automated taxonomy construction. Furthermore, our work opens up avenues\nfor future work, such as automatic taxonomy generation in other areas."
                },
                "authors": [
                    {
                        "name": "Sota Nakashima"
                    },
                    {
                        "name": "Yuta Ishimoto"
                    },
                    {
                        "name": "Masanari Kondo"
                    },
                    {
                        "name": "Tao Xiao"
                    },
                    {
                        "name": "Yasutaka Kamei"
                    }
                ],
                "author_detail": {
                    "name": "Yasutaka Kamei"
                },
                "author": "Yasutaka Kamei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09600v1",
                "updated": "2025-06-11T10:59:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    10,
                    59,
                    47,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T10:59:47Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    10,
                    59,
                    47,
                    2,
                    162,
                    0
                ],
                "title": "Effective Red-Teaming of Policy-Adherent Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective Red-Teaming of Policy-Adherent Agents"
                },
                "summary": "Task-oriented LLM-based agents are increasingly used in domains with strict\npolicies, such as refund eligibility or cancellation rules. The challenge lies\nin ensuring that the agent consistently adheres to these rules and policies,\nappropriately refusing any request that would violate them, while still\nmaintaining a helpful and natural interaction. This calls for the development\nof tailored design and evaluation methodologies to ensure agent resilience\nagainst malicious user behavior. We propose a novel threat model that focuses\non adversarial users aiming to exploit policy-adherent agents for personal\nbenefit. To address this, we present CRAFT, a multi-agent red-teaming system\nthat leverages policy-aware persuasive strategies to undermine a\npolicy-adherent agent in a customer-service scenario, outperforming\nconventional jailbreak methods such as DAN prompts, emotional manipulation, and\ncoercive. Building upon the existing tau-bench benchmark, we introduce\ntau-break, a complementary benchmark designed to rigorously assess the agent's\nrobustness against manipulative user behavior. Finally, we evaluate several\nstraightforward yet effective defense strategies. While these measures provide\nsome protection, they fall short, highlighting the need for stronger,\nresearch-driven safeguards to protect policy-adherent agents from adversarial\nattacks",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-oriented LLM-based agents are increasingly used in domains with strict\npolicies, such as refund eligibility or cancellation rules. The challenge lies\nin ensuring that the agent consistently adheres to these rules and policies,\nappropriately refusing any request that would violate them, while still\nmaintaining a helpful and natural interaction. This calls for the development\nof tailored design and evaluation methodologies to ensure agent resilience\nagainst malicious user behavior. We propose a novel threat model that focuses\non adversarial users aiming to exploit policy-adherent agents for personal\nbenefit. To address this, we present CRAFT, a multi-agent red-teaming system\nthat leverages policy-aware persuasive strategies to undermine a\npolicy-adherent agent in a customer-service scenario, outperforming\nconventional jailbreak methods such as DAN prompts, emotional manipulation, and\ncoercive. Building upon the existing tau-bench benchmark, we introduce\ntau-break, a complementary benchmark designed to rigorously assess the agent's\nrobustness against manipulative user behavior. Finally, we evaluate several\nstraightforward yet effective defense strategies. While these measures provide\nsome protection, they fall short, highlighting the need for stronger,\nresearch-driven safeguards to protect policy-adherent agents from adversarial\nattacks"
                },
                "authors": [
                    {
                        "name": "Itay Nakash"
                    },
                    {
                        "name": "George Kour"
                    },
                    {
                        "name": "Koren Lazar"
                    },
                    {
                        "name": "Matan Vetzler"
                    },
                    {
                        "name": "Guy Uziel"
                    },
                    {
                        "name": "Ateret Anaby-Tavor"
                    }
                ],
                "author_detail": {
                    "name": "Ateret Anaby-Tavor"
                },
                "author": "Ateret Anaby-Tavor",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12347v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12347v2",
                "updated": "2025-06-11T10:49:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    10,
                    49,
                    35,
                    2,
                    162,
                    0
                ],
                "published": "2025-04-15T18:31:54Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    18,
                    31,
                    54,
                    1,
                    105,
                    0
                ],
                "title": "Assessment of Evolving Large Language Models in Upper Secondary\n  Mathematics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessment of Evolving Large Language Models in Upper Secondary\n  Mathematics"
                },
                "summary": "Large language models (LLMs) have shown increasing promise in educational\nsettings, yet their mathematical reasoning has been considered evolving. This\nstudy evaluates the mathematical capabilities of various LLMs using the Finnish\nmatriculation examination, a high-stakes digital test for upper secondary\neducation. Initial tests yielded moderate performance corresponding to\nmid-range grades, but later evaluations demonstrated substantial improvements\nas the language models evolved. Remarkably, some models achieved near-perfect\nor perfect scores, matching top student performance and qualifying for\nuniversity admission. Our findings highlight the rapid advances in the\nmathematical proficiency of LLMs and illustrate their potential as underlying\ntools to support learning and teaching in a variety of ways.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown increasing promise in educational\nsettings, yet their mathematical reasoning has been considered evolving. This\nstudy evaluates the mathematical capabilities of various LLMs using the Finnish\nmatriculation examination, a high-stakes digital test for upper secondary\neducation. Initial tests yielded moderate performance corresponding to\nmid-range grades, but later evaluations demonstrated substantial improvements\nas the language models evolved. Remarkably, some models achieved near-perfect\nor perfect scores, matching top student performance and qualifying for\nuniversity admission. Our findings highlight the rapid advances in the\nmathematical proficiency of LLMs and illustrate their potential as underlying\ntools to support learning and teaching in a variety of ways."
                },
                "authors": [
                    {
                        "name": "Mika Setl"
                    },
                    {
                        "name": "Pieta Sikstrm"
                    },
                    {
                        "name": "Ville Heilala"
                    },
                    {
                        "name": "Tommi Krkkinen"
                    }
                ],
                "author_detail": {
                    "name": "Tommi Krkkinen"
                },
                "author": "Tommi Krkkinen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12347v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12347v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.3; I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01340v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01340v3",
                "updated": "2025-06-11T10:27:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    10,
                    27,
                    42,
                    2,
                    162,
                    0
                ],
                "published": "2025-02-03T13:34:00Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    13,
                    34,
                    0,
                    0,
                    34,
                    0
                ],
                "title": "Human-Agent Interaction in Synthetic Social Networks: A Framework for\n  Studying Online Polarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-Agent Interaction in Synthetic Social Networks: A Framework for\n  Studying Online Polarization"
                },
                "summary": "Online social networks have dramatically altered the landscape of public\ndiscourse, creating both opportunities for enhanced civic participation and\nrisks of deepening social divisions. Prevalent approaches to studying online\npolarization have been limited by a methodological disconnect: mathematical\nmodels excel at formal analysis but lack linguistic realism, while language\nmodel-based simulations capture natural discourse but often sacrifice\nanalytical precision. This paper introduces an innovative computational\nframework that synthesizes these approaches by embedding formal opinion\ndynamics principles within LLM-based artificial agents, enabling both rigorous\nmathematical analysis and naturalistic social interactions. We validate our\nframework through comprehensive offline testing and experimental evaluation\nwith 122 human participants engaging in a controlled social network\nenvironment. The results demonstrate our ability to systematically investigate\npolarization mechanisms while preserving ecological validity. Our findings\nreveal how polarized environments shape user perceptions and behavior:\nparticipants exposed to polarized discussions showed markedly increased\nsensitivity to emotional content and group affiliations, while perceiving\nreduced uncertainty in the agents' positions. By combining mathematical\nprecision with natural language capabilities, our framework opens new avenues\nfor investigating social media phenomena through controlled experimentation.\nThis methodological advancement allows researchers to bridge the gap between\ntheoretical models and empirical observations, offering unprecedented\nopportunities to study the causal mechanisms underlying online opinion\ndynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online social networks have dramatically altered the landscape of public\ndiscourse, creating both opportunities for enhanced civic participation and\nrisks of deepening social divisions. Prevalent approaches to studying online\npolarization have been limited by a methodological disconnect: mathematical\nmodels excel at formal analysis but lack linguistic realism, while language\nmodel-based simulations capture natural discourse but often sacrifice\nanalytical precision. This paper introduces an innovative computational\nframework that synthesizes these approaches by embedding formal opinion\ndynamics principles within LLM-based artificial agents, enabling both rigorous\nmathematical analysis and naturalistic social interactions. We validate our\nframework through comprehensive offline testing and experimental evaluation\nwith 122 human participants engaging in a controlled social network\nenvironment. The results demonstrate our ability to systematically investigate\npolarization mechanisms while preserving ecological validity. Our findings\nreveal how polarized environments shape user perceptions and behavior:\nparticipants exposed to polarized discussions showed markedly increased\nsensitivity to emotional content and group affiliations, while perceiving\nreduced uncertainty in the agents' positions. By combining mathematical\nprecision with natural language capabilities, our framework opens new avenues\nfor investigating social media phenomena through controlled experimentation.\nThis methodological advancement allows researchers to bridge the gap between\ntheoretical models and empirical observations, offering unprecedented\nopportunities to study the causal mechanisms underlying online opinion\ndynamics."
                },
                "authors": [
                    {
                        "name": "Tim Donkers"
                    },
                    {
                        "name": "Jrgen Ziegler"
                    }
                ],
                "author_detail": {
                    "name": "Jrgen Ziegler"
                },
                "author": "Jrgen Ziegler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01340v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01340v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12627v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12627v2",
                "updated": "2025-06-11T10:21:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    10,
                    21,
                    31,
                    2,
                    162,
                    0
                ],
                "published": "2025-05-19T02:20:46Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    2,
                    20,
                    46,
                    0,
                    139,
                    0
                ],
                "title": "Efficient Heuristics Generation for Solving Combinatorial Optimization\n  Problems Using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Heuristics Generation for Solving Combinatorial Optimization\n  Problems Using Large Language Models"
                },
                "summary": "Recent studies exploited Large Language Models (LLMs) to autonomously\ngenerate heuristics for solving Combinatorial Optimization Problems (COPs), by\nprompting LLMs to first provide search directions and then derive heuristics\naccordingly. However, the absence of task-specific knowledge in prompts often\nleads LLMs to provide unspecific search directions, obstructing the derivation\nof well-performing heuristics. Moreover, evaluating the derived heuristics\nremains resource-intensive, especially for those semantically equivalent ones,\noften requiring omissible resource expenditure. To enable LLMs to provide\nspecific search directions, we propose the Hercules algorithm, which leverages\nour designed Core Abstraction Prompting (CAP) method to abstract the core\ncomponents from elite heuristics and incorporate them as prior knowledge in\nprompts. We theoretically prove the effectiveness of CAP in reducing\nunspecificity and provide empirical results in this work. To reduce computing\nresources required for evaluating the derived heuristics, we propose few-shot\nPerformance Prediction Prompting (PPP), a first-of-its-kind method for the\nHeuristic Generation (HG) task. PPP leverages LLMs to predict the fitness\nvalues of newly derived heuristics by analyzing their semantic similarity to\npreviously evaluated ones. We further develop two tailored mechanisms for PPP\nto enhance predictive accuracy and determine unreliable predictions,\nrespectively. The use of PPP makes Hercules more resource-efficient and we name\nthis variant Hercules-P. Extensive experiments across four HG tasks, five COPs,\nand eight LLMs demonstrate that Hercules outperforms the state-of-the-art\nLLM-based HG algorithms, while Hercules-P excels at minimizing required\ncomputing resources. In addition, we illustrate the effectiveness of CAP, PPP,\nand the other proposed mechanisms by conducting relevant ablation studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies exploited Large Language Models (LLMs) to autonomously\ngenerate heuristics for solving Combinatorial Optimization Problems (COPs), by\nprompting LLMs to first provide search directions and then derive heuristics\naccordingly. However, the absence of task-specific knowledge in prompts often\nleads LLMs to provide unspecific search directions, obstructing the derivation\nof well-performing heuristics. Moreover, evaluating the derived heuristics\nremains resource-intensive, especially for those semantically equivalent ones,\noften requiring omissible resource expenditure. To enable LLMs to provide\nspecific search directions, we propose the Hercules algorithm, which leverages\nour designed Core Abstraction Prompting (CAP) method to abstract the core\ncomponents from elite heuristics and incorporate them as prior knowledge in\nprompts. We theoretically prove the effectiveness of CAP in reducing\nunspecificity and provide empirical results in this work. To reduce computing\nresources required for evaluating the derived heuristics, we propose few-shot\nPerformance Prediction Prompting (PPP), a first-of-its-kind method for the\nHeuristic Generation (HG) task. PPP leverages LLMs to predict the fitness\nvalues of newly derived heuristics by analyzing their semantic similarity to\npreviously evaluated ones. We further develop two tailored mechanisms for PPP\nto enhance predictive accuracy and determine unreliable predictions,\nrespectively. The use of PPP makes Hercules more resource-efficient and we name\nthis variant Hercules-P. Extensive experiments across four HG tasks, five COPs,\nand eight LLMs demonstrate that Hercules outperforms the state-of-the-art\nLLM-based HG algorithms, while Hercules-P excels at minimizing required\ncomputing resources. In addition, we illustrate the effectiveness of CAP, PPP,\nand the other proposed mechanisms by conducting relevant ablation studies."
                },
                "authors": [
                    {
                        "name": "Xuan Wu"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Chunguo Wu"
                    },
                    {
                        "name": "Lijie Wen"
                    },
                    {
                        "name": "Chunyan Miao"
                    },
                    {
                        "name": "Yubin Xiao"
                    },
                    {
                        "name": "You Zhou"
                    }
                ],
                "author_detail": {
                    "name": "You Zhou"
                },
                "author": "You Zhou",
                "arxiv_comment": "Accepted by SIGKDD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12627v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12627v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09581v1",
                "updated": "2025-06-11T10:19:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    10,
                    19,
                    49,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T10:19:49Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    10,
                    19,
                    49,
                    2,
                    162,
                    0
                ],
                "title": "Integrating Quantized LLMs into Robotics Systems as Edge AI to Leverage\n  their Natural Language Processing Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Quantized LLMs into Robotics Systems as Edge AI to Leverage\n  their Natural Language Processing Capabilities"
                },
                "summary": "Large Language Models (LLMs) have experienced great advancements in the last\nyear resulting in an increase of these models in several fields to face natural\nlanguage tasks. The integration of these models in robotics can also help to\nimprove several aspects such as human-robot interaction, navigation, planning\nand decision-making. Therefore, this paper introduces llama\\_ros, a tool\ndesigned to integrate quantized Large Language Models (LLMs) into robotic\nsystems using ROS 2. Leveraging llama.cpp, a highly optimized runtime engine,\nllama\\_ros enables the efficient execution of quantized LLMs as edge artificial\nintelligence (AI) in robotics systems with resource-constrained environments,\naddressing the challenges of computational efficiency and memory limitations.\nBy deploying quantized LLMs, llama\\_ros empowers robots to leverage the natural\nlanguage understanding and generation for enhanced decision-making and\ninteraction which can be paired with prompt engineering, knowledge graphs,\nontologies or other tools to improve the capabilities of autonomous robots.\nAdditionally, this paper provides insights into some use cases of using\nllama\\_ros for planning and explainability in robotics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have experienced great advancements in the last\nyear resulting in an increase of these models in several fields to face natural\nlanguage tasks. The integration of these models in robotics can also help to\nimprove several aspects such as human-robot interaction, navigation, planning\nand decision-making. Therefore, this paper introduces llama\\_ros, a tool\ndesigned to integrate quantized Large Language Models (LLMs) into robotic\nsystems using ROS 2. Leveraging llama.cpp, a highly optimized runtime engine,\nllama\\_ros enables the efficient execution of quantized LLMs as edge artificial\nintelligence (AI) in robotics systems with resource-constrained environments,\naddressing the challenges of computational efficiency and memory limitations.\nBy deploying quantized LLMs, llama\\_ros empowers robots to leverage the natural\nlanguage understanding and generation for enhanced decision-making and\ninteraction which can be paired with prompt engineering, knowledge graphs,\nontologies or other tools to improve the capabilities of autonomous robots.\nAdditionally, this paper provides insights into some use cases of using\nllama\\_ros for planning and explainability in robotics."
                },
                "authors": [
                    {
                        "name": "Miguel . Gonzlez-Santamarta"
                    },
                    {
                        "name": "Francisco J. Rodrguez-Lera"
                    },
                    {
                        "name": "David Sobrn-Hidalgo"
                    },
                    {
                        "name": "ngel Manuel Guerrero-Higueras"
                    },
                    {
                        "name": "Vicente Matelln-Olivera"
                    }
                ],
                "author_detail": {
                    "name": "Vicente Matelln-Olivera"
                },
                "author": "Vicente Matelln-Olivera",
                "arxiv_comment": "10 pages, 4 figures, Submitted to 3rd edition of the Workshop on\n  Ontologies and Standards for Robotics and Automation (WOSRA) at ICRA 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01067v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01067v3",
                "updated": "2025-06-11T10:15:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    10,
                    15,
                    37,
                    2,
                    162,
                    0
                ],
                "published": "2024-07-01T08:17:19Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    8,
                    17,
                    19,
                    0,
                    183,
                    0
                ],
                "title": "Human-like object concept representations emerge naturally in multimodal\n  large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-like object concept representations emerge naturally in multimodal\n  large language models"
                },
                "summary": "Understanding how humans conceptualize and categorize natural objects offers\ncritical insights into perception and cognition. With the advent of Large\nLanguage Models (LLMs), a key question arises: can these models develop\nhuman-like object representations from linguistic and multimodal data? In this\nstudy, we combined behavioral and neuroimaging analyses to explore the\nrelationship between object concept representations in LLMs and human\ncognition. We collected 4.7 million triplet judgments from LLMs and Multimodal\nLLMs (MLLMs) to derive low-dimensional embeddings that capture the similarity\nstructure of 1,854 natural objects. The resulting 66-dimensional embeddings\nwere stable, predictive, and exhibited semantic clustering similar to human\nmental representations. Remarkably, the dimensions underlying these embeddings\nwere interpretable, suggesting that LLMs and MLLMs develop human-like\nconceptual representations of objects. Further analysis showed strong alignment\nbetween model embeddings and neural activity patterns in brain regions such as\nEBA, PPA, RSC, and FFA. This provides compelling evidence that the object\nrepresentations in LLMs, while not identical to human ones, share fundamental\nsimilarities that reflect key aspects of human conceptual knowledge. Our\nfindings advance the understanding of machine intelligence and inform the\ndevelopment of more human-like artificial cognitive systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding how humans conceptualize and categorize natural objects offers\ncritical insights into perception and cognition. With the advent of Large\nLanguage Models (LLMs), a key question arises: can these models develop\nhuman-like object representations from linguistic and multimodal data? In this\nstudy, we combined behavioral and neuroimaging analyses to explore the\nrelationship between object concept representations in LLMs and human\ncognition. We collected 4.7 million triplet judgments from LLMs and Multimodal\nLLMs (MLLMs) to derive low-dimensional embeddings that capture the similarity\nstructure of 1,854 natural objects. The resulting 66-dimensional embeddings\nwere stable, predictive, and exhibited semantic clustering similar to human\nmental representations. Remarkably, the dimensions underlying these embeddings\nwere interpretable, suggesting that LLMs and MLLMs develop human-like\nconceptual representations of objects. Further analysis showed strong alignment\nbetween model embeddings and neural activity patterns in brain regions such as\nEBA, PPA, RSC, and FFA. This provides compelling evidence that the object\nrepresentations in LLMs, while not identical to human ones, share fundamental\nsimilarities that reflect key aspects of human conceptual knowledge. Our\nfindings advance the understanding of machine intelligence and inform the\ndevelopment of more human-like artificial cognitive systems."
                },
                "authors": [
                    {
                        "name": "Changde Du"
                    },
                    {
                        "name": "Kaicheng Fu"
                    },
                    {
                        "name": "Bincheng Wen"
                    },
                    {
                        "name": "Yi Sun"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Ying Gao"
                    },
                    {
                        "name": "Shengpei Wang"
                    },
                    {
                        "name": "Chuncheng Zhang"
                    },
                    {
                        "name": "Jinpeng Li"
                    },
                    {
                        "name": "Shuang Qiu"
                    },
                    {
                        "name": "Le Chang"
                    },
                    {
                        "name": "Huiguang He"
                    }
                ],
                "author_detail": {
                    "name": "Huiguang He"
                },
                "author": "Huiguang He",
                "arxiv_doi": "10.1038/s42256-025-01049-z",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1038/s42256-025-01049-z",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.01067v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01067v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published on Nature Machine Intelligence",
                "arxiv_journal_ref": "Nature Machine Intelligence, 2025",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07615v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07615v3",
                "updated": "2025-06-11T10:13:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    10,
                    13,
                    33,
                    2,
                    162,
                    0
                ],
                "published": "2024-09-11T20:55:12Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    20,
                    55,
                    12,
                    2,
                    255,
                    0
                ],
                "title": "MOSAIC: Multiple Observers Spotting AI Content",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MOSAIC: Multiple Observers Spotting AI Content"
                },
                "summary": "The dissemination of Large Language Models (LLMs), trained at scale, and\nendowed with powerful text-generating abilities, has made it easier for all to\nproduce harmful, toxic, faked or forged content. In response, various proposals\nhave been made to automatically discriminate artificially generated from\nhuman-written texts, typically framing the problem as a binary classification\nproblem. Early approaches evaluate an input document with a well-chosen\ndetector LLM, assuming that low-perplexity scores reliably signal machine-made\ncontent. More recent systems instead consider two LLMs and compare their\nprobability distributions over the document to further discriminate when\nperplexity alone cannot. However, using a fixed pair of models can induce\nbrittleness in performance. We extend these approaches to the ensembling of\nseveral LLMs and derive a new, theoretically grounded approach to combine their\nrespective strengths. Our experiments, conducted with various generator LLMs,\nindicate that this approach effectively leverages the strengths of each model,\nresulting in robust detection performance across multiple domains. Our code and\ndata are available at https://github.com/BaggerOfWords/MOSAIC .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The dissemination of Large Language Models (LLMs), trained at scale, and\nendowed with powerful text-generating abilities, has made it easier for all to\nproduce harmful, toxic, faked or forged content. In response, various proposals\nhave been made to automatically discriminate artificially generated from\nhuman-written texts, typically framing the problem as a binary classification\nproblem. Early approaches evaluate an input document with a well-chosen\ndetector LLM, assuming that low-perplexity scores reliably signal machine-made\ncontent. More recent systems instead consider two LLMs and compare their\nprobability distributions over the document to further discriminate when\nperplexity alone cannot. However, using a fixed pair of models can induce\nbrittleness in performance. We extend these approaches to the ensembling of\nseveral LLMs and derive a new, theoretically grounded approach to combine their\nrespective strengths. Our experiments, conducted with various generator LLMs,\nindicate that this approach effectively leverages the strengths of each model,\nresulting in robust detection performance across multiple domains. Our code and\ndata are available at https://github.com/BaggerOfWords/MOSAIC ."
                },
                "authors": [
                    {
                        "name": "Matthieu Dubois"
                    },
                    {
                        "name": "Franois Yvon"
                    },
                    {
                        "name": "Pablo Piantanida"
                    }
                ],
                "author_detail": {
                    "name": "Pablo Piantanida"
                },
                "author": "Pablo Piantanida",
                "arxiv_comment": "ACL 2025 Findings, code can be found at\n  https://github.com/BaggerOfWords/MOSAIC",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07615v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07615v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09566v1",
                "updated": "2025-06-11T09:58:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    9,
                    58,
                    14,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T09:58:14Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    9,
                    58,
                    14,
                    2,
                    162,
                    0
                ],
                "title": "From Symbolic to Neural and Back: Exploring Knowledge Graph-Large\n  Language Model Synergies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Symbolic to Neural and Back: Exploring Knowledge Graph-Large\n  Language Model Synergies"
                },
                "summary": "Integrating structured knowledge from Knowledge Graphs (KGs) into Large\nLanguage Models (LLMs) enhances factual grounding and reasoning capabilities.\nThis survey paper systematically examines the synergy between KGs and LLMs,\ncategorizing existing approaches into two main groups: KG-enhanced LLMs, which\nimprove reasoning, reduce hallucinations, and enable complex question\nanswering; and LLM-augmented KGs, which facilitate KG construction, completion,\nand querying. Through comprehensive analysis, we identify critical gaps and\nhighlight the mutual benefits of structured knowledge integration. Compared to\nexisting surveys, our study uniquely emphasizes scalability, computational\nefficiency, and data quality. Finally, we propose future research directions,\nincluding neuro-symbolic integration, dynamic KG updating, data reliability,\nand ethical considerations, paving the way for intelligent systems capable of\nmanaging more complex real-world knowledge tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating structured knowledge from Knowledge Graphs (KGs) into Large\nLanguage Models (LLMs) enhances factual grounding and reasoning capabilities.\nThis survey paper systematically examines the synergy between KGs and LLMs,\ncategorizing existing approaches into two main groups: KG-enhanced LLMs, which\nimprove reasoning, reduce hallucinations, and enable complex question\nanswering; and LLM-augmented KGs, which facilitate KG construction, completion,\nand querying. Through comprehensive analysis, we identify critical gaps and\nhighlight the mutual benefits of structured knowledge integration. Compared to\nexisting surveys, our study uniquely emphasizes scalability, computational\nefficiency, and data quality. Finally, we propose future research directions,\nincluding neuro-symbolic integration, dynamic KG updating, data reliability,\nand ethical considerations, paving the way for intelligent systems capable of\nmanaging more complex real-world knowledge tasks."
                },
                "authors": [
                    {
                        "name": "Bla krlj"
                    },
                    {
                        "name": "Boshko Koloski"
                    },
                    {
                        "name": "Senja Pollak"
                    },
                    {
                        "name": "Nada Lavra"
                    }
                ],
                "author_detail": {
                    "name": "Nada Lavra"
                },
                "author": "Nada Lavra",
                "arxiv_comment": "To-appear as a book chapter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09565v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09565v1",
                "updated": "2025-06-11T09:56:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    9,
                    56,
                    39,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T09:56:39Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    9,
                    56,
                    39,
                    2,
                    162,
                    0
                ],
                "title": "SemanticSplat: Feed-Forward 3D Scene Understanding with Language-Aware\n  Gaussian Fields",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SemanticSplat: Feed-Forward 3D Scene Understanding with Language-Aware\n  Gaussian Fields"
                },
                "summary": "Holistic 3D scene understanding, which jointly models geometry, appearance,\nand semantics, is crucial for applications like augmented reality and robotic\ninteraction. Existing feed-forward 3D scene understanding methods (e.g., LSM)\nare limited to extracting language-based semantics from scenes, failing to\nachieve holistic scene comprehension. Additionally, they suffer from\nlow-quality geometry reconstruction and noisy artifacts. In contrast, per-scene\noptimization methods rely on dense input views, which reduces practicality and\nincreases complexity during deployment. In this paper, we propose\nSemanticSplat, a feed-forward semantic-aware 3D reconstruction method, which\nunifies 3D Gaussians with latent semantic attributes for joint\ngeometry-appearance-semantics modeling. To predict the semantic anisotropic\nGaussians, SemanticSplat fuses diverse feature fields (e.g., LSeg, SAM) with a\ncost volume representation that stores cross-view feature similarities,\nenhancing coherent and accurate scene comprehension. Leveraging a two-stage\ndistillation framework, SemanticSplat reconstructs a holistic multi-modal\nsemantic feature field from sparse-view images. Experiments demonstrate the\neffectiveness of our method for 3D scene understanding tasks like promptable\nand open-vocabulary segmentation. Video results are available at\nhttps://semanticsplat.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Holistic 3D scene understanding, which jointly models geometry, appearance,\nand semantics, is crucial for applications like augmented reality and robotic\ninteraction. Existing feed-forward 3D scene understanding methods (e.g., LSM)\nare limited to extracting language-based semantics from scenes, failing to\nachieve holistic scene comprehension. Additionally, they suffer from\nlow-quality geometry reconstruction and noisy artifacts. In contrast, per-scene\noptimization methods rely on dense input views, which reduces practicality and\nincreases complexity during deployment. In this paper, we propose\nSemanticSplat, a feed-forward semantic-aware 3D reconstruction method, which\nunifies 3D Gaussians with latent semantic attributes for joint\ngeometry-appearance-semantics modeling. To predict the semantic anisotropic\nGaussians, SemanticSplat fuses diverse feature fields (e.g., LSeg, SAM) with a\ncost volume representation that stores cross-view feature similarities,\nenhancing coherent and accurate scene comprehension. Leveraging a two-stage\ndistillation framework, SemanticSplat reconstructs a holistic multi-modal\nsemantic feature field from sparse-view images. Experiments demonstrate the\neffectiveness of our method for 3D scene understanding tasks like promptable\nand open-vocabulary segmentation. Video results are available at\nhttps://semanticsplat.github.io."
                },
                "authors": [
                    {
                        "name": "Qijing Li"
                    },
                    {
                        "name": "Jingxiang Sun"
                    },
                    {
                        "name": "Liang An"
                    },
                    {
                        "name": "Zhaoqi Su"
                    },
                    {
                        "name": "Hongwen Zhang"
                    },
                    {
                        "name": "Yebin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yebin Liu"
                },
                "author": "Yebin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09565v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09565v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17304v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17304v2",
                "updated": "2025-06-11T09:55:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    9,
                    55,
                    38,
                    2,
                    162,
                    0
                ],
                "published": "2024-11-26T10:52:08Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    10,
                    52,
                    8,
                    1,
                    331,
                    0
                ],
                "title": "Meaningless is better: hashing bias-inducing words in LLM prompts\n  improves performance in logical reasoning and statistical learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meaningless is better: hashing bias-inducing words in LLM prompts\n  improves performance in logical reasoning and statistical learning"
                },
                "summary": "This paper introduces a novel method, referred to as \"hashing\", which\ninvolves masking potentially bias-inducing words in large language models\n(LLMs) with hash-like meaningless identifiers to reduce cognitive biases and\nreliance on external knowledge. The method was tested across three sets of\nexperiments involving a total of 490 prompts. Statistical analysis using\nchi-square tests showed significant improvements in all tested scenarios, which\ncovered LLama, ChatGPT, Copilot, Gemini and Mixtral models. In the first\nexperiment, hashing decreased the fallacy rate in a modified version of the\n\"Linda\" problem aimed at evaluating susceptibility to cognitive biases. In the\nsecond experiment, it improved LLM results on the frequent itemset extraction\ntask. In the third experiment, we found hashing is also effective when the\nLinda problem is presented in a tabular format rather than text, indicating\nthat the technique works across various input representations. Overall, the\nmethod was shown to improve bias reduction and incorporation of external\nknowledge. Despite bias reduction, hallucination rates were inconsistently\nreduced across types of LLM models. These findings suggest that masking\nbias-inducing terms can improve LLM performance, although its effectiveness is\nmodel- and task-dependent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel method, referred to as \"hashing\", which\ninvolves masking potentially bias-inducing words in large language models\n(LLMs) with hash-like meaningless identifiers to reduce cognitive biases and\nreliance on external knowledge. The method was tested across three sets of\nexperiments involving a total of 490 prompts. Statistical analysis using\nchi-square tests showed significant improvements in all tested scenarios, which\ncovered LLama, ChatGPT, Copilot, Gemini and Mixtral models. In the first\nexperiment, hashing decreased the fallacy rate in a modified version of the\n\"Linda\" problem aimed at evaluating susceptibility to cognitive biases. In the\nsecond experiment, it improved LLM results on the frequent itemset extraction\ntask. In the third experiment, we found hashing is also effective when the\nLinda problem is presented in a tabular format rather than text, indicating\nthat the technique works across various input representations. Overall, the\nmethod was shown to improve bias reduction and incorporation of external\nknowledge. Despite bias reduction, hallucination rates were inconsistently\nreduced across types of LLM models. These findings suggest that masking\nbias-inducing terms can improve LLM performance, although its effectiveness is\nmodel- and task-dependent."
                },
                "authors": [
                    {
                        "name": "Milena Chadimov"
                    },
                    {
                        "name": "Eduard Jurek"
                    },
                    {
                        "name": "Tom Kliegr"
                    }
                ],
                "author_detail": {
                    "name": "Tom Kliegr"
                },
                "author": "Tom Kliegr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17304v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17304v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08184v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08184v2",
                "updated": "2025-06-11T09:53:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    9,
                    53,
                    49,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-09T19:49:11Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    19,
                    49,
                    11,
                    0,
                    160,
                    0
                ],
                "title": "Unable to Forget: Proactive lnterference Reveals Working Memory Limits\n  in LLMs Beyond Context Length",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unable to Forget: Proactive lnterference Reveals Working Memory Limits\n  in LLMs Beyond Context Length"
                },
                "summary": "Information retrieval in Large Language Models (LLMs) is increasingly\nrecognized as intertwined with generation capabilities rather than mere lookup.\nWhile longer contexts are often assumed to improve retrieval, the effects of\nintra-context interference remain understudied. To address this, we adapt the\nproactive interference (PI) paradigm from cognitive science, where earlier\ninformation disrupts recall of newer updates. In humans, susceptibility to such\ninterference is inversely linked to working memory capacity. We introduce\nPI-LLM, an evaluation that sequentially streams semantically related key-value\nupdates and queries only the final values. Although these final values are\nclearly positioned just before the query, LLM retrieval accuracy declines\nlog-linearly toward zero as interference accumulates; errors arise from\nretrieving previously overwritten values. Attempts to mitigate interference via\nprompt engineering (e.g., instructing models to ignore earlier input) yield\nlimited success. These findings reveal a fundamental constraint on LLMs'\nability to disentangle interference and flexibly manipulate information,\nsuggesting a working memory bottleneck beyond mere context access. This calls\nfor approaches that strengthen models' ability to suppress irrelevant content\nduring retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information retrieval in Large Language Models (LLMs) is increasingly\nrecognized as intertwined with generation capabilities rather than mere lookup.\nWhile longer contexts are often assumed to improve retrieval, the effects of\nintra-context interference remain understudied. To address this, we adapt the\nproactive interference (PI) paradigm from cognitive science, where earlier\ninformation disrupts recall of newer updates. In humans, susceptibility to such\ninterference is inversely linked to working memory capacity. We introduce\nPI-LLM, an evaluation that sequentially streams semantically related key-value\nupdates and queries only the final values. Although these final values are\nclearly positioned just before the query, LLM retrieval accuracy declines\nlog-linearly toward zero as interference accumulates; errors arise from\nretrieving previously overwritten values. Attempts to mitigate interference via\nprompt engineering (e.g., instructing models to ignore earlier input) yield\nlimited success. These findings reveal a fundamental constraint on LLMs'\nability to disentangle interference and flexibly manipulate information,\nsuggesting a working memory bottleneck beyond mere context access. This calls\nfor approaches that strengthen models' ability to suppress irrelevant content\nduring retrieval."
                },
                "authors": [
                    {
                        "name": "Chupei Wang"
                    },
                    {
                        "name": "Jiaqiu Vince Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jiaqiu Vince Sun"
                },
                "arxiv_affiliation": "New York University",
                "author": "Jiaqiu Vince Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08184v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08184v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08534v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08534v3",
                "updated": "2025-06-11T09:53:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    9,
                    53,
                    35,
                    2,
                    162,
                    0
                ],
                "published": "2025-04-11T13:43:02Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    13,
                    43,
                    2,
                    4,
                    101,
                    0
                ],
                "title": "An FPGA Compiler for On-the-Fly Adaptive CNN Deployment and\n  Reconfiguration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An FPGA Compiler for On-the-Fly Adaptive CNN Deployment and\n  Reconfiguration"
                },
                "summary": "We introduce ForgeMorph, a full-stack compiler for adaptive CNN deployment on\nFPGAs, combining design-time optimization with runtime reconfigurability. At\ncompile time, the NeuroForge engine performs constraint-driven design space\nexploration, generating RTL mappings that are Pareto-optimal with respect to\nuser-defined latency and resource budgets. Unlike existing FPGA compilers,\nwhich rely on static scheduling and manual tuning, NeuroForge leverages\nanalytical performance models and multi-objective genetic algorithms to\nefficiently search large configuration spaces and propose highly optimized\nhardware implementations. At runtime, the NeuroMorph module enables dynamic\nreconfiguration of network width and depth without requiring redeployment. This\nis made possible by a novel training strategy, DistillCycle, which jointly\ntrains the full model and its subnetworks using hierarchical knowledge\ndistillation. As a result, each execution path maintains accuracy even under\naggressive resource and power constraints. We demonstrate Forge-Morph on the\nZynq-7100 using custom and benchmark models including MobileNetV2, ResNet-50,\nSqueezeNet, and YOLOv5. The system achieves up to 50x latency reduction and 32%\nlower power consumption at runtime, while matching or exceeding the efficiency\nof state-of-the-art compilers. ForgeMorph offers a unified solution for\ndeployment scenarios that demand flexibility, performance, and hardware\nefficiency",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ForgeMorph, a full-stack compiler for adaptive CNN deployment on\nFPGAs, combining design-time optimization with runtime reconfigurability. At\ncompile time, the NeuroForge engine performs constraint-driven design space\nexploration, generating RTL mappings that are Pareto-optimal with respect to\nuser-defined latency and resource budgets. Unlike existing FPGA compilers,\nwhich rely on static scheduling and manual tuning, NeuroForge leverages\nanalytical performance models and multi-objective genetic algorithms to\nefficiently search large configuration spaces and propose highly optimized\nhardware implementations. At runtime, the NeuroMorph module enables dynamic\nreconfiguration of network width and depth without requiring redeployment. This\nis made possible by a novel training strategy, DistillCycle, which jointly\ntrains the full model and its subnetworks using hierarchical knowledge\ndistillation. As a result, each execution path maintains accuracy even under\naggressive resource and power constraints. We demonstrate Forge-Morph on the\nZynq-7100 using custom and benchmark models including MobileNetV2, ResNet-50,\nSqueezeNet, and YOLOv5. The system achieves up to 50x latency reduction and 32%\nlower power consumption at runtime, while matching or exceeding the efficiency\nof state-of-the-art compilers. ForgeMorph offers a unified solution for\ndeployment scenarios that demand flexibility, performance, and hardware\nefficiency"
                },
                "authors": [
                    {
                        "name": "Alaa Mazouz"
                    },
                    {
                        "name": "Duc Han Le"
                    },
                    {
                        "name": "Van-Tam Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Van-Tam Nguyen"
                },
                "author": "Van-Tam Nguyen",
                "arxiv_comment": "Submitted to IEEE Transactions on Computer-Aided Design of Integrated\n  Circuits and Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08534v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08534v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07507v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07507v2",
                "updated": "2025-06-11T09:50:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    9,
                    50,
                    42,
                    2,
                    162,
                    0
                ],
                "published": "2024-09-11T12:27:41Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    12,
                    27,
                    41,
                    2,
                    255,
                    0
                ],
                "title": "Traceable LLM-based validation of statements in knowledge graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traceable LLM-based validation of statements in knowledge graphs"
                },
                "summary": "This article presents a method for verifying RDF triples using LLMs, with an\nemphasis on providing traceable arguments. Because the LLMs cannot currently\nreliably identify the origin of the information used to construct the response\nto the user prompt, our approach is to avoid using internal LLM factual\nknowledge altogether. Instead, verified RDF statements are compared to chunks\nof external documents retrieved through a web search or Wikipedia. To assess\nthe possible application of this retrieval augmented generation (RAG) workflow\non biosciences content, we evaluated 1,719 positive statements from the BioRED\ndataset and the same number of newly generated negative statements. The\nresulting precision is 88 %, and recall is 44 %. This indicates that the method\nrequires human oversight. We also evaluated the method on the SNLI dataset,\nwhich allowed us to compare our approach with models specifically tuned for the\nnatural language inference task. We demonstrate the method on Wikidata, where a\nSPARQL query is used to automatically retrieve statements needing verification.\nOverall, the results suggest that LLMs could be used for large-scale\nverification of statements in KGs, a task previously unfeasible due to human\nannotation costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article presents a method for verifying RDF triples using LLMs, with an\nemphasis on providing traceable arguments. Because the LLMs cannot currently\nreliably identify the origin of the information used to construct the response\nto the user prompt, our approach is to avoid using internal LLM factual\nknowledge altogether. Instead, verified RDF statements are compared to chunks\nof external documents retrieved through a web search or Wikipedia. To assess\nthe possible application of this retrieval augmented generation (RAG) workflow\non biosciences content, we evaluated 1,719 positive statements from the BioRED\ndataset and the same number of newly generated negative statements. The\nresulting precision is 88 %, and recall is 44 %. This indicates that the method\nrequires human oversight. We also evaluated the method on the SNLI dataset,\nwhich allowed us to compare our approach with models specifically tuned for the\nnatural language inference task. We demonstrate the method on Wikidata, where a\nSPARQL query is used to automatically retrieve statements needing verification.\nOverall, the results suggest that LLMs could be used for large-scale\nverification of statements in KGs, a task previously unfeasible due to human\nannotation costs."
                },
                "authors": [
                    {
                        "name": "Daniel Adam"
                    },
                    {
                        "name": "Tom Kliegr"
                    }
                ],
                "author_detail": {
                    "name": "Tom Kliegr"
                },
                "author": "Tom Kliegr",
                "arxiv_doi": "10.1016/j.ipm.2025.104128",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.ipm.2025.104128",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.07507v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07507v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Information Processing & Management 62 (2025): 104128",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06987v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06987v3",
                "updated": "2025-06-12T01:49:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    1,
                    49,
                    53,
                    3,
                    163,
                    0
                ],
                "published": "2025-05-11T14:13:58Z",
                "published_parsed": [
                    2025,
                    5,
                    11,
                    14,
                    13,
                    58,
                    6,
                    131,
                    0
                ],
                "title": "Convert Language Model into a Value-based Strategic Planner",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Convert Language Model into a Value-based Strategic Planner"
                },
                "summary": "Emotional support conversation (ESC) aims to alleviate the emotional distress\nof individuals through effective conversations. Although large language models\n(LLMs) have obtained remarkable progress on ESC, most of these studies might\nnot define the diagram from the state model perspective, therefore providing a\nsuboptimal solution for long-term satisfaction. To address such an issue, we\nleverage the Q-learning on LLMs, and propose a framework called straQ*. Our\nframework allows a plug-and-play LLM to bootstrap the planning during ESC,\ndetermine the optimal strategy based on long-term returns, and finally guide\nthe LLM to response. Substantial experiments on ESC datasets suggest that\nstraQ* outperforms many baselines, including direct inference, self-refine,\nchain of thought, finetuning, and finite state machines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emotional support conversation (ESC) aims to alleviate the emotional distress\nof individuals through effective conversations. Although large language models\n(LLMs) have obtained remarkable progress on ESC, most of these studies might\nnot define the diagram from the state model perspective, therefore providing a\nsuboptimal solution for long-term satisfaction. To address such an issue, we\nleverage the Q-learning on LLMs, and propose a framework called straQ*. Our\nframework allows a plug-and-play LLM to bootstrap the planning during ESC,\ndetermine the optimal strategy based on long-term returns, and finally guide\nthe LLM to response. Substantial experiments on ESC datasets suggest that\nstraQ* outperforms many baselines, including direct inference, self-refine,\nchain of thought, finetuning, and finite state machines."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Wang"
                    },
                    {
                        "name": "Yue Zhao"
                    },
                    {
                        "name": "Qingqing Gu"
                    },
                    {
                        "name": "Zhonglin Jiang"
                    },
                    {
                        "name": "Xiaokai Chen"
                    },
                    {
                        "name": "Yong Chen"
                    },
                    {
                        "name": "Luo Ji"
                    }
                ],
                "author_detail": {
                    "name": "Luo Ji"
                },
                "author": "Luo Ji",
                "arxiv_comment": "13 pages, 6 figures, Accepted by ACL 2025 Industry Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06987v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06987v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05342v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05342v5",
                "updated": "2025-06-11T09:49:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    9,
                    49,
                    6,
                    2,
                    162,
                    0
                ],
                "published": "2024-12-06T09:33:47Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    9,
                    33,
                    47,
                    4,
                    341,
                    0
                ],
                "title": "Multi-Party Supervised Fine-tuning of Language Models for Multi-Party\n  Dialogue Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Party Supervised Fine-tuning of Language Models for Multi-Party\n  Dialogue Generation"
                },
                "summary": "Large Language Models (LLM) are usually fine-tuned to participate in dyadic\nor two-party dialogues, which can not adapt well to multi-party dialogues\n(MPD), which hinders their applications in such scenarios including\nmulti-personal meetings, discussions and daily communication. Previous\nLLM-based researches mainly focus on the multi-agent framework, while their\nbase LLMs are still pairwisely fine-tuned. In this work, we design a\nmulti-party fine-tuning framework (MuPaS) for LLMs on the multi-party dialogue\ndatasets, and prove such a straightforward framework can let the LLM align with\nthe multi-party conversation style efficiently and effectively. We also design\ntwo training strategies which can convert MuPaS into the MPD simulator.\nSubstantial experiments show that MuPaS can achieve state-of-the-art\nmulti-party response, higher accuracy of the-next-speaker prediction, higher\nhuman and automatic evaluated utterance qualities, and can even generate\nreasonably with out-of-distribution scene, topic and role descriptions. The\nMuPaS framework bridges the LLM training with more complicated multi-party\napplications, such as conversation generation, virtual rehearsal or\nmeta-universe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLM) are usually fine-tuned to participate in dyadic\nor two-party dialogues, which can not adapt well to multi-party dialogues\n(MPD), which hinders their applications in such scenarios including\nmulti-personal meetings, discussions and daily communication. Previous\nLLM-based researches mainly focus on the multi-agent framework, while their\nbase LLMs are still pairwisely fine-tuned. In this work, we design a\nmulti-party fine-tuning framework (MuPaS) for LLMs on the multi-party dialogue\ndatasets, and prove such a straightforward framework can let the LLM align with\nthe multi-party conversation style efficiently and effectively. We also design\ntwo training strategies which can convert MuPaS into the MPD simulator.\nSubstantial experiments show that MuPaS can achieve state-of-the-art\nmulti-party response, higher accuracy of the-next-speaker prediction, higher\nhuman and automatic evaluated utterance qualities, and can even generate\nreasonably with out-of-distribution scene, topic and role descriptions. The\nMuPaS framework bridges the LLM training with more complicated multi-party\napplications, such as conversation generation, virtual rehearsal or\nmeta-universe."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Wang"
                    },
                    {
                        "name": "Ningyuan Xi"
                    },
                    {
                        "name": "Teng Chen"
                    },
                    {
                        "name": "Qingqing Gu"
                    },
                    {
                        "name": "Yue Zhao"
                    },
                    {
                        "name": "Xiaokai Chen"
                    },
                    {
                        "name": "Zhonglin Jiang"
                    },
                    {
                        "name": "Yong Chen"
                    },
                    {
                        "name": "Luo Ji"
                    }
                ],
                "author_detail": {
                    "name": "Luo Ji"
                },
                "author": "Luo Ji",
                "arxiv_comment": "Accepted by IJCNN 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05342v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05342v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09560v1",
                "updated": "2025-06-11T09:46:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    9,
                    46,
                    58,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T09:46:58Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    9,
                    46,
                    58,
                    2,
                    162,
                    0
                ],
                "title": "Towards Open Foundation Language Model and Corpus for Macedonian: A\n  Low-Resource Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Open Foundation Language Model and Corpus for Macedonian: A\n  Low-Resource Language"
                },
                "summary": "The increase in technological adoption worldwide comes with demands for novel\ntools to be used by the general population. Large Language Models (LLMs)\nprovide a great opportunity in this respect, but their capabilities remain\nlimited for low-resource languages, restricting applications in countries where\nsuch languages are spoken. We create several resources to facilitate the\nadoption of LLMs and to support research advancements for Macedonian. We\ncollect the largest Macedonian corpus to date, consisting of 40GB of textual\ndata and totaling 3.5B words. To support conversational applications, we\ncollect a 106k-instance instruction dataset, carefully built to be culturally\ngrounded. For evaluation, we construct a Macedonian evaluation suite covering\nseven benchmarks. Finally, we train domestic-yak, a state-of-the-art\n8B-parameter model, on our curated datasets and evaluate it against eight\nbaseline models using the newly constructed benchmark suite. Our model\noutperforms all existing models in the 8B parameter range across all\nbenchmarks, and achieves performance comparable to models up to 10x larger.\nFurthermore, a qualitative analysis with native speakers reveals that our model\nis preferred over larger counterparts, receiving higher ratings for grammatical\ncorrectness and cultural appropriateness. All datasets, code, and model weights\nare openly released, setting a foundation for advancing LLMs in similarly\nunderrepresented languages. These resources are publicly available at\ngithub.com/LVSTCK for source code, and at huggingface.co/LVSTCK for pretrained\nmodel weights and data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increase in technological adoption worldwide comes with demands for novel\ntools to be used by the general population. Large Language Models (LLMs)\nprovide a great opportunity in this respect, but their capabilities remain\nlimited for low-resource languages, restricting applications in countries where\nsuch languages are spoken. We create several resources to facilitate the\nadoption of LLMs and to support research advancements for Macedonian. We\ncollect the largest Macedonian corpus to date, consisting of 40GB of textual\ndata and totaling 3.5B words. To support conversational applications, we\ncollect a 106k-instance instruction dataset, carefully built to be culturally\ngrounded. For evaluation, we construct a Macedonian evaluation suite covering\nseven benchmarks. Finally, we train domestic-yak, a state-of-the-art\n8B-parameter model, on our curated datasets and evaluate it against eight\nbaseline models using the newly constructed benchmark suite. Our model\noutperforms all existing models in the 8B parameter range across all\nbenchmarks, and achieves performance comparable to models up to 10x larger.\nFurthermore, a qualitative analysis with native speakers reveals that our model\nis preferred over larger counterparts, receiving higher ratings for grammatical\ncorrectness and cultural appropriateness. All datasets, code, and model weights\nare openly released, setting a foundation for advancing LLMs in similarly\nunderrepresented languages. These resources are publicly available at\ngithub.com/LVSTCK for source code, and at huggingface.co/LVSTCK for pretrained\nmodel weights and data."
                },
                "authors": [
                    {
                        "name": "Stefan Krsteski"
                    },
                    {
                        "name": "Matea Tashkovska"
                    },
                    {
                        "name": "Borjan Sazdov"
                    },
                    {
                        "name": "Hristijan Gjoreski"
                    },
                    {
                        "name": "Branislav Gerazov"
                    }
                ],
                "author_detail": {
                    "name": "Branislav Gerazov"
                },
                "author": "Branislav Gerazov",
                "arxiv_comment": "Camera-ready version accepted at SlavNLP-2025@ACL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21298v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21298v2",
                "updated": "2025-06-11T09:42:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    9,
                    42,
                    27,
                    2,
                    162,
                    0
                ],
                "published": "2025-05-27T15:01:06Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    1,
                    6,
                    1,
                    147,
                    0
                ],
                "title": "Large Language Models Miss the Multi-Agent Mark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Miss the Multi-Agent Mark"
                },
                "summary": "Recent interest in Multi-Agent Systems of Large Language Models (MAS LLMs)\nhas led to an increase in frameworks leveraging multiple LLMs to tackle complex\ntasks. However, much of this literature appropriates the terminology of MAS\nwithout engaging with its foundational principles. In this position paper, we\nhighlight critical discrepancies between MAS theory and current MAS LLMs\nimplementations, focusing on four key areas: the social aspect of agency,\nenvironment design, coordination and communication protocols, and measuring\nemergent behaviours. Our position is that many MAS LLMs lack multi-agent\ncharacteristics such as autonomy, social interaction, and structured\nenvironments, and often rely on oversimplified, LLM-centric architectures. The\nfield may slow down and lose traction by revisiting problems the MAS literature\nhas already addressed. Therefore, we systematically analyse this issue and\noutline associated research opportunities; we advocate for better integrating\nestablished MAS concepts and more precise terminology to avoid\nmischaracterisation and missed opportunities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent interest in Multi-Agent Systems of Large Language Models (MAS LLMs)\nhas led to an increase in frameworks leveraging multiple LLMs to tackle complex\ntasks. However, much of this literature appropriates the terminology of MAS\nwithout engaging with its foundational principles. In this position paper, we\nhighlight critical discrepancies between MAS theory and current MAS LLMs\nimplementations, focusing on four key areas: the social aspect of agency,\nenvironment design, coordination and communication protocols, and measuring\nemergent behaviours. Our position is that many MAS LLMs lack multi-agent\ncharacteristics such as autonomy, social interaction, and structured\nenvironments, and often rely on oversimplified, LLM-centric architectures. The\nfield may slow down and lose traction by revisiting problems the MAS literature\nhas already addressed. Therefore, we systematically analyse this issue and\noutline associated research opportunities; we advocate for better integrating\nestablished MAS concepts and more precise terminology to avoid\nmischaracterisation and missed opportunities."
                },
                "authors": [
                    {
                        "name": "Emanuele La Malfa"
                    },
                    {
                        "name": "Gabriele La Malfa"
                    },
                    {
                        "name": "Samuele Marro"
                    },
                    {
                        "name": "Jie M. Zhang"
                    },
                    {
                        "name": "Elizabeth Black"
                    },
                    {
                        "name": "Michael Luck"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Michael Wooldridge"
                    }
                ],
                "author_detail": {
                    "name": "Michael Wooldridge"
                },
                "author": "Michael Wooldridge",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21298v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21298v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]