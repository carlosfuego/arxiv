[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.19325v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19325v2",
                "updated": "2025-04-17T15:26:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    15,
                    26,
                    4,
                    3,
                    107,
                    0
                ],
                "published": "2025-03-25T03:38:06Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    3,
                    38,
                    6,
                    1,
                    84,
                    0
                ],
                "title": "Long-Context Autoregressive Video Modeling with Next-Frame Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Context Autoregressive Video Modeling with Next-Frame Prediction"
                },
                "summary": "Long-context autoregressive modeling has significantly advanced language\ngeneration, but video generation still struggles to fully utilize extended\ntemporal contexts. To investigate long-context video modeling, we introduce\nFrame AutoRegressive (FAR), a strong baseline for video autoregressive\nmodeling. Just as language models learn causal dependencies between tokens\n(i.e., Token AR), FAR models temporal causal dependencies between continuous\nframes, achieving better convergence than Token AR and video diffusion\ntransformers. Building on FAR, we observe that long-context video modeling\nfaces challenges due to visual redundancy. Training on long videos is\ncomputationally expensive, as vision tokens grow much faster than language\ntokens. To tackle this issue, we propose balancing locality and long-range\ndependency through long short-term context modeling. A high-resolution\nshort-term context window ensures fine-grained temporal consistency, while an\nunlimited long-term context window encodes long-range information using fewer\ntokens. With this approach, we can train on long video sequences with a\nmanageable token context length, thereby significantly reducing training time\nand memory usage. Furthermore, we propose a multi-level KV cache designed to\nsupport the long short-term context modeling, which accelerating inference on\nlong video sequences. We demonstrate that FAR achieves state-of-the-art\nperformance in both short- and long-video generation, providing a simple yet\neffective baseline for video autoregressive modeling. The code is released at\nhttps://github.com/showlab/FAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context autoregressive modeling has significantly advanced language\ngeneration, but video generation still struggles to fully utilize extended\ntemporal contexts. To investigate long-context video modeling, we introduce\nFrame AutoRegressive (FAR), a strong baseline for video autoregressive\nmodeling. Just as language models learn causal dependencies between tokens\n(i.e., Token AR), FAR models temporal causal dependencies between continuous\nframes, achieving better convergence than Token AR and video diffusion\ntransformers. Building on FAR, we observe that long-context video modeling\nfaces challenges due to visual redundancy. Training on long videos is\ncomputationally expensive, as vision tokens grow much faster than language\ntokens. To tackle this issue, we propose balancing locality and long-range\ndependency through long short-term context modeling. A high-resolution\nshort-term context window ensures fine-grained temporal consistency, while an\nunlimited long-term context window encodes long-range information using fewer\ntokens. With this approach, we can train on long video sequences with a\nmanageable token context length, thereby significantly reducing training time\nand memory usage. Furthermore, we propose a multi-level KV cache designed to\nsupport the long short-term context modeling, which accelerating inference on\nlong video sequences. We demonstrate that FAR achieves state-of-the-art\nperformance in both short- and long-video generation, providing a simple yet\neffective baseline for video autoregressive modeling. The code is released at\nhttps://github.com/showlab/FAR."
                },
                "authors": [
                    {
                        "name": "Yuchao Gu"
                    },
                    {
                        "name": "Weijia Mao"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "arxiv_comment": "Project page at https://farlongctx.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19325v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19325v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12876v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12876v3",
                "updated": "2025-04-17T03:51:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    3,
                    51,
                    6,
                    3,
                    107,
                    0
                ],
                "published": "2024-10-15T05:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    1,
                    19,
                    1,
                    289,
                    0
                ],
                "title": "In-context KV-Cache Eviction for LLMs via Attention-Gate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context KV-Cache Eviction for LLMs via Attention-Gate"
                },
                "summary": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system. This paper enables a novel dynamic\nKV-Cache eviction policy by injecting a lightweight module called\nAttention-Gate to the model. It accepts the global context as input and yields\neviction flags for each token. The self-attention modules in the model proceed\naccording to the flags and cache only a subset of the KV states for next token\nprediction. The Attention-Gates can yield various flags for different heads and\nlayers and be easily tuned on top of a pre-trained LLM via continual\npre-training or supervised fine-tuning. The computational and memory overhead\nintroduced by Attention-Gates can be minimal. We empirically evaluate the\nproposed approach across multiple scenarios, showing that effective eviction of\nredundant tokens can not only improve efficiency but also enhance performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system. This paper enables a novel dynamic\nKV-Cache eviction policy by injecting a lightweight module called\nAttention-Gate to the model. It accepts the global context as input and yields\neviction flags for each token. The self-attention modules in the model proceed\naccording to the flags and cache only a subset of the KV states for next token\nprediction. The Attention-Gates can yield various flags for different heads and\nlayers and be easily tuned on top of a pre-trained LLM via continual\npre-training or supervised fine-tuning. The computational and memory overhead\nintroduced by Attention-Gates can be minimal. We empirically evaluate the\nproposed approach across multiple scenarios, showing that effective eviction of\nredundant tokens can not only improve efficiency but also enhance performance."
                },
                "authors": [
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Bokai Lin"
                    },
                    {
                        "name": "Tianqi Hou"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhijie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Deng"
                },
                "author": "Zhijie Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12876v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12876v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10074v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10074v2",
                "updated": "2025-04-17T00:38:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    0,
                    38,
                    24,
                    3,
                    107,
                    0
                ],
                "published": "2025-03-13T05:43:14Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    5,
                    43,
                    14,
                    3,
                    72,
                    0
                ],
                "title": "Demoting Security via Exploitation of Cache Demote Operation in Intel's\n  Latest ISA Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demoting Security via Exploitation of Cache Demote Operation in Intel's\n  Latest ISA Extension"
                },
                "summary": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions."
                },
                "authors": [
                    {
                        "name": "Taehun Kim"
                    },
                    {
                        "name": "Hyerean Jang"
                    },
                    {
                        "name": "Youngjoo Shin"
                    }
                ],
                "author_detail": {
                    "name": "Youngjoo Shin"
                },
                "author": "Youngjoo Shin",
                "arxiv_comment": "The modified version of this preprint has been submitted to ESORICS\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10074v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10074v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12526v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12526v1",
                "updated": "2025-04-16T23:15:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    23,
                    15,
                    9,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T23:15:09Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    23,
                    15,
                    9,
                    2,
                    106,
                    0
                ],
                "title": "MOM: Memory-Efficient Offloaded Mini-Sequence Inference for Long Context\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MOM: Memory-Efficient Offloaded Mini-Sequence Inference for Long Context\n  Language Models"
                },
                "summary": "Long-context language models exhibit impressive performance but remain\nchallenging to deploy due to high GPU memory demands during inference. We\npropose Memory-efficient Offloaded Mini-sequence Inference (MOM), a method that\npartitions critical layers into smaller \"mini-sequences\" and integrates\nseamlessly with KV cache offloading. Experiments on various Llama, Qwen, and\nMistral models demonstrate that MOM reduces peak memory usage by over 50\\% on\naverage. On Meta-Llama-3.2-8B, MOM extends the maximum context length from 155k\nto 455k tokens on a single A100 80GB GPU, while keeping outputs identical and\nnot compromising accuracy. MOM also maintains highly competitive throughput due\nto minimal computational overhead and efficient last-layer processing. Compared\nto traditional chunked prefill methods, MOM achieves a 35\\% greater context\nlength extension. More importantly, our method drastically reduces prefill\nmemory consumption, eliminating it as the longstanding dominant memory\nbottleneck during inference. This breakthrough fundamentally changes research\npriorities, redirecting future efforts from prefill-stage optimizations to\nimproving decode-stage residual KV cache efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context language models exhibit impressive performance but remain\nchallenging to deploy due to high GPU memory demands during inference. We\npropose Memory-efficient Offloaded Mini-sequence Inference (MOM), a method that\npartitions critical layers into smaller \"mini-sequences\" and integrates\nseamlessly with KV cache offloading. Experiments on various Llama, Qwen, and\nMistral models demonstrate that MOM reduces peak memory usage by over 50\\% on\naverage. On Meta-Llama-3.2-8B, MOM extends the maximum context length from 155k\nto 455k tokens on a single A100 80GB GPU, while keeping outputs identical and\nnot compromising accuracy. MOM also maintains highly competitive throughput due\nto minimal computational overhead and efficient last-layer processing. Compared\nto traditional chunked prefill methods, MOM achieves a 35\\% greater context\nlength extension. More importantly, our method drastically reduces prefill\nmemory consumption, eliminating it as the longstanding dominant memory\nbottleneck during inference. This breakthrough fundamentally changes research\npriorities, redirecting future efforts from prefill-stage optimizations to\nimproving decode-stage residual KV cache efficiency."
                },
                "authors": [
                    {
                        "name": "Junyang Zhang"
                    },
                    {
                        "name": "Tianyi Zhu"
                    },
                    {
                        "name": "Cheng Luo"
                    },
                    {
                        "name": "Anima Anandkumar"
                    }
                ],
                "author_detail": {
                    "name": "Anima Anandkumar"
                },
                "author": "Anima Anandkumar",
                "arxiv_comment": "Submitted to COLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12526v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12526v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12397v1",
                "updated": "2025-04-16T18:03:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    18,
                    3,
                    21,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T18:03:21Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    18,
                    3,
                    21,
                    2,
                    106,
                    0
                ],
                "title": "Activated LoRA: Fine-tuned LLMs for Intrinsics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activated LoRA: Fine-tuned LLMs for Intrinsics"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is highly inefficient, as the key-value (KV) cache of the\nentire turn history must be recomputed with the LoRA weights before generation\ncan begin. To address this problem, we propose Activated LoRA (aLoRA), which\nmodifies the LoRA framework to only adapt weights for the tokens in the\nsequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA\nto accept the base model's KV cache of the input string, meaning that aLoRA can\nbe instantly activated whenever needed in a chain without recomputing the\ncache. This enables building what we call \\emph{intrinsics}, i.e. highly\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\nuse aLoRA to train a set of intrinsics models, demonstrating competitive\naccuracy with standard LoRA while achieving significant inference benefits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is highly inefficient, as the key-value (KV) cache of the\nentire turn history must be recomputed with the LoRA weights before generation\ncan begin. To address this problem, we propose Activated LoRA (aLoRA), which\nmodifies the LoRA framework to only adapt weights for the tokens in the\nsequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA\nto accept the base model's KV cache of the input string, meaning that aLoRA can\nbe instantly activated whenever needed in a chain without recomputing the\ncache. This enables building what we call \\emph{intrinsics}, i.e. highly\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\nuse aLoRA to train a set of intrinsics models, demonstrating competitive\naccuracy with standard LoRA while achieving significant inference benefits."
                },
                "authors": [
                    {
                        "name": "Kristjan Greenewald"
                    },
                    {
                        "name": "Luis Lastras"
                    },
                    {
                        "name": "Thomas Parnell"
                    },
                    {
                        "name": "Vraj Shah"
                    },
                    {
                        "name": "Lucian Popa"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Chulaka Gunasekara"
                    },
                    {
                        "name": "Ambrish Rawat"
                    },
                    {
                        "name": "David Cox"
                    }
                ],
                "author_detail": {
                    "name": "David Cox"
                },
                "author": "David Cox",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2504.11704",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09775v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09775v2",
                "updated": "2025-04-16T17:34:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    17,
                    34,
                    4,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-14T00:29:49Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    0,
                    29,
                    49,
                    0,
                    104,
                    0
                ],
                "title": "Understanding and Optimizing Multi-Stage AI Inference Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and Optimizing Multi-Stage AI Inference Pipelines"
                },
                "summary": "The rapid evolution of Large Language Models (LLMs) has driven the need for\nincreasingly sophisticated inference pipelines and hardware platforms. Modern\nLLM serving extends beyond traditional prefill-decode workflows, incorporating\nmulti-stage processes such as Retrieval Augmented Generation (RAG), key-value\n(KV) cache retrieval, dynamic model routing, and multi step reasoning. These\nstages exhibit diverse computational demands, requiring distributed systems\nthat integrate GPUs, ASICs, CPUs, and memory-centric architectures. However,\nexisting simulators lack the fidelity to model these heterogeneous,\nmulti-engine workflows, limiting their ability to inform architectural\ndecisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM\ninference Execution Simulator. HERMES models diverse request stages; including\nRAG, KV retrieval, reasoning, prefill, and decode across complex hardware\nhierarchies. HERMES supports heterogeneous clients executing multiple models\nconcurrently unlike prior frameworks while incorporating advanced batching\nstrategies and multi-level memory hierarchies. By integrating real hardware\ntraces with analytical modeling, HERMES captures critical trade-offs such as\nmemory bandwidth contention, inter-cluster communication latency, and batching\nefficiency in hybrid CPU-accelerator deployments. Through case studies, we\nexplore the impact of reasoning stages on end-to-end latency, optimal batching\nstrategies for hybrid pipelines, and the architectural implications of remote\nKV cache retrieval. HERMES empowers system designers to navigate the evolving\nlandscape of LLM inference, providing actionable insights into optimizing\nhardware-software co-design for next-generation AI workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of Large Language Models (LLMs) has driven the need for\nincreasingly sophisticated inference pipelines and hardware platforms. Modern\nLLM serving extends beyond traditional prefill-decode workflows, incorporating\nmulti-stage processes such as Retrieval Augmented Generation (RAG), key-value\n(KV) cache retrieval, dynamic model routing, and multi step reasoning. These\nstages exhibit diverse computational demands, requiring distributed systems\nthat integrate GPUs, ASICs, CPUs, and memory-centric architectures. However,\nexisting simulators lack the fidelity to model these heterogeneous,\nmulti-engine workflows, limiting their ability to inform architectural\ndecisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM\ninference Execution Simulator. HERMES models diverse request stages; including\nRAG, KV retrieval, reasoning, prefill, and decode across complex hardware\nhierarchies. HERMES supports heterogeneous clients executing multiple models\nconcurrently unlike prior frameworks while incorporating advanced batching\nstrategies and multi-level memory hierarchies. By integrating real hardware\ntraces with analytical modeling, HERMES captures critical trade-offs such as\nmemory bandwidth contention, inter-cluster communication latency, and batching\nefficiency in hybrid CPU-accelerator deployments. Through case studies, we\nexplore the impact of reasoning stages on end-to-end latency, optimal batching\nstrategies for hybrid pipelines, and the architectural implications of remote\nKV cache retrieval. HERMES empowers system designers to navigate the evolving\nlandscape of LLM inference, providing actionable insights into optimizing\nhardware-software co-design for next-generation AI workloads."
                },
                "authors": [
                    {
                        "name": "Abhimanyu Rajeshkumar Bambhaniya"
                    },
                    {
                        "name": "Hanjiang Wu"
                    },
                    {
                        "name": "Suvinay Subramanian"
                    },
                    {
                        "name": "Sudarshan Srinivasan"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Amir Yazdanbakhsh"
                    },
                    {
                        "name": "Midhilesh Elavazhagan"
                    },
                    {
                        "name": "Madhu Kumar"
                    },
                    {
                        "name": "Tushar Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Krishna"
                },
                "author": "Tushar Krishna",
                "arxiv_comment": "Inference System Design for Multi-Stage AI Inference Pipelines. 13\n  Pages, 15 Figues, 3 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09775v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09775v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12240v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12240v1",
                "updated": "2025-04-16T16:45:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    45,
                    19,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T16:45:19Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    45,
                    19,
                    2,
                    106,
                    0
                ],
                "title": "Cobra: Efficient Line Art COlorization with BRoAder References",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cobra: Efficient Line Art COlorization with BRoAder References"
                },
                "summary": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/."
                },
                "authors": [
                    {
                        "name": "Junhao Zhuang"
                    },
                    {
                        "name": "Lingen Li"
                    },
                    {
                        "name": "Xuan Ju"
                    },
                    {
                        "name": "Zhaoyang Zhang"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Ying Shan"
                    }
                ],
                "author_detail": {
                    "name": "Ying Shan"
                },
                "author": "Ying Shan",
                "arxiv_comment": "Project page with code: https://zhuang2002.github.io/Cobra/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12240v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11816v1",
                "updated": "2025-04-16T07:02:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    2,
                    38,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T07:02:38Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    2,
                    38,
                    2,
                    106,
                    0
                ],
                "title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache\n  Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache\n  Offloading"
                },
                "summary": "LLM inference is essential for applications like text summarization,\ntranslation, and data analysis, but the high cost of GPU instances from Cloud\nService Providers (CSPs) like AWS is a major burden. This paper proposes\nInferSave, a cost-efficient VM selection framework for cloud based LLM\ninference. InferSave optimizes KV cache offloading based on Service Level\nObjectives (SLOs) and workload charac teristics, estimating GPU memory needs,\nand recommending cost-effective VM instances. Additionally, the Compute Time\nCalibration Function (CTCF) improves instance selection accuracy by adjusting\nfor discrepancies between theoretical and actual GPU performance. Experiments\non AWS GPU instances show that selecting lower-cost instances without KV cache\noffloading improves cost efficiency by up to 73.7% for online workloads, while\nKV cache offloading saves up to 20.19% for offline workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference is essential for applications like text summarization,\ntranslation, and data analysis, but the high cost of GPU instances from Cloud\nService Providers (CSPs) like AWS is a major burden. This paper proposes\nInferSave, a cost-efficient VM selection framework for cloud based LLM\ninference. InferSave optimizes KV cache offloading based on Service Level\nObjectives (SLOs) and workload charac teristics, estimating GPU memory needs,\nand recommending cost-effective VM instances. Additionally, the Compute Time\nCalibration Function (CTCF) improves instance selection accuracy by adjusting\nfor discrepancies between theoretical and actual GPU performance. Experiments\non AWS GPU instances show that selecting lower-cost instances without KV cache\noffloading improves cost efficiency by up to 73.7% for online workloads, while\nKV cache offloading saves up to 20.19% for offline workloads."
                },
                "authors": [
                    {
                        "name": "Kihyun Kim"
                    },
                    {
                        "name": "Jinwoo Kim"
                    },
                    {
                        "name": "Hyunsun Chung"
                    },
                    {
                        "name": "Myung-Hoon Cha"
                    },
                    {
                        "name": "Hong-Yeon Kim"
                    },
                    {
                        "name": "Youngjae Kim"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Kim"
                },
                "author": "Youngjae Kim",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08334v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08334v3",
                "updated": "2025-04-16T05:57:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    5,
                    57,
                    8,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-11T07:59:06Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    7,
                    59,
                    6,
                    4,
                    101,
                    0
                ],
                "title": "Efficient Architecture for RISC-V Vector Memory Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Architecture for RISC-V Vector Memory Access"
                },
                "summary": "Vector processors frequently suffer from inefficient memory accesses,\nparticularly for strided and segment patterns. While coalescing strided\naccesses is a natural solution, effectively gathering or scattering elements at\nfixed strides remains challenging. Naive approaches rely on high-overhead\ncrossbars that remap any byte between memory and registers, leading to physical\ndesign issues. Segment operations require row-column transpositions, typically\nhandled using either element-level in-place transposition (degrading\nperformance) or large buffer-based bulk transposition (incurring high area\noverhead). In this paper, we present EARTH, a novel vector memory access\narchitecture designed to overcome these challenges through shifting-based\noptimizations. For strided accesses, EARTH integrates specialized shift\nnetworks for gathering and scattering elements. After coalescing multiple\naccesses within the same cache line, data is routed between memory and\nregisters through the shifting network with minimal overhead. For segment\noperations, EARTH employs a shifted register bank enabling direct column-wise\naccess, eliminating dedicated segment buffers while providing high-performance,\nin-place bulk transposition. Implemented on FPGA with Chisel HDL based on an\nopen-source RISC-V vector unit, EARTH enhances performance for strided memory\naccesses, achieving 4x-8x speedups in benchmarks dominated by strided\noperations. Compared to conventional designs, EARTH reduces hardware area by 9%\nand power consumption by 41%, significantly advancing both performance and\nefficiency of vector processors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vector processors frequently suffer from inefficient memory accesses,\nparticularly for strided and segment patterns. While coalescing strided\naccesses is a natural solution, effectively gathering or scattering elements at\nfixed strides remains challenging. Naive approaches rely on high-overhead\ncrossbars that remap any byte between memory and registers, leading to physical\ndesign issues. Segment operations require row-column transpositions, typically\nhandled using either element-level in-place transposition (degrading\nperformance) or large buffer-based bulk transposition (incurring high area\noverhead). In this paper, we present EARTH, a novel vector memory access\narchitecture designed to overcome these challenges through shifting-based\noptimizations. For strided accesses, EARTH integrates specialized shift\nnetworks for gathering and scattering elements. After coalescing multiple\naccesses within the same cache line, data is routed between memory and\nregisters through the shifting network with minimal overhead. For segment\noperations, EARTH employs a shifted register bank enabling direct column-wise\naccess, eliminating dedicated segment buffers while providing high-performance,\nin-place bulk transposition. Implemented on FPGA with Chisel HDL based on an\nopen-source RISC-V vector unit, EARTH enhances performance for strided memory\naccesses, achieving 4x-8x speedups in benchmarks dominated by strided\noperations. Compared to conventional designs, EARTH reduces hardware area by 9%\nand power consumption by 41%, significantly advancing both performance and\nefficiency of vector processors."
                },
                "authors": [
                    {
                        "name": "Hongyi Guan"
                    },
                    {
                        "name": "Yichuan Gao"
                    },
                    {
                        "name": "Chenlu Miao"
                    },
                    {
                        "name": "Haoyang Wu"
                    },
                    {
                        "name": "Hang Zhu"
                    },
                    {
                        "name": "Mingfeng Lin"
                    },
                    {
                        "name": "Huayue Liang"
                    }
                ],
                "author_detail": {
                    "name": "Huayue Liang"
                },
                "author": "Huayue Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08334v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08334v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11765v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11765v1",
                "updated": "2025-04-16T04:59:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    4,
                    59,
                    18,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T04:59:18Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    4,
                    59,
                    18,
                    2,
                    106,
                    0
                ],
                "title": "Shared Disk KV Cache Management for Efficient Multi-Instance Inference\n  in RAG-Powered LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared Disk KV Cache Management for Efficient Multi-Instance Inference\n  in RAG-Powered LLMs"
                },
                "summary": "Recent large language models (LLMs) face increasing inference latency as\ninput context length and model size continue to grow. In particular, the\nretrieval-augmented generation (RAG) technique, which enhances LLM responses by\nincorporating external knowledge, exacerbates this issue by significantly\nincreasing the number of input tokens. This expansion in token length leads to\na substantial rise in computational overhead, particularly during the prefill\nstage, resulting in prolonged time-to-first-token (TTFT). To address this\nissue, this paper proposes a method to reduce TTFT by leveraging a disk-based\nkey-value (KV) cache to lessen the computational burden during the prefill\nstage. We also introduce a disk-based shared KV cache management system, called\nShared RAG-DCache, for multi-instance LLM RAG service environments. This\nsystem, together with an optimal system configuration, improves both throughput\nand latency under given resource constraints. Shared RAG-DCache exploits the\nlocality of documents related to user queries in RAG, as well as the queueing\ndelay in LLM inference services. It proactively generates and stores disk KV\ncaches for query-related documents and shares them across multiple LLM\ninstances to enhance inference performance. In experiments on a single host\nequipped with 2 GPUs and 1 CPU, Shared RAG-DCache achieved a 15~71% increase in\nthroughput and up to a 12~65% reduction in latency, depending on the resource\nconfiguration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) face increasing inference latency as\ninput context length and model size continue to grow. In particular, the\nretrieval-augmented generation (RAG) technique, which enhances LLM responses by\nincorporating external knowledge, exacerbates this issue by significantly\nincreasing the number of input tokens. This expansion in token length leads to\na substantial rise in computational overhead, particularly during the prefill\nstage, resulting in prolonged time-to-first-token (TTFT). To address this\nissue, this paper proposes a method to reduce TTFT by leveraging a disk-based\nkey-value (KV) cache to lessen the computational burden during the prefill\nstage. We also introduce a disk-based shared KV cache management system, called\nShared RAG-DCache, for multi-instance LLM RAG service environments. This\nsystem, together with an optimal system configuration, improves both throughput\nand latency under given resource constraints. Shared RAG-DCache exploits the\nlocality of documents related to user queries in RAG, as well as the queueing\ndelay in LLM inference services. It proactively generates and stores disk KV\ncaches for query-related documents and shares them across multiple LLM\ninstances to enhance inference performance. In experiments on a single host\nequipped with 2 GPUs and 1 CPU, Shared RAG-DCache achieved a 15~71% increase in\nthroughput and up to a 12~65% reduction in latency, depending on the resource\nconfiguration."
                },
                "authors": [
                    {
                        "name": "Hyungwoo Lee"
                    },
                    {
                        "name": "Kihyun Kim"
                    },
                    {
                        "name": "Jinwoo Kim"
                    },
                    {
                        "name": "Jungmin So"
                    },
                    {
                        "name": "Myung-Hoon Cha"
                    },
                    {
                        "name": "Hong-Yeon Kim"
                    },
                    {
                        "name": "James J. Kim"
                    },
                    {
                        "name": "Youngjae Kim"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Kim"
                },
                "arxiv_affiliation": "Dept. of Computer Science and Engineering, Sogang University, Seoul, Republic of Korea",
                "author": "Youngjae Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11765v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11765v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11729v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11729v1",
                "updated": "2025-04-16T03:07:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    3,
                    7,
                    7,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T03:07:07Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    3,
                    7,
                    7,
                    2,
                    106,
                    0
                ],
                "title": "EdgePrompt: A Distributed Key-Value Inference Framework for LLMs in 6G\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgePrompt: A Distributed Key-Value Inference Framework for LLMs in 6G\n  Networks"
                },
                "summary": "As sixth-generation (6G) networks advance, large language models (LLMs) are\nincreasingly integrated into 6G infrastructure to enhance network management\nand intelligence. However, traditional LLMs architecture struggle to meet the\nstringent latency and security requirements of 6G, especially as the increasing\nin sequence length leads to greater task complexity. This paper proposes\nEdge-Prompt, a cloud-edge collaborative framework based on a hierarchical\nattention splicing mechanism. EdgePrompt employs distributed key-value (KV)\npair optimization techniques to accelerate inference and adapt to network\nconditions. Additionally, to reduce the risk of data leakage, EdgePrompt\nincorporates a privacy preserving strategy by isolating sensitive information\nduring processing. Experiments on public dataset show that EdgePrompt\neffectively improves the inference throughput and reduces the latency, which\nprovides a reliable solution for LLMs deployment in 6G environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As sixth-generation (6G) networks advance, large language models (LLMs) are\nincreasingly integrated into 6G infrastructure to enhance network management\nand intelligence. However, traditional LLMs architecture struggle to meet the\nstringent latency and security requirements of 6G, especially as the increasing\nin sequence length leads to greater task complexity. This paper proposes\nEdge-Prompt, a cloud-edge collaborative framework based on a hierarchical\nattention splicing mechanism. EdgePrompt employs distributed key-value (KV)\npair optimization techniques to accelerate inference and adapt to network\nconditions. Additionally, to reduce the risk of data leakage, EdgePrompt\nincorporates a privacy preserving strategy by isolating sensitive information\nduring processing. Experiments on public dataset show that EdgePrompt\neffectively improves the inference throughput and reduces the latency, which\nprovides a reliable solution for LLMs deployment in 6G environments."
                },
                "authors": [
                    {
                        "name": "Jiahong Ning"
                    },
                    {
                        "name": "Pengyan Zhu"
                    },
                    {
                        "name": "Ce Zheng"
                    },
                    {
                        "name": "Gary Lee"
                    },
                    {
                        "name": "Sumei Sun"
                    },
                    {
                        "name": "Tingting Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tingting Yang"
                },
                "author": "Tingting Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11729v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11729v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11652v1",
                "updated": "2025-04-15T22:38:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    22,
                    38,
                    54,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T22:38:54Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    22,
                    38,
                    54,
                    1,
                    105,
                    0
                ],
                "title": "Engineering MultiQueues: Fast Relaxed Concurrent Priority Queues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Engineering MultiQueues: Fast Relaxed Concurrent Priority Queues"
                },
                "summary": "Priority queues are used in a wide range of applications, including\nprioritized online scheduling, discrete event simulation, and greedy\nalgorithms. In parallel settings, classical priority queues often become a\nsevere bottleneck, resulting in low throughput. Consequently, there has been\nsignificant interest in concurrent priority queues with relaxed semantics. In\nthis article, we present the MultiQueue, a flexible approach to relaxed\npriority queues that uses multiple internal sequential priority queues. The\nscalability of the MultiQueue is enhanced by buffering elements, batching\noperations on the internal queues, and optimizing access patterns for high\ncache locality. We investigate the complementary quality criteria of rank\nerror, which measures how close deleted elements are to the global minimum, and\ndelay, which quantifies how many smaller elements were deleted before a given\nelement. Extensive experimental evaluation shows that the MultiQueue\noutperforms competing approaches across several benchmarks. This includes\nshortest-path and branch-and-bound benchmarks that resemble real applications.\nMoreover, the MultiQueue can be configured easily to balance throughput and\nquality according to the application's requirements. We employ a seemingly\nparadoxical technique of wait-free locking that might be of broader interest\nfor converting sequential data structures into relaxed concurrent data\nstructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Priority queues are used in a wide range of applications, including\nprioritized online scheduling, discrete event simulation, and greedy\nalgorithms. In parallel settings, classical priority queues often become a\nsevere bottleneck, resulting in low throughput. Consequently, there has been\nsignificant interest in concurrent priority queues with relaxed semantics. In\nthis article, we present the MultiQueue, a flexible approach to relaxed\npriority queues that uses multiple internal sequential priority queues. The\nscalability of the MultiQueue is enhanced by buffering elements, batching\noperations on the internal queues, and optimizing access patterns for high\ncache locality. We investigate the complementary quality criteria of rank\nerror, which measures how close deleted elements are to the global minimum, and\ndelay, which quantifies how many smaller elements were deleted before a given\nelement. Extensive experimental evaluation shows that the MultiQueue\noutperforms competing approaches across several benchmarks. This includes\nshortest-path and branch-and-bound benchmarks that resemble real applications.\nMoreover, the MultiQueue can be configured easily to balance throughput and\nquality according to the application's requirements. We employ a seemingly\nparadoxical technique of wait-free locking that might be of broader interest\nfor converting sequential data structures into relaxed concurrent data\nstructures."
                },
                "authors": [
                    {
                        "name": "Marvin Williams"
                    },
                    {
                        "name": "Peter Sanders"
                    }
                ],
                "author_detail": {
                    "name": "Peter Sanders"
                },
                "author": "Peter Sanders",
                "arxiv_comment": "40 pages, extended journal version of arXiv:2107.01350",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11320v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11320v1",
                "updated": "2025-04-15T16:00:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    0,
                    21,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T16:00:21Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    0,
                    21,
                    1,
                    105,
                    0
                ],
                "title": "Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory\n  Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory\n  Constraints"
                },
                "summary": "Large Language Models (LLMs) are indispensable in today's applications, but\ntheir inference procedure -- generating responses by processing text in\nsegments and using a memory-heavy Key-Value (KV) cache -- demands significant\ncomputational resources, particularly under memory constraints. This paper\nformulates LLM inference optimization as a multi-stage online scheduling\nproblem where sequential prompt arrivals and KV cache growth render\nconventional scheduling ineffective. We develop a fluid dynamics approximation\nto provide a tractable benchmark that guides algorithm design. Building on\nthis, we propose the Waiting for Accumulated Inference Threshold (WAIT)\nalgorithm, which uses multiple thresholds to schedule incoming prompts\noptimally when output lengths are known, and extend it to Nested WAIT for cases\nwith unknown output lengths. Theoretical analysis shows that both algorithms\nachieve near-optimal performance against the fluid benchmark in heavy traffic\nconditions, balancing throughput, latency, and Time to First Token (TTFT).\nExperiments with the Llama-7B model on an A100 GPU using both synthetic and\nreal-world datasets demonstrate improved throughput and reduced latency\nrelative to established baselines like vLLM and Sarathi. This work bridges\noperations research and machine learning, offering a rigorous framework for the\nefficient deployment of LLMs under memory constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are indispensable in today's applications, but\ntheir inference procedure -- generating responses by processing text in\nsegments and using a memory-heavy Key-Value (KV) cache -- demands significant\ncomputational resources, particularly under memory constraints. This paper\nformulates LLM inference optimization as a multi-stage online scheduling\nproblem where sequential prompt arrivals and KV cache growth render\nconventional scheduling ineffective. We develop a fluid dynamics approximation\nto provide a tractable benchmark that guides algorithm design. Building on\nthis, we propose the Waiting for Accumulated Inference Threshold (WAIT)\nalgorithm, which uses multiple thresholds to schedule incoming prompts\noptimally when output lengths are known, and extend it to Nested WAIT for cases\nwith unknown output lengths. Theoretical analysis shows that both algorithms\nachieve near-optimal performance against the fluid benchmark in heavy traffic\nconditions, balancing throughput, latency, and Time to First Token (TTFT).\nExperiments with the Llama-7B model on an A100 GPU using both synthetic and\nreal-world datasets demonstrate improved throughput and reduced latency\nrelative to established baselines like vLLM and Sarathi. This work bridges\noperations research and machine learning, offering a rigorous framework for the\nefficient deployment of LLMs under memory constraints."
                },
                "authors": [
                    {
                        "name": "Ruicheng Ao"
                    },
                    {
                        "name": "Gan Luo"
                    },
                    {
                        "name": "David Simchi-Levi"
                    },
                    {
                        "name": "Xinshang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinshang Wang"
                },
                "author": "Xinshang Wang",
                "arxiv_comment": "42 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11320v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00279v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00279v3",
                "updated": "2025-04-15T15:40:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    40,
                    25,
                    1,
                    105,
                    0
                ],
                "published": "2024-12-31T05:24:30Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "title": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement"
                },
                "summary": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Li"
                },
                "author": "Junjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00279v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00279v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.13195v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.13195v5",
                "updated": "2025-04-15T15:37:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    37,
                    58,
                    1,
                    105,
                    0
                ],
                "published": "2024-04-19T22:06:14Z",
                "published_parsed": [
                    2024,
                    4,
                    19,
                    22,
                    6,
                    14,
                    4,
                    110,
                    0
                ],
                "title": "Automatic BLAS Offloading on Unified Memory Architecture: A Study on\n  NVIDIA Grace-Hopper",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic BLAS Offloading on Unified Memory Architecture: A Study on\n  NVIDIA Grace-Hopper"
                },
                "summary": "Porting codes to GPU often requires major efforts. While several tools exist\nfor automatically offload numerical libraries such as BLAS and LAPACK, they\noften prove impractical due to the high cost of mandatory data transfer. The\nnew unified memory architecture in NVIDIA Grace-Hopper allows high bandwidth\ncache-coherent memory access of all memory from both CPU and GPU, potentially\neliminating bottleneck faced in conventional architecture. This breakthrough\nopens up new avenues for application development and porting strategies. In\nthis study, we introduce a new tool for automatic BLAS offload, the tool\nleverages the high speed cache coherent NVLink C2C interconnect in\nGrace-Hopper, and enables performant GPU offload for BLAS heavy applications\nwith no code changes or recompilation. The tool was tested on two quantum\nchemistry or physics codes, great performance benefits were observed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Porting codes to GPU often requires major efforts. While several tools exist\nfor automatically offload numerical libraries such as BLAS and LAPACK, they\noften prove impractical due to the high cost of mandatory data transfer. The\nnew unified memory architecture in NVIDIA Grace-Hopper allows high bandwidth\ncache-coherent memory access of all memory from both CPU and GPU, potentially\neliminating bottleneck faced in conventional architecture. This breakthrough\nopens up new avenues for application development and porting strategies. In\nthis study, we introduce a new tool for automatic BLAS offload, the tool\nleverages the high speed cache coherent NVLink C2C interconnect in\nGrace-Hopper, and enables performant GPU offload for BLAS heavy applications\nwith no code changes or recompilation. The tool was tested on two quantum\nchemistry or physics codes, great performance benefits were observed."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    },
                    {
                        "name": "Yinzhi Wang"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Hang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hang Liu"
                },
                "author": "Hang Liu",
                "arxiv_doi": "10.1145/3626203.3670561",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3626203.3670561",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.13195v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.13195v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11208v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11208v1",
                "updated": "2025-04-15T14:11:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    11,
                    38,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T14:11:38Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    11,
                    38,
                    1,
                    105,
                    0
                ],
                "title": "Slice+Slice Baby: Generating Last-Level Cache Eviction Sets in the Blink\n  of an Eye",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Slice+Slice Baby: Generating Last-Level Cache Eviction Sets in the Blink\n  of an Eye"
                },
                "summary": "An essential step for mounting cache attacks is finding eviction sets,\ncollections of memory locations that contend on cache space. On Intel\nprocessors, one of the main challenges for identifying contending addresses is\nthe sliced cache design, where the processor hashes the physical address to\ndetermine where in the cache a memory location is stored. While past works have\ndemonstrated that the hash function can be reversed, they also showed that it\ndepends on physical address bits that the adversary does not know.\n  In this work, we make three main contributions to the art of finding eviction\nsets. We first exploit microarchitectural races to compare memory access times\nand identify the cache slice to which an address maps. We then use the known\nhash function to both reduce the error rate in our slice identification method\nand to reduce the work by extrapolating slice mappings to untested memory\naddresses. Finally, we show how to propagate information on eviction sets\nacross different page offsets for the hitherto unexplored case of non-linear\nhash functions.\n  Our contributions allow for entire LLC eviction set generation in 0.7 seconds\non the Intel i7-9850H and 1.6 seconds on the i9-10900K, both using non-linear\nfunctions. This represents a significant improvement compared to\nstate-of-the-art techniques taking 9x and 10x longer, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An essential step for mounting cache attacks is finding eviction sets,\ncollections of memory locations that contend on cache space. On Intel\nprocessors, one of the main challenges for identifying contending addresses is\nthe sliced cache design, where the processor hashes the physical address to\ndetermine where in the cache a memory location is stored. While past works have\ndemonstrated that the hash function can be reversed, they also showed that it\ndepends on physical address bits that the adversary does not know.\n  In this work, we make three main contributions to the art of finding eviction\nsets. We first exploit microarchitectural races to compare memory access times\nand identify the cache slice to which an address maps. We then use the known\nhash function to both reduce the error rate in our slice identification method\nand to reduce the work by extrapolating slice mappings to untested memory\naddresses. Finally, we show how to propagate information on eviction sets\nacross different page offsets for the hitherto unexplored case of non-linear\nhash functions.\n  Our contributions allow for entire LLC eviction set generation in 0.7 seconds\non the Intel i7-9850H and 1.6 seconds on the i9-10900K, both using non-linear\nfunctions. This represents a significant improvement compared to\nstate-of-the-art techniques taking 9x and 10x longer, respectively."
                },
                "authors": [
                    {
                        "name": "Bradley Morgan"
                    },
                    {
                        "name": "Gal Horowitz"
                    },
                    {
                        "name": "Sioli O'Connell"
                    },
                    {
                        "name": "Stephan van Schaik"
                    },
                    {
                        "name": "Chitchanok Chuengsatiansup"
                    },
                    {
                        "name": "Daniel Genkin"
                    },
                    {
                        "name": "Olaf Maennel"
                    },
                    {
                        "name": "Paul Montague"
                    },
                    {
                        "name": "Eyal Ronen"
                    },
                    {
                        "name": "Yuval Yarom"
                    }
                ],
                "author_detail": {
                    "name": "Yuval Yarom"
                },
                "author": "Yuval Yarom",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11208v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11208v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11067v1",
                "updated": "2025-04-15T11:02:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    2,
                    34,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T11:02:34Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    2,
                    34,
                    1,
                    105,
                    0
                ],
                "title": "Morphing-based Compression for Data-centric ML Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Morphing-based Compression for Data-centric ML Pipelines"
                },
                "summary": "Data-centric ML pipelines extend traditional machine learning (ML) pipelines\n-- of feature transformations and ML model training -- by outer loops for data\ncleaning, augmentation, and feature engineering to create high-quality input\ndata. Existing lossless matrix compression applies lightweight compression\nschemes to numeric matrices and performs linear algebra operations such as\nmatrix-vector multiplications directly on the compressed representation but\nstruggles to efficiently rediscover structural data redundancy. Compressed\noperations are effective at fitting data in available memory, reducing I/O\nacross the storage-memory-cache hierarchy, and improving instruction\nparallelism. The applied data cleaning, augmentation, and feature\ntransformations provide a rich source of information about data characteristics\nsuch as distinct items, column sparsity, and column correlations. In this\npaper, we introduce BWARE -- an extension of AWARE for workload-aware lossless\nmatrix compression -- that pushes compression through feature transformations\nand engineering to leverage information about structural transformations.\nBesides compressed feature transformations, we introduce a novel technique for\nlightweight morphing of a compressed representation into workload-optimized\ncompressed representations without decompression. BWARE shows substantial\nend-to-end runtime improvements, reducing the execution time for training\ndata-centric ML pipelines from days to hours.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-centric ML pipelines extend traditional machine learning (ML) pipelines\n-- of feature transformations and ML model training -- by outer loops for data\ncleaning, augmentation, and feature engineering to create high-quality input\ndata. Existing lossless matrix compression applies lightweight compression\nschemes to numeric matrices and performs linear algebra operations such as\nmatrix-vector multiplications directly on the compressed representation but\nstruggles to efficiently rediscover structural data redundancy. Compressed\noperations are effective at fitting data in available memory, reducing I/O\nacross the storage-memory-cache hierarchy, and improving instruction\nparallelism. The applied data cleaning, augmentation, and feature\ntransformations provide a rich source of information about data characteristics\nsuch as distinct items, column sparsity, and column correlations. In this\npaper, we introduce BWARE -- an extension of AWARE for workload-aware lossless\nmatrix compression -- that pushes compression through feature transformations\nand engineering to leverage information about structural transformations.\nBesides compressed feature transformations, we introduce a novel technique for\nlightweight morphing of a compressed representation into workload-optimized\ncompressed representations without decompression. BWARE shows substantial\nend-to-end runtime improvements, reducing the execution time for training\ndata-centric ML pipelines from days to hours."
                },
                "authors": [
                    {
                        "name": "Sebastian Baunsgaard"
                    },
                    {
                        "name": "Matthias Boehm"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Boehm"
                },
                "author": "Matthias Boehm",
                "arxiv_comment": "20 pages, 28 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10326v1",
                "updated": "2025-04-14T15:34:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    34,
                    26,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T15:34:26Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    34,
                    26,
                    0,
                    104,
                    0
                ],
                "title": "AlayaDB: The Data Foundation for Efficient and Effective Long-context\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlayaDB: The Data Foundation for Efficient and Effective Long-context\n  LLM Inference"
                },
                "summary": "AlayaDB is a cutting-edge vector database system natively architected for\nefficient and effective long-context inference for Large Language Models (LLMs)\nat AlayaDB AI. Specifically, it decouples the KV cache and attention\ncomputation from the LLM inference systems, and encapsulates them into a novel\nvector database system. For the Model as a Service providers (MaaS), AlayaDB\nconsumes fewer hardware resources and offers higher generation quality for\nvarious workloads with different kinds of Service Level Objectives (SLOs), when\ncomparing with the existing alternative solutions (e.g., KV cache\ndisaggregation, retrieval-based sparse attention). The crux of AlayaDB is that\nit abstracts the attention computation and cache management for LLM inference\ninto a query processing procedure, and optimizes the performance via a native\nquery optimizer. In this work, we demonstrate the effectiveness of AlayaDB via\n(i) three use cases from our industry partners, and (ii) extensive experimental\nresults on LLM inference benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlayaDB is a cutting-edge vector database system natively architected for\nefficient and effective long-context inference for Large Language Models (LLMs)\nat AlayaDB AI. Specifically, it decouples the KV cache and attention\ncomputation from the LLM inference systems, and encapsulates them into a novel\nvector database system. For the Model as a Service providers (MaaS), AlayaDB\nconsumes fewer hardware resources and offers higher generation quality for\nvarious workloads with different kinds of Service Level Objectives (SLOs), when\ncomparing with the existing alternative solutions (e.g., KV cache\ndisaggregation, retrieval-based sparse attention). The crux of AlayaDB is that\nit abstracts the attention computation and cache management for LLM inference\ninto a query processing procedure, and optimizes the performance via a native\nquery optimizer. In this work, we demonstrate the effectiveness of AlayaDB via\n(i) three use cases from our industry partners, and (ii) extensive experimental\nresults on LLM inference benchmarks."
                },
                "authors": [
                    {
                        "name": "Yangshen Deng"
                    },
                    {
                        "name": "Zhengxin You"
                    },
                    {
                        "name": "Long Xiang"
                    },
                    {
                        "name": "Qilong Li"
                    },
                    {
                        "name": "Peiqi Yuan"
                    },
                    {
                        "name": "Zhaoyang Hong"
                    },
                    {
                        "name": "Yitao Zheng"
                    },
                    {
                        "name": "Wanting Li"
                    },
                    {
                        "name": "Runzhong Li"
                    },
                    {
                        "name": "Haotian Liu"
                    },
                    {
                        "name": "Kyriakos Mouratidis"
                    },
                    {
                        "name": "Man Lung Yiu"
                    },
                    {
                        "name": "Huan Li"
                    },
                    {
                        "name": "Qiaomu Shen"
                    },
                    {
                        "name": "Rui Mao"
                    },
                    {
                        "name": "Bo Tang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Tang"
                },
                "author": "Bo Tang",
                "arxiv_comment": "14 pages, 12 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.1; H.3.2; H.3.3; H.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10318v1",
                "updated": "2025-04-14T15:27:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    27,
                    32,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T15:27:32Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    27,
                    32,
                    0,
                    104,
                    0
                ],
                "title": "Shield Bash: Abusing Defensive Coherence State Retrieval to Break Timing\n  Obfuscation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shield Bash: Abusing Defensive Coherence State Retrieval to Break Timing\n  Obfuscation"
                },
                "summary": "Microarchitectural attacks are a significant concern, leading to many\nhardware-based defense proposals. However, different defenses target different\nclasses of attacks, and their impact on each other has not been fully\nconsidered. To raise awareness of this problem, we study an interaction between\ntwo state-of-the art defenses in this paper, timing obfuscations of remote\ncache lines (TORC) and delaying speculative changes to remote cache lines\n(DSRC). TORC mitigates cache-hit based attacks and DSRC mitigates speculative\ncoherence state change attacks.\n  We observe that DSRC enables coherence information to be retrieved into the\nprocessor core, where it is out of the reach of timing obfuscations to protect.\nThis creates an unforeseen consequence that redo operations can be triggered\nwithin the core to detect the presence or absence of remote cache lines, which\nconstitutes a security vulnerability. We demonstrate that a new covert channel\nattack is possible using this vulnerability. We propose two ways to mitigate\nthe attack, whose performance varies depending on an application's cache usage.\nOne way is to never send remote exclusive coherence state (E) information to\nthe core even if it is created. The other way is to never create a remote E\nstate, which is responsible for triggering redos.\n  We demonstrate the timing difference caused by this microarchitectural\ndefense assumption violation using GEM5 simulations. Performance evaluation on\nSPECrate 2017 and PARSEC benchmarks of the two fixes show less than 32\\%\naverage overhead across both sets of benchmarks. The repair which prevented the\ncreation of remote E state had less than 2.8% average overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microarchitectural attacks are a significant concern, leading to many\nhardware-based defense proposals. However, different defenses target different\nclasses of attacks, and their impact on each other has not been fully\nconsidered. To raise awareness of this problem, we study an interaction between\ntwo state-of-the art defenses in this paper, timing obfuscations of remote\ncache lines (TORC) and delaying speculative changes to remote cache lines\n(DSRC). TORC mitigates cache-hit based attacks and DSRC mitigates speculative\ncoherence state change attacks.\n  We observe that DSRC enables coherence information to be retrieved into the\nprocessor core, where it is out of the reach of timing obfuscations to protect.\nThis creates an unforeseen consequence that redo operations can be triggered\nwithin the core to detect the presence or absence of remote cache lines, which\nconstitutes a security vulnerability. We demonstrate that a new covert channel\nattack is possible using this vulnerability. We propose two ways to mitigate\nthe attack, whose performance varies depending on an application's cache usage.\nOne way is to never send remote exclusive coherence state (E) information to\nthe core even if it is created. The other way is to never create a remote E\nstate, which is responsible for triggering redos.\n  We demonstrate the timing difference caused by this microarchitectural\ndefense assumption violation using GEM5 simulations. Performance evaluation on\nSPECrate 2017 and PARSEC benchmarks of the two fixes show less than 32\\%\naverage overhead across both sets of benchmarks. The repair which prevented the\ncreation of remote E state had less than 2.8% average overhead."
                },
                "authors": [
                    {
                        "name": "Kartik Ramkrishnan"
                    },
                    {
                        "name": "Antonia Zhai"
                    },
                    {
                        "name": "Stephen McCamant"
                    },
                    {
                        "name": "Pen Chung Yew"
                    }
                ],
                "author_detail": {
                    "name": "Pen Chung Yew"
                },
                "author": "Pen Chung Yew",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10181v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10181v1",
                "updated": "2025-04-14T12:34:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    34,
                    20,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T12:34:20Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    34,
                    20,
                    0,
                    104,
                    0
                ],
                "title": "A New Paradigm in IBR Modeling for Power Flow and Short Circuit Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Paradigm in IBR Modeling for Power Flow and Short Circuit Analysis"
                },
                "summary": "The fault characteristics of inverter-based resources (IBRs) are different\nfrom conventional synchronous generators. The fault response of IBRs is\nnon-linear due to saturation states and mainly determined by fault ride through\n(FRT) strategies of the associated voltage source converter (VSC). This results\nin prohibitively large solution times for power flows considering these short\ncircuit characteristics, especially when the power system states change fast\ndue to uncertainty in IBR generations. To overcome this, a phasor-domain steady\nstate (SS) short circuit (SC) solver for IBR dominated power systems is\nproposed in this paper, and subsequently the developed IBR models are\nincorporated with a novel Jacobian-based Power Flow (PF) solver. In this\nmultiphase PF solver, any power system components can be modeled by considering\ntheir original non-linear or linear mathematical representations. Moreover, two\nnovel FRT strategies are proposed to fully utilize the converter capacity and\nto comply with IEEE-2800 2022 std and German grid code. The results are\ncompared with the Electromagnetic Transient (EMT) simulation on the IEEE 34\ntest network and the 120 kV EPRI benchmark system. The developed IBR sequence\ndomain PF model demonstrates more accurate behavior compared to the classical\nIBR generator model. The error in calculating the short circuit current with\nthe proposed SC solver is less than 3%, while achieving significant speed\nimprovements of three order of magnitudes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fault characteristics of inverter-based resources (IBRs) are different\nfrom conventional synchronous generators. The fault response of IBRs is\nnon-linear due to saturation states and mainly determined by fault ride through\n(FRT) strategies of the associated voltage source converter (VSC). This results\nin prohibitively large solution times for power flows considering these short\ncircuit characteristics, especially when the power system states change fast\ndue to uncertainty in IBR generations. To overcome this, a phasor-domain steady\nstate (SS) short circuit (SC) solver for IBR dominated power systems is\nproposed in this paper, and subsequently the developed IBR models are\nincorporated with a novel Jacobian-based Power Flow (PF) solver. In this\nmultiphase PF solver, any power system components can be modeled by considering\ntheir original non-linear or linear mathematical representations. Moreover, two\nnovel FRT strategies are proposed to fully utilize the converter capacity and\nto comply with IEEE-2800 2022 std and German grid code. The results are\ncompared with the Electromagnetic Transient (EMT) simulation on the IEEE 34\ntest network and the 120 kV EPRI benchmark system. The developed IBR sequence\ndomain PF model demonstrates more accurate behavior compared to the classical\nIBR generator model. The error in calculating the short circuit current with\nthe proposed SC solver is less than 3%, while achieving significant speed\nimprovements of three order of magnitudes."
                },
                "authors": [
                    {
                        "name": "Zahid Javid"
                    },
                    {
                        "name": "Firdous Ul Nazir"
                    },
                    {
                        "name": "Wentao Zhu"
                    },
                    {
                        "name": "Diptargha Chakravorty"
                    },
                    {
                        "name": "Ahmed Aboushady"
                    },
                    {
                        "name": "Mohamed Galeela"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Galeela"
                },
                "author": "Mohamed Galeela",
                "arxiv_comment": "12 Pages, First Revision Submitted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10181v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10181v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00601v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00601v3",
                "updated": "2025-04-14T11:20:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    11,
                    20,
                    56,
                    0,
                    104,
                    0
                ],
                "published": "2024-11-01T14:03:21Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    3,
                    21,
                    4,
                    306,
                    0
                ],
                "title": "Diversity in Network-Friendly Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity in Network-Friendly Recommendations"
                },
                "summary": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms."
                },
                "authors": [
                    {
                        "name": "Evangelia Tzimpimpaki"
                    },
                    {
                        "name": "Thrasyvoulos Spyropoulos"
                    }
                ],
                "author_detail": {
                    "name": "Thrasyvoulos Spyropoulos"
                },
                "author": "Thrasyvoulos Spyropoulos",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00601v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00601v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09984v1",
                "updated": "2025-04-14T08:51:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    8,
                    51,
                    35,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T08:51:35Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    8,
                    51,
                    35,
                    0,
                    104,
                    0
                ],
                "title": "On Precomputation and Caching in Information Retrieval Experiments with\n  Pipeline Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Precomputation and Caching in Information Retrieval Experiments with\n  Pipeline Architectures"
                },
                "summary": "Modern information retrieval systems often rely on multiple components\nexecuted in a pipeline. In a research setting, this can lead to substantial\nredundant computations (e.g., retrieving the same query multiple times for\nevaluating different downstream rerankers). To overcome this, researchers take\ncached \"result\" files as inputs, which represent the output of another\npipeline. However, these result files can be brittle and can cause a disconnect\nbetween the conceptual design of the pipeline and its logical implementation.\nTo overcome both the redundancy problem (when executing complete pipelines) and\nthe disconnect problem (when relying on intermediate result files), we describe\nour recent efforts to improve the caching capabilities in the open-source\nPyTerrier IR platform. We focus on two main directions: (1) automatic implicit\ncaching of common pipeline prefixes when comparing systems and (2) explicit\ncaching of operations through a new extension package, pyterrier-caching. These\napproaches allow for the best of both worlds: pipelines can be fully expressed\nend-to-end, while also avoiding redundant computations between pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern information retrieval systems often rely on multiple components\nexecuted in a pipeline. In a research setting, this can lead to substantial\nredundant computations (e.g., retrieving the same query multiple times for\nevaluating different downstream rerankers). To overcome this, researchers take\ncached \"result\" files as inputs, which represent the output of another\npipeline. However, these result files can be brittle and can cause a disconnect\nbetween the conceptual design of the pipeline and its logical implementation.\nTo overcome both the redundancy problem (when executing complete pipelines) and\nthe disconnect problem (when relying on intermediate result files), we describe\nour recent efforts to improve the caching capabilities in the open-source\nPyTerrier IR platform. We focus on two main directions: (1) automatic implicit\ncaching of common pipeline prefixes when comparing systems and (2) explicit\ncaching of operations through a new extension package, pyterrier-caching. These\napproaches allow for the best of both worlds: pipelines can be fully expressed\nend-to-end, while also avoiding redundant computations between pipelines."
                },
                "authors": [
                    {
                        "name": "Sean MacAvaney"
                    },
                    {
                        "name": "Craig Macdonald"
                    }
                ],
                "author_detail": {
                    "name": "Craig Macdonald"
                },
                "author": "Craig Macdonald",
                "arxiv_comment": "WOWS @ ECIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09952v1",
                "updated": "2025-04-14T07:30:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    7,
                    30,
                    3,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T07:30:03Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    7,
                    30,
                    3,
                    0,
                    104,
                    0
                ],
                "title": "Secrecy and Privacy in Multi-Access Combinatorial Topology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secrecy and Privacy in Multi-Access Combinatorial Topology"
                },
                "summary": "In this work, we consider the multi-access combinatorial topology with $C$\ncaches where each user accesses a unique set of $r$ caches. For this setup, we\nconsider secrecy, where each user should not know anything about the files it\ndid not request, and demand privacy, where each user's demand must be kept\nprivate from other non-colluding users. We propose a scheme satisfying both\nconditions and derive a lower bound based on cut-set arguments. Also, we prove\nthat our scheme is optimal when $r\\geq C-1$, and it is order-optimal when the\ncache memory size $M$ is greater than or equal to a certain threshold for\n$r<C-1$. When $r=1$, in most of the memory region, our scheme achieves the same\nrate as the one given by the secretive scheme for the dedicated cache setup by\nRavindrakumar et al. ( 'Private Coded Caching,' in \\textit{IEEE Transactions on\nInformation Forensics and Security}, 2018), while satisfying both secrecy and\ndemand privacy conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we consider the multi-access combinatorial topology with $C$\ncaches where each user accesses a unique set of $r$ caches. For this setup, we\nconsider secrecy, where each user should not know anything about the files it\ndid not request, and demand privacy, where each user's demand must be kept\nprivate from other non-colluding users. We propose a scheme satisfying both\nconditions and derive a lower bound based on cut-set arguments. Also, we prove\nthat our scheme is optimal when $r\\geq C-1$, and it is order-optimal when the\ncache memory size $M$ is greater than or equal to a certain threshold for\n$r<C-1$. When $r=1$, in most of the memory region, our scheme achieves the same\nrate as the one given by the secretive scheme for the dedicated cache setup by\nRavindrakumar et al. ( 'Private Coded Caching,' in \\textit{IEEE Transactions on\nInformation Forensics and Security}, 2018), while satisfying both secrecy and\ndemand privacy conditions."
                },
                "authors": [
                    {
                        "name": "Mallikharjuna Chinnapadamala"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "11 pages and 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09936v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09936v1",
                "updated": "2025-04-14T06:58:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    6,
                    58,
                    0,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T06:58:00Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    6,
                    58,
                    0,
                    0,
                    104,
                    0
                ],
                "title": "KeepKV: Eliminating Output Perturbation in KV Cache Compression for\n  Efficient LLMs Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KeepKV: Eliminating Output Perturbation in KV Cache Compression for\n  Efficient LLMs Inference"
                },
                "summary": "Efficient inference of large language models (LLMs) is hindered by an\never-growing key-value (KV) cache, making KV cache compression a critical\nresearch direction. Traditional methods selectively evict less important KV\ncache entries based on attention scores or position heuristics, which leads to\ninformation loss and hallucinations. Recently, merging-based strategies have\nbeen explored to retain more information by merging KV pairs that would be\ndiscarded; however, these existing approaches inevitably introduce\ninconsistencies in attention distributions before and after merging, causing\noutput perturbation and degraded generation quality. To overcome this\nchallenge, we propose KeepKV, a novel adaptive KV cache merging method designed\nto eliminate output perturbation while preserving performance under strict\nmemory constraints. KeepKV introduces the Electoral Votes mechanism that\nrecords merging history and adaptively adjusts attention scores. Moreover, it\nfurther leverages a novel Zero Inference-Perturbation Merging methods, keeping\nattention consistency and compensating for attention loss resulting from cache\nmerging. KeepKV successfully retains essential context information within a\nsignificantly compressed cache. Extensive experiments on various benchmarks and\nLLM architectures demonstrate that KeepKV substantially reduces memory usage,\nenhances inference throughput by more than 2x and keeps superior generation\nquality even with 10% KV cache budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient inference of large language models (LLMs) is hindered by an\never-growing key-value (KV) cache, making KV cache compression a critical\nresearch direction. Traditional methods selectively evict less important KV\ncache entries based on attention scores or position heuristics, which leads to\ninformation loss and hallucinations. Recently, merging-based strategies have\nbeen explored to retain more information by merging KV pairs that would be\ndiscarded; however, these existing approaches inevitably introduce\ninconsistencies in attention distributions before and after merging, causing\noutput perturbation and degraded generation quality. To overcome this\nchallenge, we propose KeepKV, a novel adaptive KV cache merging method designed\nto eliminate output perturbation while preserving performance under strict\nmemory constraints. KeepKV introduces the Electoral Votes mechanism that\nrecords merging history and adaptively adjusts attention scores. Moreover, it\nfurther leverages a novel Zero Inference-Perturbation Merging methods, keeping\nattention consistency and compensating for attention loss resulting from cache\nmerging. KeepKV successfully retains essential context information within a\nsignificantly compressed cache. Extensive experiments on various benchmarks and\nLLM architectures demonstrate that KeepKV substantially reduces memory usage,\nenhances inference throughput by more than 2x and keeps superior generation\nquality even with 10% KV cache budgets."
                },
                "authors": [
                    {
                        "name": "Yuxuan Tian"
                    },
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Yebo Peng"
                    },
                    {
                        "name": "Aomufei Yuan"
                    },
                    {
                        "name": "Zhiming Wang"
                    },
                    {
                        "name": "Bairen Yi"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Yong Cui"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "arxiv_comment": "18 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09936v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09936v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.12280v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.12280v2",
                "updated": "2025-04-13T14:17:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    17,
                    57,
                    6,
                    103,
                    0
                ],
                "published": "2024-02-19T16:47:04Z",
                "published_parsed": [
                    2024,
                    2,
                    19,
                    16,
                    47,
                    4,
                    0,
                    50,
                    0
                ],
                "title": "Plato: Plan to Efficiently Decode for Large Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plato: Plan to Efficiently Decode for Large Language Model Inference"
                },
                "summary": "Large language models (LLMs) have achieved remarkable success in natural\nlanguage tasks, but their inference incurs substantial computational and memory\noverhead. To improve efficiency, parallel decoding methods like\nSkeleton-of-Thought (SoT) decompose prompts into sub-problems for concurrent\nprocessing. However, these methods significantly compromise answer quality by\ntreating semantically linked sub-problems as independent. We propose Plato, a\nnovel approach that co-designs algorithms and systems for semantic-aware\nparallel decoding. Plato leverages LLMs to organize sub-problems into a\ndependency graph based on logical and causal relationships, enabling concurrent\ndecoding of non-dependent nodes while preserving answer coherence and quality.\nTo further enhance efficiency, Plato pipelines planning and node decoding\nstages, implements a global context cache, and carefully structures node\ninference prompts to maximize key-value cache reuse and minimize overhead. Our\nevaluations show that Plato improves throughput by 68% over autoregressive\ndecoding while achieving a 40% net win rate in answer quality. Compared to SoT,\nPlato demonstrates a remarkable 90% quality net-win rate. Ablation studies\nreveal that our pipeline design improves speedup by 29%, while our KV cache\nreuse optimization reduces overhead by 75%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable success in natural\nlanguage tasks, but their inference incurs substantial computational and memory\noverhead. To improve efficiency, parallel decoding methods like\nSkeleton-of-Thought (SoT) decompose prompts into sub-problems for concurrent\nprocessing. However, these methods significantly compromise answer quality by\ntreating semantically linked sub-problems as independent. We propose Plato, a\nnovel approach that co-designs algorithms and systems for semantic-aware\nparallel decoding. Plato leverages LLMs to organize sub-problems into a\ndependency graph based on logical and causal relationships, enabling concurrent\ndecoding of non-dependent nodes while preserving answer coherence and quality.\nTo further enhance efficiency, Plato pipelines planning and node decoding\nstages, implements a global context cache, and carefully structures node\ninference prompts to maximize key-value cache reuse and minimize overhead. Our\nevaluations show that Plato improves throughput by 68% over autoregressive\ndecoding while achieving a 40% net win rate in answer quality. Compared to SoT,\nPlato demonstrates a remarkable 90% quality net-win rate. Ablation studies\nreveal that our pipeline design improves speedup by 29%, while our KV cache\nreuse optimization reduces overhead by 75%."
                },
                "authors": [
                    {
                        "name": "Shuowei Jin"
                    },
                    {
                        "name": "Xueshen Liu"
                    },
                    {
                        "name": "Yongji Wu"
                    },
                    {
                        "name": "Haizhong Zheng"
                    },
                    {
                        "name": "Qingzhao Zhang"
                    },
                    {
                        "name": "Atul Prakash"
                    },
                    {
                        "name": "Matthew Lentz"
                    },
                    {
                        "name": "Danyang Zhuo"
                    },
                    {
                        "name": "Feng Qian"
                    },
                    {
                        "name": "Z. Morley Mao"
                    }
                ],
                "author_detail": {
                    "name": "Z. Morley Mao"
                },
                "author": "Z. Morley Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.12280v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.12280v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09590v1",
                "updated": "2025-04-13T14:16:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    16,
                    57,
                    6,
                    103,
                    0
                ],
                "published": "2025-04-13T14:16:57Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    16,
                    57,
                    6,
                    103,
                    0
                ],
                "title": "Efficient LLM Serving on Hybrid Real-time and Best-effort Requests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Serving on Hybrid Real-time and Best-effort Requests"
                },
                "summary": "Recent breakthroughs in large Language Models (LLMs) have enabled various\ngenerative tasks on a single model. Real-world services (e.g., OpenAI's ChatGPT\n[27]) powered by an LLM often concurrently support latency-critical requests\nfor interactive applications (e.g., question-answering systems, referred to as\nreal-time or RT requests) and throughput-oriented requests for back-of-house\nprocessing (e.g., documents batch processing [28], referred to best-effort or\nBE requests), with complex hybrid inference workloads to the underlying model.\nState-of-the-art (SOTA) LLM serving systems dedicate machines to each type of\nrequest, towards either low inference latency or high serving throughput,\nrespectively. This practice simplifies request scheduling and management but\nsuffers from poor resource utilization. We propose BROS, a hybrid LLM serving\nsystem that aims to collocate RT/BE requests, meeting RT requests' latency\nrequirements while maintaining BE requests' throughput. BROS formulates the\nproblem of hybrid RT/BE request scheduling and solves it with a dynamic\npriority-based algorithm. BROS designs a bidirectional KV cache management\nmechanism, allowing RT requests to share KV memory with BE requests to remove\nthe scheduling restrictions caused by insufficient KV memory and improve\nutilization. Extensive experiments validate that BROS achieves a good trade-off\nwhen serving hybrid RT and BE requests. It significantly reduces the latency of\nRT requests (up to 74.20%), improving their fine-grained service level\nobjectives (SLOs) attainments (up to 36.38x), with negligible throughput\nreduction for BE requests, showing significant advantages over SOTA systems\nlike vLLM and TGI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent breakthroughs in large Language Models (LLMs) have enabled various\ngenerative tasks on a single model. Real-world services (e.g., OpenAI's ChatGPT\n[27]) powered by an LLM often concurrently support latency-critical requests\nfor interactive applications (e.g., question-answering systems, referred to as\nreal-time or RT requests) and throughput-oriented requests for back-of-house\nprocessing (e.g., documents batch processing [28], referred to best-effort or\nBE requests), with complex hybrid inference workloads to the underlying model.\nState-of-the-art (SOTA) LLM serving systems dedicate machines to each type of\nrequest, towards either low inference latency or high serving throughput,\nrespectively. This practice simplifies request scheduling and management but\nsuffers from poor resource utilization. We propose BROS, a hybrid LLM serving\nsystem that aims to collocate RT/BE requests, meeting RT requests' latency\nrequirements while maintaining BE requests' throughput. BROS formulates the\nproblem of hybrid RT/BE request scheduling and solves it with a dynamic\npriority-based algorithm. BROS designs a bidirectional KV cache management\nmechanism, allowing RT requests to share KV memory with BE requests to remove\nthe scheduling restrictions caused by insufficient KV memory and improve\nutilization. Extensive experiments validate that BROS achieves a good trade-off\nwhen serving hybrid RT and BE requests. It significantly reduces the latency of\nRT requests (up to 74.20%), improving their fine-grained service level\nobjectives (SLOs) attainments (up to 36.38x), with negligible throughput\nreduction for BE requests, showing significant advantages over SOTA systems\nlike vLLM and TGI."
                },
                "authors": [
                    {
                        "name": "Wan Borui"
                    },
                    {
                        "name": "Zhao Juntao"
                    },
                    {
                        "name": "Jiang Chenyu"
                    },
                    {
                        "name": "Guo Chuanxiong"
                    },
                    {
                        "name": "Wu Chuan"
                    }
                ],
                "author_detail": {
                    "name": "Wu Chuan"
                },
                "author": "Wu Chuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15355v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15355v5",
                "updated": "2025-04-13T14:02:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    2,
                    47,
                    6,
                    103,
                    0
                ],
                "published": "2024-09-14T02:34:26Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    2,
                    34,
                    26,
                    5,
                    258,
                    0
                ],
                "title": "Block-Attention for Efficient Prefilling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-Attention for Efficient Prefilling"
                },
                "summary": "We introduce Block-attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context in an\nauto-regressive manner. Instead, Block-attention divides retrieved documents\ninto discrete blocks, with each block independently calculating key-value (KV)\nstates except for the final block. In RAG scenarios, by defining each passage\nas a block, Block-attention enables us to reuse the KV states of passages that\nhave been seen before, thereby significantly reducing the latency and the\ncomputation overhead during inference. The implementation of Block-attention\ninvolves block segmentation, position re-encoding, and fine-tuning the LLM to\nadapt to the Block-attention mechanism. Experiments on 11 diverse benchmarks,\nincluding RAG, ICL, and general domains, demonstrate that after block\nfine-tuning, the Block-attention model not only achieves performance comparable\nto that of full-attention models, but can also seamlessly switch between the\nblock and full attention modes without any performance loss. Notably,\nBlock-attention significantly reduces the time to first token (TTFT) and\nfloating point operations (FLOPs) to a very low level. It only takes 45 ms to\noutput the first token for an input sequence with a total length of 32K.\nCompared to the full-attention models, the TTFT and corresponding FLOPs are\nreduced by 98.7% and 99.8%, respectively. Additionally, in Appendix A, we\nelaborate on how Block-attention is applied in Game AI scenario and the\nsubstantial potential benefits it entails. We strongly suggest researchers in\nthe gaming field not to overlook this section.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Block-attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context in an\nauto-regressive manner. Instead, Block-attention divides retrieved documents\ninto discrete blocks, with each block independently calculating key-value (KV)\nstates except for the final block. In RAG scenarios, by defining each passage\nas a block, Block-attention enables us to reuse the KV states of passages that\nhave been seen before, thereby significantly reducing the latency and the\ncomputation overhead during inference. The implementation of Block-attention\ninvolves block segmentation, position re-encoding, and fine-tuning the LLM to\nadapt to the Block-attention mechanism. Experiments on 11 diverse benchmarks,\nincluding RAG, ICL, and general domains, demonstrate that after block\nfine-tuning, the Block-attention model not only achieves performance comparable\nto that of full-attention models, but can also seamlessly switch between the\nblock and full attention modes without any performance loss. Notably,\nBlock-attention significantly reduces the time to first token (TTFT) and\nfloating point operations (FLOPs) to a very low level. It only takes 45 ms to\noutput the first token for an input sequence with a total length of 32K.\nCompared to the full-attention models, the TTFT and corresponding FLOPs are\nreduced by 98.7% and 99.8%, respectively. Additionally, in Appendix A, we\nelaborate on how Block-attention is applied in Game AI scenario and the\nsubstantial potential benefits it entails. We strongly suggest researchers in\nthe gaming field not to overlook this section."
                },
                "authors": [
                    {
                        "name": "Dongyang Ma"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Lan Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lan Tian"
                },
                "author": "Lan Tian",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15355v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15355v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10540v1",
                "updated": "2025-04-13T08:29:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    8,
                    29,
                    58,
                    6,
                    103,
                    0
                ],
                "published": "2025-04-13T08:29:58Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    8,
                    29,
                    58,
                    6,
                    103,
                    0
                ],
                "title": "AB-Cache: Training-Free Acceleration of Diffusion Models via\n  Adams-Bashforth Cached Feature Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AB-Cache: Training-Free Acceleration of Diffusion Models via\n  Adams-Bashforth Cached Feature Reuse"
                },
                "summary": "Diffusion models have demonstrated remarkable success in generative tasks,\nyet their iterative denoising process results in slow inference, limiting their\npracticality. While existing acceleration methods exploit the well-known\nU-shaped similarity pattern between adjacent steps through caching mechanisms,\nthey lack theoretical foundation and rely on simplistic computation reuse,\noften leading to performance degradation. In this work, we provide a\ntheoretical understanding by analyzing the denoising process through the\nsecond-order Adams-Bashforth method, revealing a linear relationship between\nthe outputs of consecutive steps. This analysis explains why the outputs of\nadjacent steps exhibit a U-shaped pattern. Furthermore, extending\nAdams-Bashforth method to higher order, we propose a novel caching-based\nacceleration approach for diffusion models, instead of directly reusing cached\nresults, with a truncation error bound of only \\(O(h^k)\\) where $h$ is the step\nsize. Extensive validation across diverse image and video diffusion models\n(including HunyuanVideo and FLUX.1-dev) with various schedulers demonstrates\nour method's effectiveness in achieving nearly $3\\times$ speedup while\nmaintaining original performance levels, offering a practical real-time\nsolution without compromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have demonstrated remarkable success in generative tasks,\nyet their iterative denoising process results in slow inference, limiting their\npracticality. While existing acceleration methods exploit the well-known\nU-shaped similarity pattern between adjacent steps through caching mechanisms,\nthey lack theoretical foundation and rely on simplistic computation reuse,\noften leading to performance degradation. In this work, we provide a\ntheoretical understanding by analyzing the denoising process through the\nsecond-order Adams-Bashforth method, revealing a linear relationship between\nthe outputs of consecutive steps. This analysis explains why the outputs of\nadjacent steps exhibit a U-shaped pattern. Furthermore, extending\nAdams-Bashforth method to higher order, we propose a novel caching-based\nacceleration approach for diffusion models, instead of directly reusing cached\nresults, with a truncation error bound of only \\(O(h^k)\\) where $h$ is the step\nsize. Extensive validation across diverse image and video diffusion models\n(including HunyuanVideo and FLUX.1-dev) with various schedulers demonstrates\nour method's effectiveness in achieving nearly $3\\times$ speedup while\nmaintaining original performance levels, offering a practical real-time\nsolution without compromising generation quality."
                },
                "authors": [
                    {
                        "name": "Zichao Yu"
                    },
                    {
                        "name": "Zhen Zou"
                    },
                    {
                        "name": "Guojiang Shao"
                    },
                    {
                        "name": "Chengwei Zhang"
                    },
                    {
                        "name": "Shengze Xu"
                    },
                    {
                        "name": "Jie Huang"
                    },
                    {
                        "name": "Feng Zhao"
                    },
                    {
                        "name": "Xiaodong Cun"
                    },
                    {
                        "name": "Wenyi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wenyi Zhang"
                },
                "author": "Wenyi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09431v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09431v1",
                "updated": "2025-04-13T04:46:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    4,
                    46,
                    2,
                    6,
                    103,
                    0
                ],
                "published": "2025-04-13T04:46:02Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    4,
                    46,
                    2,
                    6,
                    103,
                    0
                ],
                "title": "Sub-nanosecond in-plane magnetization switching induced by field-like\n  spin-orbit torques from ferromagnets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-nanosecond in-plane magnetization switching induced by field-like\n  spin-orbit torques from ferromagnets"
                },
                "summary": "Spin-orbit torques (SOTs) generated in SOT-material/ferromagnet structures\nare classified as damping-like (DL) and field-like (FL) torques for\ncurrent-driven magnetization switching. It is well known that both DL- and\nFL-SOTs originate from the SOT-material and DL-SOT dominates the current-driven\nswitching process while FL-SOT contributes limitedly, resulting in an\nincubation time (several nanoseconds) during collinear magnetization switching\nwith the spin polarization because of the DL attributes. Here we report a\nFL-SOT originated from the ferromagnet, different from the origin of DL-SOT,\nand demonstrate that it dominates the collinear magnetization switching. We\nshow that the FL-SOT and resultant collinear switching can be modulated, one\norder of magnitude and sign reversal, by controlling the ferromagnet. Because\nof no incubation time and higher charge-to-spin efficiencies in the FL\nswitching, we further show that the switching time can be down to 200 ps with\none order lower critical switching current density compared to DL switching.\nThese results indicate that the FL switching may provide a practical solution\nfor magnetic memory in speed-priority cache applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spin-orbit torques (SOTs) generated in SOT-material/ferromagnet structures\nare classified as damping-like (DL) and field-like (FL) torques for\ncurrent-driven magnetization switching. It is well known that both DL- and\nFL-SOTs originate from the SOT-material and DL-SOT dominates the current-driven\nswitching process while FL-SOT contributes limitedly, resulting in an\nincubation time (several nanoseconds) during collinear magnetization switching\nwith the spin polarization because of the DL attributes. Here we report a\nFL-SOT originated from the ferromagnet, different from the origin of DL-SOT,\nand demonstrate that it dominates the collinear magnetization switching. We\nshow that the FL-SOT and resultant collinear switching can be modulated, one\norder of magnitude and sign reversal, by controlling the ferromagnet. Because\nof no incubation time and higher charge-to-spin efficiencies in the FL\nswitching, we further show that the switching time can be down to 200 ps with\none order lower critical switching current density compared to DL switching.\nThese results indicate that the FL switching may provide a practical solution\nfor magnetic memory in speed-priority cache applications."
                },
                "authors": [
                    {
                        "name": "Hanying Zhang"
                    },
                    {
                        "name": "Ziqian Cui"
                    },
                    {
                        "name": "Baiqing Jiang"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "C. Bi"
                    }
                ],
                "author_detail": {
                    "name": "C. Bi"
                },
                "author": "C. Bi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09431v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09261v1",
                "updated": "2025-04-12T15:42:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    12,
                    15,
                    42,
                    17,
                    5,
                    102,
                    0
                ],
                "published": "2025-04-12T15:42:17Z",
                "published_parsed": [
                    2025,
                    4,
                    12,
                    15,
                    42,
                    17,
                    5,
                    102,
                    0
                ],
                "title": "Head-Aware KV Cache Compression for Efficient Visual Autoregressive\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Head-Aware KV Cache Compression for Efficient Visual Autoregressive\n  Modeling"
                },
                "summary": "Visual Autoregressive (VAR) models have emerged as a powerful approach for\nmulti-modal content creation, offering high efficiency and quality across\ndiverse multimedia applications. However, they face significant memory\nbottlenecks due to extensive KV cache accumulation during inference. Existing\nKV cache compression techniques for large language models are suboptimal for\nVAR models due to, as we identify in this paper, two distinct categories of\nattention heads in VAR models: Structural Heads, which preserve spatial\ncoherence through diagonal attention patterns, and Contextual Heads, which\nmaintain semantic consistency through vertical attention patterns. These\ndifferences render single-strategy KV compression techniques ineffective for\nVAR models. To address this, we propose HACK, a training-free Head-Aware\nCompression method for KV cache. HACK allocates asymmetric cache budgets and\nemploys pattern-specific compression strategies tailored to the essential\ncharacteristics of each head category. Experiments on Infinity-2B, Infinity-8B,\nand VAR-d30 demonstrate its effectiveness in text-to-image and\nclass-conditional generation tasks. HACK can hack down up to 50\\% and 70\\% of\ncache with minimal performance degradation for VAR-d30 and Infinity-8B,\nrespectively. Even with 70\\% and 90\\% KV cache compression in VAR-d30 and\nInfinity-8B, HACK still maintains high-quality generation while reducing memory\nusage by 44.2\\% and 58.9\\%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Autoregressive (VAR) models have emerged as a powerful approach for\nmulti-modal content creation, offering high efficiency and quality across\ndiverse multimedia applications. However, they face significant memory\nbottlenecks due to extensive KV cache accumulation during inference. Existing\nKV cache compression techniques for large language models are suboptimal for\nVAR models due to, as we identify in this paper, two distinct categories of\nattention heads in VAR models: Structural Heads, which preserve spatial\ncoherence through diagonal attention patterns, and Contextual Heads, which\nmaintain semantic consistency through vertical attention patterns. These\ndifferences render single-strategy KV compression techniques ineffective for\nVAR models. To address this, we propose HACK, a training-free Head-Aware\nCompression method for KV cache. HACK allocates asymmetric cache budgets and\nemploys pattern-specific compression strategies tailored to the essential\ncharacteristics of each head category. Experiments on Infinity-2B, Infinity-8B,\nand VAR-d30 demonstrate its effectiveness in text-to-image and\nclass-conditional generation tasks. HACK can hack down up to 50\\% and 70\\% of\ncache with minimal performance degradation for VAR-d30 and Infinity-8B,\nrespectively. Even with 70\\% and 90\\% KV cache compression in VAR-d30 and\nInfinity-8B, HACK still maintains high-quality generation while reducing memory\nusage by 44.2\\% and 58.9\\%, respectively."
                },
                "authors": [
                    {
                        "name": "Ziran Qin"
                    },
                    {
                        "name": "Youru Lv"
                    },
                    {
                        "name": "Mingbao Lin"
                    },
                    {
                        "name": "Zeren Zhang"
                    },
                    {
                        "name": "Danping Zou"
                    },
                    {
                        "name": "Weiyao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Weiyao Lin"
                },
                "author": "Weiyao Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17459v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17459v3",
                "updated": "2025-04-11T12:31:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    12,
                    31,
                    7,
                    4,
                    101,
                    0
                ],
                "published": "2024-11-26T14:23:53Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    23,
                    53,
                    1,
                    331,
                    0
                ],
                "title": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model"
                },
                "summary": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE."
                },
                "authors": [
                    {
                        "name": "Zongjian Li"
                    },
                    {
                        "name": "Bin Lin"
                    },
                    {
                        "name": "Yang Ye"
                    },
                    {
                        "name": "Liuhan Chen"
                    },
                    {
                        "name": "Xinhua Cheng"
                    },
                    {
                        "name": "Shenghai Yuan"
                    },
                    {
                        "name": "Li Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Li Yuan"
                },
                "author": "Li Yuan",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17459v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17459v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08378v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08378v1",
                "updated": "2025-04-11T09:26:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    26,
                    47,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T09:26:47Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    26,
                    47,
                    4,
                    101,
                    0
                ],
                "title": "Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and\n  Flash",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and\n  Flash"
                },
                "summary": "Large language models (LLMs) are increasingly being deployed on mobile\ndevices, but the limited DRAM capacity constrains the deployable model size.\nThis paper introduces ActiveFlow, the first LLM inference framework that can\nachieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the\nscaling up of deployable model sizes. The framework is based on the novel\nconcept of active weight DRAM-flash swapping and incorporates three novel\ntechniques: (1) Cross-layer active weights preloading. It uses the activations\nfrom the current layer to predict the active weights of several subsequent\nlayers, enabling computation and data loading to overlap, as well as\nfacilitating large I/O transfers. (2) Sparsity-aware self-distillation. It\nadjusts the active weights to align with the dense-model output distribution,\ncompensating for approximations introduced by contextual sparsity. (3) Active\nweight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation\namong the hot weight cache, preloaded active weights, and computation-involved\nweights based on available memory. Results show ActiveFlow achieves the\nperformance-cost Pareto frontier compared to existing efficiency optimization\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being deployed on mobile\ndevices, but the limited DRAM capacity constrains the deployable model size.\nThis paper introduces ActiveFlow, the first LLM inference framework that can\nachieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the\nscaling up of deployable model sizes. The framework is based on the novel\nconcept of active weight DRAM-flash swapping and incorporates three novel\ntechniques: (1) Cross-layer active weights preloading. It uses the activations\nfrom the current layer to predict the active weights of several subsequent\nlayers, enabling computation and data loading to overlap, as well as\nfacilitating large I/O transfers. (2) Sparsity-aware self-distillation. It\nadjusts the active weights to align with the dense-model output distribution,\ncompensating for approximations introduced by contextual sparsity. (3) Active\nweight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation\namong the hot weight cache, preloaded active weights, and computation-involved\nweights based on available memory. Results show ActiveFlow achieves the\nperformance-cost Pareto frontier compared to existing efficiency optimization\nmethods."
                },
                "authors": [
                    {
                        "name": "Fucheng Jia"
                    },
                    {
                        "name": "Zewen Wu"
                    },
                    {
                        "name": "Shiqi Jiang"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Yunxin Liu"
                    },
                    {
                        "name": "Ju Ren"
                    },
                    {
                        "name": "Deyu Zhang"
                    },
                    {
                        "name": "Ting Cao"
                    }
                ],
                "author_detail": {
                    "name": "Ting Cao"
                },
                "author": "Ting Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08378v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08378v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08204v1",
                "updated": "2025-04-11T02:10:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    10,
                    2,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T02:10:02Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    10,
                    2,
                    4,
                    101,
                    0
                ],
                "title": "II-NVM: Enhancing Map Accuracy and Consistency with Normal\n  Vector-Assisted Mapping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "II-NVM: Enhancing Map Accuracy and Consistency with Normal\n  Vector-Assisted Mapping"
                },
                "summary": "SLAM technology plays a crucial role in indoor mapping and localization. A\ncommon challenge in indoor environments is the \"double-sided mapping issue\",\nwhere closely positioned walls, doors, and other surfaces are mistakenly\nidentified as a single plane, significantly hindering map accuracy and\nconsistency. To address this issue this paper introduces a SLAM approach that\nensures accurate mapping using normal vector consistency. We enhance the voxel\nmap structure to store both point cloud data and normal vector information,\nenabling the system to evaluate consistency during nearest neighbor searches\nand map updates. This process distinguishes between the front and back sides of\nsurfaces, preventing incorrect point-to-plane constraints. Moreover, we\nimplement an adaptive radius KD-tree search method that dynamically adjusts the\nsearch radius based on the local density of the point cloud, thereby enhancing\nthe accuracy of normal vector calculations. To further improve realtime\nperformance and storage efficiency, we incorporate a Least Recently Used (LRU)\ncache strategy, which facilitates efficient incremental updates of the voxel\nmap. The code is released as open-source and validated in both simulated\nenvironments and real indoor scenarios. Experimental results demonstrate that\nthis approach effectively resolves the \"double-sided mapping issue\" and\nsignificantly improves mapping precision. Additionally, we have developed and\nopen-sourced the first simulation and real world dataset specifically tailored\nfor the \"double-sided mapping issue\".",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLAM technology plays a crucial role in indoor mapping and localization. A\ncommon challenge in indoor environments is the \"double-sided mapping issue\",\nwhere closely positioned walls, doors, and other surfaces are mistakenly\nidentified as a single plane, significantly hindering map accuracy and\nconsistency. To address this issue this paper introduces a SLAM approach that\nensures accurate mapping using normal vector consistency. We enhance the voxel\nmap structure to store both point cloud data and normal vector information,\nenabling the system to evaluate consistency during nearest neighbor searches\nand map updates. This process distinguishes between the front and back sides of\nsurfaces, preventing incorrect point-to-plane constraints. Moreover, we\nimplement an adaptive radius KD-tree search method that dynamically adjusts the\nsearch radius based on the local density of the point cloud, thereby enhancing\nthe accuracy of normal vector calculations. To further improve realtime\nperformance and storage efficiency, we incorporate a Least Recently Used (LRU)\ncache strategy, which facilitates efficient incremental updates of the voxel\nmap. The code is released as open-source and validated in both simulated\nenvironments and real indoor scenarios. Experimental results demonstrate that\nthis approach effectively resolves the \"double-sided mapping issue\" and\nsignificantly improves mapping precision. Additionally, we have developed and\nopen-sourced the first simulation and real world dataset specifically tailored\nfor the \"double-sided mapping issue\"."
                },
                "authors": [
                    {
                        "name": "Chengwei Zhao"
                    },
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Yina Jian"
                    },
                    {
                        "name": "Jie Xu"
                    },
                    {
                        "name": "Linji Wang"
                    },
                    {
                        "name": "Yongxin Ma"
                    },
                    {
                        "name": "Xinglai Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xinglai Jin"
                },
                "author": "Xinglai Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07596v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07596v2",
                "updated": "2025-04-11T02:05:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    5,
                    1,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-10T09:48:56Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    9,
                    48,
                    56,
                    3,
                    100,
                    0
                ],
                "title": "Boosting Universal LLM Reward Design through Heuristic Reward\n  Observation Space Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Universal LLM Reward Design through Heuristic Reward\n  Observation Space Evolution"
                },
                "summary": "Large Language Models (LLMs) are emerging as promising tools for automated\nreinforcement learning (RL) reward design, owing to their robust capabilities\nin commonsense reasoning and code generation. By engaging in dialogues with RL\nagents, LLMs construct a Reward Observation Space (ROS) by selecting relevant\nenvironment states and defining their internal operations. However, existing\nframeworks have not effectively leveraged historical exploration data or manual\ntask descriptions to iteratively evolve this space. In this paper, we propose a\nnovel heuristic framework that enhances LLM-driven reward design by evolving\nthe ROS through a table-based exploration caching mechanism and a text-code\nreconciliation strategy. Our framework introduces a state execution table,\nwhich tracks the historical usage and success rates of environment states,\novercoming the Markovian constraint typically found in LLM dialogues and\nfacilitating more effective exploration. Furthermore, we reconcile\nuser-provided task descriptions with expert-defined success criteria using\nstructured prompts, ensuring alignment in reward design objectives.\nComprehensive evaluations on benchmark RL tasks demonstrate the effectiveness\nand stability of the proposed framework. Code and video demos are available at\njingjjjjjie.github.io/LLM2Reward.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are emerging as promising tools for automated\nreinforcement learning (RL) reward design, owing to their robust capabilities\nin commonsense reasoning and code generation. By engaging in dialogues with RL\nagents, LLMs construct a Reward Observation Space (ROS) by selecting relevant\nenvironment states and defining their internal operations. However, existing\nframeworks have not effectively leveraged historical exploration data or manual\ntask descriptions to iteratively evolve this space. In this paper, we propose a\nnovel heuristic framework that enhances LLM-driven reward design by evolving\nthe ROS through a table-based exploration caching mechanism and a text-code\nreconciliation strategy. Our framework introduces a state execution table,\nwhich tracks the historical usage and success rates of environment states,\novercoming the Markovian constraint typically found in LLM dialogues and\nfacilitating more effective exploration. Furthermore, we reconcile\nuser-provided task descriptions with expert-defined success criteria using\nstructured prompts, ensuring alignment in reward design objectives.\nComprehensive evaluations on benchmark RL tasks demonstrate the effectiveness\nand stability of the proposed framework. Code and video demos are available at\njingjjjjjie.github.io/LLM2Reward."
                },
                "authors": [
                    {
                        "name": "Zen Kit Heng"
                    },
                    {
                        "name": "Zimeng Zhao"
                    },
                    {
                        "name": "Tianhao Wu"
                    },
                    {
                        "name": "Yuanfei Wang"
                    },
                    {
                        "name": "Mingdong Wu"
                    },
                    {
                        "name": "Yangang Wang"
                    },
                    {
                        "name": "Hao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Hao Dong"
                },
                "author": "Hao Dong",
                "arxiv_comment": "7 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07596v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07596v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07815v1",
                "updated": "2025-04-10T14:52:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    52,
                    3,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T14:52:03Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    52,
                    3,
                    3,
                    100,
                    0
                ],
                "title": "Siren Federate: Bridging document, relational, and graph models for\n  exploratory graph analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Siren Federate: Bridging document, relational, and graph models for\n  exploratory graph analysis"
                },
                "summary": "Investigative workflows require interactive exploratory analysis on large\nheterogeneous knowledge graphs. Current databases show limitations in enabling\nsuch task. This paper discusses the architecture of Siren Federate, a system\nthat efficiently supports exploratory graph analysis by bridging\ndocument-oriented, relational and graph models. Technical contributions include\ndistributed join algorithms, adaptive query planning, query plan folding,\nsemantic caching, and semi-join decomposition for path query. Semi-join\ndecomposition addresses the exponential growth of intermediate results in\npath-based queries. Experiments show that Siren Federate exhibits low latency\nand scales well with the amount of data, the number of users, and the number of\ncomputing nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigative workflows require interactive exploratory analysis on large\nheterogeneous knowledge graphs. Current databases show limitations in enabling\nsuch task. This paper discusses the architecture of Siren Federate, a system\nthat efficiently supports exploratory graph analysis by bridging\ndocument-oriented, relational and graph models. Technical contributions include\ndistributed join algorithms, adaptive query planning, query plan folding,\nsemantic caching, and semi-join decomposition for path query. Semi-join\ndecomposition addresses the exponential growth of intermediate results in\npath-based queries. Experiments show that Siren Federate exhibits low latency\nand scales well with the amount of data, the number of users, and the number of\ncomputing nodes."
                },
                "authors": [
                    {
                        "name": "Georgeta Bordea"
                    },
                    {
                        "name": "Stephane Campinas"
                    },
                    {
                        "name": "Matteo Catena"
                    },
                    {
                        "name": "Renaud Delbru"
                    }
                ],
                "author_detail": {
                    "name": "Renaud Delbru"
                },
                "author": "Renaud Delbru",
                "arxiv_comment": "36 pages, 16 figures, submitted to the ComSIS journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.11; E.1; H.2.4; H.3.3; H.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07642v1",
                "updated": "2025-04-10T10:43:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    43,
                    42,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T10:43:42Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    43,
                    42,
                    3,
                    100,
                    0
                ],
                "title": "Cache-a-lot: Pushing the Limits of Unsatisfiable Core Reuse in SMT-Based\n  Program Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-a-lot: Pushing the Limits of Unsatisfiable Core Reuse in SMT-Based\n  Program Analysis"
                },
                "summary": "Satisfiability Modulo Theories (SMT) solvers are integral to program analysis\ntechniques like concolic and symbolic execution, where they help assess the\nsatisfiability of logical formulae to explore execution paths of the program\nunder test. However, frequent solver invocations are still the main performance\nbottleneck of these techniques. One way to mitigate this challenge is through\noptimizations such as caching and reusing solver results. While current methods\ntypically focus on reusing results from fully equivalent or closely related\nformulas, they often miss broader opportunities for reuse. In this paper, we\npropose a novel approach, Cache-a-lot, that extends the reuse of unsatisfiable\n(unsat) results by systematically considering all possible variable\nsubstitutions. This enables more extensive reuse of results, thereby reducing\nthe number of SMT solver invocations and improving the overall efficiency of\nconcolic and symbolic execution. Our evaluation, conducted against the\nstate-of-the-art Utopia solution using two benchmark sets, shows significant\nimprovements, particularly with more complex formulas. Our method achieves up\nto 74% unsat core reuse, compared to Utopia's 41%, and significant increase in\nthe time savings. These results demonstrate that, despite the additional\ncomputational complexity, the broader reuse of unsat results significantly\nenhances performance, offering valuable advancements for formal verification\nand program analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Satisfiability Modulo Theories (SMT) solvers are integral to program analysis\ntechniques like concolic and symbolic execution, where they help assess the\nsatisfiability of logical formulae to explore execution paths of the program\nunder test. However, frequent solver invocations are still the main performance\nbottleneck of these techniques. One way to mitigate this challenge is through\noptimizations such as caching and reusing solver results. While current methods\ntypically focus on reusing results from fully equivalent or closely related\nformulas, they often miss broader opportunities for reuse. In this paper, we\npropose a novel approach, Cache-a-lot, that extends the reuse of unsatisfiable\n(unsat) results by systematically considering all possible variable\nsubstitutions. This enables more extensive reuse of results, thereby reducing\nthe number of SMT solver invocations and improving the overall efficiency of\nconcolic and symbolic execution. Our evaluation, conducted against the\nstate-of-the-art Utopia solution using two benchmark sets, shows significant\nimprovements, particularly with more complex formulas. Our method achieves up\nto 74% unsat core reuse, compared to Utopia's 41%, and significant increase in\nthe time savings. These results demonstrate that, despite the additional\ncomputational complexity, the broader reuse of unsat results significantly\nenhances performance, offering valuable advancements for formal verification\nand program analysis."
                },
                "authors": [
                    {
                        "name": "Rustam Sadykov"
                    },
                    {
                        "name": "Azat Abdullin"
                    },
                    {
                        "name": "Marat Akhin"
                    }
                ],
                "author_detail": {
                    "name": "Marat Akhin"
                },
                "author": "Marat Akhin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07494v1",
                "updated": "2025-04-10T06:51:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    51,
                    23,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T06:51:23Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    51,
                    23,
                    3,
                    100,
                    0
                ],
                "title": "Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM\n  Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM\n  Inference Serving"
                },
                "summary": "Large language model (LLM) inference serving systems are essential to various\nLLM-based applications. As demand for LLM services continues to grow, scaling\nthese systems to handle high request rates while meeting latency Service-Level\nObjectives (SLOs), referred to as effective throughput, becomes critical.\nHowever, existing systems often struggle to improve effective throughput,\nprimarily due to a significant decline in Time To First Token (TTFT) SLO\nattainment. We identify two major causes of this bottleneck: (1)\nmemory-intensive KV cache that limits batch size expansion under GPU memory\nconstraints, and (2) rigid batch composition enforced by the default\nFirst-Come-First-Serve scheduling policy. In this paper, we introduce\nApt-Serve, a scalable framework designed to enhance effective throughput in LLM\ninference serving. Apt-Serve features a new hybrid cache scheme that combines\nKV cache with a memory-efficient hidden cache for reusable input hidden state\nvectors, allowing large batch sizes and improving request concurrency. Based on\nthe hybrid cache, Apt-Serve employs an adaptive runtime scheduling mechanism\nthat dynamically optimizes batch composition. We formally define the adaptive\nscheduling optimization problem and propose an efficient algorithm with\ntheoretical guarantees. Extensive evaluations on three real-world datasets and\nLLMs ranging from 13B to 66B parameters demonstrate that Apt-Serve achieves up\nto 8.8x improvement in effective throughput compared to the state-of-the-art\ninference serving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference serving systems are essential to various\nLLM-based applications. As demand for LLM services continues to grow, scaling\nthese systems to handle high request rates while meeting latency Service-Level\nObjectives (SLOs), referred to as effective throughput, becomes critical.\nHowever, existing systems often struggle to improve effective throughput,\nprimarily due to a significant decline in Time To First Token (TTFT) SLO\nattainment. We identify two major causes of this bottleneck: (1)\nmemory-intensive KV cache that limits batch size expansion under GPU memory\nconstraints, and (2) rigid batch composition enforced by the default\nFirst-Come-First-Serve scheduling policy. In this paper, we introduce\nApt-Serve, a scalable framework designed to enhance effective throughput in LLM\ninference serving. Apt-Serve features a new hybrid cache scheme that combines\nKV cache with a memory-efficient hidden cache for reusable input hidden state\nvectors, allowing large batch sizes and improving request concurrency. Based on\nthe hybrid cache, Apt-Serve employs an adaptive runtime scheduling mechanism\nthat dynamically optimizes batch composition. We formally define the adaptive\nscheduling optimization problem and propose an efficient algorithm with\ntheoretical guarantees. Extensive evaluations on three real-world datasets and\nLLMs ranging from 13B to 66B parameters demonstrate that Apt-Serve achieves up\nto 8.8x improvement in effective throughput compared to the state-of-the-art\ninference serving systems."
                },
                "authors": [
                    {
                        "name": "Shihong Gao"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Yanyan Shen"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "arxiv_doi": "10.1145/3725394",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3725394",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.07494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07479v1",
                "updated": "2025-04-10T06:13:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    13,
                    30,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T06:13:30Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    13,
                    30,
                    3,
                    100,
                    0
                ],
                "title": "UniCAIM: A Unified CAM/CIM Architecture with Static-Dynamic KV Cache\n  Pruning for Efficient Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniCAIM: A Unified CAM/CIM Architecture with Static-Dynamic KV Cache\n  Pruning for Efficient Long-Context LLM Inference"
                },
                "summary": "Transformer-based large language models (LLMs) have achieved impressive\nperformance in various natural language processing (NLP) applications. However,\nthe high memory and computation cost induced by the KV cache limits the\ninference efficiency, especially for long input sequences. Compute-in-memory\n(CIM)-based accelerators have been proposed for LLM acceleration with KV cache\npruning. However, as existing accelerators only support static pruning with a\nfixed pattern or dynamic pruning with primitive implementations, they suffer\nfrom either high accuracy degradation or low efficiency. In this paper, we\npropose a ferroelectric FET (FeFET)-based unified content addressable memory\n(CAM) and CIM architecture, dubbed as UniCAIM. UniCAIM features simultaneous\nsupport for static and dynamic pruning with 3 computation modes: 1) in the CAM\nmode, UniCAIM enables approximate similarity measurement in O(1) time for\ndynamic KV cache pruning with high energy efficiency; 2) in the charge-domain\nCIM mode, static pruning can be supported based on accumulative similarity\nscore, which is much more flexible compared to fixed patterns; 3) in the\ncurrent-domain mode, exact attention computation can be conducted with a subset\nof selected KV cache. We further propose a novel CAM/CIM cell design that\nleverages the multi-level characteristics of FeFETs for signed multibit storage\nof the KV cache and in-place attention computation. With extensive experimental\nresults, we demonstrate UniCAIM can reduce the area-energy-delay product (AEDP)\nby 8.2-831x over the state-ofthe-art CIM-based LLM accelerators at the circuit\nlevel, along with high accuracy comparable with dense attention at the\napplication level, showing its great potential for efficient long-context LLM\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) have achieved impressive\nperformance in various natural language processing (NLP) applications. However,\nthe high memory and computation cost induced by the KV cache limits the\ninference efficiency, especially for long input sequences. Compute-in-memory\n(CIM)-based accelerators have been proposed for LLM acceleration with KV cache\npruning. However, as existing accelerators only support static pruning with a\nfixed pattern or dynamic pruning with primitive implementations, they suffer\nfrom either high accuracy degradation or low efficiency. In this paper, we\npropose a ferroelectric FET (FeFET)-based unified content addressable memory\n(CAM) and CIM architecture, dubbed as UniCAIM. UniCAIM features simultaneous\nsupport for static and dynamic pruning with 3 computation modes: 1) in the CAM\nmode, UniCAIM enables approximate similarity measurement in O(1) time for\ndynamic KV cache pruning with high energy efficiency; 2) in the charge-domain\nCIM mode, static pruning can be supported based on accumulative similarity\nscore, which is much more flexible compared to fixed patterns; 3) in the\ncurrent-domain mode, exact attention computation can be conducted with a subset\nof selected KV cache. We further propose a novel CAM/CIM cell design that\nleverages the multi-level characteristics of FeFETs for signed multibit storage\nof the KV cache and in-place attention computation. With extensive experimental\nresults, we demonstrate UniCAIM can reduce the area-energy-delay product (AEDP)\nby 8.2-831x over the state-ofthe-art CIM-based LLM accelerators at the circuit\nlevel, along with high accuracy comparable with dense attention at the\napplication level, showing its great potential for efficient long-context LLM\ninference."
                },
                "authors": [
                    {
                        "name": "Weikai Xu"
                    },
                    {
                        "name": "Wenxuan Zeng"
                    },
                    {
                        "name": "Qianqian Huang"
                    },
                    {
                        "name": "Meng Li"
                    },
                    {
                        "name": "Ru Huang"
                    }
                ],
                "author_detail": {
                    "name": "Ru Huang"
                },
                "author": "Ru Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19379v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19379v3",
                "updated": "2025-04-10T05:06:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    5,
                    6,
                    29,
                    3,
                    100,
                    0
                ],
                "published": "2024-11-28T21:10:20Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    21,
                    10,
                    20,
                    3,
                    333,
                    0
                ],
                "title": "Marconi: Prefix Caching for the Era of Hybrid LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Marconi: Prefix Caching for the Era of Hybrid LLMs"
                },
                "summary": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems."
                },
                "authors": [
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Zhuang Wang"
                    },
                    {
                        "name": "Zhen Jia"
                    },
                    {
                        "name": "Can Karakus"
                    },
                    {
                        "name": "Luca Zancato"
                    },
                    {
                        "name": "Tri Dao"
                    },
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Ravi Netravali"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Netravali"
                },
                "author": "Ravi Netravali",
                "arxiv_comment": "MLSys 2025 camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19379v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19379v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07056v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07056v2",
                "updated": "2025-04-09T21:47:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    21,
                    47,
                    31,
                    2,
                    99,
                    0
                ],
                "published": "2025-01-13T04:31:04Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    4,
                    31,
                    4,
                    0,
                    13,
                    0
                ],
                "title": "MAGNUS: Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAGNUS: Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs"
                },
                "summary": "Sparse general matrix-matrix multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines, including Intel MKL, for a variety of different matrices on three\nIntel architectures. For matrices from the SuiteSparse collection, MAGNUS is\nfaster than all the baselines in most cases and is often an order of magnitude\nfaster than at least one baseline. For massive random matrices, MAGNUS scales\nto the largest matrix sizes, while the baselines do not. Furthermore, MAGNUS is\nclose to the optimal bound for these matrices, regardless of the matrix size,\nstructure, and density.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse general matrix-matrix multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines, including Intel MKL, for a variety of different matrices on three\nIntel architectures. For matrices from the SuiteSparse collection, MAGNUS is\nfaster than all the baselines in most cases and is often an order of magnitude\nfaster than at least one baseline. For massive random matrices, MAGNUS scales\nto the largest matrix sizes, while the baselines do not. Furthermore, MAGNUS is\nclose to the optimal bound for these matrices, regardless of the matrix size,\nstructure, and density."
                },
                "authors": [
                    {
                        "name": "Jordi Wolfson-Pou"
                    },
                    {
                        "name": "Jan Laukemann"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Petrini"
                },
                "author": "Fabrizio Petrini",
                "arxiv_comment": "Accepted to ICS25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07056v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07056v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06425v3",
                "updated": "2025-04-09T20:51:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    20,
                    51,
                    8,
                    2,
                    99,
                    0
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "summary": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPA's memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPA's memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "arxiv_comment": "31 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06425v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06425v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06261v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06261v2",
                "updated": "2025-04-09T17:56:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    56,
                    8,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-08T17:59:41Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    59,
                    41,
                    1,
                    98,
                    0
                ],
                "title": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention"
                },
                "summary": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the instances to come up with their own collaboration strategy\nfor the problem at hand, all the while \"seeing\" each other's partial progress\nin the concurrent cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\ngenerated tokens. Hogwild! inference takes advantage of Rotary Position\nEmbeddings (RoPE) to avoid recomputation while improving parallel hardware\nutilization. We find that modern reasoning-capable LLMs can perform inference\nwith shared Key-Value cache out of the box, without additional fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the instances to come up with their own collaboration strategy\nfor the problem at hand, all the while \"seeing\" each other's partial progress\nin the concurrent cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\ngenerated tokens. Hogwild! inference takes advantage of Rotary Position\nEmbeddings (RoPE) to avoid recomputation while improving parallel hardware\nutilization. We find that modern reasoning-capable LLMs can perform inference\nwith shared Key-Value cache out of the box, without additional fine-tuning."
                },
                "authors": [
                    {
                        "name": "Gleb Rodionov"
                    },
                    {
                        "name": "Roman Garipov"
                    },
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "George Yakushev"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Anton Sinitsin"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06261v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06261v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04514v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04514v2",
                "updated": "2025-04-09T14:36:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    14,
                    36,
                    19,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-06T15:15:07Z",
                "published_parsed": [
                    2025,
                    4,
                    6,
                    15,
                    15,
                    7,
                    6,
                    96,
                    0
                ],
                "title": "Saliency-driven Dynamic Token Pruning for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Saliency-driven Dynamic Token Pruning for Large Language Models"
                },
                "summary": "Despite the recent success of large language models (LLMs), LLMs are\nparticularly challenging in long-sequence inference scenarios due to the\nquadratic computational complexity of the attention mechanism. Inspired by the\ninterpretability theory of feature attribution in neural network models, we\nobserve that not all tokens have the same contribution. Based on this\nobservation, we propose a novel token pruning framework, namely Saliency-driven\nDynamic Token Pruning (SDTP), to gradually and dynamically prune redundant\ntokens based on the input context. Specifically, a lightweight saliency-driven\nprediction module is designed to estimate the importance score of each token\nwith its hidden state, which is added to different layers of the LLM to\nhierarchically prune redundant tokens. Furthermore, a ranking-based\noptimization strategy is proposed to minimize the ranking divergence of the\nsaliency score and the predicted importance score. Extensive experiments have\nshown that our framework is generalizable to various models and datasets. By\nhierarchically pruning 65\\% of the input tokens, our method greatly reduces\n33\\% $\\sim$ 47\\% FLOPs and achieves speedup up to 1.75$\\times$ during\ninference, while maintaining comparable performance. We further demonstrate\nthat SDTP can be combined with KV cache compression method for further\ncompression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the recent success of large language models (LLMs), LLMs are\nparticularly challenging in long-sequence inference scenarios due to the\nquadratic computational complexity of the attention mechanism. Inspired by the\ninterpretability theory of feature attribution in neural network models, we\nobserve that not all tokens have the same contribution. Based on this\nobservation, we propose a novel token pruning framework, namely Saliency-driven\nDynamic Token Pruning (SDTP), to gradually and dynamically prune redundant\ntokens based on the input context. Specifically, a lightweight saliency-driven\nprediction module is designed to estimate the importance score of each token\nwith its hidden state, which is added to different layers of the LLM to\nhierarchically prune redundant tokens. Furthermore, a ranking-based\noptimization strategy is proposed to minimize the ranking divergence of the\nsaliency score and the predicted importance score. Extensive experiments have\nshown that our framework is generalizable to various models and datasets. By\nhierarchically pruning 65\\% of the input tokens, our method greatly reduces\n33\\% $\\sim$ 47\\% FLOPs and achieves speedup up to 1.75$\\times$ during\ninference, while maintaining comparable performance. We further demonstrate\nthat SDTP can be combined with KV cache compression method for further\ncompression."
                },
                "authors": [
                    {
                        "name": "Yao Tao"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Yun Wang"
                    },
                    {
                        "name": "Mingjian Zhu"
                    },
                    {
                        "name": "Hailin Hu"
                    },
                    {
                        "name": "Yunhe Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yunhe Wang"
                },
                "author": "Yunhe Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04514v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04514v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06813v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06813v1",
                "updated": "2025-04-09T12:07:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    12,
                    7,
                    26,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T12:07:26Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    12,
                    7,
                    26,
                    2,
                    99,
                    0
                ],
                "title": "Introducing the Arm-membench Throughput Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introducing the Arm-membench Throughput Benchmark"
                },
                "summary": "Application performance of modern day processors is often limited by the\nmemory subsystem rather than actual compute capabilities. Therefore, data\nthroughput specifications play a key role in modeling application performance\nand determining possible bottlenecks. However, while peak instruction\nthroughputs and bandwidths for local caches are often documented, the\nachievable throughput can also depend on the relation between memory access and\ncompute instructions. In this paper, we present an Arm version of the well\nestablished x86-membench throughput benchmark, which we have adapted to support\nall current SIMD extensions of the Armv8 instruction set architecture. We\ndescribe aspects of the Armv8 ISA that need to be considered in the portable\ndesign of this benchmark. We use the benchmark to analyze the memory subsystem\nat a fine spatial granularity and to unveil microarchitectural details of three\nprocessors: Fujitsu A64FX, Ampere Altra and Cavium ThunderX2. Based on the\nresulting performance information, we show that instruction fetch and decoder\nwidths become a potential bottleneck for cache-bandwidth-sensitive workloads\ndue to the load-store concept of the Arm ISA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application performance of modern day processors is often limited by the\nmemory subsystem rather than actual compute capabilities. Therefore, data\nthroughput specifications play a key role in modeling application performance\nand determining possible bottlenecks. However, while peak instruction\nthroughputs and bandwidths for local caches are often documented, the\nachievable throughput can also depend on the relation between memory access and\ncompute instructions. In this paper, we present an Arm version of the well\nestablished x86-membench throughput benchmark, which we have adapted to support\nall current SIMD extensions of the Armv8 instruction set architecture. We\ndescribe aspects of the Armv8 ISA that need to be considered in the portable\ndesign of this benchmark. We use the benchmark to analyze the memory subsystem\nat a fine spatial granularity and to unveil microarchitectural details of three\nprocessors: Fujitsu A64FX, Ampere Altra and Cavium ThunderX2. Based on the\nresulting performance information, we show that instruction fetch and decoder\nwidths become a potential bottleneck for cache-bandwidth-sensitive workloads\ndue to the load-store concept of the Arm ISA."
                },
                "authors": [
                    {
                        "name": "Cyrill Burth"
                    },
                    {
                        "name": "Markus Velten"
                    },
                    {
                        "name": "Robert Schöne"
                    }
                ],
                "author_detail": {
                    "name": "Robert Schöne"
                },
                "author": "Robert Schöne",
                "arxiv_doi": "10.1007/978-3-031-85697-6_7",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-85697-6_7",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.06813v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06813v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 6 figures, published in Parallel Processing and Applied\n  Mathematics (PPAM 2024), see https://doi.org/10.1007/978-3-031-85697-6_7",
                "arxiv_journal_ref": "Parallel Processing and Applied Mathematics. PPAM 2024. Lecture\n  Notes in Computer Science, vol 15579. Springer, Cham",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05821v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05821v2",
                "updated": "2025-04-09T10:23:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    10,
                    23,
                    39,
                    2,
                    99,
                    0
                ],
                "published": "2024-03-09T07:01:44Z",
                "published_parsed": [
                    2024,
                    3,
                    9,
                    7,
                    1,
                    44,
                    5,
                    69,
                    0
                ],
                "title": "Optimizing LLM Queries in Relational Data Analytics Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing LLM Queries in Relational Data Analytics Workloads"
                },
                "summary": "Batch data analytics is a growing application for Large Language Models\n(LLMs). LLMs enable users to perform a wide range of natural language tasks,\nsuch as classification, entity extraction, and translation, over large\ndatasets. However, LLM inference is highly costly and slow: for example, an\nNVIDIA L4 GPU running Llama3-8B can only process 6 KB of text per second,\ntaking about a day to handle 15 GB of data; processing a similar amount of data\ncosts around $10K on OpenAI's GPT-4o. In this paper, we propose novel\ntechniques that can significantly reduce the cost of LLM calls for relational\ndata analytics workloads. Our key contribution is developing efficient\nalgorithms for reordering the rows and the fields within each row of an input\ntable to maximize key-value (KV) cache reuse when performing LLM serving. As\nsuch, our approach can be easily applied to existing analytics systems and\nserving platforms. Our evaluation shows that our solution can yield up to 3.4x\nimprovement in job completion time on a benchmark of diverse LLM-based queries\nusing Llama 3 models. Our solution also achieves a 32% cost savings under\nOpenAI and Anthropic pricing models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch data analytics is a growing application for Large Language Models\n(LLMs). LLMs enable users to perform a wide range of natural language tasks,\nsuch as classification, entity extraction, and translation, over large\ndatasets. However, LLM inference is highly costly and slow: for example, an\nNVIDIA L4 GPU running Llama3-8B can only process 6 KB of text per second,\ntaking about a day to handle 15 GB of data; processing a similar amount of data\ncosts around $10K on OpenAI's GPT-4o. In this paper, we propose novel\ntechniques that can significantly reduce the cost of LLM calls for relational\ndata analytics workloads. Our key contribution is developing efficient\nalgorithms for reordering the rows and the fields within each row of an input\ntable to maximize key-value (KV) cache reuse when performing LLM serving. As\nsuch, our approach can be easily applied to existing analytics systems and\nserving platforms. Our evaluation shows that our solution can yield up to 3.4x\nimprovement in job completion time on a benchmark of diverse LLM-based queries\nusing Llama 3 models. Our solution also achieves a 32% cost savings under\nOpenAI and Anthropic pricing models."
                },
                "authors": [
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Asim Biswal"
                    },
                    {
                        "name": "Amog Kamsetty"
                    },
                    {
                        "name": "Audrey Cheng"
                    },
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Liana Patel"
                    },
                    {
                        "name": "Shiyi Cao"
                    },
                    {
                        "name": "Xiangxi Mo"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Matei Zaharia"
                    }
                ],
                "author_detail": {
                    "name": "Matei Zaharia"
                },
                "author": "Matei Zaharia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05821v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05821v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05591v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05591v3",
                "updated": "2025-04-09T09:09:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    9,
                    9,
                    37,
                    2,
                    99,
                    0
                ],
                "published": "2024-09-09T13:20:31Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    13,
                    20,
                    31,
                    0,
                    253,
                    0
                ],
                "title": "MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced\n  Retrieval Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced\n  Retrieval Augmentation"
                },
                "summary": "Processing long contexts presents a significant challenge for large language\nmodels (LLMs). While recent advancements allow LLMs to handle much longer\ncontexts than before (e.g., 32K or 128K tokens), it is computationally\nexpensive and can still be insufficient for many applications.\nRetrieval-Augmented Generation (RAG) is considered a promising strategy to\naddress this problem. However, conventional RAG methods face inherent\nlimitations because of two underlying requirements: 1) explicitly stated\nqueries, and 2) well-structured knowledge. These conditions, however, do not\nhold in general long-context processing tasks.\n  In this work, we propose MemoRAG, a novel RAG framework empowered by global\nmemory-augmented retrieval. MemoRAG features a dual-system architecture. First,\nit employs a light but long-range system to create a global memory of the long\ncontext. Once a task is presented, it generates draft answers, providing useful\nclues for the retrieval tools to locate relevant information within the long\ncontext. Second, it leverages an expensive but expressive system, which\ngenerates the final answer based on the retrieved information. Building upon\nthis fundamental framework, we realize the memory module in the form of KV\ncompression, and reinforce its memorization and cluing capacity from the\nGeneration quality's Feedback (a.k.a. RLGF). In our experiments, MemoRAG\nachieves superior performances across a variety of long-context evaluation\ntasks, not only complex scenarios where traditional RAG methods struggle, but\nalso simpler ones where RAG is typically applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts presents a significant challenge for large language\nmodels (LLMs). While recent advancements allow LLMs to handle much longer\ncontexts than before (e.g., 32K or 128K tokens), it is computationally\nexpensive and can still be insufficient for many applications.\nRetrieval-Augmented Generation (RAG) is considered a promising strategy to\naddress this problem. However, conventional RAG methods face inherent\nlimitations because of two underlying requirements: 1) explicitly stated\nqueries, and 2) well-structured knowledge. These conditions, however, do not\nhold in general long-context processing tasks.\n  In this work, we propose MemoRAG, a novel RAG framework empowered by global\nmemory-augmented retrieval. MemoRAG features a dual-system architecture. First,\nit employs a light but long-range system to create a global memory of the long\ncontext. Once a task is presented, it generates draft answers, providing useful\nclues for the retrieval tools to locate relevant information within the long\ncontext. Second, it leverages an expensive but expressive system, which\ngenerates the final answer based on the retrieved information. Building upon\nthis fundamental framework, we realize the memory module in the form of KV\ncompression, and reinforce its memorization and cluing capacity from the\nGeneration quality's Feedback (a.k.a. RLGF). In our experiments, MemoRAG\nachieves superior performances across a variety of long-context evaluation\ntasks, not only complex scenarios where traditional RAG methods struggle, but\nalso simpler ones where RAG is typically applied."
                },
                "authors": [
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Kelong Mao"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Tiejun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Tiejun Huang"
                },
                "author": "Tiejun Huang",
                "arxiv_comment": "theWebConf 2025. Codes and models are in\n  https://github.com/qhjqhj00/MemoRAG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05591v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05591v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18627v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18627v5",
                "updated": "2025-04-09T07:55:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    7,
                    55,
                    43,
                    2,
                    99,
                    0
                ],
                "published": "2024-10-24T10:36:16Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    36,
                    16,
                    3,
                    298,
                    0
                ],
                "title": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits"
                },
                "summary": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18627v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18627v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.14396v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.14396v5",
                "updated": "2025-04-09T03:49:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    3,
                    49,
                    16,
                    2,
                    99,
                    0
                ],
                "published": "2023-12-22T02:52:59Z",
                "published_parsed": [
                    2023,
                    12,
                    22,
                    2,
                    52,
                    59,
                    4,
                    356,
                    0
                ],
                "title": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing"
                },
                "summary": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation."
                },
                "authors": [
                    {
                        "name": "Hongfu Li"
                    },
                    {
                        "name": "Qian Tao"
                    },
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Shufeng Gong"
                    },
                    {
                        "name": "Yanfeng Zhang"
                    },
                    {
                        "name": "Feng Yao"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Ge Yu"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "This paper was accepted by VLDB2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.14396v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.14396v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06419v1",
                "updated": "2025-04-08T20:39:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    39,
                    20,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T20:39:20Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    39,
                    20,
                    1,
                    98,
                    0
                ],
                "title": "SPIRe: Boosting LLM Inference Throughput with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPIRe: Boosting LLM Inference Throughput with Speculative Decoding"
                },
                "summary": "Speculative decoding (SD) has been shown to reduce the latency of\nautoregressive decoding (AD) by 2-3x for small batch sizes. However, increasing\nthroughput and therefore reducing the cost per token requires decoding with\nlarge batch sizes. Recent work shows that SD can accelerate decoding with large\nbatch sizes too if the context is sufficiently long and the draft model's KV\ncache is sparse. We introduce SPIRe, a draft model that combines static sparse\nattention, pruned initialization, and feedback memory to increase the modeled\nthroughput of speculative decoding by over 100% compared to speculation with a\nmuch smaller draft model and by over 35% compared to the strong baseline of\nsparse self-speculation. Our approach is particularly effective when context\nlengths vary significantly across requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding (SD) has been shown to reduce the latency of\nautoregressive decoding (AD) by 2-3x for small batch sizes. However, increasing\nthroughput and therefore reducing the cost per token requires decoding with\nlarge batch sizes. Recent work shows that SD can accelerate decoding with large\nbatch sizes too if the context is sufficiently long and the draft model's KV\ncache is sparse. We introduce SPIRe, a draft model that combines static sparse\nattention, pruned initialization, and feedback memory to increase the modeled\nthroughput of speculative decoding by over 100% compared to speculation with a\nmuch smaller draft model and by over 35% compared to the strong baseline of\nsparse self-speculation. Our approach is particularly effective when context\nlengths vary significantly across requests."
                },
                "authors": [
                    {
                        "name": "Sanjit Neelam"
                    },
                    {
                        "name": "Daniel Heinlein"
                    },
                    {
                        "name": "Vaclav Cvicek"
                    },
                    {
                        "name": "Akshay Mishra"
                    },
                    {
                        "name": "Reiner Pope"
                    }
                ],
                "author_detail": {
                    "name": "Reiner Pope"
                },
                "author": "Reiner Pope",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06416v1",
                "updated": "2025-04-08T20:32:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    32,
                    10,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T20:32:10Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    32,
                    10,
                    1,
                    98,
                    0
                ],
                "title": "Unifying Autoregressive and Diffusion-Based Sequence Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying Autoregressive and Diffusion-Based Sequence Generation"
                },
                "summary": "We present significant extensions to diffusion-based sequence generation\nmodels, blurring the line with autoregressive language models. We introduce\nhyperschedules, which assign distinct noise schedules to individual token\npositions, generalizing both autoregressive models (e.g., GPT) and conventional\ndiffusion models (e.g., SEDD, MDLM) as special cases. Second, we propose two\nhybrid token-wise noising processes that interpolate between absorbing and\nuniform processes, enabling the model to fix past mistakes, and we introduce a\nnovel inference algorithm that leverages this new feature in a simplified\ncontext inspired from MDLM. To support efficient training and inference, we\ndesign attention masks compatible with KV-caching. Our methods achieve\nstate-of-the-art perplexity and generate diverse, high-quality sequences across\nstandard benchmarks, suggesting a promising path for autoregressive\ndiffusion-based sequence generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present significant extensions to diffusion-based sequence generation\nmodels, blurring the line with autoregressive language models. We introduce\nhyperschedules, which assign distinct noise schedules to individual token\npositions, generalizing both autoregressive models (e.g., GPT) and conventional\ndiffusion models (e.g., SEDD, MDLM) as special cases. Second, we propose two\nhybrid token-wise noising processes that interpolate between absorbing and\nuniform processes, enabling the model to fix past mistakes, and we introduce a\nnovel inference algorithm that leverages this new feature in a simplified\ncontext inspired from MDLM. To support efficient training and inference, we\ndesign attention masks compatible with KV-caching. Our methods achieve\nstate-of-the-art perplexity and generate diverse, high-quality sequences across\nstandard benchmarks, suggesting a promising path for autoregressive\ndiffusion-based sequence generation."
                },
                "authors": [
                    {
                        "name": "Nima Fathi"
                    },
                    {
                        "name": "Torsten Scholak"
                    },
                    {
                        "name": "Pierre-André Noël"
                    }
                ],
                "author_detail": {
                    "name": "Pierre-André Noël"
                },
                "author": "Pierre-André Noël",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.17692v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.17692v2",
                "updated": "2025-04-08T19:26:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    19,
                    26,
                    41,
                    1,
                    98,
                    0
                ],
                "published": "2024-04-26T20:44:36Z",
                "published_parsed": [
                    2024,
                    4,
                    26,
                    20,
                    44,
                    36,
                    4,
                    117,
                    0
                ],
                "title": "Walking on Spheres and Talking to Neighbors: Variance Reduction for\n  Laplace's Equation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Walking on Spheres and Talking to Neighbors: Variance Reduction for\n  Laplace's Equation"
                },
                "summary": "Walk on Spheres algorithms leverage properties of Brownian Motion to create\nMonte Carlo estimates of solutions to a class of elliptic partial differential\nequations. We propose a new caching strategy which leverages the continuity of\npaths of Brownian Motion. In the case of Laplace's equation with Dirichlet\nboundary conditions, our algorithm has improved asymptotic runtime compared to\nprevious approaches. Until recently, estimates were constructed pointwise and\ndid not use the relationship between solutions at nearby points within a\ndomain. Instead, our results are achieved by passing information from a cache\nof fixed size. We also provide bounds on the performance of our algorithm and\ndemonstrate its performance on example problems of increasing complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Walk on Spheres algorithms leverage properties of Brownian Motion to create\nMonte Carlo estimates of solutions to a class of elliptic partial differential\nequations. We propose a new caching strategy which leverages the continuity of\npaths of Brownian Motion. In the case of Laplace's equation with Dirichlet\nboundary conditions, our algorithm has improved asymptotic runtime compared to\nprevious approaches. Until recently, estimates were constructed pointwise and\ndid not use the relationship between solutions at nearby points within a\ndomain. Instead, our results are achieved by passing information from a cache\nof fixed size. We also provide bounds on the performance of our algorithm and\ndemonstrate its performance on example problems of increasing complexity."
                },
                "authors": [
                    {
                        "name": "Michael Czekanski"
                    },
                    {
                        "name": "Benjamin Faber"
                    },
                    {
                        "name": "Margaret Fairborn"
                    },
                    {
                        "name": "Adelle Wright"
                    },
                    {
                        "name": "David Bindel"
                    }
                ],
                "author_detail": {
                    "name": "David Bindel"
                },
                "author": "David Bindel",
                "arxiv_comment": "21 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.17692v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.17692v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06067v1",
                "updated": "2025-04-08T14:09:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    9,
                    23,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T14:09:23Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    9,
                    23,
                    1,
                    98,
                    0
                ],
                "title": "GPU-accelerated Evolutionary Many-objective Optimization Using\n  Tensorized NSGA-III",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU-accelerated Evolutionary Many-objective Optimization Using\n  Tensorized NSGA-III"
                },
                "summary": "NSGA-III is one of the most widely adopted algorithms for tackling\nmany-objective optimization problems. However, its CPU-based design severely\nlimits scalability and computational efficiency. To address the limitations, we\npropose {TensorNSGA-III}, a fully tensorized implementation of NSGA-III that\nleverages GPU parallelism for large-scale many-objective optimization. Unlike\nconventional GPU-accelerated evolutionary algorithms that rely on heuristic\napproximations to improve efficiency, TensorNSGA-III maintains the exact\nselection and variation mechanisms of NSGA-III while achieving significant\nacceleration. By reformulating the selection process with tensorized data\nstructures and an optimized caching strategy, our approach effectively\neliminates computational bottlenecks inherent in traditional CPU-based and\nna\\\"ive GPU implementations. Experimental results on widely used numerical\nbenchmarks show that TensorNSGA-III achieves speedups of up to $3629\\times$\nover the CPU version of NSGA-III. Additionally, we validate its effectiveness\nin multiobjective robotic control tasks, where it discovers diverse and\nhigh-quality behavioral solutions. Furthermore, we investigate the critical\nrole of large population sizes in many-objective optimization and demonstrate\nthe scalability of TensorNSGA-III in such scenarios. The source code is\navailable at https://github.com/EMI-Group/evomo",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NSGA-III is one of the most widely adopted algorithms for tackling\nmany-objective optimization problems. However, its CPU-based design severely\nlimits scalability and computational efficiency. To address the limitations, we\npropose {TensorNSGA-III}, a fully tensorized implementation of NSGA-III that\nleverages GPU parallelism for large-scale many-objective optimization. Unlike\nconventional GPU-accelerated evolutionary algorithms that rely on heuristic\napproximations to improve efficiency, TensorNSGA-III maintains the exact\nselection and variation mechanisms of NSGA-III while achieving significant\nacceleration. By reformulating the selection process with tensorized data\nstructures and an optimized caching strategy, our approach effectively\neliminates computational bottlenecks inherent in traditional CPU-based and\nna\\\"ive GPU implementations. Experimental results on widely used numerical\nbenchmarks show that TensorNSGA-III achieves speedups of up to $3629\\times$\nover the CPU version of NSGA-III. Additionally, we validate its effectiveness\nin multiobjective robotic control tasks, where it discovers diverse and\nhigh-quality behavioral solutions. Furthermore, we investigate the critical\nrole of large population sizes in many-objective optimization and demonstrate\nthe scalability of TensorNSGA-III in such scenarios. The source code is\navailable at https://github.com/EMI-Group/evomo"
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Zhenyu Liang"
                    },
                    {
                        "name": "Ran Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Ran Cheng"
                },
                "author": "Ran Cheng",
                "arxiv_comment": "Accepted by IEEE CEC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03131v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03131v2",
                "updated": "2025-04-08T14:05:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    5,
                    12,
                    1,
                    98,
                    0
                ],
                "published": "2024-12-04T08:51:23Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "title": "Unifying KV Cache Compression for Large Language Models with LeanKV",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying KV Cache Compression for Large Language Models with LeanKV"
                },
                "summary": "Large language models (LLMs) exhibit exceptional performance but incur\nsignificant serving costs due to their substantial memory requirements, with\nthe key-value (KV) cache being a primary bottleneck. Existing KV cache\ncompression techniques, such as quantization and pruning, apply uniform\ntreatment to both keys and values, and discard unimportant tokens entirely,\noverlooking the fine-grained differences in significance of various components\nwithin the KV cache. To address these limitations, we introduce LeanKV, a\nframework that advances KV cache compression by exploiting three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. At the core of LeanKV\nis an on-GPU memory manager that compacts fragmented free memory list into\ncontiguous regions in parallel, effectively translating sparsity in the KV\ncache into performance gains. We evaluate LeanKV on several mainstream models,\nincluding the recent \"thinking model\". LeanKV is able to compress the KV cache\nby $2.7\\times$ to $5.7\\times$ with near-lossless accuracy on complex workloads\nrequiring sophisticated reasoning and long-generation capabilities, and\nenhances throughput by $1.9\\times$ to $5.4\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit exceptional performance but incur\nsignificant serving costs due to their substantial memory requirements, with\nthe key-value (KV) cache being a primary bottleneck. Existing KV cache\ncompression techniques, such as quantization and pruning, apply uniform\ntreatment to both keys and values, and discard unimportant tokens entirely,\noverlooking the fine-grained differences in significance of various components\nwithin the KV cache. To address these limitations, we introduce LeanKV, a\nframework that advances KV cache compression by exploiting three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. At the core of LeanKV\nis an on-GPU memory manager that compacts fragmented free memory list into\ncontiguous regions in parallel, effectively translating sparsity in the KV\ncache into performance gains. We evaluate LeanKV on several mainstream models,\nincluding the recent \"thinking model\". LeanKV is able to compress the KV cache\nby $2.7\\times$ to $5.7\\times$ with near-lossless accuracy on complex workloads\nrequiring sophisticated reasoning and long-generation capabilities, and\nenhances throughput by $1.9\\times$ to $5.4\\times$."
                },
                "authors": [
                    {
                        "name": "Yanqi Zhang"
                    },
                    {
                        "name": "Yuwei Hu"
                    },
                    {
                        "name": "Runyuan Zhao"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03131v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03131v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04760v2",
                "updated": "2025-04-08T12:46:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    12,
                    46,
                    45,
                    1,
                    98,
                    0
                ],
                "published": "2025-02-07T08:48:06Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    48,
                    6,
                    4,
                    38,
                    0
                ],
                "title": "Graph Federated Learning Based Proactive Content Caching in Edge\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Federated Learning Based Proactive Content Caching in Edge\n  Computing"
                },
                "summary": "With the rapid growth of mobile data traffic and the increasing prevalence of\nvideo streaming, proactive content caching in edge computing has become crucial\nfor reducing latency and alleviating network congestion. However, traditional\ncaching strategies such as FIFO, LRU, and LFU fail to effectively predict\nfuture content popularity, while existing proactive caching approaches often\nrequire users to upload data to a central server, raising concerns regarding\nprivacy and scalability. To address these challenges, this paper proposes a\nGraph Federated Learning-based Proactive Content Caching (GFPCC) scheme that\nenhances caching efficiency while preserving user privacy. The proposed\napproach integrates federated learning and graph neural networks, enabling\nusers to locally train Light Graph Convolutional Networks (LightGCN) to capture\nuser-item relationships and predict content popularity. Instead of sharing raw\ndata, only the trained model parameters are transmitted to the central server,\nwhere a federated averaging algorithm aggregates updates, refines the global\nmodel, and selects the most popular files for proactive caching. Experimental\nevaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC\noutperforms baseline caching algorithms by achieving higher cache efficiency\nthrough more accurate content popularity predictions. Moreover, the federated\nlearning framework strengthens privacy protection while maintaining efficient\nmodel training; however, scalability remains a challenge in large-scale\nnetworks with dynamic user preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of mobile data traffic and the increasing prevalence of\nvideo streaming, proactive content caching in edge computing has become crucial\nfor reducing latency and alleviating network congestion. However, traditional\ncaching strategies such as FIFO, LRU, and LFU fail to effectively predict\nfuture content popularity, while existing proactive caching approaches often\nrequire users to upload data to a central server, raising concerns regarding\nprivacy and scalability. To address these challenges, this paper proposes a\nGraph Federated Learning-based Proactive Content Caching (GFPCC) scheme that\nenhances caching efficiency while preserving user privacy. The proposed\napproach integrates federated learning and graph neural networks, enabling\nusers to locally train Light Graph Convolutional Networks (LightGCN) to capture\nuser-item relationships and predict content popularity. Instead of sharing raw\ndata, only the trained model parameters are transmitted to the central server,\nwhere a federated averaging algorithm aggregates updates, refines the global\nmodel, and selects the most popular files for proactive caching. Experimental\nevaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC\noutperforms baseline caching algorithms by achieving higher cache efficiency\nthrough more accurate content popularity predictions. Moreover, the federated\nlearning framework strengthens privacy protection while maintaining efficient\nmodel training; however, scalability remains a challenge in large-scale\nnetworks with dynamic user preferences."
                },
                "authors": [
                    {
                        "name": "Rui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Wang"
                },
                "author": "Rui Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05897v1",
                "updated": "2025-04-08T10:47:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    10,
                    47,
                    37,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T10:47:37Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    10,
                    47,
                    37,
                    1,
                    98,
                    0
                ],
                "title": "HybriMoE: Hybrid CPU-GPU Scheduling and Cache Management for Efficient\n  MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HybriMoE: Hybrid CPU-GPU Scheduling and Cache Management for Efficient\n  MoE Inference"
                },
                "summary": "The Mixture of Experts (MoE) architecture has demonstrated significant\nadvantages as it enables to increase the model capacity without a proportional\nincrease in computation. However, the large MoE model size still introduces\nsubstantial memory demands, which usually requires expert offloading on\nresource-constrained platforms and incurs significant overhead. Hybrid CPU-GPU\ninference has been proposed to leverage CPU computation to reduce expert\nloading overhead but faces major challenges: on one hand, the expert activation\npatterns of MoE models are highly unstable, rendering the fixed mapping\nstrategies in existing works inefficient; on the other hand, the hybrid CPU-GPU\nschedule for MoE is inherently complex due to the diverse expert sizes,\nstructures, uneven workload distribution, etc. To address these challenges, in\nthis paper, we propose HybriMoE, a hybrid CPU-GPU inference framework that\nimproves resource utilization through a novel CPU-GPU scheduling and cache\nmanagement system. HybriMoE introduces (i) a dynamic intra-layer scheduling\nstrategy to balance workloads across CPU and GPU, (ii) an impact-driven\ninter-layer prefetching algorithm, and (iii) a score-based caching algorithm to\nmitigate expert activation instability. We implement HybriMoE on top of the\nkTransformers framework and evaluate it on three widely used MoE-based LLMs.\nExperimental results demonstrate that HybriMoE achieves an average speedup of\n1.33$\\times$ in the prefill stage and 1.70$\\times$ in the decode stage compared\nto state-of-the-art hybrid MoE inference framework. Our code is available at:\nhttps://github.com/PKU-SEC-Lab/HybriMoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture of Experts (MoE) architecture has demonstrated significant\nadvantages as it enables to increase the model capacity without a proportional\nincrease in computation. However, the large MoE model size still introduces\nsubstantial memory demands, which usually requires expert offloading on\nresource-constrained platforms and incurs significant overhead. Hybrid CPU-GPU\ninference has been proposed to leverage CPU computation to reduce expert\nloading overhead but faces major challenges: on one hand, the expert activation\npatterns of MoE models are highly unstable, rendering the fixed mapping\nstrategies in existing works inefficient; on the other hand, the hybrid CPU-GPU\nschedule for MoE is inherently complex due to the diverse expert sizes,\nstructures, uneven workload distribution, etc. To address these challenges, in\nthis paper, we propose HybriMoE, a hybrid CPU-GPU inference framework that\nimproves resource utilization through a novel CPU-GPU scheduling and cache\nmanagement system. HybriMoE introduces (i) a dynamic intra-layer scheduling\nstrategy to balance workloads across CPU and GPU, (ii) an impact-driven\ninter-layer prefetching algorithm, and (iii) a score-based caching algorithm to\nmitigate expert activation instability. We implement HybriMoE on top of the\nkTransformers framework and evaluate it on three widely used MoE-based LLMs.\nExperimental results demonstrate that HybriMoE achieves an average speedup of\n1.33$\\times$ in the prefill stage and 1.70$\\times$ in the decode stage compared\nto state-of-the-art hybrid MoE inference framework. Our code is available at:\nhttps://github.com/PKU-SEC-Lab/HybriMoE."
                },
                "authors": [
                    {
                        "name": "Shuzhang Zhong"
                    },
                    {
                        "name": "Yanfan Sun"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Ru Huang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_comment": "Accepted by DAC 25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06319v1",
                "updated": "2025-04-08T09:17:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    17,
                    35,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T09:17:35Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    17,
                    35,
                    1,
                    98,
                    0
                ],
                "title": "Accelerating LLM Inference Throughput via Asynchronous KV Cache\n  Prefetching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating LLM Inference Throughput via Asynchronous KV Cache\n  Prefetching"
                },
                "summary": "Large Language Models (LLMs) exhibit pronounced memory-bound characteristics\nduring inference due to High Bandwidth Memory (HBM) bandwidth constraints. In\nthis paper, we propose an L2 Cache-oriented asynchronous KV Cache prefetching\nmethod to break through the memory bandwidth bottleneck in LLM inference\nthrough computation-load overlap. By strategically scheduling idle memory\nbandwidth during active computation windows, our method proactively prefetches\nrequired KV Cache into GPU L2 cache, enabling high-speed L2 cache hits for\nsubsequent accesses and effectively hiding HBM access latency within\ncomputational cycles. Extensive experiments on NVIDIA H20 GPUs demonstrate that\nthe proposed method achieves 2.15x improvement in attention kernel efficiency\nand up to 1.97x end-to-end throughput enhancement, surpassing state-of-the-art\nbaseline FlashAttention-3. Notably, our solution maintains orthogonality to\nexisting optimization techniques and can be integrated with current inference\nframeworks, providing a scalable latency-hiding solution for next-generation\nLLM inference engines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit pronounced memory-bound characteristics\nduring inference due to High Bandwidth Memory (HBM) bandwidth constraints. In\nthis paper, we propose an L2 Cache-oriented asynchronous KV Cache prefetching\nmethod to break through the memory bandwidth bottleneck in LLM inference\nthrough computation-load overlap. By strategically scheduling idle memory\nbandwidth during active computation windows, our method proactively prefetches\nrequired KV Cache into GPU L2 cache, enabling high-speed L2 cache hits for\nsubsequent accesses and effectively hiding HBM access latency within\ncomputational cycles. Extensive experiments on NVIDIA H20 GPUs demonstrate that\nthe proposed method achieves 2.15x improvement in attention kernel efficiency\nand up to 1.97x end-to-end throughput enhancement, surpassing state-of-the-art\nbaseline FlashAttention-3. Notably, our solution maintains orthogonality to\nexisting optimization techniques and can be integrated with current inference\nframeworks, providing a scalable latency-hiding solution for next-generation\nLLM inference engines."
                },
                "authors": [
                    {
                        "name": "Yanhao Dong"
                    },
                    {
                        "name": "Yubo Miao"
                    },
                    {
                        "name": "Weinan Li"
                    },
                    {
                        "name": "Xiao Zheng"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Feng Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Lyu"
                },
                "author": "Feng Lyu",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05807v1",
                "updated": "2025-04-08T08:40:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    40,
                    36,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T08:40:36Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    40,
                    36,
                    1,
                    98,
                    0
                ],
                "title": "Low-Complexity AoI-Optimal Status Update Control with Partial Battery\n  State Information in Energy Harvesting IoT Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Complexity AoI-Optimal Status Update Control with Partial Battery\n  State Information in Energy Harvesting IoT Networks"
                },
                "summary": "For a two-hop IoT system consisting of multiple energy harvesting sensors, a\ncache-enabled edge node, and multiple monitors, the status update control at\nthe edge node, which has partial battery state information (pBSI) of the\nsensors, is formulated as a pBSI problem. The concept of inferred pBSI is\nintroduced to reduce the noiseless single-sensor pBSI problem to a Markov\ndecision process with a moderate state-space size, enabling the optimal policy\nto be obtained through a value iteration algorithm. A lower bound on the\nexpected time-average on-demand age of information performance is established\nfor the general single-sensor status update problem. For the single-sensor pBSI\nproblem, a semi-closed-form policy called the current-next (CN) policy is\nproposed, along with an efficient post-update value iteration algorithm with a\nper-iteration time complexity proportional to the square of the battery\ncapacity. A weighted-update-gain-competition (WUGC) approach is further\nleveraged to extend the CN policy to the multi-sensor case. Numerical results\nin the single-sensor case demonstrate the near-optimal performance of the CN\npolicy across various energy arrival processes. Simulations for an IoT system\nwith $100$ sensors reveal that the WUGC-CN policy outperforms the\nmaximum-age-first policy and the random-scheduling-based CN policy under\nBernoulli energy arrival processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For a two-hop IoT system consisting of multiple energy harvesting sensors, a\ncache-enabled edge node, and multiple monitors, the status update control at\nthe edge node, which has partial battery state information (pBSI) of the\nsensors, is formulated as a pBSI problem. The concept of inferred pBSI is\nintroduced to reduce the noiseless single-sensor pBSI problem to a Markov\ndecision process with a moderate state-space size, enabling the optimal policy\nto be obtained through a value iteration algorithm. A lower bound on the\nexpected time-average on-demand age of information performance is established\nfor the general single-sensor status update problem. For the single-sensor pBSI\nproblem, a semi-closed-form policy called the current-next (CN) policy is\nproposed, along with an efficient post-update value iteration algorithm with a\nper-iteration time complexity proportional to the square of the battery\ncapacity. A weighted-update-gain-competition (WUGC) approach is further\nleveraged to extend the CN policy to the multi-sensor case. Numerical results\nin the single-sensor case demonstrate the near-optimal performance of the CN\npolicy across various energy arrival processes. Simulations for an IoT system\nwith $100$ sensors reveal that the WUGC-CN policy outperforms the\nmaximum-age-first policy and the random-scheduling-based CN policy under\nBernoulli energy arrival processes."
                },
                "authors": [
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Shengtian Yang"
                    },
                    {
                        "name": "Jun Chen"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Anding Wang"
                    }
                ],
                "author_detail": {
                    "name": "Anding Wang"
                },
                "author": "Anding Wang",
                "arxiv_comment": "18 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05718v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05718v1",
                "updated": "2025-04-08T06:38:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    6,
                    38,
                    27,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T06:38:27Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    6,
                    38,
                    27,
                    1,
                    98,
                    0
                ],
                "title": "CVA6-VMRT: A Modular Approach Towards Time-Predictable Virtual Memory in\n  a 64-bit Application Class RISC-V Processor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CVA6-VMRT: A Modular Approach Towards Time-Predictable Virtual Memory in\n  a 64-bit Application Class RISC-V Processor"
                },
                "summary": "The increasing complexity of autonomous systems has driven a shift to\nintegrated heterogeneous SoCs with real-time and safety demands. Ensuring\ndeterministic WCETs and low-latency for critical tasks requires minimizing\ninterference on shared resources like virtual memory. Existing techniques, such\nas software coloring and memory replication, introduce significant area and\nperformance overhead, especially with virtualized memory where address\ntranslation adds latency uncertainty. To address these limitations, we propose\nCVA6-VMRT, an extension of the open-source RISC-V CVA6 core, adding hardware\nsupport for predictability in virtual memory access with minimal area overhead.\nCVA6-VMRT features dynamically partitioned Translation Look-aside Buffers\n(TLBs) and hybrid L1 cache/scratchpad memory (SPM) functionality. It allows\nfine-grained per-thread control of resources, enabling the operating system to\nmanage TLB replacements, including static overwrites, to ensure single-cycle\naddress translation for critical memory regions. Additionally, CVA6-VMRT\nenables runtime partitioning of data and instruction caches into cache and SPM\nsections, providing low and predictable access times for critical data without\nimpacting other accesses. In a virtualized setting, CVA6-VMRT enhances\nexecution time determinism for critical guests by 94% during interference from\nnon-critical guests, with minimal impact on their average absolute execution\ntime compared to isolated execution of the critical guests only. This\ninterference-aware behaviour is achieved with just a 4% area overhead and no\ntiming penalty compared to the baseline CVA6 core.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of autonomous systems has driven a shift to\nintegrated heterogeneous SoCs with real-time and safety demands. Ensuring\ndeterministic WCETs and low-latency for critical tasks requires minimizing\ninterference on shared resources like virtual memory. Existing techniques, such\nas software coloring and memory replication, introduce significant area and\nperformance overhead, especially with virtualized memory where address\ntranslation adds latency uncertainty. To address these limitations, we propose\nCVA6-VMRT, an extension of the open-source RISC-V CVA6 core, adding hardware\nsupport for predictability in virtual memory access with minimal area overhead.\nCVA6-VMRT features dynamically partitioned Translation Look-aside Buffers\n(TLBs) and hybrid L1 cache/scratchpad memory (SPM) functionality. It allows\nfine-grained per-thread control of resources, enabling the operating system to\nmanage TLB replacements, including static overwrites, to ensure single-cycle\naddress translation for critical memory regions. Additionally, CVA6-VMRT\nenables runtime partitioning of data and instruction caches into cache and SPM\nsections, providing low and predictable access times for critical data without\nimpacting other accesses. In a virtualized setting, CVA6-VMRT enhances\nexecution time determinism for critical guests by 94% during interference from\nnon-critical guests, with minimal impact on their average absolute execution\ntime compared to isolated execution of the critical guests only. This\ninterference-aware behaviour is achieved with just a 4% area overhead and no\ntiming penalty compared to the baseline CVA6 core."
                },
                "authors": [
                    {
                        "name": "Christopher Reinwardt"
                    },
                    {
                        "name": "Robert Balas"
                    },
                    {
                        "name": "Alessandro Ottaviano"
                    },
                    {
                        "name": "Angelo Garofalo"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_comment": "8 pages, 7 figures, accepted at the 22nd ACM International Conference\n  on Computing Frontiers 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05718v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05718v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22926v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22926v2",
                "updated": "2025-04-08T05:27:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    5,
                    27,
                    15,
                    1,
                    98,
                    0
                ],
                "published": "2025-03-29T01:06:54Z",
                "published_parsed": [
                    2025,
                    3,
                    29,
                    1,
                    6,
                    54,
                    5,
                    88,
                    0
                ],
                "title": "SR-LIO++: Efficient LiDAR-Inertial Odometry and Quantized Mapping with\n  Sweep Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SR-LIO++: Efficient LiDAR-Inertial Odometry and Quantized Mapping with\n  Sweep Reconstruction"
                },
                "summary": "Addressing the inherent low acquisition frequency limitation of 3D LiDAR to\nachieve high-frequency output has become a critical research focus in the\nLiDAR-Inertial Odometry (LIO) domain. To ensure real-time performance,\nfrequency-enhanced LIO systems must process each sweep within significantly\nreduced timeframe, which presents substantial challenges for deployment on\nlow-computational-power platforms. To address these limitations, we introduce\nSR-LIO++, an innovative LIO system capable of achieving doubled output\nfrequency relative to input frequency on resource-constrained hardware\nplatforms, including the Raspberry Pi 4B. Our system employs a sweep\nreconstruction methodology to enhance LiDAR sweep frequency, generating\nhigh-frequency reconstructed sweeps. Building upon this foundation, we propose\na caching mechanism for intermediate results (i.e., surface parameters) of the\nmost recent segments, effectively minimizing redundant processing of common\nsegments in adjacent reconstructed sweeps. This method decouples processing\ntime from the traditionally linear dependence on reconstructed sweep frequency.\nFurthermore, we present a quantized map point management based on index table\nmapping, significantly reducing memory usage by converting global 3D point\nstorage from 64-bit double precision to 8-bit char representation. This method\nalso converts the computationally intensive Euclidean distance calculations in\nnearest neighbor searches from 64-bit double precision to 16-bit short and\n32-bit integer formats, significantly reducing both memory and computational\ncost. Extensive experimental evaluations across three distinct computing\nplatforms and four public datasets demonstrate that SR-LIO++ maintains\nstate-of-the-art accuracy while substantially enhancing efficiency. Notably,\nour system successfully achieves 20Hz state output on Raspberry Pi 4B hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing the inherent low acquisition frequency limitation of 3D LiDAR to\nachieve high-frequency output has become a critical research focus in the\nLiDAR-Inertial Odometry (LIO) domain. To ensure real-time performance,\nfrequency-enhanced LIO systems must process each sweep within significantly\nreduced timeframe, which presents substantial challenges for deployment on\nlow-computational-power platforms. To address these limitations, we introduce\nSR-LIO++, an innovative LIO system capable of achieving doubled output\nfrequency relative to input frequency on resource-constrained hardware\nplatforms, including the Raspberry Pi 4B. Our system employs a sweep\nreconstruction methodology to enhance LiDAR sweep frequency, generating\nhigh-frequency reconstructed sweeps. Building upon this foundation, we propose\na caching mechanism for intermediate results (i.e., surface parameters) of the\nmost recent segments, effectively minimizing redundant processing of common\nsegments in adjacent reconstructed sweeps. This method decouples processing\ntime from the traditionally linear dependence on reconstructed sweep frequency.\nFurthermore, we present a quantized map point management based on index table\nmapping, significantly reducing memory usage by converting global 3D point\nstorage from 64-bit double precision to 8-bit char representation. This method\nalso converts the computationally intensive Euclidean distance calculations in\nnearest neighbor searches from 64-bit double precision to 16-bit short and\n32-bit integer formats, significantly reducing both memory and computational\ncost. Extensive experimental evaluations across three distinct computing\nplatforms and four public datasets demonstrate that SR-LIO++ maintains\nstate-of-the-art accuracy while substantially enhancing efficiency. Notably,\nour system successfully achieves 20Hz state output on Raspberry Pi 4B hardware."
                },
                "authors": [
                    {
                        "name": "Zikang Yuan"
                    },
                    {
                        "name": "Ruiye Ming"
                    },
                    {
                        "name": "Chengwei Zhao"
                    },
                    {
                        "name": "Yonghao Tan"
                    },
                    {
                        "name": "Pingcheng Dong"
                    },
                    {
                        "name": "Hongcheng Luo"
                    },
                    {
                        "name": "Yuzhong Jiao"
                    },
                    {
                        "name": "Xin Yang"
                    },
                    {
                        "name": "Kwang-Ting Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Kwang-Ting Cheng"
                },
                "author": "Kwang-Ting Cheng",
                "arxiv_comment": "10 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22926v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22926v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03661v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03661v2",
                "updated": "2025-04-08T04:34:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    4,
                    34,
                    44,
                    1,
                    98,
                    0
                ],
                "published": "2025-03-12T13:32:50Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    32,
                    50,
                    2,
                    71,
                    0
                ],
                "title": "MILLION: Mastering Long-Context LLM Inference Via Outlier-Immunized KV\n  Product Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MILLION: Mastering Long-Context LLM Inference Via Outlier-Immunized KV\n  Product Quantization"
                },
                "summary": "Large language models (LLMs) are increasingly utilized for complex tasks\nrequiring longer context lengths, with some models supporting up to 128K or 1M\ntokens. This trend, however, presents significant challenges in inference speed\nand memory management. Quantization emerges as a promising approach to address\nthe widening gap between LLM size and memory capacity. However, traditional\nquantization schemes often yield suboptimal compression results for KV caches\ndue to two key factors: i) On-the-fly quantization and de-quantization, causing\nsignificant performance overhead; ii) Prevalence of outliers in KV values,\nchallenging low-bitwidth uniform quantization. To this end, we propose MILLION,\na novel quantization framework achieving low-bitwidth KV cache through product\nquantization. First, we conduct a thorough analysis of KV cache distribution,\nrevealing the limitations of existing quantization schemes. Second, we\nintroduce a non-uniform quantization algorithm based on product quantization,\nwhich efficiently compresses data while preserving accuracy. Third, we develop\na high-performance GPU inference framework with efficient attention kernel and\npipeline design for MILLION that leverages sparse computation and asynchronous\nquantization, significantly enhancing inference speed. Comprehensive evaluation\nresults demonstrate that MILLION can achieve 4 bits quantization with trivial\nperplexity and accuracy loss, and achieve 2.09x end-to-end performance gains at\n32K context length. Code is released at https://github.com/ZongwuWang/MILLION.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly utilized for complex tasks\nrequiring longer context lengths, with some models supporting up to 128K or 1M\ntokens. This trend, however, presents significant challenges in inference speed\nand memory management. Quantization emerges as a promising approach to address\nthe widening gap between LLM size and memory capacity. However, traditional\nquantization schemes often yield suboptimal compression results for KV caches\ndue to two key factors: i) On-the-fly quantization and de-quantization, causing\nsignificant performance overhead; ii) Prevalence of outliers in KV values,\nchallenging low-bitwidth uniform quantization. To this end, we propose MILLION,\na novel quantization framework achieving low-bitwidth KV cache through product\nquantization. First, we conduct a thorough analysis of KV cache distribution,\nrevealing the limitations of existing quantization schemes. Second, we\nintroduce a non-uniform quantization algorithm based on product quantization,\nwhich efficiently compresses data while preserving accuracy. Third, we develop\na high-performance GPU inference framework with efficient attention kernel and\npipeline design for MILLION that leverages sparse computation and asynchronous\nquantization, significantly enhancing inference speed. Comprehensive evaluation\nresults demonstrate that MILLION can achieve 4 bits quantization with trivial\nperplexity and accuracy loss, and achieve 2.09x end-to-end performance gains at\n32K context length. Code is released at https://github.com/ZongwuWang/MILLION."
                },
                "authors": [
                    {
                        "name": "Zongwu Wang"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Fangxin Liu"
                    },
                    {
                        "name": "Yiwei Hu"
                    },
                    {
                        "name": "Qingxiao Sun"
                    },
                    {
                        "name": "Gezi Li"
                    },
                    {
                        "name": "Cheng Li"
                    },
                    {
                        "name": "Xuan Wang"
                    },
                    {
                        "name": "Li Jiang"
                    },
                    {
                        "name": "Haibing Guan"
                    }
                ],
                "author_detail": {
                    "name": "Haibing Guan"
                },
                "author": "Haibing Guan",
                "arxiv_comment": "7 pages, 7 figures and 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03661v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03661v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05646v1",
                "updated": "2025-04-08T03:48:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    3,
                    48,
                    43,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T03:48:43Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    3,
                    48,
                    43,
                    1,
                    98,
                    0
                ],
                "title": "Lattice: Learning to Efficiently Compress the Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lattice: Learning to Efficiently Compress the Memory"
                },
                "summary": "Attention mechanisms have revolutionized sequence learning but suffer from\nquadratic computational complexity. This paper introduces Lattice, a novel\nrecurrent neural network (RNN) mechanism that leverages the inherent low-rank\nstructure of K-V matrices to efficiently compress the cache into a fixed number\nof memory slots, achieving sub-quadratic complexity. We formulate this\ncompression as an online optimization problem and derive a dynamic memory\nupdate rule based on a single gradient descent step. The resulting recurrence\nfeatures a state- and input-dependent gating mechanism, offering an\ninterpretable memory update process. The core innovation is the orthogonal\nupdate: each memory slot is updated exclusively with information orthogonal to\nits current state hence incorporation of only novel, non-redundant data, which\nminimizes the interference with previously stored information. The experimental\nresults show that Lattice achieves the best perplexity compared to all\nbaselines across diverse context lengths, with performance improvement becoming\nmore pronounced as the context length increases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention mechanisms have revolutionized sequence learning but suffer from\nquadratic computational complexity. This paper introduces Lattice, a novel\nrecurrent neural network (RNN) mechanism that leverages the inherent low-rank\nstructure of K-V matrices to efficiently compress the cache into a fixed number\nof memory slots, achieving sub-quadratic complexity. We formulate this\ncompression as an online optimization problem and derive a dynamic memory\nupdate rule based on a single gradient descent step. The resulting recurrence\nfeatures a state- and input-dependent gating mechanism, offering an\ninterpretable memory update process. The core innovation is the orthogonal\nupdate: each memory slot is updated exclusively with information orthogonal to\nits current state hence incorporation of only novel, non-redundant data, which\nminimizes the interference with previously stored information. The experimental\nresults show that Lattice achieves the best perplexity compared to all\nbaselines across diverse context lengths, with performance improvement becoming\nmore pronounced as the context length increases."
                },
                "authors": [
                    {
                        "name": "Mahdi Karami"
                    },
                    {
                        "name": "Vahab Mirrokni"
                    }
                ],
                "author_detail": {
                    "name": "Vahab Mirrokni"
                },
                "author": "Vahab Mirrokni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02533v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02533v3",
                "updated": "2025-04-07T22:48:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    22,
                    48,
                    33,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-03T12:36:01Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    12,
                    36,
                    1,
                    3,
                    93,
                    0
                ],
                "title": "ARCANE: Adaptive RISC-V Cache Architecture for Near-memory Extensions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARCANE: Adaptive RISC-V Cache Architecture for Near-memory Extensions"
                },
                "summary": "Modern data-driven applications expose limitations of von Neumann\narchitectures - extensive data movement, low throughput, and poor energy\nefficiency. Accelerators improve performance but lack flexibility and require\ndata transfers. Existing compute in- and near-memory solutions mitigate these\nissues but face usability challenges due to data placement constraints. We\npropose a novel cache architecture that doubles as a tightly-coupled\ncompute-near-memory coprocessor. Our RISC-V cache controller executes custom\ninstructions from the host CPU using vector operations dispatched to\nnear-memory vector processing units within the cache memory subsystem. This\narchitecture abstracts memory synchronization and data mapping from application\nsoftware while offering software-based Instruction Set Architecture\nextensibility. Our implementation shows $30\\times$ to $84\\times$ performance\nimprovement when operating on 8-bit data over the same system with a\ntraditional cache when executing a worst-case 32-bit CNN workload, with only\n$41.3\\%$ area overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern data-driven applications expose limitations of von Neumann\narchitectures - extensive data movement, low throughput, and poor energy\nefficiency. Accelerators improve performance but lack flexibility and require\ndata transfers. Existing compute in- and near-memory solutions mitigate these\nissues but face usability challenges due to data placement constraints. We\npropose a novel cache architecture that doubles as a tightly-coupled\ncompute-near-memory coprocessor. Our RISC-V cache controller executes custom\ninstructions from the host CPU using vector operations dispatched to\nnear-memory vector processing units within the cache memory subsystem. This\narchitecture abstracts memory synchronization and data mapping from application\nsoftware while offering software-based Instruction Set Architecture\nextensibility. Our implementation shows $30\\times$ to $84\\times$ performance\nimprovement when operating on 8-bit data over the same system with a\ntraditional cache when executing a worst-case 32-bit CNN workload, with only\n$41.3\\%$ area overhead."
                },
                "authors": [
                    {
                        "name": "Vincenzo Petrolo"
                    },
                    {
                        "name": "Flavia Guella"
                    },
                    {
                        "name": "Michele Caon"
                    },
                    {
                        "name": "Pasquale Davide Schiavone"
                    },
                    {
                        "name": "Guido Masera"
                    },
                    {
                        "name": "Maurizio Martina"
                    }
                ],
                "author_detail": {
                    "name": "Maurizio Martina"
                },
                "author": "Maurizio Martina",
                "arxiv_comment": "6 pages, 4 figures, accepted at the Design Automation Conference\n  (DAC) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02533v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02533v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07467v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07467v2",
                "updated": "2025-04-07T20:52:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    20,
                    52,
                    4,
                    0,
                    97,
                    0
                ],
                "published": "2024-06-11T17:13:18Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    17,
                    13,
                    18,
                    1,
                    163,
                    0
                ],
                "title": "LLM meets ML: Data-efficient Anomaly Detection on Unseen Unstable Logs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM meets ML: Data-efficient Anomaly Detection on Unseen Unstable Logs"
                },
                "summary": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for ULAD, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\nin F1 score while using 62.87 percentage points less labeled data. When trained\non the same amount of data as the baselines, FlexLog achieves up to a 13\npercentage points increase in F1 score on ADFA-U across varying training\ndataset sizes. Additionally, FlexLog maintains inference time under one second\nper log sequence, making it suitable for most applications except\nlatency-sensitive systems. Further analysis reveals the positive impact of\nFlexLog's key components: cache, RAG and ensemble learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for ULAD, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\nin F1 score while using 62.87 percentage points less labeled data. When trained\non the same amount of data as the baselines, FlexLog achieves up to a 13\npercentage points increase in F1 score on ADFA-U across varying training\ndataset sizes. Additionally, FlexLog maintains inference time under one second\nper log sequence, making it suitable for most applications except\nlatency-sensitive systems. Further analysis reveals the positive impact of\nFlexLog's key components: cache, RAG and ensemble learning."
                },
                "authors": [
                    {
                        "name": "Fatemeh Hadadi"
                    },
                    {
                        "name": "Qinghua Xu"
                    },
                    {
                        "name": "Domenico Bianculli"
                    },
                    {
                        "name": "Lionel Briand"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Briand"
                },
                "author": "Lionel Briand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07467v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07467v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05097v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05097v1",
                "updated": "2025-04-07T14:04:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    4,
                    30,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T14:04:30Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    4,
                    30,
                    0,
                    97,
                    0
                ],
                "title": "State Tuning: State-based Test-Time Scaling on RWKV-7",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State Tuning: State-based Test-Time Scaling on RWKV-7"
                },
                "summary": "Test-time scaling has emerged as a prominent research direction in machine\nlearning, enabling models to enhance their expressive capabilities during\ninference.Transformers, renowned for striking a delicate balance between\nefficiency and expressiveness, have benefited from test-time scaling techniques\nthat leverage an expanding key-value (KV) cache to significantly improve\nperformance.In this paper, we introduce a novel state-based approach to\ntest-time scaling, which we term state tuning, tailored to the RNN-based RWKV-7\nmodel.By exploiting the unique strengths of RWKV-7, our method achieves\nstate-of-the-art performance on the target task without altering the model's\npre-trained weights. Our approach centers on three key innovations. First, we\ndevelop an observer framework that allows a smaller model to replicate and\nlearn the state dynamics of the RWKV-7 model. Second, we employ a kernel method\nto dynamically upscale the state size, enhancing the model's capacity to\ncapture intricate patterns. Third, we integrate Decorrelated Backpropagation\n(DBP) to optimize the upscaled state matrix, thereby improving convergence and\nexpressivity. By tuning only the state matrix, we demonstrate that a smaller\nmodel can outperform larger models on the given task. This method preserves the\nefficiency of the original RWKV-7 architecture while harnessing the power of\ntest-time scaling to deliver superior results. Our findings underscore the\npotential of state tuning as an effective strategy for advancing model\nperformance in resource-constrained settings. Our code is\nhttps://github.com/TorchRWKV/flash-linear-attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling has emerged as a prominent research direction in machine\nlearning, enabling models to enhance their expressive capabilities during\ninference.Transformers, renowned for striking a delicate balance between\nefficiency and expressiveness, have benefited from test-time scaling techniques\nthat leverage an expanding key-value (KV) cache to significantly improve\nperformance.In this paper, we introduce a novel state-based approach to\ntest-time scaling, which we term state tuning, tailored to the RNN-based RWKV-7\nmodel.By exploiting the unique strengths of RWKV-7, our method achieves\nstate-of-the-art performance on the target task without altering the model's\npre-trained weights. Our approach centers on three key innovations. First, we\ndevelop an observer framework that allows a smaller model to replicate and\nlearn the state dynamics of the RWKV-7 model. Second, we employ a kernel method\nto dynamically upscale the state size, enhancing the model's capacity to\ncapture intricate patterns. Third, we integrate Decorrelated Backpropagation\n(DBP) to optimize the upscaled state matrix, thereby improving convergence and\nexpressivity. By tuning only the state matrix, we demonstrate that a smaller\nmodel can outperform larger models on the given task. This method preserves the\nefficiency of the original RWKV-7 architecture while harnessing the power of\ntest-time scaling to deliver superior results. Our findings underscore the\npotential of state tuning as an effective strategy for advancing model\nperformance in resource-constrained settings. Our code is\nhttps://github.com/TorchRWKV/flash-linear-attention."
                },
                "authors": [
                    {
                        "name": "Liu Xiao"
                    },
                    {
                        "name": "Li Zhiyuan"
                    },
                    {
                        "name": "Lin Yueyu"
                    }
                ],
                "author_detail": {
                    "name": "Lin Yueyu"
                },
                "author": "Lin Yueyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05097v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05097v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04823v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04823v1",
                "updated": "2025-04-07T08:22:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    8,
                    22,
                    45,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T08:22:45Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    8,
                    22,
                    45,
                    0,
                    97,
                    0
                ],
                "title": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models"
                },
                "summary": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this study, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, and QwQ-32B. Our investigation covers weight, KV cache, and\nactivation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes will be open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this study, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, and QwQ-32B. Our investigation covers weight, KV cache, and\nactivation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes will be open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models."
                },
                "authors": [
                    {
                        "name": "Ruikang Liu"
                    },
                    {
                        "name": "Yuxuan Sun"
                    },
                    {
                        "name": "Manyi Zhang"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Tiezheng Yu"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Lu Hou"
                    }
                ],
                "author_detail": {
                    "name": "Lu Hou"
                },
                "author": "Lu Hou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04823v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04823v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00414v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00414v3",
                "updated": "2025-04-07T06:27:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    6,
                    27,
                    48,
                    0,
                    97,
                    0
                ],
                "published": "2024-10-01T05:46:22Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    5,
                    46,
                    22,
                    1,
                    275,
                    0
                ],
                "title": "Semantic Parsing with Candidate Expressions for Knowledge Base Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Parsing with Candidate Expressions for Knowledge Base Question\n  Answering"
                },
                "summary": "Semantic parsers convert natural language to logical forms, which can be\nevaluated on knowledge bases (KBs) to produce denotations. Recent semantic\nparsers have been developed with sequence-to-sequence (seq2seq) pre-trained\nlanguage models (PLMs) or large language models, where the models treat logical\nforms as sequences of tokens. For syntactic and semantic validity, the semantic\nparsers use grammars that enable constrained decoding. However, the grammars\nlack the ability to utilize large information of KBs, although logical forms\ncontain representations of KB elements, such as entities or relations. In this\nwork, we propose a grammar augmented with candidate expressions for semantic\nparsing on a large KB with a seq2seq PLM. The grammar defines actions as\nproduction rules, and our semantic parser predicts actions during inference\nunder the constraints by types and candidate expressions. We apply the grammar\nto knowledge base question answering, where the constraints by candidate\nexpressions assist a semantic parser to generate valid KB elements. We also\nintroduce two special rules, sub-type inference and union types, and a mask\ncaching algorithm. In particular, sub-type inference and the mask caching\nalgorithm greatly increase the decoding speed of our semantic parser. We\nexperimented on two benchmarks, KQA Pro and Overnight, where the constraints by\ncandidate expressions increased the accuracy of our semantic parser, whether it\nwas trained with strong supervision or weak supervision. In addition, our\nsemantic parser had a fast decoding speed in the experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic parsers convert natural language to logical forms, which can be\nevaluated on knowledge bases (KBs) to produce denotations. Recent semantic\nparsers have been developed with sequence-to-sequence (seq2seq) pre-trained\nlanguage models (PLMs) or large language models, where the models treat logical\nforms as sequences of tokens. For syntactic and semantic validity, the semantic\nparsers use grammars that enable constrained decoding. However, the grammars\nlack the ability to utilize large information of KBs, although logical forms\ncontain representations of KB elements, such as entities or relations. In this\nwork, we propose a grammar augmented with candidate expressions for semantic\nparsing on a large KB with a seq2seq PLM. The grammar defines actions as\nproduction rules, and our semantic parser predicts actions during inference\nunder the constraints by types and candidate expressions. We apply the grammar\nto knowledge base question answering, where the constraints by candidate\nexpressions assist a semantic parser to generate valid KB elements. We also\nintroduce two special rules, sub-type inference and union types, and a mask\ncaching algorithm. In particular, sub-type inference and the mask caching\nalgorithm greatly increase the decoding speed of our semantic parser. We\nexperimented on two benchmarks, KQA Pro and Overnight, where the constraints by\ncandidate expressions increased the accuracy of our semantic parser, whether it\nwas trained with strong supervision or weak supervision. In addition, our\nsemantic parser had a fast decoding speed in the experiments."
                },
                "authors": [
                    {
                        "name": "Daehwan Nam"
                    },
                    {
                        "name": "Gary Geunbae Lee"
                    }
                ],
                "author_detail": {
                    "name": "Gary Geunbae Lee"
                },
                "author": "Gary Geunbae Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00414v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00414v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04704v1",
                "updated": "2025-04-07T03:22:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    3,
                    22,
                    15,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T03:22:15Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    3,
                    22,
                    15,
                    0,
                    97,
                    0
                ],
                "title": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important"
                },
                "summary": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modifiation of the inference infrastructure and\nsignificant computation overhead. Base on the fact that the Large Lanuage\nmodels are autoregresssive models, we propose {\\it LagKV}, a KV allocation\nstrategy only relying on straight forward comparison among KV themself. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on LongBench and PasskeyRetrieval show that, our\napproach achieves nearly zero loss when the ratio is $2\\times$ and $\\approx\n90\\%$ of the original model performance for $8\\times$. Especially in the\n64-digit passkey retrieval task, our mehod outperforms the attention weight\nbased method $H_2O$ over $60\\%$ with same compression ratios. Our code is\navailable at \\url{https://github.com/AI-Lab-China-Merchants-Bank/LagKV}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modifiation of the inference infrastructure and\nsignificant computation overhead. Base on the fact that the Large Lanuage\nmodels are autoregresssive models, we propose {\\it LagKV}, a KV allocation\nstrategy only relying on straight forward comparison among KV themself. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on LongBench and PasskeyRetrieval show that, our\napproach achieves nearly zero loss when the ratio is $2\\times$ and $\\approx\n90\\%$ of the original model performance for $8\\times$. Especially in the\n64-digit passkey retrieval task, our mehod outperforms the attention weight\nbased method $H_2O$ over $60\\%$ with same compression ratios. Our code is\navailable at \\url{https://github.com/AI-Lab-China-Merchants-Bank/LagKV}."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "JiaMing Zhang"
                    },
                    {
                        "name": "Xiong Li"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23367v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23367v2",
                "updated": "2025-04-07T01:35:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    1,
                    35,
                    39,
                    0,
                    97,
                    0
                ],
                "published": "2025-03-30T08:51:19Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    8,
                    51,
                    19,
                    6,
                    89,
                    0
                ],
                "title": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning"
                },
                "summary": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR."
                },
                "authors": [
                    {
                        "name": "Hang Guo"
                    },
                    {
                        "name": "Yawei Li"
                    },
                    {
                        "name": "Taolin Zhang"
                    },
                    {
                        "name": "Jiangshan Wang"
                    },
                    {
                        "name": "Tao Dai"
                    },
                    {
                        "name": "Shu-Tao Xia"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23367v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23367v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10714v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10714v2",
                "updated": "2025-04-06T12:20:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    6,
                    12,
                    20,
                    25,
                    6,
                    96,
                    0
                ],
                "published": "2025-03-13T03:36:03Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    36,
                    3,
                    3,
                    72,
                    0
                ],
                "title": "ZSMerge: Zero-Shot KV Cache Compression for Memory-Efficient\n  Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZSMerge: Zero-Shot KV Cache Compression for Memory-Efficient\n  Long-Context LLMs"
                },
                "summary": "The linear growth of key-value (KV) cache memory and quadratic computational\nin attention mechanisms complexity pose significant bottlenecks for large\nlanguage models (LLMs) in long-context processing. While existing KV cache\noptimization methods address these challenges through token pruning or feature\nmerging, they often incur irreversible information loss or require costly\nparameter retraining. To this end, we propose ZSMerge, a dynamic KV cache\ncompression framework designed for efficient cache management, featuring three\nkey operations: (1) fine-grained memory allocation guided by multi-dimensional\ntoken importance metrics at head-level granularity, (2) a residual merging\nmechanism that preserves critical context through compensated attention\nscoring, and (3) a zero-shot adaptation mechanism compatible with diverse LLM\narchitectures without requiring retraining. ZSMerge significantly enhances\nmemory efficiency and inference speed with negligible performance degradation\nacross LLMs. When applied to LLaMA2-7B, it demonstrates a 20:1 compression\nratio for key-value cache retention (reducing memory footprint to 5\\% of\nbaseline) while sustaining comparable generation quality, coupled with triple\nthroughput gains at extreme 54k-token contexts that eliminate out-of-memory\nfailures. The code is available at https://github.com/SusCom-Lab/ZSMerge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The linear growth of key-value (KV) cache memory and quadratic computational\nin attention mechanisms complexity pose significant bottlenecks for large\nlanguage models (LLMs) in long-context processing. While existing KV cache\noptimization methods address these challenges through token pruning or feature\nmerging, they often incur irreversible information loss or require costly\nparameter retraining. To this end, we propose ZSMerge, a dynamic KV cache\ncompression framework designed for efficient cache management, featuring three\nkey operations: (1) fine-grained memory allocation guided by multi-dimensional\ntoken importance metrics at head-level granularity, (2) a residual merging\nmechanism that preserves critical context through compensated attention\nscoring, and (3) a zero-shot adaptation mechanism compatible with diverse LLM\narchitectures without requiring retraining. ZSMerge significantly enhances\nmemory efficiency and inference speed with negligible performance degradation\nacross LLMs. When applied to LLaMA2-7B, it demonstrates a 20:1 compression\nratio for key-value cache retention (reducing memory footprint to 5\\% of\nbaseline) while sustaining comparable generation quality, coupled with triple\nthroughput gains at extreme 54k-token contexts that eliminate out-of-memory\nfailures. The code is available at https://github.com/SusCom-Lab/ZSMerge."
                },
                "authors": [
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Pei Liu"
                    },
                    {
                        "name": "Guoming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Guoming Tang"
                },
                "author": "Guoming Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10714v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10714v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04005v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04005v1",
                "updated": "2025-04-05T00:59:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    5,
                    0,
                    59,
                    52,
                    5,
                    95,
                    0
                ],
                "published": "2025-04-05T00:59:52Z",
                "published_parsed": [
                    2025,
                    4,
                    5,
                    0,
                    59,
                    52,
                    5,
                    95,
                    0
                ],
                "title": "Learning Cache Coherence Traffic for NoC Routing Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Cache Coherence Traffic for NoC Routing Design"
                },
                "summary": "The rapid growth of multi-core systems highlights the need for efficient\nNetwork-on-Chip (NoC) design to ensure seamless communication. Cache coherence,\nessential for data consistency, substantially reduces task computation time by\nenabling data sharing among caches. As a result, routing serves two roles:\nfacilitating data sharing (influenced by topology) and managing NoC-level\ncommunication. However, cache coherence is often overlooked in routing, causing\nmismatches between design expectations and evaluation outcomes. Two main\nchallenges are the lack of specialized tools to assess cache coherence's impact\nand the neglect of topology selection in routing. In this work, we propose a\ncache coherence-aware routing approach with integrated topology selection,\nguided by our Cache Coherence Traffic Analyzer (CCTA). Our method achieves up\nto 10.52% lower packet latency, 55.51% faster execution time, and 49.02% total\nenergy savings, underscoring the critical role of cache coherence in NoC design\nand enabling effective co-design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of multi-core systems highlights the need for efficient\nNetwork-on-Chip (NoC) design to ensure seamless communication. Cache coherence,\nessential for data consistency, substantially reduces task computation time by\nenabling data sharing among caches. As a result, routing serves two roles:\nfacilitating data sharing (influenced by topology) and managing NoC-level\ncommunication. However, cache coherence is often overlooked in routing, causing\nmismatches between design expectations and evaluation outcomes. Two main\nchallenges are the lack of specialized tools to assess cache coherence's impact\nand the neglect of topology selection in routing. In this work, we propose a\ncache coherence-aware routing approach with integrated topology selection,\nguided by our Cache Coherence Traffic Analyzer (CCTA). Our method achieves up\nto 10.52% lower packet latency, 55.51% faster execution time, and 49.02% total\nenergy savings, underscoring the critical role of cache coherence in NoC design\nand enabling effective co-design."
                },
                "authors": [
                    {
                        "name": "Guochu Xiong"
                    },
                    {
                        "name": "Xiangzhong Luo"
                    },
                    {
                        "name": "Weichen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Weichen Liu"
                },
                "author": "Weichen Liu",
                "arxiv_comment": "7 pages, 14 figures. Preprint version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04005v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04005v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03632v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03632v1",
                "updated": "2025-04-04T17:56:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    56,
                    44,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T17:56:44Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    56,
                    44,
                    4,
                    94,
                    0
                ],
                "title": "Performance Analysis of HPC applications on the Aurora Supercomputer:\n  Exploring the Impact of HBM-Enabled Intel Xeon Max CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Analysis of HPC applications on the Aurora Supercomputer:\n  Exploring the Impact of HBM-Enabled Intel Xeon Max CPUs"
                },
                "summary": "The Aurora supercomputer is an exascale-class system designed to tackle some\nof the most demanding computational workloads. Equipped with both High\nBandwidth Memory (HBM) and DDR memory, it provides unique trade-offs in\nperformance, latency, and capacity. This paper presents a comprehensive\nanalysis of the memory systems on the Aurora supercomputer, with a focus on\nevaluating the trade-offs between HBM and DDR memory systems. We explore how\ndifferent memory configurations, including memory modes (Flat and Cache) and\nclustering modes (Quad and SNC4), influence key system performance metrics such\nas memory bandwidth, latency, CPU-GPU PCIe bandwidth, and MPI communication\nbandwidth. Additionally, we examine the performance of three representative HPC\napplications -- HACC, QMCPACK, and BFS -- each illustrating the impact of\nmemory configurations on performance. By using microbenchmarks and\napplication-level analysis, we provide insights into how to select the optimal\nmemory system and configuration to maximize performance based on the\napplication characteristics. The findings presented in this paper offer\nguidance for users of the Aurora system and similar exascale systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Aurora supercomputer is an exascale-class system designed to tackle some\nof the most demanding computational workloads. Equipped with both High\nBandwidth Memory (HBM) and DDR memory, it provides unique trade-offs in\nperformance, latency, and capacity. This paper presents a comprehensive\nanalysis of the memory systems on the Aurora supercomputer, with a focus on\nevaluating the trade-offs between HBM and DDR memory systems. We explore how\ndifferent memory configurations, including memory modes (Flat and Cache) and\nclustering modes (Quad and SNC4), influence key system performance metrics such\nas memory bandwidth, latency, CPU-GPU PCIe bandwidth, and MPI communication\nbandwidth. Additionally, we examine the performance of three representative HPC\napplications -- HACC, QMCPACK, and BFS -- each illustrating the impact of\nmemory configurations on performance. By using microbenchmarks and\napplication-level analysis, we provide insights into how to select the optimal\nmemory system and configuration to maximize performance based on the\napplication characteristics. The findings presented in this paper offer\nguidance for users of the Aurora system and similar exascale systems."
                },
                "authors": [
                    {
                        "name": "Huda Ibeid"
                    },
                    {
                        "name": "Vikram Narayana"
                    },
                    {
                        "name": "Jeongnim Kim"
                    },
                    {
                        "name": "Anthony Nguyen"
                    },
                    {
                        "name": "Vitali Morozov"
                    },
                    {
                        "name": "Ye Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ye Luo"
                },
                "author": "Ye Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03632v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03632v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03771v2",
                "updated": "2025-04-04T16:51:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    16,
                    51,
                    15,
                    4,
                    94,
                    0
                ],
                "published": "2025-02-06T04:16:20Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "title": "Adaptive Semantic Prompt Caching with VectorQ",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Semantic Prompt Caching with VectorQ"
                },
                "summary": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different embeddings.\nWe propose VectorQ, an online framework with a threshold convergence guarantee\nto learn embedding-specific threshold regions that adapt to the uncertainty of\nan embedding. Through evaluations on a combination of three diverse datasets,\nwe show that VectorQ consistently outperforms state-of-the-art systems across\nall static thresholds, achieving up to 26x increases in cache hit rate and\nerror rate reductions up to 74%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different embeddings.\nWe propose VectorQ, an online framework with a threshold convergence guarantee\nto learn embedding-specific threshold regions that adapt to the uncertainty of\nan embedding. Through evaluations on a combination of three diverse datasets,\nwe show that VectorQ consistently outperforms state-of-the-art systems across\nall static thresholds, achieving up to 26x increases in cache hit rate and\nerror rate reductions up to 74%."
                },
                "authors": [
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Alejandro Cuadron"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Stephan Krusche"
                    },
                    {
                        "name": "Alfons Kemper"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E. Gonzalez"
                },
                "author": "Joseph E. Gonzalez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.02073v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.02073v2",
                "updated": "2025-04-04T15:30:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    15,
                    30,
                    20,
                    4,
                    94,
                    0
                ],
                "published": "2023-07-05T07:30:53Z",
                "published_parsed": [
                    2023,
                    7,
                    5,
                    7,
                    30,
                    53,
                    2,
                    186,
                    0
                ],
                "title": "Performance Modeling of Data Storage Systems using Generative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Modeling of Data Storage Systems using Generative Models"
                },
                "summary": "High-precision modeling of systems is one of the main areas of industrial\ndata analysis. Models of systems, their digital twins, are used to predict\ntheir behavior under various conditions. We have developed several models of a\nstorage system using machine learning-based generative models. The system\nconsists of several components: hard disk drive (HDD) and solid-state drive\n(SSD) storage pools with different RAID schemes and cache. Each storage\ncomponent is represented by a probabilistic model that describes the\nprobability distribution of the component performance in terms of IOPS and\nlatency, depending on their configuration and external data load parameters.\nThe results of the experiments demonstrate the errors of 4-10 % for IOPS and\n3-16 % for latency predictions depending on the components and models of the\nsystem. The predictions show up to 0.99 Pearson correlation with Little's law,\nwhich can be used for unsupervised reliability checks of the models. In\naddition, we present novel data sets that can be used for benchmarking\nregression algorithms, conditional generative models, and uncertainty\nestimation methods in machine learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-precision modeling of systems is one of the main areas of industrial\ndata analysis. Models of systems, their digital twins, are used to predict\ntheir behavior under various conditions. We have developed several models of a\nstorage system using machine learning-based generative models. The system\nconsists of several components: hard disk drive (HDD) and solid-state drive\n(SSD) storage pools with different RAID schemes and cache. Each storage\ncomponent is represented by a probabilistic model that describes the\nprobability distribution of the component performance in terms of IOPS and\nlatency, depending on their configuration and external data load parameters.\nThe results of the experiments demonstrate the errors of 4-10 % for IOPS and\n3-16 % for latency predictions depending on the components and models of the\nsystem. The predictions show up to 0.99 Pearson correlation with Little's law,\nwhich can be used for unsupervised reliability checks of the models. In\naddition, we present novel data sets that can be used for benchmarking\nregression algorithms, conditional generative models, and uncertainty\nestimation methods in machine learning."
                },
                "authors": [
                    {
                        "name": "Abdalaziz Rashid Al-Maeeni"
                    },
                    {
                        "name": "Aziz Temirkhanov"
                    },
                    {
                        "name": "Artem Ryzhikov"
                    },
                    {
                        "name": "Mikhail Hushchyn"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Hushchyn"
                },
                "author": "Mikhail Hushchyn",
                "arxiv_doi": "10.1109/ACCESS.2025.3552409",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ACCESS.2025.3552409",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2307.02073v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.02073v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Access 2025 ( Volume: 13) 49643 - 49658",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03499v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03499v1",
                "updated": "2025-04-04T14:55:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    55,
                    27,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T14:55:27Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    55,
                    27,
                    4,
                    94,
                    0
                ],
                "title": "Optimistic Learning for Communication Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimistic Learning for Communication Networks"
                },
                "summary": "AI/ML-based tools are at the forefront of resource management solutions for\ncommunication networks. Deep learning, in particular, is highly effective in\nfacilitating fast and high-performing decision-making whenever representative\ntraining data is available to build offline accurate models. Conversely, online\nlearning solutions do not require training and enable adaptive decisions based\non runtime observations, alas are often overly conservative. This extensive\ntutorial proposes the use of optimistic learning (OpL) as a decision engine for\nresource management frameworks in modern communication systems. When properly\ndesigned, such solutions can achieve fast and high-performing decisions --\ncomparable to offline-trained models -- while preserving the robustness and\nperformance guarantees of the respective online learning approaches. We\nintroduce the fundamental concepts, algorithms and results of OpL, discuss the\nroots of this theory and present different approaches to defining and achieving\noptimism. We proceed to showcase how OpL can enhance resource management in\ncommunication networks for several key problems such as caching, edge\ncomputing, network slicing, and workload assignment in decentralized O-RAN\nplatforms. Finally, we discuss the open challenges that must be addressed to\nunlock the full potential of this new resource management approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI/ML-based tools are at the forefront of resource management solutions for\ncommunication networks. Deep learning, in particular, is highly effective in\nfacilitating fast and high-performing decision-making whenever representative\ntraining data is available to build offline accurate models. Conversely, online\nlearning solutions do not require training and enable adaptive decisions based\non runtime observations, alas are often overly conservative. This extensive\ntutorial proposes the use of optimistic learning (OpL) as a decision engine for\nresource management frameworks in modern communication systems. When properly\ndesigned, such solutions can achieve fast and high-performing decisions --\ncomparable to offline-trained models -- while preserving the robustness and\nperformance guarantees of the respective online learning approaches. We\nintroduce the fundamental concepts, algorithms and results of OpL, discuss the\nroots of this theory and present different approaches to defining and achieving\noptimism. We proceed to showcase how OpL can enhance resource management in\ncommunication networks for several key problems such as caching, edge\ncomputing, network slicing, and workload assignment in decentralized O-RAN\nplatforms. Finally, we discuss the open challenges that must be addressed to\nunlock the full potential of this new resource management approach."
                },
                "authors": [
                    {
                        "name": "George Iosifidis"
                    },
                    {
                        "name": "Naram Mhaisen"
                    },
                    {
                        "name": "Douglas J. Leith"
                    }
                ],
                "author_detail": {
                    "name": "Douglas J. Leith"
                },
                "author": "Douglas J. Leith",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03499v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03499v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10153v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10153v3",
                "updated": "2025-04-04T13:27:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    13,
                    27,
                    49,
                    4,
                    94,
                    0
                ],
                "published": "2024-12-13T14:11:42Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    11,
                    42,
                    4,
                    348,
                    0
                ],
                "title": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector"
                },
                "summary": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies."
                },
                "authors": [
                    {
                        "name": "Weixiang Zhang"
                    },
                    {
                        "name": "Shuzhao Xie"
                    },
                    {
                        "name": "Chengwei Ren"
                    },
                    {
                        "name": "Siyi Xie"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Shijia Ge"
                    },
                    {
                        "name": "Mingzi Wang"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10153v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10153v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03140v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03140v1",
                "updated": "2025-04-04T03:30:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    3,
                    30,
                    15,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T03:30:15Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    3,
                    30,
                    15,
                    4,
                    94,
                    0
                ],
                "title": "Model Reveals What to Cache: Profiling-Based Feature Reuse for Video\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Reveals What to Cache: Profiling-Based Feature Reuse for Video\n  Diffusion Models"
                },
                "summary": "Recent advances in diffusion models have demonstrated remarkable capabilities\nin video generation. However, the computational intensity remains a significant\nchallenge for practical applications. While feature caching has been proposed\nto reduce the computational burden of diffusion models, existing methods\ntypically overlook the heterogeneous significance of individual blocks,\nresulting in suboptimal reuse and degraded output quality. To this end, we\naddress this gap by introducing ProfilingDiT, a novel adaptive caching strategy\nthat explicitly disentangles foreground and background-focused blocks. Through\na systematic analysis of attention distributions in diffusion models, we reveal\na key observation: 1) Most layers exhibit a consistent preference for either\nforeground or background regions. 2) Predicted noise shows low inter-step\nsimilarity initially, which stabilizes as denoising progresses. This finding\ninspires us to formulate a selective caching strategy that preserves full\ncomputation for dynamic foreground elements while efficiently caching static\nbackground features. Our approach substantially reduces computational overhead\nwhile preserving visual fidelity. Extensive experiments demonstrate that our\nframework achieves significant acceleration (e.g., 2.01 times speedup for\nWan2.1) while maintaining visual fidelity across comprehensive quality metrics,\nestablishing a viable method for efficient video generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in diffusion models have demonstrated remarkable capabilities\nin video generation. However, the computational intensity remains a significant\nchallenge for practical applications. While feature caching has been proposed\nto reduce the computational burden of diffusion models, existing methods\ntypically overlook the heterogeneous significance of individual blocks,\nresulting in suboptimal reuse and degraded output quality. To this end, we\naddress this gap by introducing ProfilingDiT, a novel adaptive caching strategy\nthat explicitly disentangles foreground and background-focused blocks. Through\na systematic analysis of attention distributions in diffusion models, we reveal\na key observation: 1) Most layers exhibit a consistent preference for either\nforeground or background regions. 2) Predicted noise shows low inter-step\nsimilarity initially, which stabilizes as denoising progresses. This finding\ninspires us to formulate a selective caching strategy that preserves full\ncomputation for dynamic foreground elements while efficiently caching static\nbackground features. Our approach substantially reduces computational overhead\nwhile preserving visual fidelity. Extensive experiments demonstrate that our\nframework achieves significant acceleration (e.g., 2.01 times speedup for\nWan2.1) while maintaining visual fidelity across comprehensive quality metrics,\nestablishing a viable method for efficient video generation."
                },
                "authors": [
                    {
                        "name": "Xuran Ma"
                    },
                    {
                        "name": "Yexin Liu"
                    },
                    {
                        "name": "Yaofu Liu"
                    },
                    {
                        "name": "Xianfeng Wu"
                    },
                    {
                        "name": "Mingzhe Zheng"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Ser-Nam Lim"
                    },
                    {
                        "name": "Harry Yang"
                    }
                ],
                "author_detail": {
                    "name": "Harry Yang"
                },
                "author": "Harry Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03140v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03140v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16444v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16444v3",
                "updated": "2025-04-03T22:49:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    22,
                    49,
                    22,
                    3,
                    93,
                    0
                ],
                "published": "2024-05-26T06:00:17Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    6,
                    0,
                    17,
                    6,
                    147,
                    0
                ],
                "title": "CacheBlend: Fast Large Language Model Serving for RAG with Cached\n  Knowledge Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheBlend: Fast Large Language Model Serving for RAG with Cached\n  Knowledge Fusion"
                },
                "summary": "Large language models (LLMs) often incorporate multiple text chunks in their\ninputs to provide the necessary contexts. To speed up the prefill of the long\nLLM inputs, one can pre-compute the KV cache of a text and re-use the KV cache\nwhen the context is reused as the prefix of another LLM input. However, the\nreused text chunks are not always the input prefix, which makes precomputed KV\ncaches not directly usable since they ignore the text's cross-attention with\nthe preceding texts. Thus, the benefits of reusing KV caches remain largely\nunrealized.\n  This paper tackles just one challenge: when an LLM input contains multiple\ntext chunks, how to quickly combine their precomputed KV caches in order to\nachieve the same generation quality as the expensive full prefill (i.e.,\nwithout reusing KV cache)? This challenge naturally arises in\nretrieval-augmented generation (RAG) where the input is supplemented with\nmultiple retrieved texts as the context. We present CacheBlend, a scheme that\nreuses the precomputed KV caches, regardless prefix or not, and selectively\nrecomputes the KV values of a small subset of tokens to partially update each\nreused KV cache. In the meantime, the small extra delay for recomputing some\ntokens can be pipelined with the retrieval of KV caches within the same job,\nallowing CacheBlend to store KV caches in slower devices with more storage\ncapacity while retrieving them without increasing the inference delay. By\ncomparing CacheBlend with the state-of-the-art KV cache reusing schemes on\nthree open-source LLMs of various sizes and four popular benchmark datasets of\ndifferent tasks, we show that CacheBlend reduces time-to-first-token (TTFT) by\n2.2-3.3x and increases the inference throughput by 2.8-5x from full KV\nrecompute without compromising generation quality. The code is available at\nhttps://github.com/LMCache/LMCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often incorporate multiple text chunks in their\ninputs to provide the necessary contexts. To speed up the prefill of the long\nLLM inputs, one can pre-compute the KV cache of a text and re-use the KV cache\nwhen the context is reused as the prefix of another LLM input. However, the\nreused text chunks are not always the input prefix, which makes precomputed KV\ncaches not directly usable since they ignore the text's cross-attention with\nthe preceding texts. Thus, the benefits of reusing KV caches remain largely\nunrealized.\n  This paper tackles just one challenge: when an LLM input contains multiple\ntext chunks, how to quickly combine their precomputed KV caches in order to\nachieve the same generation quality as the expensive full prefill (i.e.,\nwithout reusing KV cache)? This challenge naturally arises in\nretrieval-augmented generation (RAG) where the input is supplemented with\nmultiple retrieved texts as the context. We present CacheBlend, a scheme that\nreuses the precomputed KV caches, regardless prefix or not, and selectively\nrecomputes the KV values of a small subset of tokens to partially update each\nreused KV cache. In the meantime, the small extra delay for recomputing some\ntokens can be pipelined with the retrieval of KV caches within the same job,\nallowing CacheBlend to store KV caches in slower devices with more storage\ncapacity while retrieving them without increasing the inference delay. By\ncomparing CacheBlend with the state-of-the-art KV cache reusing schemes on\nthree open-source LLMs of various sizes and four popular benchmark datasets of\ndifferent tasks, we show that CacheBlend reduces time-to-first-token (TTFT) by\n2.2-3.3x and increases the inference throughput by 2.8-5x from full KV\nrecompute without compromising generation quality. The code is available at\nhttps://github.com/LMCache/LMCache."
                },
                "authors": [
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Siddhant Ray"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Qizheng Zhang"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16444v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16444v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03048v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03048v1",
                "updated": "2025-04-03T21:53:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    21,
                    53,
                    51,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T21:53:51Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    21,
                    53,
                    51,
                    3,
                    93,
                    0
                ],
                "title": "LLM Library Learning Fails: A LEGO-Prover Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Library Learning Fails: A LEGO-Prover Case Study"
                },
                "summary": "Recent advancements in the coding, reasoning, and tool-using abilities of\nLLMs have spurred interest in library learning (i.e., online learning through\nthe creation, storage, and retrieval of reusable and composable functions,\nknowledge, checklists, or lemmas). Such systems often promise improved task\nperformance through the automatic creation of broadly applicable tools, as well\nas superior computational performance through the caching of reasoning (i.e.,\nthe storage of generated tools). However, we find strong reason to be\nskeptical. We perform a deep dive into one such system, LEGO-Prover, which\npurports to learn reusable lemmas for mathematical reasoning. We find no\nevidence of the direct reuse of learned lemmas, and find evidence against the\nsoft reuse of learned lemmas (i.e., reuse by modifying relevant examples).\nCrucially, we find that LEGO-Prover does not in fact improve over the simple\nbaseline of prompting the model - the improvements in task accuracy vanish once\ncomputational cost is accounted for. Our findings suggest that serious\nmisconceptions exist as to the effectiveness of these techniques, that a\nserious re-examination of the state of LLM-based library learning is required,\nand that we require much stronger standards for evaluation including\nbehavioural analysis and ensuring that an equal computational budget is used\nfor baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in the coding, reasoning, and tool-using abilities of\nLLMs have spurred interest in library learning (i.e., online learning through\nthe creation, storage, and retrieval of reusable and composable functions,\nknowledge, checklists, or lemmas). Such systems often promise improved task\nperformance through the automatic creation of broadly applicable tools, as well\nas superior computational performance through the caching of reasoning (i.e.,\nthe storage of generated tools). However, we find strong reason to be\nskeptical. We perform a deep dive into one such system, LEGO-Prover, which\npurports to learn reusable lemmas for mathematical reasoning. We find no\nevidence of the direct reuse of learned lemmas, and find evidence against the\nsoft reuse of learned lemmas (i.e., reuse by modifying relevant examples).\nCrucially, we find that LEGO-Prover does not in fact improve over the simple\nbaseline of prompting the model - the improvements in task accuracy vanish once\ncomputational cost is accounted for. Our findings suggest that serious\nmisconceptions exist as to the effectiveness of these techniques, that a\nserious re-examination of the state of LLM-based library learning is required,\nand that we require much stronger standards for evaluation including\nbehavioural analysis and ensuring that an equal computational budget is used\nfor baselines."
                },
                "authors": [
                    {
                        "name": "Ian Berlot-Attwell"
                    },
                    {
                        "name": "Frank Rudzicz"
                    },
                    {
                        "name": "Xujie Si"
                    }
                ],
                "author_detail": {
                    "name": "Xujie Si"
                },
                "author": "Xujie Si",
                "arxiv_comment": "24 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03048v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03048v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02976v1",
                "updated": "2025-04-03T18:54:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    18,
                    54,
                    50,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T18:54:50Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    18,
                    54,
                    50,
                    3,
                    93,
                    0
                ],
                "title": "Localized Definitions and Distributed Reasoning: A Proof-of-Concept\n  Mechanistic Interpretability Study via Activation Patching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Localized Definitions and Distributed Reasoning: A Proof-of-Concept\n  Mechanistic Interpretability Study via Activation Patching"
                },
                "summary": "This study investigates the localization of knowledge representation in\nfine-tuned GPT-2 models using Causal Layer Attribution via Activation Patching\n(CLAP), a method that identifies critical neural layers responsible for correct\nanswer generation. The model was fine-tuned on 9,958 PubMed abstracts\n(epilepsy: 20,595 mentions, EEG: 11,674 mentions, seizure: 13,921 mentions)\nusing two configurations with validation loss monitoring for early stopping.\nCLAP involved (1) caching clean (correct answer) and corrupted (incorrect\nanswer) activations, (2) computing logit difference to quantify model\npreference, and (3) patching corrupted activations with clean ones to assess\nrecovery. Results revealed three findings: First, patching the first\nfeedforward layer recovered 56% of correct preference, demonstrating that\nassociative knowledge is distributed across multiple layers. Second, patching\nthe final output layer completely restored accuracy (100% recovery), indicating\nthat definitional knowledge is localised. The stronger clean logit difference\nfor definitional questions further supports this localized representation.\nThird, minimal recovery from convolutional layer patching (13.6%) suggests\nlow-level features contribute marginally to high-level reasoning. Statistical\nanalysis confirmed significant layer-specific effects (p<0.01). These findings\ndemonstrate that factual knowledge is more localized and associative knowledge\ndepends on distributed representations. We also showed that editing efficacy\ndepends on task type. Our findings not only reconcile conflicting observations\nabout localization in model editing but also emphasize on using task-adaptive\ntechniques for reliable, interpretable updates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the localization of knowledge representation in\nfine-tuned GPT-2 models using Causal Layer Attribution via Activation Patching\n(CLAP), a method that identifies critical neural layers responsible for correct\nanswer generation. The model was fine-tuned on 9,958 PubMed abstracts\n(epilepsy: 20,595 mentions, EEG: 11,674 mentions, seizure: 13,921 mentions)\nusing two configurations with validation loss monitoring for early stopping.\nCLAP involved (1) caching clean (correct answer) and corrupted (incorrect\nanswer) activations, (2) computing logit difference to quantify model\npreference, and (3) patching corrupted activations with clean ones to assess\nrecovery. Results revealed three findings: First, patching the first\nfeedforward layer recovered 56% of correct preference, demonstrating that\nassociative knowledge is distributed across multiple layers. Second, patching\nthe final output layer completely restored accuracy (100% recovery), indicating\nthat definitional knowledge is localised. The stronger clean logit difference\nfor definitional questions further supports this localized representation.\nThird, minimal recovery from convolutional layer patching (13.6%) suggests\nlow-level features contribute marginally to high-level reasoning. Statistical\nanalysis confirmed significant layer-specific effects (p<0.01). These findings\ndemonstrate that factual knowledge is more localized and associative knowledge\ndepends on distributed representations. We also showed that editing efficacy\ndepends on task type. Our findings not only reconcile conflicting observations\nabout localization in model editing but also emphasize on using task-adaptive\ntechniques for reliable, interpretable updates."
                },
                "authors": [
                    {
                        "name": "Nooshin Bahador"
                    }
                ],
                "author_detail": {
                    "name": "Nooshin Bahador"
                },
                "author": "Nooshin Bahador",
                "arxiv_comment": "15 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02972v1",
                "updated": "2025-04-03T18:47:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    18,
                    47,
                    26,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T18:47:26Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    18,
                    47,
                    26,
                    3,
                    93,
                    0
                ],
                "title": "Improved Compact Genetic Algorithms with Efficient Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improved Compact Genetic Algorithms with Efficient Caching"
                },
                "summary": "Compact Genetic Algorithms (cGAs) are condensed variants of classical Genetic\nAlgorithms (GAs) that use a probability vector representation of the population\ninstead of the complete population. cGAs have been shown to significantly\nreduce the number of function evaluations required while producing outcomes\nsimilar to those of classical GAs. However, cGAs have a tendency to repeatedly\ngenerate the same chromosomes as they approach convergence, resulting in\nunnecessary evaluations of identical chromosomes. This article introduces the\nconcept of caching in cGAs as a means of avoiding redundant evaluations of the\nsame chromosomes. Our proposed approach operates equivalently to cGAs, but\nenhances the algorithm's time efficiency by reducing the number of function\nevaluations. We also present a data structure for efficient cache maintenance\nto ensure low overhead. The proposed caching approach has an asymptotically\nconstant time complexity on average. The proposed method further generalizes\nthe caching mechanism with higher selection pressure for elitism-based cGAs. We\nconduct a rigorous analysis based on experiments on benchmark optimization\nproblems using two well-known cache replacement strategies. The results\ndemonstrate that caching significantly reduces the number of function\nevaluations required while maintaining the same level of performance accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compact Genetic Algorithms (cGAs) are condensed variants of classical Genetic\nAlgorithms (GAs) that use a probability vector representation of the population\ninstead of the complete population. cGAs have been shown to significantly\nreduce the number of function evaluations required while producing outcomes\nsimilar to those of classical GAs. However, cGAs have a tendency to repeatedly\ngenerate the same chromosomes as they approach convergence, resulting in\nunnecessary evaluations of identical chromosomes. This article introduces the\nconcept of caching in cGAs as a means of avoiding redundant evaluations of the\nsame chromosomes. Our proposed approach operates equivalently to cGAs, but\nenhances the algorithm's time efficiency by reducing the number of function\nevaluations. We also present a data structure for efficient cache maintenance\nto ensure low overhead. The proposed caching approach has an asymptotically\nconstant time complexity on average. The proposed method further generalizes\nthe caching mechanism with higher selection pressure for elitism-based cGAs. We\nconduct a rigorous analysis based on experiments on benchmark optimization\nproblems using two well-known cache replacement strategies. The results\ndemonstrate that caching significantly reduces the number of function\nevaluations required while maintaining the same level of performance accuracy."
                },
                "authors": [
                    {
                        "name": "Prasanta Dutta"
                    },
                    {
                        "name": "Anirban Mukhopadhyay"
                    }
                ],
                "author_detail": {
                    "name": "Anirban Mukhopadhyay"
                },
                "author": "Anirban Mukhopadhyay",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02921v1",
                "updated": "2025-04-03T17:08:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    8,
                    42,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T17:08:42Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    8,
                    42,
                    3,
                    93,
                    0
                ],
                "title": "HyperRAG: Enhancing Quality-Efficiency Tradeoffs in Retrieval-Augmented\n  Generation with Reranker KV-Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyperRAG: Enhancing Quality-Efficiency Tradeoffs in Retrieval-Augmented\n  Generation with Reranker KV-Cache Reuse"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for\nenhancing the performance of large language models (LLMs) by integrating\nexternal knowledge into the generation process. A key component of RAG\npipelines is the reranker, which selects the most relevant documents from a\npool of retrieved candidates and significantly improves the quality of the\ngenerated responses. While rerankers refine the selection of retrieved\ndocuments in RAG pipelines, they introduce computational challenges that hinder\nhigh throughput and low latency. To address this problem, we propose HyperRAG,\na system that optimizes the trade-off between quality and efficiency in RAG\npipelines by leveraging KV-cache reuse for efficient reranker inference. By\nreusing document-side KV-cache, HyperRAG achieves both high-quality generation\nand system-level efficiency. To fully realize the benefits of KV-cache reuse,\nHyperRAG incorporates a range of system-level optimizations designed to enhance\nefficiency and scalability. Experiments show that HyperRAG achieves a 2 - 3\nthroughput improvement with decoder-only rerankers while also delivering higher\ndownstream performance compared with traditional RAG service.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for\nenhancing the performance of large language models (LLMs) by integrating\nexternal knowledge into the generation process. A key component of RAG\npipelines is the reranker, which selects the most relevant documents from a\npool of retrieved candidates and significantly improves the quality of the\ngenerated responses. While rerankers refine the selection of retrieved\ndocuments in RAG pipelines, they introduce computational challenges that hinder\nhigh throughput and low latency. To address this problem, we propose HyperRAG,\na system that optimizes the trade-off between quality and efficiency in RAG\npipelines by leveraging KV-cache reuse for efficient reranker inference. By\nreusing document-side KV-cache, HyperRAG achieves both high-quality generation\nand system-level efficiency. To fully realize the benefits of KV-cache reuse,\nHyperRAG incorporates a range of system-level optimizations designed to enhance\nefficiency and scalability. Experiments show that HyperRAG achieves a 2 - 3\nthroughput improvement with decoder-only rerankers while also delivering higher\ndownstream performance compared with traditional RAG service."
                },
                "authors": [
                    {
                        "name": "Yuwei An"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Seo Jin Park"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01380v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01380v2",
                "updated": "2025-04-03T13:28:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    13,
                    28,
                    51,
                    3,
                    93,
                    0
                ],
                "published": "2024-12-02T11:07:51Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    7,
                    51,
                    0,
                    337,
                    0
                ],
                "title": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking"
                },
                "summary": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which results in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46\\% reduction in memory and 40\\% increase in throughput with $<$\n0.1 loss in perplexity when compared to streaming the dense model from Flash.\nThe open source code for HW simulator, methods, and experiments in this paper\nis available at https://github.com/Qualcomm-AI-research/dynamic-sparsity .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which results in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46\\% reduction in memory and 40\\% increase in throughput with $<$\n0.1 loss in perplexity when compared to streaming the dense model from Flash.\nThe open source code for HW simulator, methods, and experiments in this paper\nis available at https://github.com/Qualcomm-AI-research/dynamic-sparsity ."
                },
                "authors": [
                    {
                        "name": "Marco Federici"
                    },
                    {
                        "name": "Davide Belli"
                    },
                    {
                        "name": "Mart van Baalen"
                    },
                    {
                        "name": "Amir Jalalirad"
                    },
                    {
                        "name": "Andrii Skliar"
                    },
                    {
                        "name": "Bence Major"
                    },
                    {
                        "name": "Markus Nagel"
                    },
                    {
                        "name": "Paul Whatmough"
                    }
                ],
                "author_detail": {
                    "name": "Paul Whatmough"
                },
                "author": "Paul Whatmough",
                "arxiv_comment": "Main Text: 10 pages, 11 figures. Appendix: 6 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01380v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01380v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02441v1",
                "updated": "2025-04-03T09:58:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    58,
                    19,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T09:58:19Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    58,
                    19,
                    3,
                    93,
                    0
                ],
                "title": "Cognitive Memory in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Memory in Large Language Models"
                },
                "summary": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions."
                },
                "authors": [
                    {
                        "name": "Lianlei Shan"
                    },
                    {
                        "name": "Shixian Luo"
                    },
                    {
                        "name": "Zezhou Zhu"
                    },
                    {
                        "name": "Yu Yuan"
                    },
                    {
                        "name": "Yong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Wu"
                },
                "author": "Yong Wu",
                "arxiv_comment": "37 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03775v1",
                "updated": "2025-04-03T08:58:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    8,
                    58,
                    5,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T08:58:05Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    8,
                    58,
                    5,
                    3,
                    93,
                    0
                ],
                "title": "FlowKV: A Disaggregated Inference Framework with Low-Latency KV Cache\n  Transfer and Load-Aware Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowKV: A Disaggregated Inference Framework with Low-Latency KV Cache\n  Transfer and Load-Aware Scheduling"
                },
                "summary": "Disaggregated inference has become an essential framework that separates the\nprefill (P) and decode (D) stages in large language model inference to improve\nthroughput. However, the KV cache transfer faces significant delays between\nprefill and decode nodes. The block-wise calling method and discontinuous KV\ncache memory allocation increase the number of calls to the transmission\nkernel. Additionally, existing frameworks often fix the roles of P and D nodes,\nleading to computational imbalances. In this paper, we propose FlowKV, a novel\ndisaggregated inference framework, which reduces the average transmission\nlatency of KV cache by 96%, from 0.944s to 0.053s, almost eliminating the\ntransfer time relative to the total request latency by optimizing the KV cache\ntransfer. FlowKV introduces the Load-Aware Scheduler for balanced request\nscheduling and flexible PD node allocation. This design maximizes hardware\nresource utilization, achieving peak system throughput across various\nscenarios, including normal, computational imbalance, and extreme overload\nconditions. Experimental results demonstrate that FlowKV significantly\naccelerates inference by 15.2%-48.9% on LongBench dataset compared to the\nbaseline and supports applications with heterogeneous GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated inference has become an essential framework that separates the\nprefill (P) and decode (D) stages in large language model inference to improve\nthroughput. However, the KV cache transfer faces significant delays between\nprefill and decode nodes. The block-wise calling method and discontinuous KV\ncache memory allocation increase the number of calls to the transmission\nkernel. Additionally, existing frameworks often fix the roles of P and D nodes,\nleading to computational imbalances. In this paper, we propose FlowKV, a novel\ndisaggregated inference framework, which reduces the average transmission\nlatency of KV cache by 96%, from 0.944s to 0.053s, almost eliminating the\ntransfer time relative to the total request latency by optimizing the KV cache\ntransfer. FlowKV introduces the Load-Aware Scheduler for balanced request\nscheduling and flexible PD node allocation. This design maximizes hardware\nresource utilization, achieving peak system throughput across various\nscenarios, including normal, computational imbalance, and extreme overload\nconditions. Experimental results demonstrate that FlowKV significantly\naccelerates inference by 15.2%-48.9% on LongBench dataset compared to the\nbaseline and supports applications with heterogeneous GPUs."
                },
                "authors": [
                    {
                        "name": "Weiqing Li"
                    },
                    {
                        "name": "Guochao Jiang"
                    },
                    {
                        "name": "Xiangyong Ding"
                    },
                    {
                        "name": "Zhangcheng Tao"
                    },
                    {
                        "name": "Chuzhan Hao"
                    },
                    {
                        "name": "Chenfeng Xu"
                    },
                    {
                        "name": "Yuewei Zhang"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02268v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02268v1",
                "updated": "2025-04-03T04:27:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    4,
                    27,
                    2,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T04:27:02Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    4,
                    27,
                    2,
                    3,
                    93,
                    0
                ],
                "title": "Advancing Semantic Caching for LLMs with Domain-Specific Embeddings and\n  Synthetic Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Semantic Caching for LLMs with Domain-Specific Embeddings and\n  Synthetic Data"
                },
                "summary": "This report investigates enhancing semantic caching effectiveness by\nemploying specialized, fine-tuned embedding models. Semantic caching relies on\nembedding similarity rather than exact key matching, presenting unique\nchallenges in balancing precision, query latency, and computational efficiency.\nWe propose leveraging smaller, domain-specific embedding models, fine-tuned\nwith targeted real-world and synthetically generated datasets. Our empirical\nevaluations demonstrate that compact embedding models fine-tuned for just one\nepoch on specialized datasets significantly surpass both state-of-the-art\nopen-source and proprietary alternatives in precision and recall. Moreover, we\nintroduce a novel synthetic data generation pipeline for the semantic cache\nthat mitigates the challenge of limited domain-specific annotated data, further\nboosting embedding performance. Our approach effectively balances computational\noverhead and accuracy, establishing a viable and efficient strategy for\npractical semantic caching implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report investigates enhancing semantic caching effectiveness by\nemploying specialized, fine-tuned embedding models. Semantic caching relies on\nembedding similarity rather than exact key matching, presenting unique\nchallenges in balancing precision, query latency, and computational efficiency.\nWe propose leveraging smaller, domain-specific embedding models, fine-tuned\nwith targeted real-world and synthetically generated datasets. Our empirical\nevaluations demonstrate that compact embedding models fine-tuned for just one\nepoch on specialized datasets significantly surpass both state-of-the-art\nopen-source and proprietary alternatives in precision and recall. Moreover, we\nintroduce a novel synthetic data generation pipeline for the semantic cache\nthat mitigates the challenge of limited domain-specific annotated data, further\nboosting embedding performance. Our approach effectively balances computational\noverhead and accuracy, establishing a viable and efficient strategy for\npractical semantic caching implementations."
                },
                "authors": [
                    {
                        "name": "Waris Gill"
                    },
                    {
                        "name": "Justin Cechmanek"
                    },
                    {
                        "name": "Tyler Hutcherson"
                    },
                    {
                        "name": "Srijith Rajamohan"
                    },
                    {
                        "name": "Jen Agarwal"
                    },
                    {
                        "name": "Muhammad Ali Gulzar"
                    },
                    {
                        "name": "Manvinder Singh"
                    },
                    {
                        "name": "Benoit Dion"
                    }
                ],
                "author_detail": {
                    "name": "Benoit Dion"
                },
                "arxiv_affiliation": "Redis",
                "author": "Benoit Dion",
                "arxiv_comment": "Initial study on embedding fine tuning for semantic cache. It also\n  explores synthetic data. Total pages are 12, including refrences",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02268v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02268v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02220v1",
                "updated": "2025-04-03T02:24:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    2,
                    24,
                    21,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T02:24:21Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    2,
                    24,
                    21,
                    3,
                    93,
                    0
                ],
                "title": "Comparative Analysis of Distributed Caching Algorithms: Performance\n  Metrics and Implementation Considerations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparative Analysis of Distributed Caching Algorithms: Performance\n  Metrics and Implementation Considerations"
                },
                "summary": "This paper presents a comprehensive comparison of distributed caching\nalgorithms employed in modern distributed systems. We evaluate various caching\nstrategies including Least Recently Used (LRU), Least Frequently Used (LFU),\nAdaptive Replacement Cache (ARC), and Time-Aware Least Recently Used (TLRU)\nagainst metrics such as hit ratio, latency reduction, memory overhead, and\nscalability. Our analysis reveals that while traditional algorithms like LRU\nremain prevalent, hybrid approaches incorporating machine learning techniques\ndemonstrate superior performance in dynamic environments. Additionally, we\nanalyze implementation patterns across different distributed architectures and\nprovide recommendations for algorithm selection based on specific workload\ncharacteristics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a comprehensive comparison of distributed caching\nalgorithms employed in modern distributed systems. We evaluate various caching\nstrategies including Least Recently Used (LRU), Least Frequently Used (LFU),\nAdaptive Replacement Cache (ARC), and Time-Aware Least Recently Used (TLRU)\nagainst metrics such as hit ratio, latency reduction, memory overhead, and\nscalability. Our analysis reveals that while traditional algorithms like LRU\nremain prevalent, hybrid approaches incorporating machine learning techniques\ndemonstrate superior performance in dynamic environments. Additionally, we\nanalyze implementation patterns across different distributed architectures and\nprovide recommendations for algorithm selection based on specific workload\ncharacteristics."
                },
                "authors": [
                    {
                        "name": "Helen Mayer"
                    },
                    {
                        "name": "James Richards"
                    }
                ],
                "author_detail": {
                    "name": "James Richards"
                },
                "author": "James Richards",
                "arxiv_comment": "International Conference on Computing Technologies and Artificial\n  Intelligence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01281v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01281v2",
                "updated": "2025-04-03T01:23:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    1,
                    23,
                    22,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-02T01:16:10Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    16,
                    10,
                    2,
                    92,
                    0
                ],
                "title": "Scaling Test-Time Inference with Policy-Optimized, Dynamic\n  Retrieval-Augmented Generation via KV Caching and Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Test-Time Inference with Policy-Optimized, Dynamic\n  Retrieval-Augmented Generation via KV Caching and Decoding"
                },
                "summary": "We present a comprehensive framework for enhancing Retrieval-Augmented\nGeneration (RAG) systems through dynamic retrieval strategies and reinforcement\nfine-tuning. This approach significantly improves large language models on\nknowledge-intensive tasks, including opendomain question answering and complex\nreasoning. Our framework integrates two complementary techniques:\nPolicy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use\nof retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS),\nwhich dynamically determines retrieval timing and content based on contextual\nneeds. Together, these techniques enhance both the utilization and relevance of\nretrieved content, improving factual accuracy and response quality. Designed as\na lightweight solution compatible with any Transformer-based LLM without\nrequiring additional training, our framework excels in knowledge-intensive\ntasks, boosting output accuracy in RAG settings. We further propose CRITIC, a\nnovel method to selectively compress key-value caches by token importance,\nmitigating memory bottlenecks in long-context applications. The framework also\nincorporates test-time scaling techniques to dynamically balance reasoning\ndepth and computational resources, alongside optimized decoding strategies for\nfaster inference. Experiments on benchmark datasets show that our framework\nreduces hallucinations, strengthens domain-specific reasoning, and achieves\nsignificant efficiency and scalability gains over traditional RAG systems. This\nintegrated approach advances the development of robust, efficient, and scalable\nRAG systems across diverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a comprehensive framework for enhancing Retrieval-Augmented\nGeneration (RAG) systems through dynamic retrieval strategies and reinforcement\nfine-tuning. This approach significantly improves large language models on\nknowledge-intensive tasks, including opendomain question answering and complex\nreasoning. Our framework integrates two complementary techniques:\nPolicy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use\nof retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS),\nwhich dynamically determines retrieval timing and content based on contextual\nneeds. Together, these techniques enhance both the utilization and relevance of\nretrieved content, improving factual accuracy and response quality. Designed as\na lightweight solution compatible with any Transformer-based LLM without\nrequiring additional training, our framework excels in knowledge-intensive\ntasks, boosting output accuracy in RAG settings. We further propose CRITIC, a\nnovel method to selectively compress key-value caches by token importance,\nmitigating memory bottlenecks in long-context applications. The framework also\nincorporates test-time scaling techniques to dynamically balance reasoning\ndepth and computational resources, alongside optimized decoding strategies for\nfaster inference. Experiments on benchmark datasets show that our framework\nreduces hallucinations, strengthens domain-specific reasoning, and achieves\nsignificant efficiency and scalability gains over traditional RAG systems. This\nintegrated approach advances the development of robust, efficient, and scalable\nRAG systems across diverse applications."
                },
                "authors": [
                    {
                        "name": "Sakhinana Sagar Srinivas"
                    },
                    {
                        "name": "Venkataramana Runkana"
                    }
                ],
                "author_detail": {
                    "name": "Venkataramana Runkana"
                },
                "author": "Venkataramana Runkana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01281v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01281v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22875v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22875v2",
                "updated": "2025-04-02T18:51:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    18,
                    51,
                    53,
                    2,
                    92,
                    0
                ],
                "published": "2025-03-28T21:02:32Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    21,
                    2,
                    32,
                    4,
                    87,
                    0
                ],
                "title": "A Pilot Study on Tunable Precision Emulation via Automatic BLAS\n  Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Pilot Study on Tunable Precision Emulation via Automatic BLAS\n  Offloading"
                },
                "summary": "This study explores the use of automatic BLAS offloading and INT8-based\nemulation for accelerating traditional HPC workloads on modern GPU\narchitectures. Through the use of low-bitwidth integer units and cache-coherent\nUnified Memory Architecture, we emulate double-precision matrix multiplications\nin the MuST application without code changes. We find that accuracy depends on\nboth arithmetic precision and the properties of the operator, which can be\ndealt with through tunable precision emulation. Unlike traditional\nmixed-precision approaches, this method preserves original algorithms while\noptimizing hardware utilization. We showcases the potential of improving\naccuracy and performance at the same time. This work highlights the potential\nof AI-driven hardware to transform HPC, advocating for adaptive precision\nstrategies in future scientific computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the use of automatic BLAS offloading and INT8-based\nemulation for accelerating traditional HPC workloads on modern GPU\narchitectures. Through the use of low-bitwidth integer units and cache-coherent\nUnified Memory Architecture, we emulate double-precision matrix multiplications\nin the MuST application without code changes. We find that accuracy depends on\nboth arithmetic precision and the properties of the operator, which can be\ndealt with through tunable precision emulation. Unlike traditional\nmixed-precision approaches, this method preserves original algorithms while\noptimizing hardware utilization. We showcases the potential of improving\naccuracy and performance at the same time. This work highlights the potential\nof AI-driven hardware to transform HPC, advocating for adaptive precision\nstrategies in future scientific computing."
                },
                "authors": [
                    {
                        "name": "Hang Liu"
                    },
                    {
                        "name": "Junjie Li"
                    },
                    {
                        "name": "Yinzhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yinzhi Wang"
                },
                "author": "Yinzhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22875v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22875v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01582v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01582v1",
                "updated": "2025-04-02T10:38:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    10,
                    38,
                    25,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T10:38:25Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    10,
                    38,
                    25,
                    2,
                    92,
                    0
                ],
                "title": "MERE: Hardware-Software Co-Design for Masking Cache Miss Latency in\n  Embedded Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MERE: Hardware-Software Co-Design for Masking Cache Miss Latency in\n  Embedded Processors"
                },
                "summary": "Runahead execution is a technique to mask memory latency caused by irregular\nmemory accesses. By pre-executing the application code during occurrences of\nlong-latency operations and prefetching anticipated cache-missed data into the\ncache hierarchy, runahead effectively masks memory latency for subsequent cache\nmisses and achieves high prefetching accuracy; however, this technique has been\nlimited to superscalar out-of-order and superscalar in-order cores. For\nimplementation in scalar in-order cores, the challenges of\narea-/energy-constraint and severe cache contention remain.\n  Here, we build the first full-stack system featuring runahead, MERE, from SoC\nand a dedicated ISA to the OS and programming model. Through this deployment,\nwe show that enabling runahead in scalar in-order cores is possible, with\nminimal area and power overheads, while still achieving high performance. By\nre-constructing the sequential runahead employing a hardware/software co-design\napproach, the system can be implemented on a mature processor and SoC. Building\non this, an adaptive runahead mechanism is proposed to mitigate the severe\ncache contention in scalar in-order cores. Combining this, we provide a\ncomprehensive solution for embedded processors managing irregular workloads.\nOur evaluation demonstrates that the proposed MERE attains 93.5% of a 2-wide\nout-of-order core's performance while constraining area and power overheads\nbelow 5%, with the adaptive runahead mechanism delivering an additional 20.1%\nperformance gain through mitigating the severe cache contention issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Runahead execution is a technique to mask memory latency caused by irregular\nmemory accesses. By pre-executing the application code during occurrences of\nlong-latency operations and prefetching anticipated cache-missed data into the\ncache hierarchy, runahead effectively masks memory latency for subsequent cache\nmisses and achieves high prefetching accuracy; however, this technique has been\nlimited to superscalar out-of-order and superscalar in-order cores. For\nimplementation in scalar in-order cores, the challenges of\narea-/energy-constraint and severe cache contention remain.\n  Here, we build the first full-stack system featuring runahead, MERE, from SoC\nand a dedicated ISA to the OS and programming model. Through this deployment,\nwe show that enabling runahead in scalar in-order cores is possible, with\nminimal area and power overheads, while still achieving high performance. By\nre-constructing the sequential runahead employing a hardware/software co-design\napproach, the system can be implemented on a mature processor and SoC. Building\non this, an adaptive runahead mechanism is proposed to mitigate the severe\ncache contention in scalar in-order cores. Combining this, we provide a\ncomprehensive solution for embedded processors managing irregular workloads.\nOur evaluation demonstrates that the proposed MERE attains 93.5% of a 2-wide\nout-of-order core's performance while constraining area and power overheads\nbelow 5%, with the adaptive runahead mechanism delivering an additional 20.1%\nperformance gain through mitigating the severe cache contention issues."
                },
                "authors": [
                    {
                        "name": "Dean You"
                    },
                    {
                        "name": "Jieyu Jiang"
                    },
                    {
                        "name": "Xiaoxuan Wang"
                    },
                    {
                        "name": "Yushu Du"
                    },
                    {
                        "name": "Zhihang Tan"
                    },
                    {
                        "name": "Wenbo Xu"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Jiapeng Guan"
                    },
                    {
                        "name": "Zhenyuan Wang"
                    },
                    {
                        "name": "Ran Wei"
                    },
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Zhe Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Jiang"
                },
                "author": "Zhe Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01582v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01582v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01288v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01288v4",
                "updated": "2025-04-02T04:57:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    4,
                    57,
                    15,
                    2,
                    92,
                    0
                ],
                "published": "2024-11-02T15:45:54Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    45,
                    54,
                    5,
                    307,
                    0
                ],
                "title": "Hexa-MoE: Efficient and Heterogeneous-aware Training for\n  Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hexa-MoE: Efficient and Heterogeneous-aware Training for\n  Mixture-of-Experts"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Hanrui Wang"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01288v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01288v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11049v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11049v5",
                "updated": "2025-04-02T01:58:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    58,
                    38,
                    2,
                    92,
                    0
                ],
                "published": "2024-08-20T17:57:31Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    31,
                    1,
                    233,
                    0
                ],
                "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency losslessly, but the conventional wisdom suggests\nthat its efficacy is limited to small batch sizes. In MagicDec, we show that\nsurprisingly SD can achieve speedup even for a high throughput inference regime\nfor moderate to long sequences. More interestingly, an intelligent drafting\nstrategy can achieve better speedup with increasing batch size based on our\nrigorous analysis. MagicDec first identifies the bottleneck shifts with\nincreasing batch size and sequence length, and uses these insights to deploy SD\nmore effectively for high throughput inference. We leverage draft model with\nsparse KV cache to address the KV bottleneck, which scales with both sequence\nlength and batch size. Additionally, we propose a theoretical model to select\nthe optimal drafting strategy for maximum speedup. Our work highlights the\nbroad applicability of speculative decoding in long-context serving, as it can\nenhance throughput and reduce latency without compromising accuracy. For\nmoderate to long sequences, we demonstrate up to 2.51x speedup for Llama3.1-8B\nwhen serving batch sizes ranging from 32 to 256 on various types of hardware\nand tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency losslessly, but the conventional wisdom suggests\nthat its efficacy is limited to small batch sizes. In MagicDec, we show that\nsurprisingly SD can achieve speedup even for a high throughput inference regime\nfor moderate to long sequences. More interestingly, an intelligent drafting\nstrategy can achieve better speedup with increasing batch size based on our\nrigorous analysis. MagicDec first identifies the bottleneck shifts with\nincreasing batch size and sequence length, and uses these insights to deploy SD\nmore effectively for high throughput inference. We leverage draft model with\nsparse KV cache to address the KV bottleneck, which scales with both sequence\nlength and batch size. Additionally, we propose a theoretical model to select\nthe optimal drafting strategy for maximum speedup. Our work highlights the\nbroad applicability of speculative decoding in long-context serving, as it can\nenhance throughput and reduce latency without compromising accuracy. For\nmoderate to long sequences, we demonstrate up to 2.51x speedup for Llama3.1-8B\nwhen serving batch sizes ranging from 32 to 256 on various types of hardware\nand tasks."
                },
                "authors": [
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Vashisth Tiwari"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Jinyuan Shi"
                    },
                    {
                        "name": "Ian En-Hsu Yen"
                    },
                    {
                        "name": "Avner May"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11049v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11049v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01291v2",
                "updated": "2025-04-17T23:45:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    23,
                    45,
                    51,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-02T01:49:58Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    49,
                    58,
                    2,
                    92,
                    0
                ],
                "title": "Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN\n  Heterostructures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN\n  Heterostructures"
                },
                "summary": "We report on energy bands and breakdown characteristics of Al2O3 dielectrics\non ultra-wide bandgap (UWBG) AlGaN heterostructures.\nMetal-dielectric-semiconductor structures are important to sustain high fields\nneeded for future high-performance UWBG transistors. Using systematic\nexperiments, we determined the fixed charge density (> 1013 cm-2), the\ndielectric/interface, and electric fields in the oxide of under flat-band\nconditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x\n10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In\nlateral metal-semiconductor-insulator test structures, breakdown voltage\nexceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013\ncm-2. The effective peak electric field and average breakdown field were\nestimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings\ndemonstrate the potential of Al2O2 integration for enhancing the breakdown\nperformance of UWBG AlGaN HEMTs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on energy bands and breakdown characteristics of Al2O3 dielectrics\non ultra-wide bandgap (UWBG) AlGaN heterostructures.\nMetal-dielectric-semiconductor structures are important to sustain high fields\nneeded for future high-performance UWBG transistors. Using systematic\nexperiments, we determined the fixed charge density (> 1013 cm-2), the\ndielectric/interface, and electric fields in the oxide of under flat-band\nconditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x\n10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In\nlateral metal-semiconductor-insulator test structures, breakdown voltage\nexceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013\ncm-2. The effective peak electric field and average breakdown field were\nestimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings\ndemonstrate the potential of Al2O2 integration for enhancing the breakdown\nperformance of UWBG AlGaN HEMTs."
                },
                "authors": [
                    {
                        "name": "Seungheon Shin"
                    },
                    {
                        "name": "Kyle Liddy"
                    },
                    {
                        "name": "Yinxuan Zhu"
                    },
                    {
                        "name": "Chandan Joishi"
                    },
                    {
                        "name": "Brianna A. Klein"
                    },
                    {
                        "name": "Andrew Armstrong"
                    },
                    {
                        "name": "Andrew A. Allerman"
                    },
                    {
                        "name": "Siddharth Rajan"
                    }
                ],
                "author_detail": {
                    "name": "Siddharth Rajan"
                },
                "author": "Siddharth Rajan",
                "arxiv_comment": "12 pages, 7 figures, and 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01157v1",
                "updated": "2025-04-01T19:48:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    19,
                    48,
                    17,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T19:48:17Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    19,
                    48,
                    17,
                    1,
                    91,
                    0
                ],
                "title": "Beyond Quacking: Deep Integration of Language Models and RAG into DuckDB",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Quacking: Deep Integration of Language Models and RAG into DuckDB"
                },
                "summary": "Knowledge-intensive analytical applications retrieve context from both\nstructured tabular data and unstructured, text-free documents for effective\ndecision-making. Large language models (LLMs) have made it significantly easier\nto prototype such retrieval and reasoning data pipelines. However, implementing\nthese pipelines efficiently still demands significant effort and has several\nchallenges. This often involves orchestrating heterogeneous data systems,\nmanaging data movement, and handling low-level implementation details, e.g.,\nLLM context management.\n  To address these challenges, we introduce FlockMTL: an extension for DBMSs\nthat deeply integrates LLM capabilities and retrieval-augmented generation\n(RAG). FlockMTL includes model-driven scalar and aggregate functions, enabling\nchained predictions through tuple-level mappings and reductions. Drawing\ninspiration from the relational model, FlockMTL incorporates: (i) cost-based\noptimizations, which seamlessly apply techniques such as batching and caching;\nand (ii) resource independence, enabled through novel SQL DDL abstractions:\nPROMPT and MODEL, introduced as first-class schema objects alongside TABLE.\nFlockMTL streamlines the development of knowledge-intensive analytical\napplications, and its optimizations ease the implementation burden.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-intensive analytical applications retrieve context from both\nstructured tabular data and unstructured, text-free documents for effective\ndecision-making. Large language models (LLMs) have made it significantly easier\nto prototype such retrieval and reasoning data pipelines. However, implementing\nthese pipelines efficiently still demands significant effort and has several\nchallenges. This often involves orchestrating heterogeneous data systems,\nmanaging data movement, and handling low-level implementation details, e.g.,\nLLM context management.\n  To address these challenges, we introduce FlockMTL: an extension for DBMSs\nthat deeply integrates LLM capabilities and retrieval-augmented generation\n(RAG). FlockMTL includes model-driven scalar and aggregate functions, enabling\nchained predictions through tuple-level mappings and reductions. Drawing\ninspiration from the relational model, FlockMTL incorporates: (i) cost-based\noptimizations, which seamlessly apply techniques such as batching and caching;\nand (ii) resource independence, enabled through novel SQL DDL abstractions:\nPROMPT and MODEL, introduced as first-class schema objects alongside TABLE.\nFlockMTL streamlines the development of knowledge-intensive analytical\napplications, and its optimizations ease the implementation burden."
                },
                "authors": [
                    {
                        "name": "Anas Dorbani"
                    },
                    {
                        "name": "Sunny Yasser"
                    },
                    {
                        "name": "Jimmy Lin"
                    },
                    {
                        "name": "Amine Mhedhbi"
                    }
                ],
                "author_detail": {
                    "name": "Amine Mhedhbi"
                },
                "author": "Amine Mhedhbi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01104v1",
                "updated": "2025-04-01T18:21:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    21,
                    43,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T18:21:43Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    21,
                    43,
                    1,
                    91,
                    0
                ],
                "title": "Fundamentals of Caching Layered Data objects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fundamentals of Caching Layered Data objects"
                },
                "summary": "The effective management of large amounts of data processed or required by\ntoday's cloud or edge computing systems remains a fundamental challenge. This\npaper focuses on cache management for applications where data objects can be\nstored in layered representations. In such representations, each additional\ndata layer enhances the \"quality\" of the object's version but comes with an\nincremental cost of memory space. This layered approach proves beneficial in\nvarious scenarios, including the delivery of zoomable maps, video coding,\nfuture Virtual Reality gaming, and layered neural network models where\nadditional data layers improve inference accuracy. In systems where users or\ndevices demand different versions of a data object, layered representations\noffer flexibility for caching policies to achieve improved hit rates.\n  In this paper, we explore the performance of various traditionally studied\ncaching policies, such as Belady, LRU, and LFU, both with and without layering.\nTo this end, we develop an asymptotically accurate analytical model for Layered\nLRU (LLRU). We study how the performance of LLRU is impacted by factors such as\nthe number of layers, the popularity of different objects and layers, and\noverheads associated with storing layered representations. For instance, we\nshow that, for LLRU, more layers are not always beneficial and indeed\nperformance depends in subtle ways on the popularity and size profiles of\nlayers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The effective management of large amounts of data processed or required by\ntoday's cloud or edge computing systems remains a fundamental challenge. This\npaper focuses on cache management for applications where data objects can be\nstored in layered representations. In such representations, each additional\ndata layer enhances the \"quality\" of the object's version but comes with an\nincremental cost of memory space. This layered approach proves beneficial in\nvarious scenarios, including the delivery of zoomable maps, video coding,\nfuture Virtual Reality gaming, and layered neural network models where\nadditional data layers improve inference accuracy. In systems where users or\ndevices demand different versions of a data object, layered representations\noffer flexibility for caching policies to achieve improved hit rates.\n  In this paper, we explore the performance of various traditionally studied\ncaching policies, such as Belady, LRU, and LFU, both with and without layering.\nTo this end, we develop an asymptotically accurate analytical model for Layered\nLRU (LLRU). We study how the performance of LLRU is impacted by factors such as\nthe number of layers, the popularity of different objects and layers, and\noverheads associated with storing layered representations. For instance, we\nshow that, for LLRU, more layers are not always beneficial and indeed\nperformance depends in subtle ways on the popularity and size profiles of\nlayers."
                },
                "authors": [
                    {
                        "name": "Agrim Bari"
                    },
                    {
                        "name": "Gustavo de Veciana"
                    },
                    {
                        "name": "George Kesidis"
                    }
                ],
                "author_detail": {
                    "name": "George Kesidis"
                },
                "author": "George Kesidis",
                "arxiv_comment": "An abridged version of this paper has been accepted at the 45th IEEE\n  International Conference on Distributed Computing Systems (ICDCS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01084v1",
                "updated": "2025-04-01T18:00:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    0,
                    48,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T18:00:48Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    0,
                    48,
                    1,
                    91,
                    0
                ],
                "title": "Surfactants Screen Slide Electrification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surfactants Screen Slide Electrification"
                },
                "summary": "Water drops spontaneously accumulate charges when they move on hydrophobic\ndielectric surfaces by slide electrification. On the one hand, slide\nelectrification generates electricity with possible applications on tiny\ndevices. On the other hand, the potential of up to 1 KV generated by slide\nelectrification alters wetting and drop motion. Therefore, it is important to\nknow the factors that affect slide electrification. To find out how surfactants\naffect slide electrification, we measured drop charges of aqueous drops\ncontaining cationic CTAB, anionic SDS and neutral C8E3 sliding on different\nhydrophobic surfaces. The result is: addition of surfactant significantly\nreduces the spontaneous charging of moving water drops. Based on zeta potential\nmeasurements, confocal microscopy of deposited surface-active dyes and drop\nimpact studies, we propose that several factors contribute to this suppression\nof charge separation: (1) Surfactants tend to lower the contact angles, which\nreduces charge separation. (2) Surfactant adsorption at the solid-liquid\ninterface can reduce the density of primary ions, particularly for anionic\nsurfactants. (3) Anionic and neutral surfactants are mostly transferred to the\nliquid-air interface at the rear of the sliding drop, retaining primary ions\nwithin the drop. (4) Deposited cationic surfactant directly reduces the charge\nof the drop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Water drops spontaneously accumulate charges when they move on hydrophobic\ndielectric surfaces by slide electrification. On the one hand, slide\nelectrification generates electricity with possible applications on tiny\ndevices. On the other hand, the potential of up to 1 KV generated by slide\nelectrification alters wetting and drop motion. Therefore, it is important to\nknow the factors that affect slide electrification. To find out how surfactants\naffect slide electrification, we measured drop charges of aqueous drops\ncontaining cationic CTAB, anionic SDS and neutral C8E3 sliding on different\nhydrophobic surfaces. The result is: addition of surfactant significantly\nreduces the spontaneous charging of moving water drops. Based on zeta potential\nmeasurements, confocal microscopy of deposited surface-active dyes and drop\nimpact studies, we propose that several factors contribute to this suppression\nof charge separation: (1) Surfactants tend to lower the contact angles, which\nreduces charge separation. (2) Surfactant adsorption at the solid-liquid\ninterface can reduce the density of primary ions, particularly for anionic\nsurfactants. (3) Anionic and neutral surfactants are mostly transferred to the\nliquid-air interface at the rear of the sliding drop, retaining primary ions\nwithin the drop. (4) Deposited cationic surfactant directly reduces the charge\nof the drop."
                },
                "authors": [
                    {
                        "name": "Xiaomei Li"
                    },
                    {
                        "name": "Zhongyuan Ni"
                    },
                    {
                        "name": "Xiaoteng Zhou"
                    },
                    {
                        "name": "Lisa S. Bauer"
                    },
                    {
                        "name": "Diego Diaz"
                    },
                    {
                        "name": "Gabriele Schäfer"
                    },
                    {
                        "name": "Hans-Jürgen Butt"
                    }
                ],
                "author_detail": {
                    "name": "Hans-Jürgen Butt"
                },
                "author": "Hans-Jürgen Butt",
                "arxiv_comment": "13 pages, 4 figures, 50 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.soft",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00999v1",
                "updated": "2025-04-01T17:39:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    39,
                    19,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T17:39:19Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    39,
                    19,
                    1,
                    91,
                    0
                ],
                "title": "MergeVQ: A Unified Framework for Visual Generation and Representation\n  with Disentangled Token Merging and Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MergeVQ: A Unified Framework for Visual Generation and Representation\n  with Disentangled Token Merging and Quantization"
                },
                "summary": "Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great\nsuccess in both self-supervised pre-training and image generation. However,\nmost existing methods struggle to address the trade-off in shared latent space\nfor generation quality vs. representation learning and efficiency. To push the\nlimits of this paradigm, we propose MergeVQ, which incorporates token merging\ntechniques into VQ-based generative models to bridge the gap between image\ngeneration and visual representation learning in a unified architecture. During\npre-training, MergeVQ decouples top-k semantics from latent space with the\ntoken merge module after self-attention blocks in the encoder for subsequent\nLook-up Free Quantization (LFQ) and global alignment and recovers their\nfine-grained details through cross-attention in the decoder for reconstruction.\nAs for the second-stage generation, we introduce MergeAR, which performs KV\nCache compression for efficient raster-order prediction. Extensive experiments\non ImageNet verify that MergeVQ as an AR generative model achieves competitive\nperformance in both visual representation learning and image generation tasks\nwhile maintaining favorable token efficiency and inference speed. The code and\nmodel will be available at https://apexgen-x.github.io/MergeVQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great\nsuccess in both self-supervised pre-training and image generation. However,\nmost existing methods struggle to address the trade-off in shared latent space\nfor generation quality vs. representation learning and efficiency. To push the\nlimits of this paradigm, we propose MergeVQ, which incorporates token merging\ntechniques into VQ-based generative models to bridge the gap between image\ngeneration and visual representation learning in a unified architecture. During\npre-training, MergeVQ decouples top-k semantics from latent space with the\ntoken merge module after self-attention blocks in the encoder for subsequent\nLook-up Free Quantization (LFQ) and global alignment and recovers their\nfine-grained details through cross-attention in the decoder for reconstruction.\nAs for the second-stage generation, we introduce MergeAR, which performs KV\nCache compression for efficient raster-order prediction. Extensive experiments\non ImageNet verify that MergeVQ as an AR generative model achieves competitive\nperformance in both visual representation learning and image generation tasks\nwhile maintaining favorable token efficiency and inference speed. The code and\nmodel will be available at https://apexgen-x.github.io/MergeVQ."
                },
                "authors": [
                    {
                        "name": "Siyuan Li"
                    },
                    {
                        "name": "Luyuan Zhang"
                    },
                    {
                        "name": "Zedong Wang"
                    },
                    {
                        "name": "Juanxi Tian"
                    },
                    {
                        "name": "Cheng Tan"
                    },
                    {
                        "name": "Zicheng Liu"
                    },
                    {
                        "name": "Chang Yu"
                    },
                    {
                        "name": "Qingsong Xie"
                    },
                    {
                        "name": "Haonan Lu"
                    },
                    {
                        "name": "Haoqian Wang"
                    },
                    {
                        "name": "Zhen Lei"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Lei"
                },
                "author": "Zhen Lei",
                "arxiv_comment": "CVPR2025 (in process for more analysis and extension)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00970v1",
                "updated": "2025-04-01T17:08:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    8,
                    57,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T17:08:57Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    8,
                    57,
                    1,
                    91,
                    0
                ],
                "title": "SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV\n  Caching"
                },
                "summary": "Large language models face significant computational and memory challenges\nwhen processing long contexts. During inference, efficient management of the\nkey-value (KV) cache, which stores intermediate activations for autoregressive\ngeneration, is critical to reducing memory overhead and improving computational\nefficiency. Traditional token-level efficient KV caching methods overlook\nsemantic information, treating tokens independently without considering their\nsemantic relationships. Meanwhile, existing semantic-preserving KV cache\nmanagement approaches often suffer from substantial memory usage and high\ntime-to-first-token. To address these limitations, we propose SentenceKV, a\nnovel sentence-level semantic KV caching approach designed to enhance inference\nefficiency while preserving semantic coherence. During prefilling, SentenceKV\ngroups tokens based on sentence-level semantic similarity, compressing sentence\nrepresentations into concise semantic vectors stored directly on the GPU, while\nindividual KV pairs are offloaded to CPU. During decoding, SentenceKV generates\ntokens by selectively retrieving semantically relevant sentence-level KV\nentries, leveraging the semantic similarity between the prefilling-stage\nsemantic vectors and decoding-stage queries. This ensures efficient and\ncontextually accurate predictions, minimizing the loading of redundant or\nirrelevant data into GPU memory and significantly reducing memory overhead\nwhile maintaining stable inference latency, even for extremely long contexts.\nExtensive evaluations on benchmarks including PG-19, LongBench, and\nNeedle-In-A-Haystack demonstrate that SentenceKV significantly outperforms\nstate-of-the-art methods in both efficiency and memory usage, without\ncompromising model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models face significant computational and memory challenges\nwhen processing long contexts. During inference, efficient management of the\nkey-value (KV) cache, which stores intermediate activations for autoregressive\ngeneration, is critical to reducing memory overhead and improving computational\nefficiency. Traditional token-level efficient KV caching methods overlook\nsemantic information, treating tokens independently without considering their\nsemantic relationships. Meanwhile, existing semantic-preserving KV cache\nmanagement approaches often suffer from substantial memory usage and high\ntime-to-first-token. To address these limitations, we propose SentenceKV, a\nnovel sentence-level semantic KV caching approach designed to enhance inference\nefficiency while preserving semantic coherence. During prefilling, SentenceKV\ngroups tokens based on sentence-level semantic similarity, compressing sentence\nrepresentations into concise semantic vectors stored directly on the GPU, while\nindividual KV pairs are offloaded to CPU. During decoding, SentenceKV generates\ntokens by selectively retrieving semantically relevant sentence-level KV\nentries, leveraging the semantic similarity between the prefilling-stage\nsemantic vectors and decoding-stage queries. This ensures efficient and\ncontextually accurate predictions, minimizing the loading of redundant or\nirrelevant data into GPU memory and significantly reducing memory overhead\nwhile maintaining stable inference latency, even for extremely long contexts.\nExtensive evaluations on benchmarks including PG-19, LongBench, and\nNeedle-In-A-Haystack demonstrate that SentenceKV significantly outperforms\nstate-of-the-art methods in both efficiency and memory usage, without\ncompromising model accuracy."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhu"
                    },
                    {
                        "name": "Ali Falahati"
                    },
                    {
                        "name": "David H. Yang"
                    },
                    {
                        "name": "Mohammad Mohammadi Amiri"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Mohammadi Amiri"
                },
                "author": "Mohammad Mohammadi Amiri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13275v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13275v2",
                "updated": "2025-04-01T14:21:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    14,
                    21,
                    15,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-17T15:27:02Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    27,
                    2,
                    0,
                    76,
                    0
                ],
                "title": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems"
                },
                "summary": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability."
                },
                "authors": [
                    {
                        "name": "Seyoung Song"
                    }
                ],
                "author_detail": {
                    "name": "Seyoung Song"
                },
                "author": "Seyoung Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13275v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13275v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7; I.2.11; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00726v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00726v1",
                "updated": "2025-04-01T12:34:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    34,
                    58,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T12:34:58Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    34,
                    58,
                    1,
                    91,
                    0
                ],
                "title": "EMO: Edge Model Overlays to Scale Model Size in Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMO: Edge Model Overlays to Scale Model Size in Federated Learning"
                },
                "summary": "Federated Learning (FL) trains machine learning models on edge devices with\ndistributed data. However, the computational and memory limitations of these\ndevices restrict the training of large models using FL. Split Federated\nLearning (SFL) addresses this challenge by distributing the model across the\ndevice and server, but it introduces a tightly coupled data flow, leading to\ncomputational bottlenecks and high communication costs. We propose EMO as a\nsolution to enable the training of large models in FL while mitigating the\nchallenges of SFL. EMO introduces Edge Model Overlay(s) between the device and\nserver, enabling the creation of a larger ensemble model without modifying the\nFL workflow. The key innovation in EMO is Augmented Federated Learning (AFL),\nwhich builds an ensemble model by connecting the original (smaller) FL model\nwith model(s) trained in the overlay(s) to facilitate horizontal or vertical\nscaling. This is accomplished through three key modules: a hierarchical\nactivation replay cache to decouple AFL from FL, a convergence-aware\ncommunication controller to optimize communication overhead, and an ensemble\ninference module. Evaluations on a real-world prototype show that EMO improves\naccuracy by up to 17.77% compared to FL, and reduces communication costs by up\nto 7.17x and decreases training time by up to 6.9x compared to SFL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) trains machine learning models on edge devices with\ndistributed data. However, the computational and memory limitations of these\ndevices restrict the training of large models using FL. Split Federated\nLearning (SFL) addresses this challenge by distributing the model across the\ndevice and server, but it introduces a tightly coupled data flow, leading to\ncomputational bottlenecks and high communication costs. We propose EMO as a\nsolution to enable the training of large models in FL while mitigating the\nchallenges of SFL. EMO introduces Edge Model Overlay(s) between the device and\nserver, enabling the creation of a larger ensemble model without modifying the\nFL workflow. The key innovation in EMO is Augmented Federated Learning (AFL),\nwhich builds an ensemble model by connecting the original (smaller) FL model\nwith model(s) trained in the overlay(s) to facilitate horizontal or vertical\nscaling. This is accomplished through three key modules: a hierarchical\nactivation replay cache to decouple AFL from FL, a convergence-aware\ncommunication controller to optimize communication overhead, and an ensemble\ninference module. Evaluations on a real-world prototype show that EMO improves\naccuracy by up to 17.77% compared to FL, and reduces communication costs by up\nto 7.17x and decreases training time by up to 6.9x compared to SFL."
                },
                "authors": [
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Weibo He"
                    },
                    {
                        "name": "Wanglei Feng"
                    },
                    {
                        "name": "Zhenyu Wen"
                    },
                    {
                        "name": "Bin Qian"
                    },
                    {
                        "name": "Blesson Varghese"
                    }
                ],
                "author_detail": {
                    "name": "Blesson Varghese"
                },
                "author": "Blesson Varghese",
                "arxiv_comment": "Poster accepted at IEEE ICDCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00726v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00557v1",
                "updated": "2025-04-01T09:10:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    9,
                    10,
                    32,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T09:10:32Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    9,
                    10,
                    32,
                    1,
                    91,
                    0
                ],
                "title": "Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features"
                },
                "summary": "Visual token reduction lowers inference costs caused by extensive image\nfeatures in large vision-language models (LVLMs). Unlike relevant studies that\nprune tokens in self-attention-only LVLMs, our work uniquely addresses\ncross-attention-based models, which achieve superior performance. We identify\nthat the key-value (KV) cache size for image tokens in cross-attention layers\nsignificantly exceeds that of text tokens in self-attention layers, posing a\nmajor compute bottleneck. To mitigate this issue, we exploit the sparse nature\nin cross-attention maps to selectively prune redundant visual features. Our\nTrimmed Llama effectively reduces KV cache demands without requiring additional\ntraining. By benefiting from 50%-reduced visual features, our model can reduce\ninference latency and memory usage while achieving benchmark parity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual token reduction lowers inference costs caused by extensive image\nfeatures in large vision-language models (LVLMs). Unlike relevant studies that\nprune tokens in self-attention-only LVLMs, our work uniquely addresses\ncross-attention-based models, which achieve superior performance. We identify\nthat the key-value (KV) cache size for image tokens in cross-attention layers\nsignificantly exceeds that of text tokens in self-attention layers, posing a\nmajor compute bottleneck. To mitigate this issue, we exploit the sparse nature\nin cross-attention maps to selectively prune redundant visual features. Our\nTrimmed Llama effectively reduces KV cache demands without requiring additional\ntraining. By benefiting from 50%-reduced visual features, our model can reduce\ninference latency and memory usage while achieving benchmark parity."
                },
                "authors": [
                    {
                        "name": "Jewon Lee"
                    },
                    {
                        "name": "Ki-Ung Song"
                    },
                    {
                        "name": "Seungmin Yang"
                    },
                    {
                        "name": "Donguk Lim"
                    },
                    {
                        "name": "Jaeyeon Kim"
                    },
                    {
                        "name": "Wooksu Shin"
                    },
                    {
                        "name": "Bo-Kyeong Kim"
                    },
                    {
                        "name": "Yong Jae Lee"
                    },
                    {
                        "name": "Tae-Ho Kim"
                    }
                ],
                "author_detail": {
                    "name": "Tae-Ho Kim"
                },
                "author": "Tae-Ho Kim",
                "arxiv_comment": "accepted at CVPR 2025 Workshop on ELVM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.13178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13178v1",
                "updated": "2025-04-17T17:59:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    59,
                    54,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T17:59:54Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    59,
                    54,
                    3,
                    107,
                    0
                ],
                "title": "Aligning Constraint Generation with Design Intent in Parametric CAD",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Constraint Generation with Design Intent in Parametric CAD"
                },
                "summary": "We adapt alignment techniques from reasoning LLMs to the task of generating\nengineering sketch constraints found in computer-aided design (CAD) models.\nEngineering sketches consist of geometric primitives (e.g. points, lines)\nconnected by constraints (e.g. perpendicular, tangent) that define the\nrelationships between them. For a design to be easily editable, the constraints\nmust effectively capture design intent, ensuring the geometry updates\npredictably when parameters change. Although current approaches can generate\nCAD designs, an open challenge remains to align model outputs with design\nintent, we label this problem `design alignment'. A critical first step towards\naligning generative CAD models is to generate constraints which fully-constrain\nall geometric primitives, without over-constraining or distorting sketch\ngeometry. Using alignment techniques to train an existing constraint generation\nmodel with feedback from a constraint solver, we are able to fully-constrain\n93% of sketches compared to 34% when using a na\\\"ive supervised fine-tuning\n(SFT) baseline and only 8.9% without alignment. Our approach can be applied to\nany existing constraint generation model and sets the stage for further\nresearch bridging alignment strategies between the language and design domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We adapt alignment techniques from reasoning LLMs to the task of generating\nengineering sketch constraints found in computer-aided design (CAD) models.\nEngineering sketches consist of geometric primitives (e.g. points, lines)\nconnected by constraints (e.g. perpendicular, tangent) that define the\nrelationships between them. For a design to be easily editable, the constraints\nmust effectively capture design intent, ensuring the geometry updates\npredictably when parameters change. Although current approaches can generate\nCAD designs, an open challenge remains to align model outputs with design\nintent, we label this problem `design alignment'. A critical first step towards\naligning generative CAD models is to generate constraints which fully-constrain\nall geometric primitives, without over-constraining or distorting sketch\ngeometry. Using alignment techniques to train an existing constraint generation\nmodel with feedback from a constraint solver, we are able to fully-constrain\n93% of sketches compared to 34% when using a na\\\"ive supervised fine-tuning\n(SFT) baseline and only 8.9% without alignment. Our approach can be applied to\nany existing constraint generation model and sets the stage for further\nresearch bridging alignment strategies between the language and design domains."
                },
                "authors": [
                    {
                        "name": "Evan Casey"
                    },
                    {
                        "name": "Tianyu Zhang"
                    },
                    {
                        "name": "Shu Ishida"
                    },
                    {
                        "name": "John Roger Thompson"
                    },
                    {
                        "name": "Amir Khasahmadi"
                    },
                    {
                        "name": "Joseph George Lambourne"
                    },
                    {
                        "name": "Pradeep Kumar Jayaraman"
                    },
                    {
                        "name": "Karl D. D. Willis"
                    }
                ],
                "author_detail": {
                    "name": "Karl D. D. Willis"
                },
                "author": "Karl D. D. Willis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13176v1",
                "updated": "2025-04-17T17:59:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    59,
                    47,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T17:59:47Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    59,
                    47,
                    3,
                    107,
                    0
                ],
                "title": "IMAGGarment-1: Fine-Grained Garment Generation for Controllable Fashion\n  Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IMAGGarment-1: Fine-Grained Garment Generation for Controllable Fashion\n  Design"
                },
                "summary": "This paper presents IMAGGarment-1, a fine-grained garment generation (FGG)\nframework that enables high-fidelity garment synthesis with precise control\nover silhouette, color, and logo placement. Unlike existing methods that are\nlimited to single-condition inputs, IMAGGarment-1 addresses the challenges of\nmulti-conditional controllability in personalized fashion design and digital\napparel applications. Specifically, IMAGGarment-1 employs a two-stage training\nstrategy to separately model global appearance and local details, while\nenabling unified and controllable generation through end-to-end inference. In\nthe first stage, we propose a global appearance model that jointly encodes\nsilhouette and color using a mixed attention module and a color adapter. In the\nsecond stage, we present a local enhancement model with an adaptive\nappearance-aware module to inject user-defined logos and spatial constraints,\nenabling accurate placement and visual consistency. To support this task, we\nrelease GarmentBench, a large-scale dataset comprising over 180K garment\nsamples paired with multi-level design conditions, including sketches, color\nreferences, logo placements, and textual prompts. Extensive experiments\ndemonstrate that our method outperforms existing baselines, achieving superior\nstructural stability, color fidelity, and local controllability performance.\nThe code and model are available at https://github.com/muzishen/IMAGGarment-1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents IMAGGarment-1, a fine-grained garment generation (FGG)\nframework that enables high-fidelity garment synthesis with precise control\nover silhouette, color, and logo placement. Unlike existing methods that are\nlimited to single-condition inputs, IMAGGarment-1 addresses the challenges of\nmulti-conditional controllability in personalized fashion design and digital\napparel applications. Specifically, IMAGGarment-1 employs a two-stage training\nstrategy to separately model global appearance and local details, while\nenabling unified and controllable generation through end-to-end inference. In\nthe first stage, we propose a global appearance model that jointly encodes\nsilhouette and color using a mixed attention module and a color adapter. In the\nsecond stage, we present a local enhancement model with an adaptive\nappearance-aware module to inject user-defined logos and spatial constraints,\nenabling accurate placement and visual consistency. To support this task, we\nrelease GarmentBench, a large-scale dataset comprising over 180K garment\nsamples paired with multi-level design conditions, including sketches, color\nreferences, logo placements, and textual prompts. Extensive experiments\ndemonstrate that our method outperforms existing baselines, achieving superior\nstructural stability, color fidelity, and local controllability performance.\nThe code and model are available at https://github.com/muzishen/IMAGGarment-1."
                },
                "authors": [
                    {
                        "name": "Fei Shen"
                    },
                    {
                        "name": "Jian Yu"
                    },
                    {
                        "name": "Cong Wang"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Xiaoyu Du"
                    },
                    {
                        "name": "Jinhui Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jinhui Tang"
                },
                "author": "Jinhui Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13171v1",
                "updated": "2025-04-17T17:59:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    59,
                    25,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T17:59:25Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    59,
                    25,
                    3,
                    107,
                    0
                ],
                "title": "Sleep-time Compute: Beyond Inference Scaling at Test-time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sleep-time Compute: Beyond Inference Scaling at Test-time"
                },
                "summary": "Scaling test-time compute has emerged as a key ingredient for enabling large\nlanguage models (LLMs) to solve difficult problems, but comes with high latency\nand inference cost. We introduce sleep-time compute, which allows models to\n\"think\" offline about contexts before queries are presented: by anticipating\nwhat queries users might ask and pre-computing useful quantities, we can\nsignificantly reduce the compute requirements at test-time. To demonstrate the\nefficacy of our method, we create modified versions of two reasoning tasks -\nStateful GSM-Symbolic and Stateful AIME. We find that sleep-time compute can\nreduce the amount of test-time compute needed to achieve the same accuracy by ~\n5x on Stateful GSM-Symbolic and Stateful AIME and that by scaling sleep-time\ncompute we can further increase accuracy by up to 13% on Stateful GSM-Symbolic\nand 18% on Stateful AIME. Furthermore, we introduce Multi-Query GSM-Symbolic,\nwhich extends GSM-Symbolic by including multiple related queries per context.\nBy amortizing sleep-time compute across related queries about the same context\nusing Multi-Query GSM-Symbolic, we can decrease the average cost per query by\n2.5x. We then conduct additional analysis to understand when sleep-time compute\nis most effective, finding the predictability of the user query to be well\ncorrelated with the efficacy of sleep-time compute. Finally, we conduct a\ncase-study of applying sleep-time compute to a realistic agentic SWE task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling test-time compute has emerged as a key ingredient for enabling large\nlanguage models (LLMs) to solve difficult problems, but comes with high latency\nand inference cost. We introduce sleep-time compute, which allows models to\n\"think\" offline about contexts before queries are presented: by anticipating\nwhat queries users might ask and pre-computing useful quantities, we can\nsignificantly reduce the compute requirements at test-time. To demonstrate the\nefficacy of our method, we create modified versions of two reasoning tasks -\nStateful GSM-Symbolic and Stateful AIME. We find that sleep-time compute can\nreduce the amount of test-time compute needed to achieve the same accuracy by ~\n5x on Stateful GSM-Symbolic and Stateful AIME and that by scaling sleep-time\ncompute we can further increase accuracy by up to 13% on Stateful GSM-Symbolic\nand 18% on Stateful AIME. Furthermore, we introduce Multi-Query GSM-Symbolic,\nwhich extends GSM-Symbolic by including multiple related queries per context.\nBy amortizing sleep-time compute across related queries about the same context\nusing Multi-Query GSM-Symbolic, we can decrease the average cost per query by\n2.5x. We then conduct additional analysis to understand when sleep-time compute\nis most effective, finding the predictability of the user query to be well\ncorrelated with the efficacy of sleep-time compute. Finally, we conduct a\ncase-study of applying sleep-time compute to a realistic agentic SWE task."
                },
                "authors": [
                    {
                        "name": "Kevin Lin"
                    },
                    {
                        "name": "Charlie Snell"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Charles Packer"
                    },
                    {
                        "name": "Sarah Wooders"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E. Gonzalez"
                },
                "author": "Joseph E. Gonzalez",
                "arxiv_comment": "Code and data released at:\n  https://github.com/letta-ai/sleep-time-compute",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13169v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13169v1",
                "updated": "2025-04-17T17:59:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    59,
                    22,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T17:59:22Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    59,
                    22,
                    3,
                    107,
                    0
                ],
                "title": "Generate, but Verify: Reducing Hallucination in Vision-Language Models\n  with Retrospective Resampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generate, but Verify: Reducing Hallucination in Vision-Language Models\n  with Retrospective Resampling"
                },
                "summary": "Vision-Language Models (VLMs) excel at visual understanding but often suffer\nfrom visual hallucinations, where they generate descriptions of nonexistent\nobjects, actions, or concepts, posing significant risks in safety-critical\napplications. Existing hallucination mitigation methods typically follow one of\ntwo paradigms: generation adjustment, which modifies decoding behavior to align\ntext with visual inputs, and post-hoc verification, where external models\nassess and correct outputs. While effective, generation adjustment methods\noften rely on heuristics and lack correction mechanisms, while post-hoc\nverification is complicated, typically requiring multiple models and tending to\nreject outputs rather than refine them. In this work, we introduce REVERSE, a\nunified framework that integrates hallucination-aware training with on-the-fly\nself-verification. By leveraging a new hallucination-verification dataset\ncontaining over 1.3M semi-synthetic samples, along with a novel inference-time\nretrospective resampling technique, our approach enables VLMs to both detect\nhallucinations during generation and dynamically revise those hallucinations.\nOur evaluations show that REVERSE achieves state-of-the-art hallucination\nreduction, outperforming the best existing methods by up to 12% on CHAIR-MSCOCO\nand 28% on HaloQuest. Our dataset, model, and code are available at:\nhttps://reverse-vlm.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) excel at visual understanding but often suffer\nfrom visual hallucinations, where they generate descriptions of nonexistent\nobjects, actions, or concepts, posing significant risks in safety-critical\napplications. Existing hallucination mitigation methods typically follow one of\ntwo paradigms: generation adjustment, which modifies decoding behavior to align\ntext with visual inputs, and post-hoc verification, where external models\nassess and correct outputs. While effective, generation adjustment methods\noften rely on heuristics and lack correction mechanisms, while post-hoc\nverification is complicated, typically requiring multiple models and tending to\nreject outputs rather than refine them. In this work, we introduce REVERSE, a\nunified framework that integrates hallucination-aware training with on-the-fly\nself-verification. By leveraging a new hallucination-verification dataset\ncontaining over 1.3M semi-synthetic samples, along with a novel inference-time\nretrospective resampling technique, our approach enables VLMs to both detect\nhallucinations during generation and dynamically revise those hallucinations.\nOur evaluations show that REVERSE achieves state-of-the-art hallucination\nreduction, outperforming the best existing methods by up to 12% on CHAIR-MSCOCO\nand 28% on HaloQuest. Our dataset, model, and code are available at:\nhttps://reverse-vlm.github.io."
                },
                "authors": [
                    {
                        "name": "Tsung-Han Wu"
                    },
                    {
                        "name": "Heekyung Lee"
                    },
                    {
                        "name": "Jiaxin Ge"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Trevor Darrell"
                    },
                    {
                        "name": "David M. Chan"
                    }
                ],
                "author_detail": {
                    "name": "David M. Chan"
                },
                "author": "David M. Chan",
                "arxiv_comment": "Preprint. Project Page: https://reverse-vlm.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13169v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13169v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13145v1",
                "updated": "2025-04-17T17:53:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    53,
                    54,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T17:53:54Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    53,
                    54,
                    3,
                    107,
                    0
                ],
                "title": "Exploring Expert Failures Improves LLM Agent Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Expert Failures Improves LLM Agent Tuning"
                },
                "summary": "Large Language Models (LLMs) have shown tremendous potential as agents,\nexcelling at tasks that require multiple rounds of reasoning and interactions.\nRejection Sampling Fine-Tuning (RFT) has emerged as an effective method for\nfinetuning LLMs as agents: it first imitates expert-generated successful\ntrajectories and further improves agentic skills through iterative fine-tuning\non successful, self-generated trajectories. However, since the expert (e.g.,\nGPT-4) succeeds primarily on simpler subtasks and RFT inherently favors simpler\nscenarios, many complex subtasks remain unsolved and persistently\nout-of-distribution (OOD). Upon investigating these challenging subtasks, we\ndiscovered that previously failed expert trajectories can often provide\nvaluable guidance, e.g., plans and key actions, that can significantly improve\nagent exploration efficiency and acquisition of critical skills. Motivated by\nthese observations, we propose Exploring Expert Failures (EEF), which\nidentifies beneficial actions from failed expert trajectories and integrates\nthem into the training dataset. Potentially harmful actions are meticulously\nexcluded to prevent contamination of the model learning process. By leveraging\nthe beneficial actions in expert failures, EEF successfully solves some\npreviously unsolvable subtasks and improves agent tuning performance.\nRemarkably, our approach achieved a 62\\% win rate in WebShop, outperforming RFT\n(53. 6\\%) and GPT-4 (35. 6\\%), and to the best of our knowledge, setting a new\nstate-of-the-art as the first method to surpass a score of 0.81 in WebShop and\nexceed 81 in SciWorld.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown tremendous potential as agents,\nexcelling at tasks that require multiple rounds of reasoning and interactions.\nRejection Sampling Fine-Tuning (RFT) has emerged as an effective method for\nfinetuning LLMs as agents: it first imitates expert-generated successful\ntrajectories and further improves agentic skills through iterative fine-tuning\non successful, self-generated trajectories. However, since the expert (e.g.,\nGPT-4) succeeds primarily on simpler subtasks and RFT inherently favors simpler\nscenarios, many complex subtasks remain unsolved and persistently\nout-of-distribution (OOD). Upon investigating these challenging subtasks, we\ndiscovered that previously failed expert trajectories can often provide\nvaluable guidance, e.g., plans and key actions, that can significantly improve\nagent exploration efficiency and acquisition of critical skills. Motivated by\nthese observations, we propose Exploring Expert Failures (EEF), which\nidentifies beneficial actions from failed expert trajectories and integrates\nthem into the training dataset. Potentially harmful actions are meticulously\nexcluded to prevent contamination of the model learning process. By leveraging\nthe beneficial actions in expert failures, EEF successfully solves some\npreviously unsolvable subtasks and improves agent tuning performance.\nRemarkably, our approach achieved a 62\\% win rate in WebShop, outperforming RFT\n(53. 6\\%) and GPT-4 (35. 6\\%), and to the best of our knowledge, setting a new\nstate-of-the-art as the first method to surpass a score of 0.81 in WebShop and\nexceed 81 in SciWorld."
                },
                "authors": [
                    {
                        "name": "Li-Cheng Lan"
                    },
                    {
                        "name": "Andrew Bai"
                    },
                    {
                        "name": "Minhao Cheng"
                    },
                    {
                        "name": "Ruochen Wang"
                    },
                    {
                        "name": "Cho-Jui Hsieh"
                    },
                    {
                        "name": "Tianyi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Zhou"
                },
                "author": "Tianyi Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13144v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13144v1",
                "updated": "2025-04-17T17:53:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    53,
                    39,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T17:53:39Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    53,
                    39,
                    3,
                    107,
                    0
                ],
                "title": "Bayesian model-data comparison incorporating theoretical uncertainties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian model-data comparison incorporating theoretical uncertainties"
                },
                "summary": "Accurate comparisons between theoretical models and experimental data are\ncritical for scientific progress. However, inferred model parameters can vary\nsignificantly with the chosen physics model, highlighting the importance of\nproperly accounting for theoretical uncertainties. In this article, we\nexplicitly incorporate these uncertainties using Gaussian processes that model\nthe domain of validity of theoretical models, integrating prior knowledge about\nwhere a theory applies and where it does not. We demonstrate the effectiveness\nof this approach using two systems: a simple ball drop experiment and\nmulti-stage heavy-ion simulations. In both cases incorporating model\ndiscrepancy leads to improved parameter estimates, with systematic improvements\nobserved as additional experimental observables are integrated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate comparisons between theoretical models and experimental data are\ncritical for scientific progress. However, inferred model parameters can vary\nsignificantly with the chosen physics model, highlighting the importance of\nproperly accounting for theoretical uncertainties. In this article, we\nexplicitly incorporate these uncertainties using Gaussian processes that model\nthe domain of validity of theoretical models, integrating prior knowledge about\nwhere a theory applies and where it does not. We demonstrate the effectiveness\nof this approach using two systems: a simple ball drop experiment and\nmulti-stage heavy-ion simulations. In both cases incorporating model\ndiscrepancy leads to improved parameter estimates, with systematic improvements\nobserved as additional experimental observables are integrated."
                },
                "authors": [
                    {
                        "name": "Sunil Jaiswal"
                    },
                    {
                        "name": "Chun Shen"
                    },
                    {
                        "name": "Richard J. Furnstahl"
                    },
                    {
                        "name": "Ulrich Heinz"
                    },
                    {
                        "name": "Matthew T. Pratola"
                    }
                ],
                "author_detail": {
                    "name": "Matthew T. Pratola"
                },
                "author": "Matthew T. Pratola",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13144v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13144v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13139v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13139v1",
                "updated": "2025-04-17T17:49:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    49,
                    40,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T17:49:40Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    49,
                    40,
                    3,
                    107,
                    0
                ],
                "title": "Syntactic and Semantic Control of Large Language Models via Sequential\n  Monte Carlo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Syntactic and Semantic Control of Large Language Models via Sequential\n  Monte Carlo"
                },
                "summary": "A wide range of LM applications require generating text that conforms to\nsyntactic or semantic constraints. Imposing such constraints can be naturally\nframed as probabilistic conditioning, but exact generation from the resulting\ndistribution -- which can differ substantially from the LM's base distribution\n-- is generally intractable. In this work, we develop an architecture for\ncontrolled LM generation based on sequential Monte Carlo (SMC). Our SMC\nframework allows us to flexibly incorporate domain- and problem-specific\nconstraints at inference time, and efficiently reallocate computational\nresources in light of new information during the course of generation. By\ncomparing to a number of alternatives and ablations on four challenging domains\n-- Python code generation for data science, text-to-SQL, goal inference, and\nmolecule synthesis -- we demonstrate that, with little overhead, our approach\nallows small open-source language models to outperform models over 8x larger,\nas well as closed-source, fine-tuned ones. In support of the probabilistic\nperspective, we show that these performance improvements are driven by better\napproximation to the posterior distribution. Our system builds on the framework\nof Lew et al. (2023) and integrates with its language model probabilistic\nprogramming language, giving users a simple, programmable way to apply SMC to a\nbroad variety of controlled generation problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A wide range of LM applications require generating text that conforms to\nsyntactic or semantic constraints. Imposing such constraints can be naturally\nframed as probabilistic conditioning, but exact generation from the resulting\ndistribution -- which can differ substantially from the LM's base distribution\n-- is generally intractable. In this work, we develop an architecture for\ncontrolled LM generation based on sequential Monte Carlo (SMC). Our SMC\nframework allows us to flexibly incorporate domain- and problem-specific\nconstraints at inference time, and efficiently reallocate computational\nresources in light of new information during the course of generation. By\ncomparing to a number of alternatives and ablations on four challenging domains\n-- Python code generation for data science, text-to-SQL, goal inference, and\nmolecule synthesis -- we demonstrate that, with little overhead, our approach\nallows small open-source language models to outperform models over 8x larger,\nas well as closed-source, fine-tuned ones. In support of the probabilistic\nperspective, we show that these performance improvements are driven by better\napproximation to the posterior distribution. Our system builds on the framework\nof Lew et al. (2023) and integrates with its language model probabilistic\nprogramming language, giving users a simple, programmable way to apply SMC to a\nbroad variety of controlled generation problems."
                },
                "authors": [
                    {
                        "name": "João Loula"
                    },
                    {
                        "name": "Benjamin LeBrun"
                    },
                    {
                        "name": "Li Du"
                    },
                    {
                        "name": "Ben Lipkin"
                    },
                    {
                        "name": "Clemente Pasti"
                    },
                    {
                        "name": "Gabriel Grand"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Yahya Emara"
                    },
                    {
                        "name": "Marjorie Freedman"
                    },
                    {
                        "name": "Jason Eisner"
                    },
                    {
                        "name": "Ryan Cotterel"
                    },
                    {
                        "name": "Vikash Mansinghka"
                    },
                    {
                        "name": "Alexander K. Lew"
                    },
                    {
                        "name": "Tim Vieira"
                    },
                    {
                        "name": "Timothy J. O'Donnell"
                    }
                ],
                "author_detail": {
                    "name": "Timothy J. O'Donnell"
                },
                "author": "Timothy J. O'Donnell",
                "arxiv_comment": "34 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13139v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13139v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13134v1",
                "updated": "2025-04-17T17:47:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    47,
                    15,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T17:47:15Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    47,
                    15,
                    3,
                    107,
                    0
                ],
                "title": "Energy-Based Reward Models for Robust Language Model Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy-Based Reward Models for Robust Language Model Alignment"
                },
                "summary": "Reward models (RMs) are essential for aligning Large Language Models (LLMs)\nwith human preferences. However, they often struggle with capturing complex\nhuman preferences and generalizing to unseen data. To address these challenges,\nwe introduce Energy-Based Reward Model (EBRM), a lightweight post-hoc\nrefinement framework that enhances RM robustness and generalization. EBRM\nmodels the reward distribution explicitly, capturing uncertainty in human\npreferences and mitigating the impact of noisy or misaligned annotations. It\nachieves this through conflict-aware data filtering, label-noise-aware\ncontrastive training, and hybrid initialization. Notably, EBRM enhances RMs\nwithout retraining, making it computationally efficient and adaptable across\ndifferent models and tasks. Empirical evaluations on RM benchmarks demonstrate\nsignificant improvements in both robustness and generalization, achieving up to\na 5.97% improvement in safety-critical alignment tasks compared to standard\nRMs. Furthermore, reinforcement learning experiments confirm that our refined\nrewards enhance alignment quality, effectively delaying reward hacking. These\nresults demonstrate our approach as a scalable and effective enhancement for\nexisting RMs and alignment pipelines. The code is available at EBRM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward models (RMs) are essential for aligning Large Language Models (LLMs)\nwith human preferences. However, they often struggle with capturing complex\nhuman preferences and generalizing to unseen data. To address these challenges,\nwe introduce Energy-Based Reward Model (EBRM), a lightweight post-hoc\nrefinement framework that enhances RM robustness and generalization. EBRM\nmodels the reward distribution explicitly, capturing uncertainty in human\npreferences and mitigating the impact of noisy or misaligned annotations. It\nachieves this through conflict-aware data filtering, label-noise-aware\ncontrastive training, and hybrid initialization. Notably, EBRM enhances RMs\nwithout retraining, making it computationally efficient and adaptable across\ndifferent models and tasks. Empirical evaluations on RM benchmarks demonstrate\nsignificant improvements in both robustness and generalization, achieving up to\na 5.97% improvement in safety-critical alignment tasks compared to standard\nRMs. Furthermore, reinforcement learning experiments confirm that our refined\nrewards enhance alignment quality, effectively delaying reward hacking. These\nresults demonstrate our approach as a scalable and effective enhancement for\nexisting RMs and alignment pipelines. The code is available at EBRM."
                },
                "authors": [
                    {
                        "name": "Anamika Lochab"
                    },
                    {
                        "name": "Ruqi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ruqi Zhang"
                },
                "author": "Ruqi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13125v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13125v1",
                "updated": "2025-04-17T17:42:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    42,
                    2,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T17:42:02Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    42,
                    2,
                    3,
                    107,
                    0
                ],
                "title": "LLMs Meet Finance: Fine-Tuning Foundation Models for the Open FinLLM\n  Leaderboard",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Meet Finance: Fine-Tuning Foundation Models for the Open FinLLM\n  Leaderboard"
                },
                "summary": "This paper investigates the application of large language models (LLMs) to\nfinancial tasks. We fine-tuned foundation models using the Open FinLLM\nLeaderboard as a benchmark. Building on Qwen2.5 and Deepseek-R1, we employed\ntechniques including supervised fine-tuning (SFT), direct preference\noptimization (DPO), and reinforcement learning (RL) to enhance their financial\ncapabilities. The fine-tuned models demonstrated substantial performance gains\nacross a wide range of financial tasks. Moreover, we measured the data scaling\nlaw in the financial domain. Our work demonstrates the potential of large\nlanguage models (LLMs) in financial applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the application of large language models (LLMs) to\nfinancial tasks. We fine-tuned foundation models using the Open FinLLM\nLeaderboard as a benchmark. Building on Qwen2.5 and Deepseek-R1, we employed\ntechniques including supervised fine-tuning (SFT), direct preference\noptimization (DPO), and reinforcement learning (RL) to enhance their financial\ncapabilities. The fine-tuned models demonstrated substantial performance gains\nacross a wide range of financial tasks. Moreover, we measured the data scaling\nlaw in the financial domain. Our work demonstrates the potential of large\nlanguage models (LLMs) in financial applications."
                },
                "authors": [
                    {
                        "name": "Varun Rao"
                    },
                    {
                        "name": "Youran Sun"
                    },
                    {
                        "name": "Mahendra Kumar"
                    },
                    {
                        "name": "Tejas Mutneja"
                    },
                    {
                        "name": "Agastya Mukherjee"
                    },
                    {
                        "name": "Haizhao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Haizhao Yang"
                },
                "author": "Haizhao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13125v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13125v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13122v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13122v1",
                "updated": "2025-04-17T17:39:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    39,
                    41,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T17:39:41Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    39,
                    41,
                    3,
                    107,
                    0
                ],
                "title": "VistaDPO: Video Hierarchical Spatial-Temporal Direct Preference\n  Optimization for Large Video Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VistaDPO: Video Hierarchical Spatial-Temporal Direct Preference\n  Optimization for Large Video Models"
                },
                "summary": "Large Video Models (LVMs) built upon Large Language Models (LLMs) have shown\npromise in video understanding but often suffer from misalignment with human\nintuition and video hallucination issues. To address these challenges, we\nintroduce VistaDPO, a novel framework for Video Hierarchical Spatial-Temporal\nDirect Preference Optimization. VistaDPO enhances text-video preference\nalignment across three hierarchical levels: i) Instance Level, aligning overall\nvideo content with responses; ii) Temporal Level, aligning video temporal\nsemantics with event descriptions; and iii) Perceptive Level, aligning spatial\nobjects with language tokens. Given the lack of datasets for fine-grained\nvideo-language preference alignment, we construct VistaDPO-7k, a dataset of\n7.2K QA pairs annotated with chosen and rejected responses, along with\nspatial-temporal grounding information such as timestamps, keyframes, and\nbounding boxes. Extensive experiments on benchmarks such as Video\nHallucination, Video QA, and Captioning performance tasks demonstrate that\nVistaDPO significantly improves the performance of existing LVMs, effectively\nmitigating video-language misalignment and hallucination. The code and data are\navailable at https://github.com/HaroldChen19/VistaDPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Video Models (LVMs) built upon Large Language Models (LLMs) have shown\npromise in video understanding but often suffer from misalignment with human\nintuition and video hallucination issues. To address these challenges, we\nintroduce VistaDPO, a novel framework for Video Hierarchical Spatial-Temporal\nDirect Preference Optimization. VistaDPO enhances text-video preference\nalignment across three hierarchical levels: i) Instance Level, aligning overall\nvideo content with responses; ii) Temporal Level, aligning video temporal\nsemantics with event descriptions; and iii) Perceptive Level, aligning spatial\nobjects with language tokens. Given the lack of datasets for fine-grained\nvideo-language preference alignment, we construct VistaDPO-7k, a dataset of\n7.2K QA pairs annotated with chosen and rejected responses, along with\nspatial-temporal grounding information such as timestamps, keyframes, and\nbounding boxes. Extensive experiments on benchmarks such as Video\nHallucination, Video QA, and Captioning performance tasks demonstrate that\nVistaDPO significantly improves the performance of existing LVMs, effectively\nmitigating video-language misalignment and hallucination. The code and data are\navailable at https://github.com/HaroldChen19/VistaDPO."
                },
                "authors": [
                    {
                        "name": "Haojian Huang"
                    },
                    {
                        "name": "Haodong Chen"
                    },
                    {
                        "name": "Shengqiong Wu"
                    },
                    {
                        "name": "Meng Luo"
                    },
                    {
                        "name": "Jinlan Fu"
                    },
                    {
                        "name": "Xinya Du"
                    },
                    {
                        "name": "Hanwang Zhang"
                    },
                    {
                        "name": "Hao Fei"
                    }
                ],
                "author_detail": {
                    "name": "Hao Fei"
                },
                "author": "Hao Fei",
                "arxiv_comment": "Code and Data: https://github.com/HaroldChen19/VistaDPO",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13122v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13122v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09081v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09081v2",
                "updated": "2025-04-17T17:34:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    34,
                    21,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-12T04:45:48Z",
                "published_parsed": [
                    2025,
                    4,
                    12,
                    4,
                    45,
                    48,
                    5,
                    102,
                    0
                ],
                "title": "SIFT-50M: A Large-Scale Multilingual Dataset for Speech Instruction\n  Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SIFT-50M: A Large-Scale Multilingual Dataset for Speech Instruction\n  Fine-Tuning"
                },
                "summary": "We introduce SIFT (Speech Instruction Fine-Tuning), a 50M-example dataset\ndesigned for instruction fine-tuning and pre-training of speech-text large\nlanguage models (LLMs). SIFT-50M is built from publicly available speech\ncorpora, which collectively contain 14K hours of speech, and leverages LLMs\nalong with off-the-shelf expert models. The dataset spans five languages,\nencompassing a diverse range of speech understanding as well as controllable\nspeech generation instructions. Using SIFT-50M, we train SIFT-LLM, which\noutperforms existing speech-text LLMs on instruction-following benchmarks while\nachieving competitive performance on foundational speech tasks. To support\nfurther research, we also introduce EvalSIFT, a benchmark dataset specifically\ndesigned to evaluate the instruction-following capabilities of speech-text\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SIFT (Speech Instruction Fine-Tuning), a 50M-example dataset\ndesigned for instruction fine-tuning and pre-training of speech-text large\nlanguage models (LLMs). SIFT-50M is built from publicly available speech\ncorpora, which collectively contain 14K hours of speech, and leverages LLMs\nalong with off-the-shelf expert models. The dataset spans five languages,\nencompassing a diverse range of speech understanding as well as controllable\nspeech generation instructions. Using SIFT-50M, we train SIFT-LLM, which\noutperforms existing speech-text LLMs on instruction-following benchmarks while\nachieving competitive performance on foundational speech tasks. To support\nfurther research, we also introduce EvalSIFT, a benchmark dataset specifically\ndesigned to evaluate the instruction-following capabilities of speech-text\nLLMs."
                },
                "authors": [
                    {
                        "name": "Prabhat Pandey"
                    },
                    {
                        "name": "Rupak Vignesh Swaminathan"
                    },
                    {
                        "name": "K V Vijay Girish"
                    },
                    {
                        "name": "Arunasish Sen"
                    },
                    {
                        "name": "Jian Xie"
                    },
                    {
                        "name": "Grant P. Strimel"
                    },
                    {
                        "name": "Andreas Schwarz"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Schwarz"
                },
                "author": "Andreas Schwarz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09081v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09081v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16063v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16063v4",
                "updated": "2025-04-17T17:28:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    28,
                    29,
                    3,
                    107,
                    0
                ],
                "published": "2024-02-25T11:24:41Z",
                "published_parsed": [
                    2024,
                    2,
                    25,
                    11,
                    24,
                    41,
                    6,
                    56,
                    0
                ],
                "title": "Citation-Enhanced Generation for LLM-based Chatbots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Citation-Enhanced Generation for LLM-based Chatbots"
                },
                "summary": "Large language models (LLMs) exhibit powerful general intelligence across\ndiverse scenarios, including their integration into chatbots. However, a vital\nchallenge of LLM-based chatbots is that they may produce hallucinated content\nin responses, which significantly limits their applicability. Various efforts\nhave been made to alleviate hallucination, such as retrieval augmented\ngeneration and reinforcement learning with human feedback, but most of them\nrequire additional training and data annotation. In this paper, we propose a\nnovel post-hoc Citation-Enhanced Generation (CEG) approach combined with\nretrieval argumentation. Unlike previous studies that focus on preventing\nhallucinations during generation, our method addresses this issue in a post-hoc\nway. It incorporates a retrieval module to search for supporting documents\nrelevant to the generated content, and employs a natural language\ninference-based citation generation module. Once the statements in the\ngenerated content lack of reference, our model can regenerate responses until\nall statements are supported by citations. Note that our method is a\ntraining-free plug-and-play plugin that is capable of various LLMs. Experiments\non various hallucination-related datasets show our framework outperforms\nstate-of-the-art methods in both hallucination detection and response\nregeneration on three benchmarks. Our codes and dataset will be publicly\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit powerful general intelligence across\ndiverse scenarios, including their integration into chatbots. However, a vital\nchallenge of LLM-based chatbots is that they may produce hallucinated content\nin responses, which significantly limits their applicability. Various efforts\nhave been made to alleviate hallucination, such as retrieval augmented\ngeneration and reinforcement learning with human feedback, but most of them\nrequire additional training and data annotation. In this paper, we propose a\nnovel post-hoc Citation-Enhanced Generation (CEG) approach combined with\nretrieval argumentation. Unlike previous studies that focus on preventing\nhallucinations during generation, our method addresses this issue in a post-hoc\nway. It incorporates a retrieval module to search for supporting documents\nrelevant to the generated content, and employs a natural language\ninference-based citation generation module. Once the statements in the\ngenerated content lack of reference, our model can regenerate responses until\nall statements are supported by citations. Note that our method is a\ntraining-free plug-and-play plugin that is capable of various LLMs. Experiments\non various hallucination-related datasets show our framework outperforms\nstate-of-the-art methods in both hallucination detection and response\nregeneration on three benchmarks. Our codes and dataset will be publicly\navailable."
                },
                "authors": [
                    {
                        "name": "Weitao Li"
                    },
                    {
                        "name": "Junkai Li"
                    },
                    {
                        "name": "Weizhi Ma"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_doi": "10.18653/v1/2024.acl-long.79",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2024.acl-long.79",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.16063v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16063v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proc. 62nd ACL Vol. 1 Long Papers, Bangkok Thailand, pp.\n  1451-1466, 2024",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.11514v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.11514v3",
                "updated": "2025-04-17T17:18:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    18,
                    24,
                    3,
                    107,
                    0
                ],
                "published": "2024-05-19T10:54:03Z",
                "published_parsed": [
                    2024,
                    5,
                    19,
                    10,
                    54,
                    3,
                    6,
                    140,
                    0
                ],
                "title": "Towards Translating Real-World Code with LLMs: A Study of Translating to\n  Rust",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Translating Real-World Code with LLMs: A Study of Translating to\n  Rust"
                },
                "summary": "Large language models (LLMs) show promise in code translation - the task of\ntranslating code written in one programming language to another language - due\nto their ability to write code in most programming languages. However, LLM's\neffectiveness on translating real-world code remains largely unstudied. In this\nwork, we perform the first substantial study on LLM-based translation to Rust\nby assessing the ability of five state-of-the-art LLMs, GPT4, Claude 3, Claude\n2.1, Gemini Pro, and Mixtral. We conduct our study on code extracted from\nreal-world open source projects. To enable our study, we develop FLOURINE, an\nend-to-end code translation tool that uses differential fuzzing to check if a\nRust translation is I/O equivalent to the original source program, eliminating\nthe need for pre-existing test cases. As part of our investigation, we assess\nboth the LLM's ability to produce an initially successful translation, as well\nas their capacity to fix a previously generated buggy one. If the original and\nthe translated programs are not I/O equivalent, we apply a set of automated\nfeedback strategies, including feedback to the LLM with counterexamples. Our\nresults show that the most successful LLM can translate 47% of our benchmarks,\nand also provides insights into next steps for improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) show promise in code translation - the task of\ntranslating code written in one programming language to another language - due\nto their ability to write code in most programming languages. However, LLM's\neffectiveness on translating real-world code remains largely unstudied. In this\nwork, we perform the first substantial study on LLM-based translation to Rust\nby assessing the ability of five state-of-the-art LLMs, GPT4, Claude 3, Claude\n2.1, Gemini Pro, and Mixtral. We conduct our study on code extracted from\nreal-world open source projects. To enable our study, we develop FLOURINE, an\nend-to-end code translation tool that uses differential fuzzing to check if a\nRust translation is I/O equivalent to the original source program, eliminating\nthe need for pre-existing test cases. As part of our investigation, we assess\nboth the LLM's ability to produce an initially successful translation, as well\nas their capacity to fix a previously generated buggy one. If the original and\nthe translated programs are not I/O equivalent, we apply a set of automated\nfeedback strategies, including feedback to the LLM with counterexamples. Our\nresults show that the most successful LLM can translate 47% of our benchmarks,\nand also provides insights into next steps for improvements."
                },
                "authors": [
                    {
                        "name": "Hasan Ferit Eniser"
                    },
                    {
                        "name": "Hanliang Zhang"
                    },
                    {
                        "name": "Cristina David"
                    },
                    {
                        "name": "Meng Wang"
                    },
                    {
                        "name": "Maria Christakis"
                    },
                    {
                        "name": "Brandon Paulsen"
                    },
                    {
                        "name": "Joey Dodds"
                    },
                    {
                        "name": "Daniel Kroening"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Kroening"
                },
                "author": "Daniel Kroening",
                "arxiv_comment": "12 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.11514v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.11514v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09012v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09012v2",
                "updated": "2025-04-17T17:14:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    14,
                    9,
                    3,
                    107,
                    0
                ],
                "published": "2025-01-15T18:56:22Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    18,
                    56,
                    22,
                    2,
                    15,
                    0
                ],
                "title": "Multimodal LLMs Can Reason about Aesthetics in Zero-Shot",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal LLMs Can Reason about Aesthetics in Zero-Shot"
                },
                "summary": "The rapid progress of generative art has democratized the creation of\nvisually pleasing imagery. However, achieving genuine artistic impact - the\nkind that resonates with viewers on a deeper, more meaningful level - requires\na sophisticated aesthetic sensibility. This sensibility involves a\nmulti-faceted reasoning process extending beyond mere visual appeal, which is\noften overlooked by current computational models. This paper pioneers an\napproach to capture this complex process by investigating how the reasoning\ncapabilities of Multimodal LLMs (MLLMs) can be effectively elicited for\naesthetic judgment. Our analysis reveals a critical challenge: MLLMs exhibit a\ntendency towards hallucinations during aesthetic reasoning, characterized by\nsubjective opinions and unsubstantiated artistic interpretations. We further\ndemonstrate that these limitations can be overcome by employing an\nevidence-based, objective reasoning process, as substantiated by our proposed\nbaseline, ArtCoT. MLLMs prompted by this principle produce multi-faceted and\nin-depth aesthetic reasoning that aligns significantly better with human\njudgment. These findings have direct applications in areas such as AI art\ntutoring and as reward models for generative art. Ultimately, our work paves\nthe way for AI systems that can truly understand, appreciate, and generate\nartworks that align with the sensible human aesthetic standard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid progress of generative art has democratized the creation of\nvisually pleasing imagery. However, achieving genuine artistic impact - the\nkind that resonates with viewers on a deeper, more meaningful level - requires\na sophisticated aesthetic sensibility. This sensibility involves a\nmulti-faceted reasoning process extending beyond mere visual appeal, which is\noften overlooked by current computational models. This paper pioneers an\napproach to capture this complex process by investigating how the reasoning\ncapabilities of Multimodal LLMs (MLLMs) can be effectively elicited for\naesthetic judgment. Our analysis reveals a critical challenge: MLLMs exhibit a\ntendency towards hallucinations during aesthetic reasoning, characterized by\nsubjective opinions and unsubstantiated artistic interpretations. We further\ndemonstrate that these limitations can be overcome by employing an\nevidence-based, objective reasoning process, as substantiated by our proposed\nbaseline, ArtCoT. MLLMs prompted by this principle produce multi-faceted and\nin-depth aesthetic reasoning that aligns significantly better with human\njudgment. These findings have direct applications in areas such as AI art\ntutoring and as reward models for generative art. Ultimately, our work paves\nthe way for AI systems that can truly understand, appreciate, and generate\nartworks that align with the sensible human aesthetic standard."
                },
                "authors": [
                    {
                        "name": "Ruixiang Jiang"
                    },
                    {
                        "name": "Changwen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Changwen Chen"
                },
                "author": "Changwen Chen",
                "arxiv_comment": "WIP, Homepage https://github.com/songrise/MLLM4Art",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09012v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09012v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02676v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02676v4",
                "updated": "2025-04-17T17:07:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    7,
                    58,
                    3,
                    107,
                    0
                ],
                "published": "2024-07-02T21:29:14Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    21,
                    29,
                    14,
                    1,
                    184,
                    0
                ],
                "title": "Covariate-dependent hierarchical Dirichlet processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Covariate-dependent hierarchical Dirichlet processes"
                },
                "summary": "Bayesian hierarchical modelling is a natural framework to effectively\nintegrate data and borrow information across groups. In this paper, we address\nproblems related to density estimation and identifying clusters across related\ngroups, by proposing a hierarchical Bayesian approach that incorporates\nadditional covariate information. To achieve flexibility, our approach builds\non ideas from Bayesian nonparametrics, combining the hierarchical Dirichlet\nprocess with dependent Dirichlet processes. The proposed model is widely\napplicable, accommodating multiple and mixed covariate types through\nappropriate kernel functions as well as different output types through suitable\nlikelihoods. This extends our ability to discern the relationship between\ncovariates and clusters, while also effectively borrowing information and\nquantifying differences across groups. By employing a data augmentation trick,\nwe are able to tackle the intractable normalized weights and construct a Markov\nchain Monte Carlo algorithm for posterior inference. The proposed method is\nillustrated on simulated data and two real data sets on single-cell RNA\nsequencing (scRNA-seq) and calcium imaging. For scRNA-seq data, we show that\nthe incorporation of cell dynamics facilitates the discovery of additional cell\nsubgroups. On calcium imaging data, our method identifies interpretable\nclusters of time frames with similar neural activity, aligning with the\nobserved behavior of the animal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian hierarchical modelling is a natural framework to effectively\nintegrate data and borrow information across groups. In this paper, we address\nproblems related to density estimation and identifying clusters across related\ngroups, by proposing a hierarchical Bayesian approach that incorporates\nadditional covariate information. To achieve flexibility, our approach builds\non ideas from Bayesian nonparametrics, combining the hierarchical Dirichlet\nprocess with dependent Dirichlet processes. The proposed model is widely\napplicable, accommodating multiple and mixed covariate types through\nappropriate kernel functions as well as different output types through suitable\nlikelihoods. This extends our ability to discern the relationship between\ncovariates and clusters, while also effectively borrowing information and\nquantifying differences across groups. By employing a data augmentation trick,\nwe are able to tackle the intractable normalized weights and construct a Markov\nchain Monte Carlo algorithm for posterior inference. The proposed method is\nillustrated on simulated data and two real data sets on single-cell RNA\nsequencing (scRNA-seq) and calcium imaging. For scRNA-seq data, we show that\nthe incorporation of cell dynamics facilitates the discovery of additional cell\nsubgroups. On calcium imaging data, our method identifies interpretable\nclusters of time frames with similar neural activity, aligning with the\nobserved behavior of the animal."
                },
                "authors": [
                    {
                        "name": "Huizi Zhang"
                    },
                    {
                        "name": "Sara Wade"
                    },
                    {
                        "name": "Natalia Bochkina"
                    }
                ],
                "author_detail": {
                    "name": "Natalia Bochkina"
                },
                "author": "Natalia Bochkina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02676v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02676v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22512v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22512v3",
                "updated": "2025-04-17T17:00:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    0,
                    56,
                    3,
                    107,
                    0
                ],
                "published": "2025-03-28T15:15:56Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    15,
                    15,
                    56,
                    4,
                    87,
                    0
                ],
                "title": "Unlocking LLM Repair Capabilities in Low-Resource Programming Languages\n  Through Cross-Language Translation and Multi-Agent Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking LLM Repair Capabilities in Low-Resource Programming Languages\n  Through Cross-Language Translation and Multi-Agent Refinement"
                },
                "summary": "Recent advances in leveraging LLMs for APR have demonstrated impressive\ncapabilities in fixing software defects. However, current LLM-based approaches\npredominantly focus on mainstream programming languages like Java and Python,\nneglecting less prevalent but emerging languages such as Rust due to expensive\ntraining resources, limited datasets, and insufficient community support. This\nnarrow focus creates a significant gap in repair capabilities across the\nprogramming language spectrum, where the full potential of LLMs for\ncomprehensive multilingual program repair remains largely unexplored. To\naddress this limitation, we introduce a novel cross-language program repair\napproach LANTERN that leverages LLMs' differential proficiency across languages\nthrough a multi-agent iterative repair paradigm. Our technique strategically\ntranslates defective code from languages where LLMs exhibit weaker repair\ncapabilities to languages where they demonstrate stronger performance, without\nrequiring additional training. A key innovation of our approach is an LLM-based\ndecision-making system that dynamically selects optimal target languages based\non bug characteristics and continuously incorporates feedback from previous\nrepair attempts. We evaluate our method on xCodeEval, a comprehensive\nmultilingual benchmark comprising 5,068 bugs across 11 programming languages.\nResults demonstrate significant enhancement in repair effectiveness,\nparticularly for underrepresented languages, with Rust showing a 22.09%\nimprovement in Pass@10 metrics. Our research provides the first empirical\nevidence that cross-language translation significantly expands the repair\ncapabilities of LLMs and effectively bridges the performance gap between\nprogramming languages with different levels of popularity, opening new avenues\nfor truly language-agnostic automated program repair.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in leveraging LLMs for APR have demonstrated impressive\ncapabilities in fixing software defects. However, current LLM-based approaches\npredominantly focus on mainstream programming languages like Java and Python,\nneglecting less prevalent but emerging languages such as Rust due to expensive\ntraining resources, limited datasets, and insufficient community support. This\nnarrow focus creates a significant gap in repair capabilities across the\nprogramming language spectrum, where the full potential of LLMs for\ncomprehensive multilingual program repair remains largely unexplored. To\naddress this limitation, we introduce a novel cross-language program repair\napproach LANTERN that leverages LLMs' differential proficiency across languages\nthrough a multi-agent iterative repair paradigm. Our technique strategically\ntranslates defective code from languages where LLMs exhibit weaker repair\ncapabilities to languages where they demonstrate stronger performance, without\nrequiring additional training. A key innovation of our approach is an LLM-based\ndecision-making system that dynamically selects optimal target languages based\non bug characteristics and continuously incorporates feedback from previous\nrepair attempts. We evaluate our method on xCodeEval, a comprehensive\nmultilingual benchmark comprising 5,068 bugs across 11 programming languages.\nResults demonstrate significant enhancement in repair effectiveness,\nparticularly for underrepresented languages, with Rust showing a 22.09%\nimprovement in Pass@10 metrics. Our research provides the first empirical\nevidence that cross-language translation significantly expands the repair\ncapabilities of LLMs and effectively bridges the performance gap between\nprogramming languages with different levels of popularity, opening new avenues\nfor truly language-agnostic automated program repair."
                },
                "authors": [
                    {
                        "name": "Wenqiang Luo"
                    },
                    {
                        "name": "Jacky Wai Keung"
                    },
                    {
                        "name": "Boyang Yang"
                    },
                    {
                        "name": "Jacques Klein"
                    },
                    {
                        "name": "Tegawende F. Bissyande"
                    },
                    {
                        "name": "Haoye Tian"
                    },
                    {
                        "name": "Bach Le"
                    }
                ],
                "author_detail": {
                    "name": "Bach Le"
                },
                "author": "Bach Le",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22512v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22512v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13092v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13092v1",
                "updated": "2025-04-17T16:59:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    16,
                    59,
                    4,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T16:59:04Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    16,
                    59,
                    4,
                    3,
                    107,
                    0
                ],
                "title": "EventVAD: Training-Free Event-Aware Video Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EventVAD: Training-Free Event-Aware Video Anomaly Detection"
                },
                "summary": "Video Anomaly Detection~(VAD) focuses on identifying anomalies within videos.\nSupervised methods require an amount of in-domain training data and often\nstruggle to generalize to unseen anomalies. In contrast, training-free methods\nleverage the intrinsic world knowledge of large language models (LLMs) to\ndetect anomalies but face challenges in localizing fine-grained visual\ntransitions and diverse events. Therefore, we propose EventVAD, an event-aware\nvideo anomaly detection framework that combines tailored dynamic graph\narchitectures and multimodal LLMs through temporal-event reasoning.\nSpecifically, EventVAD first employs dynamic spatiotemporal graph modeling with\ntime-decay constraints to capture event-aware video features. Then, it performs\nadaptive noise filtering and uses signal ratio thresholding to detect event\nboundaries via unsupervised statistical features. The statistical boundary\ndetection module reduces the complexity of processing long videos for MLLMs and\nimproves their temporal reasoning through event consistency. Finally, it\nutilizes a hierarchical prompting strategy to guide MLLMs in performing\nreasoning before determining final decisions. We conducted extensive\nexperiments on the UCF-Crime and XD-Violence datasets. The results demonstrate\nthat EventVAD with a 7B MLLM achieves state-of-the-art (SOTA) in training-free\nsettings, outperforming strong baselines that use 7B or larger MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Anomaly Detection~(VAD) focuses on identifying anomalies within videos.\nSupervised methods require an amount of in-domain training data and often\nstruggle to generalize to unseen anomalies. In contrast, training-free methods\nleverage the intrinsic world knowledge of large language models (LLMs) to\ndetect anomalies but face challenges in localizing fine-grained visual\ntransitions and diverse events. Therefore, we propose EventVAD, an event-aware\nvideo anomaly detection framework that combines tailored dynamic graph\narchitectures and multimodal LLMs through temporal-event reasoning.\nSpecifically, EventVAD first employs dynamic spatiotemporal graph modeling with\ntime-decay constraints to capture event-aware video features. Then, it performs\nadaptive noise filtering and uses signal ratio thresholding to detect event\nboundaries via unsupervised statistical features. The statistical boundary\ndetection module reduces the complexity of processing long videos for MLLMs and\nimproves their temporal reasoning through event consistency. Finally, it\nutilizes a hierarchical prompting strategy to guide MLLMs in performing\nreasoning before determining final decisions. We conducted extensive\nexperiments on the UCF-Crime and XD-Violence datasets. The results demonstrate\nthat EventVAD with a 7B MLLM achieves state-of-the-art (SOTA) in training-free\nsettings, outperforming strong baselines that use 7B or larger MLLMs."
                },
                "authors": [
                    {
                        "name": "Yihua Shao"
                    },
                    {
                        "name": "Haojin He"
                    },
                    {
                        "name": "Sijie Li"
                    },
                    {
                        "name": "Siyu Chen"
                    },
                    {
                        "name": "Xinwei Long"
                    },
                    {
                        "name": "Fanhu Zeng"
                    },
                    {
                        "name": "Yuxuan Fan"
                    },
                    {
                        "name": "Muyang Zhang"
                    },
                    {
                        "name": "Ziyang Yan"
                    },
                    {
                        "name": "Ao Ma"
                    },
                    {
                        "name": "Xiaochen Wang"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Shuyan Li"
                    }
                ],
                "author_detail": {
                    "name": "Shuyan Li"
                },
                "author": "Shuyan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13092v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09597v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09597v2",
                "updated": "2025-04-17T16:53:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    16,
                    53,
                    17,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-13T14:31:52Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    31,
                    52,
                    6,
                    103,
                    0
                ],
                "title": "Understanding LLM Behaviors via Compression: Data Generation, Knowledge\n  Acquisition and Scaling Laws",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding LLM Behaviors via Compression: Data Generation, Knowledge\n  Acquisition and Scaling Laws"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nnumerous tasks, yet principled explanations for their underlying mechanisms and\nseveral phenomena, such as scaling laws, hallucinations, and related behaviors,\nremain elusive. In this work, we revisit the classical relationship between\ncompression and prediction, grounded in Kolmogorov complexity and Shannon\ninformation theory, to provide deeper insights into LLM behaviors. By\nleveraging the Kolmogorov Structure Function and interpreting LLM compression\nas a two-part coding process, we offer a detailed view of how LLMs acquire and\nstore information across increasing model and data scales -- from pervasive\nsyntactic patterns to progressively rarer knowledge elements. Motivated by this\ntheoretical perspective and natural assumptions inspired by Heap's and Zipf's\nlaws, we introduce a simplified yet representative hierarchical data-generation\nframework called the Syntax-Knowledge model. Under the Bayesian setting, we\nshow that prediction and compression within this model naturally lead to\ndiverse learning and scaling behaviors of LLMs. In particular, our theoretical\nanalysis offers intuitive and principled explanations for both data and model\nscaling laws, the dynamics of knowledge acquisition during training and\nfine-tuning, factual knowledge hallucinations in LLMs. The experimental results\nvalidate our theoretical predictions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nnumerous tasks, yet principled explanations for their underlying mechanisms and\nseveral phenomena, such as scaling laws, hallucinations, and related behaviors,\nremain elusive. In this work, we revisit the classical relationship between\ncompression and prediction, grounded in Kolmogorov complexity and Shannon\ninformation theory, to provide deeper insights into LLM behaviors. By\nleveraging the Kolmogorov Structure Function and interpreting LLM compression\nas a two-part coding process, we offer a detailed view of how LLMs acquire and\nstore information across increasing model and data scales -- from pervasive\nsyntactic patterns to progressively rarer knowledge elements. Motivated by this\ntheoretical perspective and natural assumptions inspired by Heap's and Zipf's\nlaws, we introduce a simplified yet representative hierarchical data-generation\nframework called the Syntax-Knowledge model. Under the Bayesian setting, we\nshow that prediction and compression within this model naturally lead to\ndiverse learning and scaling behaviors of LLMs. In particular, our theoretical\nanalysis offers intuitive and principled explanations for both data and model\nscaling laws, the dynamics of knowledge acquisition during training and\nfine-tuning, factual knowledge hallucinations in LLMs. The experimental results\nvalidate our theoretical predictions."
                },
                "authors": [
                    {
                        "name": "Zhixuan Pan"
                    },
                    {
                        "name": "Shaowen Wang"
                    },
                    {
                        "name": "Jian Li"
                    }
                ],
                "author_detail": {
                    "name": "Jian Li"
                },
                "author": "Jian Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09597v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09597v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13079v1",
                "updated": "2025-04-17T16:46:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    16,
                    46,
                    11,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T16:46:11Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    16,
                    46,
                    11,
                    3,
                    107,
                    0
                ],
                "title": "Retrieval-Augmented Generation with Conflicting Evidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation with Conflicting Evidence"
                },
                "summary": "Large language model (LLM) agents are increasingly employing\nretrieval-augmented generation (RAG) to improve the factuality of their\nresponses. However, in practice, these systems often need to handle ambiguous\nuser queries and potentially conflicting information from multiple sources\nwhile also suppressing inaccurate information from noisy or irrelevant\ndocuments. Prior work has generally studied and addressed these challenges in\nisolation, considering only one aspect at a time, such as handling ambiguity or\nrobustness to noise and misinformation. We instead consider multiple factors\nsimultaneously, proposing (i) RAMDocs (Retrieval with Ambiguity and\nMisinformation in Documents), a new dataset that simulates complex and\nrealistic scenarios for conflicting evidence for a user query, including\nambiguity, misinformation, and noise; and (ii) MADAM-RAG, a multi-agent\napproach in which LLM agents debate over the merits of an answer over multiple\nrounds, allowing an aggregator to collate responses corresponding to\ndisambiguated entities while discarding misinformation and noise, thereby\nhandling diverse sources of conflict jointly. We demonstrate the effectiveness\nof MADAM-RAG using both closed and open-source models on AmbigDocs -- which\nrequires presenting all valid answers for ambiguous queries -- improving over\nstrong RAG baselines by up to 11.40% and on FaithEval -- which requires\nsuppressing misinformation -- where we improve by up to 15.80% (absolute) with\nLlama3.3-70B-Instruct. Furthermore, we find that RAMDocs poses a challenge for\nexisting RAG baselines (Llama3.3-70B-Instruct only obtains 32.60 exact match\nscore). While MADAM-RAG begins to address these conflicting factors, our\nanalysis indicates that a substantial gap remains especially when increasing\nthe level of imbalance in supporting evidence and misinformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) agents are increasingly employing\nretrieval-augmented generation (RAG) to improve the factuality of their\nresponses. However, in practice, these systems often need to handle ambiguous\nuser queries and potentially conflicting information from multiple sources\nwhile also suppressing inaccurate information from noisy or irrelevant\ndocuments. Prior work has generally studied and addressed these challenges in\nisolation, considering only one aspect at a time, such as handling ambiguity or\nrobustness to noise and misinformation. We instead consider multiple factors\nsimultaneously, proposing (i) RAMDocs (Retrieval with Ambiguity and\nMisinformation in Documents), a new dataset that simulates complex and\nrealistic scenarios for conflicting evidence for a user query, including\nambiguity, misinformation, and noise; and (ii) MADAM-RAG, a multi-agent\napproach in which LLM agents debate over the merits of an answer over multiple\nrounds, allowing an aggregator to collate responses corresponding to\ndisambiguated entities while discarding misinformation and noise, thereby\nhandling diverse sources of conflict jointly. We demonstrate the effectiveness\nof MADAM-RAG using both closed and open-source models on AmbigDocs -- which\nrequires presenting all valid answers for ambiguous queries -- improving over\nstrong RAG baselines by up to 11.40% and on FaithEval -- which requires\nsuppressing misinformation -- where we improve by up to 15.80% (absolute) with\nLlama3.3-70B-Instruct. Furthermore, we find that RAMDocs poses a challenge for\nexisting RAG baselines (Llama3.3-70B-Instruct only obtains 32.60 exact match\nscore). While MADAM-RAG begins to address these conflicting factors, our\nanalysis indicates that a substantial gap remains especially when increasing\nthe level of imbalance in supporting evidence and misinformation."
                },
                "authors": [
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Archiki Prasad"
                    },
                    {
                        "name": "Elias Stengel-Eskin"
                    },
                    {
                        "name": "Mohit Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Bansal"
                },
                "author": "Mohit Bansal",
                "arxiv_comment": "Our data and code is available at:\n  https://github.com/HanNight/RAMDocs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11536v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11536v2",
                "updated": "2025-04-17T16:46:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    16,
                    46,
                    7,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-15T18:10:22Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    18,
                    10,
                    22,
                    1,
                    105,
                    0
                ],
                "title": "ReTool: Reinforcement Learning for Strategic Tool Use in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReTool: Reinforcement Learning for Strategic Tool Use in LLMs"
                },
                "summary": "While reasoning models (e.g., DeepSeek R1) trained with reinforcement\nlearning (RL), excel in textual reasoning, they struggle in scenarios requiring\nstructured problem-solving, such as geometric reasoning, concise computation,\nor complex equation solving-areas where computational tools like code\ninterpreters (CI) demonstrate distinct advantages. To bridge this gap, we\npropose ReTool, which enhances long-form reasoning with tool-integrated\nlearning, including two key features: (1) dynamic interleaving of real-time\ncode execution within natural language reasoning processes, and (2) an\nautomated RL paradigm that allows policy rollouts with multi-turn real-time\ncode execution and teaches the model in learning when and how to invoke tools\nbased on outcome feedback. ReTool employs a systematic training framework,\nbeginning with synthetic cold-start data generation to produce code-augmented\nlong-form reasoning traces for fine-tuning base models. Subsequent RL training\nleverages task outcomes as rewards to iteratively refine the model's tool use\nstrategy, enabling autonomous discovery of optimal tool invocation patterns\nwithout human priors. Experiments on the challenging MATH Olympiad benchmark\nAIME demonstrate ReTool's superiority: Our 32B model achieves 67% accuracy with\n400 training steps, outperforming text-based RL baseline (40% accuracy, 1080\nsteps) in efficiency and performance. Remarkably, ReTool-32B attains 72.5%\naccuracy in extended settings, surpassing OpenAI's o1-preview by 27.9%. Further\nanalysis reveals emergent behaviors such as code self-correction, signaling an\n''aha moment'' in which the model autonomously masters adaptive tool use. These\nfindings highlight the promise of outcome-driven tool integration for advancing\ncomplex mathematical reasoning and offer new insights into hybrid\nneuro-symbolic systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While reasoning models (e.g., DeepSeek R1) trained with reinforcement\nlearning (RL), excel in textual reasoning, they struggle in scenarios requiring\nstructured problem-solving, such as geometric reasoning, concise computation,\nor complex equation solving-areas where computational tools like code\ninterpreters (CI) demonstrate distinct advantages. To bridge this gap, we\npropose ReTool, which enhances long-form reasoning with tool-integrated\nlearning, including two key features: (1) dynamic interleaving of real-time\ncode execution within natural language reasoning processes, and (2) an\nautomated RL paradigm that allows policy rollouts with multi-turn real-time\ncode execution and teaches the model in learning when and how to invoke tools\nbased on outcome feedback. ReTool employs a systematic training framework,\nbeginning with synthetic cold-start data generation to produce code-augmented\nlong-form reasoning traces for fine-tuning base models. Subsequent RL training\nleverages task outcomes as rewards to iteratively refine the model's tool use\nstrategy, enabling autonomous discovery of optimal tool invocation patterns\nwithout human priors. Experiments on the challenging MATH Olympiad benchmark\nAIME demonstrate ReTool's superiority: Our 32B model achieves 67% accuracy with\n400 training steps, outperforming text-based RL baseline (40% accuracy, 1080\nsteps) in efficiency and performance. Remarkably, ReTool-32B attains 72.5%\naccuracy in extended settings, surpassing OpenAI's o1-preview by 27.9%. Further\nanalysis reveals emergent behaviors such as code self-correction, signaling an\n''aha moment'' in which the model autonomously masters adaptive tool use. These\nfindings highlight the promise of outcome-driven tool integration for advancing\ncomplex mathematical reasoning and offer new insights into hybrid\nneuro-symbolic systems."
                },
                "authors": [
                    {
                        "name": "Jiazhan Feng"
                    },
                    {
                        "name": "Shijue Huang"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Yujia Qin"
                    },
                    {
                        "name": "Baoquan Zhong"
                    },
                    {
                        "name": "Chengquan Jiang"
                    },
                    {
                        "name": "Jinxin Chi"
                    },
                    {
                        "name": "Wanjun Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Wanjun Zhong"
                },
                "author": "Wanjun Zhong",
                "arxiv_comment": "fix typos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11536v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11536v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13074v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13074v2",
                "updated": "2025-04-18T09:46:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    9,
                    46,
                    32,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-17T16:37:27Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    16,
                    37,
                    27,
                    3,
                    107,
                    0
                ],
                "title": "SkyReels-V2: Infinite-length Film Generative Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SkyReels-V2: Infinite-length Film Generative Model"
                },
                "summary": "Recent advances in video generation have been driven by diffusion models and\nautoregressive frameworks, yet critical challenges persist in harmonizing\nprompt adherence, visual quality, motion dynamics, and duration: compromises in\nmotion dynamics to enhance temporal visual quality, constrained video duration\n(5-10 seconds) to prioritize resolution, and inadequate shot-aware generation\nstemming from general-purpose MLLMs' inability to interpret cinematic grammar,\nsuch as shot composition, actor expressions, and camera motions. These\nintertwined limitations hinder realistic long-form synthesis and professional\nfilm-style generation. To address these limitations, we propose SkyReels-V2, an\nInfinite-length Film Generative Model, that synergizes Multi-modal Large\nLanguage Model (MLLM), Multi-stage Pretraining, Reinforcement Learning, and\nDiffusion Forcing Framework. Firstly, we design a comprehensive structural\nrepresentation of video that combines the general descriptions by the\nMulti-modal LLM and the detailed shot language by sub-expert models. Aided with\nhuman annotation, we then train a unified Video Captioner, named\nSkyCaptioner-V1, to efficiently label the video data. Secondly, we establish\nprogressive-resolution pretraining for the fundamental video generation,\nfollowed by a four-stage post-training enhancement: Initial concept-balanced\nSupervised Fine-Tuning (SFT) improves baseline quality; Motion-specific\nReinforcement Learning (RL) training with human-annotated and synthetic\ndistortion data addresses dynamic artifacts; Our diffusion forcing framework\nwith non-decreasing noise schedules enables long-video synthesis in an\nefficient search space; Final high-quality SFT refines visual fidelity. All the\ncode and models are available at https://github.com/SkyworkAI/SkyReels-V2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in video generation have been driven by diffusion models and\nautoregressive frameworks, yet critical challenges persist in harmonizing\nprompt adherence, visual quality, motion dynamics, and duration: compromises in\nmotion dynamics to enhance temporal visual quality, constrained video duration\n(5-10 seconds) to prioritize resolution, and inadequate shot-aware generation\nstemming from general-purpose MLLMs' inability to interpret cinematic grammar,\nsuch as shot composition, actor expressions, and camera motions. These\nintertwined limitations hinder realistic long-form synthesis and professional\nfilm-style generation. To address these limitations, we propose SkyReels-V2, an\nInfinite-length Film Generative Model, that synergizes Multi-modal Large\nLanguage Model (MLLM), Multi-stage Pretraining, Reinforcement Learning, and\nDiffusion Forcing Framework. Firstly, we design a comprehensive structural\nrepresentation of video that combines the general descriptions by the\nMulti-modal LLM and the detailed shot language by sub-expert models. Aided with\nhuman annotation, we then train a unified Video Captioner, named\nSkyCaptioner-V1, to efficiently label the video data. Secondly, we establish\nprogressive-resolution pretraining for the fundamental video generation,\nfollowed by a four-stage post-training enhancement: Initial concept-balanced\nSupervised Fine-Tuning (SFT) improves baseline quality; Motion-specific\nReinforcement Learning (RL) training with human-annotated and synthetic\ndistortion data addresses dynamic artifacts; Our diffusion forcing framework\nwith non-decreasing noise schedules enables long-video synthesis in an\nefficient search space; Final high-quality SFT refines visual fidelity. All the\ncode and models are available at https://github.com/SkyworkAI/SkyReels-V2."
                },
                "authors": [
                    {
                        "name": "Guibin Chen"
                    },
                    {
                        "name": "Dixuan Lin"
                    },
                    {
                        "name": "Jiangping Yang"
                    },
                    {
                        "name": "Chunze Lin"
                    },
                    {
                        "name": "Juncheng Zhu"
                    },
                    {
                        "name": "Mingyuan Fan"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Sheng Chen"
                    },
                    {
                        "name": "Zheng Chen"
                    },
                    {
                        "name": "Chengchen Ma"
                    },
                    {
                        "name": "Weiming Xiong"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Nuo Pang"
                    },
                    {
                        "name": "Kang Kang"
                    },
                    {
                        "name": "Zhiheng Xu"
                    },
                    {
                        "name": "Yuzhe Jin"
                    },
                    {
                        "name": "Yupeng Liang"
                    },
                    {
                        "name": "Yubing Song"
                    },
                    {
                        "name": "Peng Zhao"
                    },
                    {
                        "name": "Boyuan Xu"
                    },
                    {
                        "name": "Di Qiu"
                    },
                    {
                        "name": "Debang Li"
                    },
                    {
                        "name": "Zhengcong Fei"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Yahui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yahui Zhou"
                },
                "author": "Yahui Zhou",
                "arxiv_comment": "31 pages,10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13074v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13074v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13070v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13070v1",
                "updated": "2025-04-17T16:31:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    16,
                    31,
                    14,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T16:31:14Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    16,
                    31,
                    14,
                    3,
                    107,
                    0
                ],
                "title": "A quadratic estimator view of the transfer function correction in\n  intensity mapping surveys",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A quadratic estimator view of the transfer function correction in\n  intensity mapping surveys"
                },
                "summary": "In single dish neutral hydrogen (HI) intensity mapping, signal separation\nmethods such as Principal Component Analysis (PCA) are used to clean the\nastrophysical foregrounds. PCA induces a signal loss in the estimated power\nspectrum, which can be corrected by a transfer function (TF). By injecting mock\nsignals of HI into the data and performing the PCA cleaning, we can use the\ncleaned mock HI signal to cross-correlate with the original mock, and estimate\nthe signal loss as a TF, $T(\\vec{k})$. As expected, a correction of\n${T}(\\vec{k})^{-1}$ restores the cross-power between the HI and optical\ngalaxies. However, contrary to intuition, the HI auto-power also requires a\n${T}(\\vec{k})^{-1}$ correction, not ${T}(\\vec{k})^{-2}$. The\n${T}(\\vec{k})^{-1}$ correction is only known empirically through simulations.\nIn this Letter, we show that the ${T}(\\vec{k})^{-1}$ correction in auto-power\nis universal, and can be analytically proven using the quadratic estimator\nformalism through window function normalisation. The normalisation can also be\nused to determine the TF correction for any type of linear process. Using the\nwindow function, we demonstrate that PCA induces mode-mixing in the power\nspectrum estimation, which may lead to biases in the model inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In single dish neutral hydrogen (HI) intensity mapping, signal separation\nmethods such as Principal Component Analysis (PCA) are used to clean the\nastrophysical foregrounds. PCA induces a signal loss in the estimated power\nspectrum, which can be corrected by a transfer function (TF). By injecting mock\nsignals of HI into the data and performing the PCA cleaning, we can use the\ncleaned mock HI signal to cross-correlate with the original mock, and estimate\nthe signal loss as a TF, $T(\\vec{k})$. As expected, a correction of\n${T}(\\vec{k})^{-1}$ restores the cross-power between the HI and optical\ngalaxies. However, contrary to intuition, the HI auto-power also requires a\n${T}(\\vec{k})^{-1}$ correction, not ${T}(\\vec{k})^{-2}$. The\n${T}(\\vec{k})^{-1}$ correction is only known empirically through simulations.\nIn this Letter, we show that the ${T}(\\vec{k})^{-1}$ correction in auto-power\nis universal, and can be analytically proven using the quadratic estimator\nformalism through window function normalisation. The normalisation can also be\nused to determine the TF correction for any type of linear process. Using the\nwindow function, we demonstrate that PCA induces mode-mixing in the power\nspectrum estimation, which may lead to biases in the model inference."
                },
                "authors": [
                    {
                        "name": "Zhaoting Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhaoting Chen"
                },
                "author": "Zhaoting Chen",
                "arxiv_comment": "To be submitted to MNRAS letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13070v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13070v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13068v1",
                "updated": "2025-04-17T16:29:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    16,
                    29,
                    8,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T16:29:08Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    16,
                    29,
                    8,
                    3,
                    107,
                    0
                ],
                "title": "Accuracy is Not Agreement: Expert-Aligned Evaluation of Crash Narrative\n  Classification Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accuracy is Not Agreement: Expert-Aligned Evaluation of Crash Narrative\n  Classification Models"
                },
                "summary": "This study explores the relationship between deep learning (DL) model\naccuracy and expert agreement in the classification of crash narratives. We\nevaluate five DL models -- including BERT variants, the Universal Sentence\nEncoder (USE), and a zero-shot classifier -- against expert-labeled data and\nnarrative text. The analysis is further extended to four large language models\n(LLMs): GPT-4, LLaMA 3, Qwen, and Claude. Our results reveal a counterintuitive\ntrend: models with higher technical accuracy often exhibit lower agreement with\ndomain experts, whereas LLMs demonstrate greater expert alignment despite\nrelatively lower accuracy scores. To quantify and interpret model-expert\nagreement, we employ Cohen's Kappa, Principal Component Analysis (PCA), and\nSHAP-based explainability techniques. Findings indicate that expert-aligned\nmodels tend to rely more on contextual and temporal language cues, rather than\nlocation-specific keywords. These results underscore that accuracy alone is\ninsufficient for evaluating models in safety-critical NLP applications. We\nadvocate for incorporating expert agreement as a complementary metric in model\nevaluation frameworks and highlight the promise of LLMs as interpretable,\nscalable tools for crash analysis pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the relationship between deep learning (DL) model\naccuracy and expert agreement in the classification of crash narratives. We\nevaluate five DL models -- including BERT variants, the Universal Sentence\nEncoder (USE), and a zero-shot classifier -- against expert-labeled data and\nnarrative text. The analysis is further extended to four large language models\n(LLMs): GPT-4, LLaMA 3, Qwen, and Claude. Our results reveal a counterintuitive\ntrend: models with higher technical accuracy often exhibit lower agreement with\ndomain experts, whereas LLMs demonstrate greater expert alignment despite\nrelatively lower accuracy scores. To quantify and interpret model-expert\nagreement, we employ Cohen's Kappa, Principal Component Analysis (PCA), and\nSHAP-based explainability techniques. Findings indicate that expert-aligned\nmodels tend to rely more on contextual and temporal language cues, rather than\nlocation-specific keywords. These results underscore that accuracy alone is\ninsufficient for evaluating models in safety-critical NLP applications. We\nadvocate for incorporating expert agreement as a complementary metric in model\nevaluation frameworks and highlight the promise of LLMs as interpretable,\nscalable tools for crash analysis pipelines."
                },
                "authors": [
                    {
                        "name": "Sudesh Ramesh Bhagat"
                    },
                    {
                        "name": "Ibne Farabi Shihab"
                    },
                    {
                        "name": "Anuj Sharma"
                    }
                ],
                "author_detail": {
                    "name": "Anuj Sharma"
                },
                "author": "Anuj Sharma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11543v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11543v2",
                "updated": "2025-04-17T16:28:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    16,
                    28,
                    46,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-15T18:22:55Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    18,
                    22,
                    55,
                    1,
                    105,
                    0
                ],
                "title": "REAL: Benchmarking Autonomous Agents on Deterministic Simulations of\n  Real Websites",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REAL: Benchmarking Autonomous Agents on Deterministic Simulations of\n  Real Websites"
                },
                "summary": "We introduce REAL, a benchmark and framework for multi-turn agent evaluations\non deterministic simulations of real-world websites. REAL comprises\nhigh-fidelity, deterministic replicas of 11 widely-used websites across domains\nsuch as e-commerce, travel, communication, and professional networking. We also\nrelease a benchmark consisting of 112 practical tasks that mirror everyday\ncomplex user interactions requiring both accurate information retrieval and\nstate-changing actions. All interactions occur within this fully controlled\nsetting, eliminating safety risks and enabling robust, reproducible evaluation\nof agent capability and reliability. Our novel evaluation framework combines\nprogrammatic checks of website state for action-based tasks with rubric-guided\nLLM-based judgments for information retrieval. The framework supports both\nopen-source and proprietary agent systems through a flexible evaluation harness\nthat accommodates black-box commands within browser environments, allowing\nresearch labs to test agentic systems without modification. Our empirical\nresults show that frontier language models achieve at most a 41% success rate\non REAL, highlighting critical gaps in autonomous web navigation and task\ncompletion capabilities. Our framework supports easy integration of new tasks,\nreproducible evaluation, and scalable post-training data generation, marking a\nsignificant step forward in evaluating and advancing agent capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce REAL, a benchmark and framework for multi-turn agent evaluations\non deterministic simulations of real-world websites. REAL comprises\nhigh-fidelity, deterministic replicas of 11 widely-used websites across domains\nsuch as e-commerce, travel, communication, and professional networking. We also\nrelease a benchmark consisting of 112 practical tasks that mirror everyday\ncomplex user interactions requiring both accurate information retrieval and\nstate-changing actions. All interactions occur within this fully controlled\nsetting, eliminating safety risks and enabling robust, reproducible evaluation\nof agent capability and reliability. Our novel evaluation framework combines\nprogrammatic checks of website state for action-based tasks with rubric-guided\nLLM-based judgments for information retrieval. The framework supports both\nopen-source and proprietary agent systems through a flexible evaluation harness\nthat accommodates black-box commands within browser environments, allowing\nresearch labs to test agentic systems without modification. Our empirical\nresults show that frontier language models achieve at most a 41% success rate\non REAL, highlighting critical gaps in autonomous web navigation and task\ncompletion capabilities. Our framework supports easy integration of new tasks,\nreproducible evaluation, and scalable post-training data generation, marking a\nsignificant step forward in evaluating and advancing agent capabilities."
                },
                "authors": [
                    {
                        "name": "Divyansh Garg"
                    },
                    {
                        "name": "Shaun VanWeelden"
                    },
                    {
                        "name": "Diego Caples"
                    },
                    {
                        "name": "Andis Draguns"
                    },
                    {
                        "name": "Nikil Ravi"
                    },
                    {
                        "name": "Pranav Putta"
                    },
                    {
                        "name": "Naman Garg"
                    },
                    {
                        "name": "Tomas Abraham"
                    },
                    {
                        "name": "Michael Lara"
                    },
                    {
                        "name": "Federico Lopez"
                    },
                    {
                        "name": "James Liu"
                    },
                    {
                        "name": "Atharva Gundawar"
                    },
                    {
                        "name": "Prannay Hebbar"
                    },
                    {
                        "name": "Youngchul Joo"
                    },
                    {
                        "name": "Jindong Gu"
                    },
                    {
                        "name": "Charles London"
                    },
                    {
                        "name": "Christian Schroeder de Witt"
                    },
                    {
                        "name": "Sumeet Motwani"
                    }
                ],
                "author_detail": {
                    "name": "Sumeet Motwani"
                },
                "author": "Sumeet Motwani",
                "arxiv_comment": "The websites, framework, and leaderboard are available at\n  https://realevals.xyz and https://github.com/agi-inc/REAL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11543v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11543v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13062v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13062v1",
                "updated": "2025-04-17T16:15:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    16,
                    15,
                    56,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T16:15:56Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    16,
                    15,
                    56,
                    3,
                    107,
                    0
                ],
                "title": "Seeing Beyond Dark-Field RGB Capabilities: Deep Spectral Extrapolation\n  of Ultrasmall Plasmonic Nanogaps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seeing Beyond Dark-Field RGB Capabilities: Deep Spectral Extrapolation\n  of Ultrasmall Plasmonic Nanogaps"
                },
                "summary": "Localized surface plasmons can confine light within a deep-subwavelength\nvolume comparable to the scale of atoms and molecules, enabling ultrasensitive\nresponses to near-field variations. On the other hand, this extreme\nlocalization also inevitably amplifies the unwanted noise from the response of\nlocal morphological imperfections, leading to complex spectral variations and\nreduced consistency across the plasmonic nanostructures. Seeking uniform\noptical responses has therefore long been a sought-after goal in\nnanoplasmonics. However, conventional probing techniques by dark-field (DF)\nconfocal microscopy, such as image analysis or spectral measurements, can be\ninaccurate and time-consuming, respectively. Here, we introduce SPARX, a\ndeep-learning-powered paradigm that surpasses conventional imaging and\nspectroscopic capabilities. In particular, SPARX can batch-predict broadband DF\nspectra (e.g., 500-1000 nm) of numerous nanoparticles simultaneously from an\ninformation-limited RGB image (i.e., below 700 nm). It achieves this\nextrapolative inference beyond the camera's capture capabilities by learning\nthe underlying physical relationships among multiple orders of optical\nresonances. The spectral predictions only take milliseconds, achieving a\nspeedup of three to four orders of magnitude compared to traditional spectral\nacquisition, which may take from hours to days. As a proof-of-principle\ndemonstration for screening identical resonances, the selection accuracy\nachieved by SPARX is comparable to that of conventional spectroscopy\ntechniques. This breakthrough paves the way for consistent plasmonic\napplications and next-generation microscopies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Localized surface plasmons can confine light within a deep-subwavelength\nvolume comparable to the scale of atoms and molecules, enabling ultrasensitive\nresponses to near-field variations. On the other hand, this extreme\nlocalization also inevitably amplifies the unwanted noise from the response of\nlocal morphological imperfections, leading to complex spectral variations and\nreduced consistency across the plasmonic nanostructures. Seeking uniform\noptical responses has therefore long been a sought-after goal in\nnanoplasmonics. However, conventional probing techniques by dark-field (DF)\nconfocal microscopy, such as image analysis or spectral measurements, can be\ninaccurate and time-consuming, respectively. Here, we introduce SPARX, a\ndeep-learning-powered paradigm that surpasses conventional imaging and\nspectroscopic capabilities. In particular, SPARX can batch-predict broadband DF\nspectra (e.g., 500-1000 nm) of numerous nanoparticles simultaneously from an\ninformation-limited RGB image (i.e., below 700 nm). It achieves this\nextrapolative inference beyond the camera's capture capabilities by learning\nthe underlying physical relationships among multiple orders of optical\nresonances. The spectral predictions only take milliseconds, achieving a\nspeedup of three to four orders of magnitude compared to traditional spectral\nacquisition, which may take from hours to days. As a proof-of-principle\ndemonstration for screening identical resonances, the selection accuracy\nachieved by SPARX is comparable to that of conventional spectroscopy\ntechniques. This breakthrough paves the way for consistent plasmonic\napplications and next-generation microscopies."
                },
                "authors": [
                    {
                        "name": "Mohammadrahim Kazemzadeh"
                    },
                    {
                        "name": "Banghuan Zhang"
                    },
                    {
                        "name": "Tao He"
                    },
                    {
                        "name": "Haoran Liu"
                    },
                    {
                        "name": "Zihe Jiang"
                    },
                    {
                        "name": "Zhiwei Hu"
                    },
                    {
                        "name": "Xiaohui Dong"
                    },
                    {
                        "name": "Chaowei Sun"
                    },
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Xiaobo He"
                    },
                    {
                        "name": "Shuyan Li"
                    },
                    {
                        "name": "Gonzalo Alvarez-Perez"
                    },
                    {
                        "name": "Ferruccio Pisanello"
                    },
                    {
                        "name": "Huatian Hu"
                    },
                    {
                        "name": "Wen Chen"
                    },
                    {
                        "name": "Hongxing Xu"
                    }
                ],
                "author_detail": {
                    "name": "Hongxing Xu"
                },
                "author": "Hongxing Xu",
                "arxiv_comment": "22 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13062v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13062v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13057v1",
                "updated": "2025-04-17T16:11:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    16,
                    11,
                    42,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T16:11:42Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    16,
                    11,
                    42,
                    3,
                    107,
                    0
                ],
                "title": "Covariate balancing estimation and model selection for\n  difference-in-differences approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Covariate balancing estimation and model selection for\n  difference-in-differences approach"
                },
                "summary": "In causal inference, remarkable progress has been made in\ndifference-in-differences (DID) approaches to estimate the average effect of\ntreatment on the treated (ATT). Of these, the semiparametric DID (SDID)\napproach incorporates a propensity score analysis into the DID setup. Supposing\nthat the ATT is a function of covariates, we estimate it by weighting the\ninverse of the propensity score. As one method to make the estimation robust to\nthe propensity score modeling, we incorporate covariate balancing. Then, by\nattentively constructing the moment conditions used in the covariate balancing,\nwe show that the proposed estimator has doubly robustness. In addition to the\nestimation, model selection is also addressed. In practice, covariate selection\nis an essential task in statistical analysis, but even in the basic setting of\nthe SDID approach, there are no reasonable information criteria. Therefore, we\nderive a model selection criterion as an asymptotically bias-corrected\nestimator of risk based on the loss function used in the SDID estimation. As a\nresult, we show that a penalty term is derived that is considerably different\nfrom almost twice the number of parameters that often appears in AIC-type\ninformation criteria. Numerical experiments show that the proposed method\nestimates the ATT robustly compared to the method using propensity scores given\nby the maximum likelihood estimation (MLE), and that the proposed criterion\nclearly reduces the risk targeted in the SDID approach compared to the\nintuitive generalization of the existing information criterion. In addition,\nreal data analysis confirms that there is a large difference between the\nresults of the proposed method and the existing method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In causal inference, remarkable progress has been made in\ndifference-in-differences (DID) approaches to estimate the average effect of\ntreatment on the treated (ATT). Of these, the semiparametric DID (SDID)\napproach incorporates a propensity score analysis into the DID setup. Supposing\nthat the ATT is a function of covariates, we estimate it by weighting the\ninverse of the propensity score. As one method to make the estimation robust to\nthe propensity score modeling, we incorporate covariate balancing. Then, by\nattentively constructing the moment conditions used in the covariate balancing,\nwe show that the proposed estimator has doubly robustness. In addition to the\nestimation, model selection is also addressed. In practice, covariate selection\nis an essential task in statistical analysis, but even in the basic setting of\nthe SDID approach, there are no reasonable information criteria. Therefore, we\nderive a model selection criterion as an asymptotically bias-corrected\nestimator of risk based on the loss function used in the SDID estimation. As a\nresult, we show that a penalty term is derived that is considerably different\nfrom almost twice the number of parameters that often appears in AIC-type\ninformation criteria. Numerical experiments show that the proposed method\nestimates the ATT robustly compared to the method using propensity scores given\nby the maximum likelihood estimation (MLE), and that the proposed criterion\nclearly reduces the risk targeted in the SDID approach compared to the\nintuitive generalization of the existing information criterion. In addition,\nreal data analysis confirms that there is a large difference between the\nresults of the proposed method and the existing method."
                },
                "authors": [
                    {
                        "name": "Takamichi Baba"
                    },
                    {
                        "name": "Yoshiyuki Ninomiya"
                    }
                ],
                "author_detail": {
                    "name": "Yoshiyuki Ninomiya"
                },
                "author": "Yoshiyuki Ninomiya",
                "arxiv_comment": "24 pages, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62D20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13052v1",
                "updated": "2025-04-17T16:09:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    16,
                    9,
                    12,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T16:09:12Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    16,
                    9,
                    12,
                    3,
                    107,
                    0
                ],
                "title": "GraphAttack: Exploiting Representational Blindspots in LLM Safety\n  Mechanisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphAttack: Exploiting Representational Blindspots in LLM Safety\n  Mechanisms"
                },
                "summary": "Large Language Models (LLMs) have been equipped with safety mechanisms to\nprevent harmful outputs, but these guardrails can often be bypassed through\n\"jailbreak\" prompts. This paper introduces a novel graph-based approach to\nsystematically generate jailbreak prompts through semantic transformations. We\nrepresent malicious prompts as nodes in a graph structure with edges denoting\ndifferent transformations, leveraging Abstract Meaning Representation (AMR) and\nResource Description Framework (RDF) to parse user goals into semantic\ncomponents that can be manipulated to evade safety filters. We demonstrate a\nparticularly effective exploitation vector by instructing LLMs to generate code\nthat realizes the intent described in these semantic graphs, achieving success\nrates of up to 87% against leading commercial LLMs. Our analysis reveals that\ncontextual framing and abstraction are particularly effective at circumventing\nsafety measures, highlighting critical gaps in current safety alignment\ntechniques that focus primarily on surface-level patterns. These findings\nprovide insights for developing more robust safeguards against structured\nsemantic attacks. Our research contributes both a theoretical framework and\npractical methodology for systematically stress-testing LLM safety mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been equipped with safety mechanisms to\nprevent harmful outputs, but these guardrails can often be bypassed through\n\"jailbreak\" prompts. This paper introduces a novel graph-based approach to\nsystematically generate jailbreak prompts through semantic transformations. We\nrepresent malicious prompts as nodes in a graph structure with edges denoting\ndifferent transformations, leveraging Abstract Meaning Representation (AMR) and\nResource Description Framework (RDF) to parse user goals into semantic\ncomponents that can be manipulated to evade safety filters. We demonstrate a\nparticularly effective exploitation vector by instructing LLMs to generate code\nthat realizes the intent described in these semantic graphs, achieving success\nrates of up to 87% against leading commercial LLMs. Our analysis reveals that\ncontextual framing and abstraction are particularly effective at circumventing\nsafety measures, highlighting critical gaps in current safety alignment\ntechniques that focus primarily on surface-level patterns. These findings\nprovide insights for developing more robust safeguards against structured\nsemantic attacks. Our research contributes both a theoretical framework and\npractical methodology for systematically stress-testing LLM safety mechanisms."
                },
                "authors": [
                    {
                        "name": "Sinan He"
                    },
                    {
                        "name": "An Wang"
                    }
                ],
                "author_detail": {
                    "name": "An Wang"
                },
                "author": "An Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20973v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20973v2",
                "updated": "2025-04-17T16:07:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    16,
                    7,
                    44,
                    3,
                    107,
                    0
                ],
                "published": "2025-02-28T11:37:52Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    11,
                    37,
                    52,
                    4,
                    59,
                    0
                ],
                "title": "Arabizi vs LLMs: Can the Genie Understand the Language of Aladdin?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arabizi vs LLMs: Can the Genie Understand the Language of Aladdin?"
                },
                "summary": "In this era of rapid technological advancements, communication continues to\nevolve as new linguistic phenomena emerge. Among these is Arabizi, a hybrid\nform of Arabic that incorporates Latin characters and numbers to represent the\nspoken dialects of Arab communities. Arabizi is widely used on social media and\nallows people to communicate in an informal and dynamic way, but it poses\nsignificant challenges for machine translation due to its lack of formal\nstructure and deeply embedded cultural nuances. This case study arises from a\ngrowing need to translate Arabizi for gisting purposes. It evaluates the\ncapacity of different LLMs to decode and translate Arabizi, focusing on\nmultiple Arabic dialects that have rarely been studied up until now. Using a\ncombination of human evaluators and automatic metrics, this research project\ninvestigates the model's performance in translating Arabizi into both Modern\nStandard Arabic and English. Key questions explored include which dialects are\ntranslated most effectively and whether translations into English surpass those\ninto Arabic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this era of rapid technological advancements, communication continues to\nevolve as new linguistic phenomena emerge. Among these is Arabizi, a hybrid\nform of Arabic that incorporates Latin characters and numbers to represent the\nspoken dialects of Arab communities. Arabizi is widely used on social media and\nallows people to communicate in an informal and dynamic way, but it poses\nsignificant challenges for machine translation due to its lack of formal\nstructure and deeply embedded cultural nuances. This case study arises from a\ngrowing need to translate Arabizi for gisting purposes. It evaluates the\ncapacity of different LLMs to decode and translate Arabizi, focusing on\nmultiple Arabic dialects that have rarely been studied up until now. Using a\ncombination of human evaluators and automatic metrics, this research project\ninvestigates the model's performance in translating Arabizi into both Modern\nStandard Arabic and English. Key questions explored include which dialects are\ntranslated most effectively and whether translations into English surpass those\ninto Arabic."
                },
                "authors": [
                    {
                        "name": "Perla Al Almaoui"
                    },
                    {
                        "name": "Pierrette Bouillon"
                    },
                    {
                        "name": "Simon Hengchen"
                    }
                ],
                "author_detail": {
                    "name": "Simon Hengchen"
                },
                "author": "Simon Hengchen",
                "arxiv_comment": "Accepted to MT Summit 2025 (Track: Implementation and Case Studies)\n  https://mtsummit2025.unige.ch/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20973v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20973v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10445v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10445v4",
                "updated": "2025-04-17T16:05:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    16,
                    5,
                    20,
                    3,
                    107,
                    0
                ],
                "published": "2024-04-16T10:31:06Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    10,
                    31,
                    6,
                    1,
                    107,
                    0
                ],
                "title": "SparseDM: Toward Sparse Efficient Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseDM: Toward Sparse Efficient Diffusion Models"
                },
                "summary": "Diffusion models represent a powerful family of generative models widely used\nfor image and video generation. However, the time-consuming deployment, long\ninference time, and requirements on large memory hinder their applications on\nresource constrained devices. In this paper, we propose a method based on the\nimproved Straight-Through Estimator to improve the deployment efficiency of\ndiffusion models. Specifically, we add sparse masks to the Convolution and\nLinear layers in a pre-trained diffusion model, then transfer learn the sparse\nmodel during the fine-tuning stage and turn on the sparse masks during\ninference. Experimental results on a Transformer and UNet-based diffusion\nmodels demonstrate that our method reduces MACs by 50% while maintaining FID.\nSparse models are accelerated by approximately 1.2x on the GPU. Under other\nMACs conditions, the FID is also lower than 1 compared to other methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models represent a powerful family of generative models widely used\nfor image and video generation. However, the time-consuming deployment, long\ninference time, and requirements on large memory hinder their applications on\nresource constrained devices. In this paper, we propose a method based on the\nimproved Straight-Through Estimator to improve the deployment efficiency of\ndiffusion models. Specifically, we add sparse masks to the Convolution and\nLinear layers in a pre-trained diffusion model, then transfer learn the sparse\nmodel during the fine-tuning stage and turn on the sparse masks during\ninference. Experimental results on a Transformer and UNet-based diffusion\nmodels demonstrate that our method reduces MACs by 50% while maintaining FID.\nSparse models are accelerated by approximately 1.2x on the GPU. Under other\nMACs conditions, the FID is also lower than 1 compared to other methods."
                },
                "authors": [
                    {
                        "name": "Kafeng Wang"
                    },
                    {
                        "name": "Jianfei Chen"
                    },
                    {
                        "name": "He Li"
                    },
                    {
                        "name": "Zhenpeng Mi"
                    },
                    {
                        "name": "Jun Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhu"
                },
                "author": "Jun Zhu",
                "arxiv_comment": "This paper has been accepted by ICME 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10445v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10445v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13043v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13043v1",
                "updated": "2025-04-17T15:57:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    15,
                    57,
                    16,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T15:57:16Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    15,
                    57,
                    16,
                    3,
                    107,
                    0
                ],
                "title": "Machine Learning Decoding of Circuit-Level Noise for Bivariate Bicycle\n  Codes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine Learning Decoding of Circuit-Level Noise for Bivariate Bicycle\n  Codes"
                },
                "summary": "Fault-tolerant quantum computers will depend crucially on the performance of\nthe classical decoding algorithm which takes in the results of measurements and\noutputs corrections to the errors inferred to have occurred. Machine learning\nmodels have shown great promise as decoders for the surface code; however, this\npromise has not yet been substantiated for the more challenging task of\ndecoding quantum low-density parity-check (QLDPC) codes. In this paper, we\npresent a recurrent, transformer-based neural network designed to decode\ncircuit-level noise on Bivariate Bicycle (BB) codes, introduced recently by\nBravyi et al (Nature 627, 778-782, 2024). For the $[[72,12,6]]$ BB code, at a\nphysical error rate of $p=0.1\\%$, our model achieves a logical error rate\nalmost $5$ times lower than belief propagation with ordered statistics decoding\n(BP-OSD). Moreover, while BP-OSD has a wide distribution of runtimes with\nsignificant outliers, our model has a consistent runtime and is an\norder-of-magnitude faster than the worst-case times from a benchmark BP-OSD\nimplementation. On the $[[144,12,12]]$ BB code, our model obtains worse logical\nerror rates but maintains the speed advantage. These results demonstrate that\nmachine learning decoders can out-perform conventional decoders on QLDPC codes,\nin regimes of current interest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fault-tolerant quantum computers will depend crucially on the performance of\nthe classical decoding algorithm which takes in the results of measurements and\noutputs corrections to the errors inferred to have occurred. Machine learning\nmodels have shown great promise as decoders for the surface code; however, this\npromise has not yet been substantiated for the more challenging task of\ndecoding quantum low-density parity-check (QLDPC) codes. In this paper, we\npresent a recurrent, transformer-based neural network designed to decode\ncircuit-level noise on Bivariate Bicycle (BB) codes, introduced recently by\nBravyi et al (Nature 627, 778-782, 2024). For the $[[72,12,6]]$ BB code, at a\nphysical error rate of $p=0.1\\%$, our model achieves a logical error rate\nalmost $5$ times lower than belief propagation with ordered statistics decoding\n(BP-OSD). Moreover, while BP-OSD has a wide distribution of runtimes with\nsignificant outliers, our model has a consistent runtime and is an\norder-of-magnitude faster than the worst-case times from a benchmark BP-OSD\nimplementation. On the $[[144,12,12]]$ BB code, our model obtains worse logical\nerror rates but maintains the speed advantage. These results demonstrate that\nmachine learning decoders can out-perform conventional decoders on QLDPC codes,\nin regimes of current interest."
                },
                "authors": [
                    {
                        "name": "John Blue"
                    },
                    {
                        "name": "Harshil Avlani"
                    },
                    {
                        "name": "Zhiyang He"
                    },
                    {
                        "name": "Liu Ziyin"
                    },
                    {
                        "name": "Isaac L. Chuang"
                    }
                ],
                "author_detail": {
                    "name": "Isaac L. Chuang"
                },
                "author": "Isaac L. Chuang",
                "arxiv_comment": "19 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13043v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13043v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13039v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13039v1",
                "updated": "2025-04-17T15:52:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    15,
                    52,
                    23,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T15:52:23Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    15,
                    52,
                    23,
                    3,
                    107,
                    0
                ],
                "title": "Evidence for sulfur chemistry in the atmosphere of the warm sub-Neptune\n  TOI-270 d",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evidence for sulfur chemistry in the atmosphere of the warm sub-Neptune\n  TOI-270 d"
                },
                "summary": "Context: Recent JWST measurements allow access to the near-infrared spectrum\nof the sub-Neptune TOI-270 d, for which two different interpretations, a\nhigh-metallicity miscible envelope and a lower metallicity hycean world, are\ncurrently in conflict. Aims: Here, we reanalyze the published data and\nreproduce previously retrieved molecular abundances based on an independent\ndata reduction and a different retrieval framework. The aim of this study is to\nrefine the understanding of TOI-270 d and highlight considerations for JWST\ndata analysis. Additionally, we test the impact of data resolution on\natmospheric retrieval calculations. Methods: We reduce one JWST NIRSpec G395H\nand one NIRISS SOSS GR700XD transit dataset using the Eureka! pipeline and a\ncustom MCMC-based light curve fitting algorithm at the instruments' native\nresolutions. The atmospheric composition is estimated with the updated BeAR\nretrieval code across a grid of retrieval setups and spectral resolutions.\nResults: Our transit spectrum is consistent with previous studies, except at\nthe red end of the NIRISS data. Our retrievals support a higher mean molecular\nweight atmosphere for TOI-270 d. We provide refined abundance constraints and\nfind statistically favored model extensions indicating either sulfur-rich\nchemistry with species such as CS2, CS, and H2CS, or the possible presence of\nCH3Cl or CH3F. However, Bayesian inference cannot distinguish between these\nscenarios due to similar opacities below 4 microns. Conclusions: Our analysis\nreinforces TOI-270 d as a highly interesting warm sub-Neptune for atmospheric\nstudies, with a complex chemistry in a cloud-free upper atmosphere. However,\nits exact nature remains uncertain and warrants further detailed photochemical\nmodeling and observations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: Recent JWST measurements allow access to the near-infrared spectrum\nof the sub-Neptune TOI-270 d, for which two different interpretations, a\nhigh-metallicity miscible envelope and a lower metallicity hycean world, are\ncurrently in conflict. Aims: Here, we reanalyze the published data and\nreproduce previously retrieved molecular abundances based on an independent\ndata reduction and a different retrieval framework. The aim of this study is to\nrefine the understanding of TOI-270 d and highlight considerations for JWST\ndata analysis. Additionally, we test the impact of data resolution on\natmospheric retrieval calculations. Methods: We reduce one JWST NIRSpec G395H\nand one NIRISS SOSS GR700XD transit dataset using the Eureka! pipeline and a\ncustom MCMC-based light curve fitting algorithm at the instruments' native\nresolutions. The atmospheric composition is estimated with the updated BeAR\nretrieval code across a grid of retrieval setups and spectral resolutions.\nResults: Our transit spectrum is consistent with previous studies, except at\nthe red end of the NIRISS data. Our retrievals support a higher mean molecular\nweight atmosphere for TOI-270 d. We provide refined abundance constraints and\nfind statistically favored model extensions indicating either sulfur-rich\nchemistry with species such as CS2, CS, and H2CS, or the possible presence of\nCH3Cl or CH3F. However, Bayesian inference cannot distinguish between these\nscenarios due to similar opacities below 4 microns. Conclusions: Our analysis\nreinforces TOI-270 d as a highly interesting warm sub-Neptune for atmospheric\nstudies, with a complex chemistry in a cloud-free upper atmosphere. However,\nits exact nature remains uncertain and warrants further detailed photochemical\nmodeling and observations."
                },
                "authors": [
                    {
                        "name": "Lukas Felix"
                    },
                    {
                        "name": "Daniel Kitzmann"
                    },
                    {
                        "name": "Brice-Olivier Demory"
                    },
                    {
                        "name": "Christoph Mordasini"
                    }
                ],
                "author_detail": {
                    "name": "Christoph Mordasini"
                },
                "author": "Christoph Mordasini",
                "arxiv_comment": "Submitted to A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13039v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13039v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13038v1",
                "updated": "2025-04-17T15:51:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    15,
                    51,
                    59,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T15:51:59Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    15,
                    51,
                    59,
                    3,
                    107,
                    0
                ],
                "title": "How Large Language Models Are Changing MOOC Essay Answers: A Comparison\n  of Pre- and Post-LLM Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Large Language Models Are Changing MOOC Essay Answers: A Comparison\n  of Pre- and Post-LLM Responses"
                },
                "summary": "The release of ChatGPT in late 2022 caused a flurry of activity and concern\nin the academic and educational communities. Some see the tool's ability to\ngenerate human-like text that passes at least cursory inspections for factual\naccuracy ``often enough'' a golden age of information retrieval and\ncomputer-assisted learning. Some, on the other hand, worry the tool may lead to\nunprecedented levels of academic dishonesty and cheating. In this work, we\nquantify some of the effects of the emergence of Large Language Models (LLMs)\non online education by analyzing a multi-year dataset of student essay\nresponses from a free university-level MOOC on AI ethics. Our dataset includes\nessays submitted both before and after ChatGPT's release. We find that the\nlaunch of ChatGPT coincided with significant changes in both the length and\nstyle of student essays, mirroring observations in other contexts such as\nacademic publishing. We also observe -- as expected based on related public\ndiscourse -- changes in prevalence of key content words related to AI and LLMs,\nbut not necessarily the general themes or topics discussed in the student\nessays as identified through (dynamic) topic modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The release of ChatGPT in late 2022 caused a flurry of activity and concern\nin the academic and educational communities. Some see the tool's ability to\ngenerate human-like text that passes at least cursory inspections for factual\naccuracy ``often enough'' a golden age of information retrieval and\ncomputer-assisted learning. Some, on the other hand, worry the tool may lead to\nunprecedented levels of academic dishonesty and cheating. In this work, we\nquantify some of the effects of the emergence of Large Language Models (LLMs)\non online education by analyzing a multi-year dataset of student essay\nresponses from a free university-level MOOC on AI ethics. Our dataset includes\nessays submitted both before and after ChatGPT's release. We find that the\nlaunch of ChatGPT coincided with significant changes in both the length and\nstyle of student essays, mirroring observations in other contexts such as\nacademic publishing. We also observe -- as expected based on related public\ndiscourse -- changes in prevalence of key content words related to AI and LLMs,\nbut not necessarily the general themes or topics discussed in the student\nessays as identified through (dynamic) topic modeling."
                },
                "authors": [
                    {
                        "name": "Leo Leppänen"
                    },
                    {
                        "name": "Lili Aunimo"
                    },
                    {
                        "name": "Arto Hellas"
                    },
                    {
                        "name": "Jukka K. Nurminen"
                    },
                    {
                        "name": "Linda Mannila"
                    }
                ],
                "author_detail": {
                    "name": "Linda Mannila"
                },
                "author": "Linda Mannila",
                "arxiv_comment": "10 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.3.1; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13034v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13034v1",
                "updated": "2025-04-17T15:42:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    15,
                    42,
                    13,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T15:42:13Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    15,
                    42,
                    13,
                    3,
                    107,
                    0
                ],
                "title": "Inference-friendly Graph Compression for Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-friendly Graph Compression for Graph Neural Networks"
                },
                "summary": "Graph Neural Networks (GNNs) have demonstrated promising performance in graph\nanalysis. Nevertheless, the inference process of GNNs remains costly, hindering\ntheir applications for large graphs. This paper proposes inference-friendly\ngraph compression (IFGC), a graph compression scheme to accelerate GNNs\ninference. Given a graph $G$ and a GNN $M$, an IFGC computes a small compressed\ngraph $G_c$, to best preserve the inference results of $M$ over $G$, such that\nthe result can be directly inferred by accessing $G_c$ with no or little\ndecompression cost. (1) We characterize IFGC with a class of inference\nequivalence relation. The relation captures the node pairs in $G$ that are not\ndistinguishable for GNN inference. (2) We introduce three practical\nspecifications of IFGC for representative GNNs: structural preserving\ncompression (SPGC), which computes $G_c$ that can be directly processed by GNN\ninference without decompression; ($\\alpha$, $r$)-compression, that allows for a\nconfigurable trade-off between compression ratio and inference quality, and\nanchored compression that preserves inference results for specific nodes of\ninterest. For each scheme, we introduce compression and inference algorithms\nwith guarantees of efficiency and quality of the inferred results. We conduct\nextensive experiments on diverse sets of large-scale graphs, which verifies the\neffectiveness and efficiency of our graph compression approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have demonstrated promising performance in graph\nanalysis. Nevertheless, the inference process of GNNs remains costly, hindering\ntheir applications for large graphs. This paper proposes inference-friendly\ngraph compression (IFGC), a graph compression scheme to accelerate GNNs\ninference. Given a graph $G$ and a GNN $M$, an IFGC computes a small compressed\ngraph $G_c$, to best preserve the inference results of $M$ over $G$, such that\nthe result can be directly inferred by accessing $G_c$ with no or little\ndecompression cost. (1) We characterize IFGC with a class of inference\nequivalence relation. The relation captures the node pairs in $G$ that are not\ndistinguishable for GNN inference. (2) We introduce three practical\nspecifications of IFGC for representative GNNs: structural preserving\ncompression (SPGC), which computes $G_c$ that can be directly processed by GNN\ninference without decompression; ($\\alpha$, $r$)-compression, that allows for a\nconfigurable trade-off between compression ratio and inference quality, and\nanchored compression that preserves inference results for specific nodes of\ninterest. For each scheme, we introduce compression and inference algorithms\nwith guarantees of efficiency and quality of the inferred results. We conduct\nextensive experiments on diverse sets of large-scale graphs, which verifies the\neffectiveness and efficiency of our graph compression approaches."
                },
                "authors": [
                    {
                        "name": "Yangxin Fan"
                    },
                    {
                        "name": "Haolai Che"
                    },
                    {
                        "name": "Yinghui Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yinghui Wu"
                },
                "author": "Yinghui Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13034v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13034v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13032v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13032v1",
                "updated": "2025-04-17T15:41:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    15,
                    41,
                    39,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T15:41:39Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    15,
                    41,
                    39,
                    3,
                    107,
                    0
                ],
                "title": "InstructRAG: Leveraging Retrieval-Augmented Generation on Instruction\n  Graphs for LLM-Based Task Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstructRAG: Leveraging Retrieval-Augmented Generation on Instruction\n  Graphs for LLM-Based Task Planning"
                },
                "summary": "Recent advancements in large language models (LLMs) have enabled their use as\nagents for planning complex tasks. Existing methods typically rely on a\nthought-action-observation (TAO) process to enhance LLM performance, but these\napproaches are often constrained by the LLMs' limited knowledge of complex\ntasks. Retrieval-augmented generation (RAG) offers new opportunities by\nleveraging external databases to ground generation in retrieved information. In\nthis paper, we identify two key challenges (enlargability and transferability)\nin applying RAG to task planning. We propose InstructRAG, a novel solution\nwithin a multi-agent meta-reinforcement learning framework, to address these\nchallenges. InstructRAG includes a graph to organize past instruction paths\n(sequences of correct actions), an RL-Agent with Reinforcement Learning to\nexpand graph coverage for enlargability, and an ML-Agent with Meta-Learning to\nimprove task generalization for transferability. The two agents are trained\nend-to-end to optimize overall planning performance. Our experiments on four\nwidely used task planning datasets demonstrate that InstructRAG significantly\nenhances performance and adapts efficiently to new tasks, achieving up to a\n19.2% improvement over the best existing approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have enabled their use as\nagents for planning complex tasks. Existing methods typically rely on a\nthought-action-observation (TAO) process to enhance LLM performance, but these\napproaches are often constrained by the LLMs' limited knowledge of complex\ntasks. Retrieval-augmented generation (RAG) offers new opportunities by\nleveraging external databases to ground generation in retrieved information. In\nthis paper, we identify two key challenges (enlargability and transferability)\nin applying RAG to task planning. We propose InstructRAG, a novel solution\nwithin a multi-agent meta-reinforcement learning framework, to address these\nchallenges. InstructRAG includes a graph to organize past instruction paths\n(sequences of correct actions), an RL-Agent with Reinforcement Learning to\nexpand graph coverage for enlargability, and an ML-Agent with Meta-Learning to\nimprove task generalization for transferability. The two agents are trained\nend-to-end to optimize overall planning performance. Our experiments on four\nwidely used task planning datasets demonstrate that InstructRAG significantly\nenhances performance and adapts efficiently to new tasks, achieving up to a\n19.2% improvement over the best existing approach."
                },
                "authors": [
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Shu Xian Teo"
                    },
                    {
                        "name": "Jun Jie Chew"
                    },
                    {
                        "name": "Wei Shi"
                    }
                ],
                "author_detail": {
                    "name": "Wei Shi"
                },
                "author": "Wei Shi",
                "arxiv_comment": "This paper has been accepted by SIGIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13032v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13032v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13023v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13023v1",
                "updated": "2025-04-17T15:33:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    15,
                    33,
                    17,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T15:33:17Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    15,
                    33,
                    17,
                    3,
                    107,
                    0
                ],
                "title": "ChatEXAONEPath: An Expert-level Multimodal Large Language Model for\n  Histopathology Using Whole Slide Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatEXAONEPath: An Expert-level Multimodal Large Language Model for\n  Histopathology Using Whole Slide Images"
                },
                "summary": "Recent studies have made significant progress in developing large language\nmodels (LLMs) in the medical domain, which can answer expert-level questions\nand demonstrate the potential to assist clinicians in real-world clinical\nscenarios. Studies have also witnessed the importance of integrating various\nmodalities with the existing LLMs for a better understanding of complex\nclinical contexts, which are innately multi-faceted by nature. Although studies\nhave demonstrated the ability of multimodal LLMs in histopathology to answer\nquestions from given images, they lack in understanding of thorough clinical\ncontext due to the patch-level data with limited information from public\ndatasets. Thus, developing WSI-level MLLMs is significant in terms of the\nscalability and applicability of MLLMs in histopathology. In this study, we\nintroduce an expert-level MLLM for histopathology using WSIs, dubbed as\nChatEXAONEPath. We present a retrieval-based data generation pipeline using\n10,094 pairs of WSIs and histopathology reports from The Cancer Genome Atlas\n(TCGA). We also showcase an AI-based evaluation protocol for a comprehensive\nunderstanding of the medical context from given multimodal information and\nevaluate generated answers compared to the original histopathology reports. We\ndemonstrate the ability of diagnosing the given histopathology images using\nChatEXAONEPath with the acceptance rate of 62.9% from 1,134 pairs of WSIs and\nreports. Our proposed model can understand pan-cancer WSIs and clinical context\nfrom various cancer types. We argue that our proposed model has the potential\nto assist clinicians by comprehensively understanding complex morphology of\nWSIs for cancer diagnosis through the integration of multiple modalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have made significant progress in developing large language\nmodels (LLMs) in the medical domain, which can answer expert-level questions\nand demonstrate the potential to assist clinicians in real-world clinical\nscenarios. Studies have also witnessed the importance of integrating various\nmodalities with the existing LLMs for a better understanding of complex\nclinical contexts, which are innately multi-faceted by nature. Although studies\nhave demonstrated the ability of multimodal LLMs in histopathology to answer\nquestions from given images, they lack in understanding of thorough clinical\ncontext due to the patch-level data with limited information from public\ndatasets. Thus, developing WSI-level MLLMs is significant in terms of the\nscalability and applicability of MLLMs in histopathology. In this study, we\nintroduce an expert-level MLLM for histopathology using WSIs, dubbed as\nChatEXAONEPath. We present a retrieval-based data generation pipeline using\n10,094 pairs of WSIs and histopathology reports from The Cancer Genome Atlas\n(TCGA). We also showcase an AI-based evaluation protocol for a comprehensive\nunderstanding of the medical context from given multimodal information and\nevaluate generated answers compared to the original histopathology reports. We\ndemonstrate the ability of diagnosing the given histopathology images using\nChatEXAONEPath with the acceptance rate of 62.9% from 1,134 pairs of WSIs and\nreports. Our proposed model can understand pan-cancer WSIs and clinical context\nfrom various cancer types. We argue that our proposed model has the potential\nto assist clinicians by comprehensively understanding complex morphology of\nWSIs for cancer diagnosis through the integration of multiple modalities."
                },
                "authors": [
                    {
                        "name": "Sangwook Kim"
                    },
                    {
                        "name": "Soonyoung Lee"
                    },
                    {
                        "name": "Jongseong Jang"
                    }
                ],
                "author_detail": {
                    "name": "Jongseong Jang"
                },
                "author": "Jongseong Jang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13023v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13023v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19325v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19325v2",
                "updated": "2025-04-17T15:26:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    15,
                    26,
                    4,
                    3,
                    107,
                    0
                ],
                "published": "2025-03-25T03:38:06Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    3,
                    38,
                    6,
                    1,
                    84,
                    0
                ],
                "title": "Long-Context Autoregressive Video Modeling with Next-Frame Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Context Autoregressive Video Modeling with Next-Frame Prediction"
                },
                "summary": "Long-context autoregressive modeling has significantly advanced language\ngeneration, but video generation still struggles to fully utilize extended\ntemporal contexts. To investigate long-context video modeling, we introduce\nFrame AutoRegressive (FAR), a strong baseline for video autoregressive\nmodeling. Just as language models learn causal dependencies between tokens\n(i.e., Token AR), FAR models temporal causal dependencies between continuous\nframes, achieving better convergence than Token AR and video diffusion\ntransformers. Building on FAR, we observe that long-context video modeling\nfaces challenges due to visual redundancy. Training on long videos is\ncomputationally expensive, as vision tokens grow much faster than language\ntokens. To tackle this issue, we propose balancing locality and long-range\ndependency through long short-term context modeling. A high-resolution\nshort-term context window ensures fine-grained temporal consistency, while an\nunlimited long-term context window encodes long-range information using fewer\ntokens. With this approach, we can train on long video sequences with a\nmanageable token context length, thereby significantly reducing training time\nand memory usage. Furthermore, we propose a multi-level KV cache designed to\nsupport the long short-term context modeling, which accelerating inference on\nlong video sequences. We demonstrate that FAR achieves state-of-the-art\nperformance in both short- and long-video generation, providing a simple yet\neffective baseline for video autoregressive modeling. The code is released at\nhttps://github.com/showlab/FAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context autoregressive modeling has significantly advanced language\ngeneration, but video generation still struggles to fully utilize extended\ntemporal contexts. To investigate long-context video modeling, we introduce\nFrame AutoRegressive (FAR), a strong baseline for video autoregressive\nmodeling. Just as language models learn causal dependencies between tokens\n(i.e., Token AR), FAR models temporal causal dependencies between continuous\nframes, achieving better convergence than Token AR and video diffusion\ntransformers. Building on FAR, we observe that long-context video modeling\nfaces challenges due to visual redundancy. Training on long videos is\ncomputationally expensive, as vision tokens grow much faster than language\ntokens. To tackle this issue, we propose balancing locality and long-range\ndependency through long short-term context modeling. A high-resolution\nshort-term context window ensures fine-grained temporal consistency, while an\nunlimited long-term context window encodes long-range information using fewer\ntokens. With this approach, we can train on long video sequences with a\nmanageable token context length, thereby significantly reducing training time\nand memory usage. Furthermore, we propose a multi-level KV cache designed to\nsupport the long short-term context modeling, which accelerating inference on\nlong video sequences. We demonstrate that FAR achieves state-of-the-art\nperformance in both short- and long-video generation, providing a simple yet\neffective baseline for video autoregressive modeling. The code is released at\nhttps://github.com/showlab/FAR."
                },
                "authors": [
                    {
                        "name": "Yuchao Gu"
                    },
                    {
                        "name": "Weijia Mao"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "arxiv_comment": "Project page at https://farlongctx.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19325v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19325v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11672v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11672v3",
                "updated": "2025-04-17T15:10:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    15,
                    10,
                    52,
                    3,
                    107,
                    0
                ],
                "published": "2024-04-17T18:13:16Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    18,
                    13,
                    16,
                    2,
                    108,
                    0
                ],
                "title": "MemLLM: Finetuning LLMs to Use An Explicit Read-Write Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemLLM: Finetuning LLMs to Use An Explicit Read-Write Memory"
                },
                "summary": "While current large language models (LLMs) perform well on many\nknowledge-related tasks, they are limited by relying on their parameters as an\nimplicit storage mechanism. As a result, they struggle with memorizing rare\nevents and with updating their memory as facts change over time. In addition,\nthe uninterpretable nature of parametric memory makes it challenging to prevent\nhallucination. Model editing and augmenting LLMs with parameters specialized\nfor memory are only partial solutions. In this paper, we introduce MemLLM, a\nnovel method of enhancing LLMs by integrating a structured and explicit\nread-and-write memory module. MemLLM tackles the aforementioned challenges by\nenabling dynamic interaction with the memory and improving the LLM's\ncapabilities in using stored knowledge. Our experiments indicate that MemLLM\nenhances the LLM's performance and interpretability, in language modeling in\ngeneral and knowledge-intensive tasks in particular. We see MemLLM as an\nimportant step towards making LLMs more grounded and factual through memory\naugmentation. The project repository is publicly available at\nhttps://github.com/amodaresi/MemLLM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While current large language models (LLMs) perform well on many\nknowledge-related tasks, they are limited by relying on their parameters as an\nimplicit storage mechanism. As a result, they struggle with memorizing rare\nevents and with updating their memory as facts change over time. In addition,\nthe uninterpretable nature of parametric memory makes it challenging to prevent\nhallucination. Model editing and augmenting LLMs with parameters specialized\nfor memory are only partial solutions. In this paper, we introduce MemLLM, a\nnovel method of enhancing LLMs by integrating a structured and explicit\nread-and-write memory module. MemLLM tackles the aforementioned challenges by\nenabling dynamic interaction with the memory and improving the LLM's\ncapabilities in using stored knowledge. Our experiments indicate that MemLLM\nenhances the LLM's performance and interpretability, in language modeling in\ngeneral and knowledge-intensive tasks in particular. We see MemLLM as an\nimportant step towards making LLMs more grounded and factual through memory\naugmentation. The project repository is publicly available at\nhttps://github.com/amodaresi/MemLLM"
                },
                "authors": [
                    {
                        "name": "Ali Modarressi"
                    },
                    {
                        "name": "Abdullatif Köksal"
                    },
                    {
                        "name": "Ayyoob Imani"
                    },
                    {
                        "name": "Mohsen Fayyaz"
                    },
                    {
                        "name": "Hinrich Schütze"
                    }
                ],
                "author_detail": {
                    "name": "Hinrich Schütze"
                },
                "author": "Hinrich Schütze",
                "arxiv_comment": "Published in Transactions on Machine Learning Research (TMLR)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11672v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11672v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12996v1",
                "updated": "2025-04-17T15:05:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    15,
                    5,
                    40,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T15:05:40Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    15,
                    5,
                    40,
                    3,
                    107,
                    0
                ],
                "title": "SHA256 at SemEval-2025 Task 4: Selective Amnesia -- Constrained\n  Unlearning for Large Language Models via Knowledge Isolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SHA256 at SemEval-2025 Task 4: Selective Amnesia -- Constrained\n  Unlearning for Large Language Models via Knowledge Isolation"
                },
                "summary": "Large language models (LLMs) frequently memorize sensitive information during\ntraining, posing risks when deploying publicly accessible models. Current\nmachine unlearning methods struggle to selectively remove specific data\nassociations without degrading overall model capabilities. This paper presents\nour solution to SemEval-2025 Task 4 on targeted unlearning, which introduces a\ntwo-stage methodology that combines causal mediation analysis with\nlayer-specific optimization. Through systematic causal tracing experiments on\nOLMo architectures (1B and 7B parameters), we identify the critical role of the\nfirst few transformer layers (layers 0-5) in storing subject-attribute\nassociations within MLP modules. Building on this insight, we develop a\nconstrained optimization approach that freezes upper layers while applying a\nnovel joint loss function to lower layers-simultaneously maximizing forget set\nloss via output token cross-entropy penalties and minimizing retain set\ndeviation through adaptive regularization. Our method achieves 2nd place in the\n1B model track, demonstrating strong task performance while maintaining 88% of\nbaseline MMLU accuracy. These results establish causal-informed layer\noptimization as a promising paradigm for efficient, precise unlearning in LLMs,\noffering a significant step forward in addressing data privacy concerns in AI\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) frequently memorize sensitive information during\ntraining, posing risks when deploying publicly accessible models. Current\nmachine unlearning methods struggle to selectively remove specific data\nassociations without degrading overall model capabilities. This paper presents\nour solution to SemEval-2025 Task 4 on targeted unlearning, which introduces a\ntwo-stage methodology that combines causal mediation analysis with\nlayer-specific optimization. Through systematic causal tracing experiments on\nOLMo architectures (1B and 7B parameters), we identify the critical role of the\nfirst few transformer layers (layers 0-5) in storing subject-attribute\nassociations within MLP modules. Building on this insight, we develop a\nconstrained optimization approach that freezes upper layers while applying a\nnovel joint loss function to lower layers-simultaneously maximizing forget set\nloss via output token cross-entropy penalties and minimizing retain set\ndeviation through adaptive regularization. Our method achieves 2nd place in the\n1B model track, demonstrating strong task performance while maintaining 88% of\nbaseline MMLU accuracy. These results establish causal-informed layer\noptimization as a promising paradigm for efficient, precise unlearning in LLMs,\noffering a significant step forward in addressing data privacy concerns in AI\nsystems."
                },
                "authors": [
                    {
                        "name": "Saransh Agrawal"
                    },
                    {
                        "name": "Kuan-Hao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kuan-Hao Huang"
                },
                "author": "Kuan-Hao Huang",
                "arxiv_comment": "8 pages, In Proceedings of The 19th International Workshop on\n  Semantic Evaluation (SemEval), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12991v1",
                "updated": "2025-04-17T14:59:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    14,
                    59,
                    29,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T14:59:29Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    14,
                    59,
                    29,
                    3,
                    107,
                    0
                ],
                "title": "Chain-of-Thought Prompting for Out-of-Distribution Samples: A\n  Latent-Variable Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought Prompting for Out-of-Distribution Samples: A\n  Latent-Variable Study"
                },
                "summary": "Chain-of-Thought (CoT) prompting has emerged as a powerful technique to\nimprove in-context learning (ICL) in large language models (LLMs) by breaking\ncomplex reasoning into intermediate steps. However, the ability of CoT to\ngeneralize under distribution shift remains poorly understood. In this work, we\nextend a latent-variable framework for CoT prompting and study its behavior on\ntwo prototypical out-of-distribution (OOD) scenarios: (i) the latent variables\nfor CoT steps are permuted into novel combinations, and (ii) the latent\nvariables uniformly scaled by a factor. Our experiments demonstrate that CoT\ninference generalizes effectively to OOD samples whose latent variables closely\nresemble those seen during training, but its performance degrades as this\nsimilarity decreases. These findings provide foundational insights into the\nstrengths and limitations of CoT prompting under OOD conditions and suggest\ndirections for developing more resilient reasoning strategies in future LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) prompting has emerged as a powerful technique to\nimprove in-context learning (ICL) in large language models (LLMs) by breaking\ncomplex reasoning into intermediate steps. However, the ability of CoT to\ngeneralize under distribution shift remains poorly understood. In this work, we\nextend a latent-variable framework for CoT prompting and study its behavior on\ntwo prototypical out-of-distribution (OOD) scenarios: (i) the latent variables\nfor CoT steps are permuted into novel combinations, and (ii) the latent\nvariables uniformly scaled by a factor. Our experiments demonstrate that CoT\ninference generalizes effectively to OOD samples whose latent variables closely\nresemble those seen during training, but its performance degrades as this\nsimilarity decreases. These findings provide foundational insights into the\nstrengths and limitations of CoT prompting under OOD conditions and suggest\ndirections for developing more resilient reasoning strategies in future LLMs."
                },
                "authors": [
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Fu-Chieh Chang"
                    },
                    {
                        "name": "Pei-Yuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Pei-Yuan Wu"
                },
                "author": "Pei-Yuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11061v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11061v2",
                "updated": "2025-04-17T14:53:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    14,
                    53,
                    45,
                    3,
                    107,
                    0
                ],
                "published": "2024-07-10T16:05:43Z",
                "published_parsed": [
                    2024,
                    7,
                    10,
                    16,
                    5,
                    43,
                    2,
                    192,
                    0
                ],
                "title": "Exploring the Boundaries of On-Device Inference: When Tiny Falls Short,\n  Go Hierarchical",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Boundaries of On-Device Inference: When Tiny Falls Short,\n  Go Hierarchical"
                },
                "summary": "On-device inference holds great potential for increased energy efficiency,\nresponsiveness, and privacy in edge ML systems. However, due to less capable ML\nmodels that can be embedded in resource-limited devices, use cases are limited\nto simple inference tasks such as visual keyword spotting, gesture recognition,\nand predictive analytics. In this context, the Hierarchical Inference (HI)\nsystem has emerged as a promising solution that augments the capabilities of\nthe local ML by offloading selected samples to an edge server or cloud for\nremote ML inference. Existing works demonstrate through simulation that HI\nimproves accuracy. However, they do not account for the latency and energy\nconsumption on the device, nor do they consider three key heterogeneous\ndimensions that characterize ML systems: hardware, network connectivity, and\nmodels. In contrast, this paper systematically compares the performance of HI\nwith on-device inference based on measurements of accuracy, latency, and energy\nfor running embedded ML models on five devices with different capabilities and\nthree image classification datasets. For a given accuracy requirement, the HI\nsystems we designed achieved up to 73% lower latency and up to 77% lower device\nenergy consumption than an on-device inference system. The key to building an\nefficient HI system is the availability of small-size, reasonably accurate\non-device models whose outputs can be effectively differentiated for samples\nthat require remote inference. Despite the performance gains, HI requires\non-device inference for all samples, which adds a fixed overhead to its latency\nand energy consumption. Therefore, we design a hybrid system, Early Exit with\nHI (EE-HI), and demonstrate that compared to HI, EE-HI reduces the latency by\nup to 59.7% and lowers the device's energy consumption by up to 60.4%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-device inference holds great potential for increased energy efficiency,\nresponsiveness, and privacy in edge ML systems. However, due to less capable ML\nmodels that can be embedded in resource-limited devices, use cases are limited\nto simple inference tasks such as visual keyword spotting, gesture recognition,\nand predictive analytics. In this context, the Hierarchical Inference (HI)\nsystem has emerged as a promising solution that augments the capabilities of\nthe local ML by offloading selected samples to an edge server or cloud for\nremote ML inference. Existing works demonstrate through simulation that HI\nimproves accuracy. However, they do not account for the latency and energy\nconsumption on the device, nor do they consider three key heterogeneous\ndimensions that characterize ML systems: hardware, network connectivity, and\nmodels. In contrast, this paper systematically compares the performance of HI\nwith on-device inference based on measurements of accuracy, latency, and energy\nfor running embedded ML models on five devices with different capabilities and\nthree image classification datasets. For a given accuracy requirement, the HI\nsystems we designed achieved up to 73% lower latency and up to 77% lower device\nenergy consumption than an on-device inference system. The key to building an\nefficient HI system is the availability of small-size, reasonably accurate\non-device models whose outputs can be effectively differentiated for samples\nthat require remote inference. Despite the performance gains, HI requires\non-device inference for all samples, which adds a fixed overhead to its latency\nand energy consumption. Therefore, we design a hybrid system, Early Exit with\nHI (EE-HI), and demonstrate that compared to HI, EE-HI reduces the latency by\nup to 59.7% and lowers the device's energy consumption by up to 60.4%."
                },
                "authors": [
                    {
                        "name": "Adarsh Prasad Behera"
                    },
                    {
                        "name": "Paulius Daubaris"
                    },
                    {
                        "name": "Iñaki Bravo"
                    },
                    {
                        "name": "José Gallego"
                    },
                    {
                        "name": "Roberto Morabito"
                    },
                    {
                        "name": "Joerg Widmer"
                    },
                    {
                        "name": "Jaya Prakash Varma Champati"
                    }
                ],
                "author_detail": {
                    "name": "Jaya Prakash Varma Champati"
                },
                "author": "Jaya Prakash Varma Champati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11061v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11061v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12984v1",
                "updated": "2025-04-17T14:45:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    14,
                    45,
                    3,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T14:45:03Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    14,
                    45,
                    3,
                    3,
                    107,
                    0
                ],
                "title": "A Virtual Machine for Arbitrary Low-Precision GPGPU Computation in LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Virtual Machine for Arbitrary Low-Precision GPGPU Computation in LLM\n  Serving"
                },
                "summary": "Serving Large Language Models (LLMs) is critical for AI-powered applications\nbut demands substantial computational resources, particularly in memory\nbandwidth and computational throughput. Low-precision computation has emerged\nas a key technique to improve efficiency while reducing resource consumption.\nExisting approaches for generating low-precision kernels are limited to weight\nbit widths that are powers of two and suffer from suboptimal performance due to\nhigh-level GPU programming abstractions. These abstractions restrict critical\noptimizations, such as fine-grained register management and optimized memory\naccess patterns, which are essential for efficient low-precision computations.\nIn this paper, we introduce a virtual machine (VM) designed for General-Purpose\nGPU (GPGPU) computing, enabling support for low-precision data types with\narbitrary bit widths while maintaining GPU programmability. The proposed VM\nfeatures a thread-block-level programming model, a hierarchical memory space, a\nnovel algebraic layout system, and extensive support for diverse low-precision\ndata types. VM programs are compiled into highly efficient GPU programs with\nautomatic vectorization and instruction selection. Extensive experiments\ndemonstrate that our VM efficiently supports a full spectrum of low-precision\ndata types, and outperforms state-of-the-art low-precision kernels on their\nsupported types. Compared to existing compilers like Triton and Ladder, as well\nas hand-optimized kernels such as QuantLLM and Marlin, our VM achieves\nperformance improvements of 1.75x, 2.61x, 1.29x and 1.03x, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Large Language Models (LLMs) is critical for AI-powered applications\nbut demands substantial computational resources, particularly in memory\nbandwidth and computational throughput. Low-precision computation has emerged\nas a key technique to improve efficiency while reducing resource consumption.\nExisting approaches for generating low-precision kernels are limited to weight\nbit widths that are powers of two and suffer from suboptimal performance due to\nhigh-level GPU programming abstractions. These abstractions restrict critical\noptimizations, such as fine-grained register management and optimized memory\naccess patterns, which are essential for efficient low-precision computations.\nIn this paper, we introduce a virtual machine (VM) designed for General-Purpose\nGPU (GPGPU) computing, enabling support for low-precision data types with\narbitrary bit widths while maintaining GPU programmability. The proposed VM\nfeatures a thread-block-level programming model, a hierarchical memory space, a\nnovel algebraic layout system, and extensive support for diverse low-precision\ndata types. VM programs are compiled into highly efficient GPU programs with\nautomatic vectorization and instruction selection. Extensive experiments\ndemonstrate that our VM efficiently supports a full spectrum of low-precision\ndata types, and outperforms state-of-the-art low-precision kernels on their\nsupported types. Compared to existing compilers like Triton and Ladder, as well\nas hand-optimized kernels such as QuantLLM and Marlin, our VM achieves\nperformance improvements of 1.75x, 2.61x, 1.29x and 1.03x, respectively."
                },
                "authors": [
                    {
                        "name": "Yaoyao Ding"
                    },
                    {
                        "name": "Bohan Hou"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Allan Lin"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Cody Yu Hao"
                    },
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Gennady Pekhimenko"
                    }
                ],
                "author_detail": {
                    "name": "Gennady Pekhimenko"
                },
                "author": "Gennady Pekhimenko",
                "arxiv_comment": "18 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12983v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12983v1",
                "updated": "2025-04-17T14:41:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    14,
                    41,
                    56,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T14:41:56Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    14,
                    41,
                    56,
                    3,
                    107,
                    0
                ],
                "title": "Adaptive Modeling of Correlated Noise in Space-Based Gravitational Wave\n  Detectors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Modeling of Correlated Noise in Space-Based Gravitational Wave\n  Detectors"
                },
                "summary": "Accurately estimating the statistical properties of noise is important in\nspace-based gravitational wave data analysis. Traditional methods often assume\nuncorrelated noise or impose restrictive parametric forms on cross-channel\ncorrelations, which could lead to biased estimation in complex instrumental\nnoise. This paper introduces a spline-based framework with trans-dimensional\nBayesian inference to reconstruct the full noise covariance matrix, including\nfrequency-dependent auto- and cross-power spectral densities, without prior\nassumptions on noise shapes. The developed software $\\mathtt{NOISAR}$ can\nrecover the features of the noise power spectrum curves with a relative error\n$\\leq 10\\%$ for both auto- and cross-one.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately estimating the statistical properties of noise is important in\nspace-based gravitational wave data analysis. Traditional methods often assume\nuncorrelated noise or impose restrictive parametric forms on cross-channel\ncorrelations, which could lead to biased estimation in complex instrumental\nnoise. This paper introduces a spline-based framework with trans-dimensional\nBayesian inference to reconstruct the full noise covariance matrix, including\nfrequency-dependent auto- and cross-power spectral densities, without prior\nassumptions on noise shapes. The developed software $\\mathtt{NOISAR}$ can\nrecover the features of the noise power spectrum curves with a relative error\n$\\leq 10\\%$ for both auto- and cross-one."
                },
                "authors": [
                    {
                        "name": "Ya-Nan Li"
                    },
                    {
                        "name": "Yi-Ming Hu"
                    },
                    {
                        "name": "En-Kun Li"
                    }
                ],
                "author_detail": {
                    "name": "En-Kun Li"
                },
                "author": "En-Kun Li",
                "arxiv_comment": "5 figures, submitted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12983v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12983v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12982v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12982v1",
                "updated": "2025-04-17T14:40:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    14,
                    40,
                    31,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T14:40:31Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    14,
                    40,
                    31,
                    3,
                    107,
                    0
                ],
                "title": "Accommodate Knowledge Conflicts in Retrieval-augmented LLMs: Towards\n  Reliable Response Generation in the Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accommodate Knowledge Conflicts in Retrieval-augmented LLMs: Towards\n  Reliable Response Generation in the Wild"
                },
                "summary": "The proliferation of large language models (LLMs) has significantly advanced\ninformation retrieval systems, particularly in response generation (RG).\nUnfortunately, LLMs often face knowledge conflicts between internal memory and\nretrievaled external information, arising from misinformation, biases, or\noutdated knowledge. These conflicts undermine response reliability and\nintroduce uncertainty in decision-making. In this work, we analyze how LLMs\nnavigate knowledge conflicts from an information-theoretic perspective and\nreveal that when conflicting and supplementary information exhibit significant\ndifferences, LLMs confidently resolve their preferences. However, when the\ndistinction is ambiguous, LLMs experience heightened uncertainty. Based on this\ninsight, we propose Swin-VIB, a novel framework that integrates a pipeline of\nvariational information bottleneck models into adaptive augmentation of\nretrieved information and guiding LLM preference in response generation.\nExtensive experiments on single-choice, open-ended question-answering (QA), and\nretrieval augmented generation (RAG) validate our theoretical findings and\ndemonstrate the efficacy of Swin-VIB. Notably, our method improves\nsingle-choice task accuracy by at least 7.54\\% over competitive baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of large language models (LLMs) has significantly advanced\ninformation retrieval systems, particularly in response generation (RG).\nUnfortunately, LLMs often face knowledge conflicts between internal memory and\nretrievaled external information, arising from misinformation, biases, or\noutdated knowledge. These conflicts undermine response reliability and\nintroduce uncertainty in decision-making. In this work, we analyze how LLMs\nnavigate knowledge conflicts from an information-theoretic perspective and\nreveal that when conflicting and supplementary information exhibit significant\ndifferences, LLMs confidently resolve their preferences. However, when the\ndistinction is ambiguous, LLMs experience heightened uncertainty. Based on this\ninsight, we propose Swin-VIB, a novel framework that integrates a pipeline of\nvariational information bottleneck models into adaptive augmentation of\nretrieved information and guiding LLM preference in response generation.\nExtensive experiments on single-choice, open-ended question-answering (QA), and\nretrieval augmented generation (RAG) validate our theoretical findings and\ndemonstrate the efficacy of Swin-VIB. Notably, our method improves\nsingle-choice task accuracy by at least 7.54\\% over competitive baselines."
                },
                "authors": [
                    {
                        "name": "Jiatai Wang"
                    },
                    {
                        "name": "Zhiwei Xu"
                    },
                    {
                        "name": "Di Jin"
                    },
                    {
                        "name": "Xuewen Yang"
                    },
                    {
                        "name": "Tao Li"
                    }
                ],
                "author_detail": {
                    "name": "Tao Li"
                },
                "author": "Tao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12982v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12976v1",
                "updated": "2025-04-17T14:29:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    14,
                    29,
                    18,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T14:29:18Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    14,
                    29,
                    18,
                    3,
                    107,
                    0
                ],
                "title": "Sparks of Science: Hypothesis Generation Using Structured Paper Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparks of Science: Hypothesis Generation Using Structured Paper Data"
                },
                "summary": "Generating novel and creative scientific hypotheses is a cornerstone in\nachieving Artificial General Intelligence. Large language and reasoning models\nhave the potential to aid in the systematic creation, selection, and validation\nof scientifically informed hypotheses. However, current foundation models often\nstruggle to produce scientific ideas that are both novel and feasible. One\nreason is the lack of a dedicated dataset that frames Scientific Hypothesis\nGeneration (SHG) as a Natural Language Generation (NLG) task. In this paper, we\nintroduce HypoGen, the first dataset of approximately 5500 structured\nproblem-hypothesis pairs extracted from top-tier computer science conferences\nstructured with a Bit-Flip-Spark schema, where the Bit is the conventional\nassumption, the Spark is the key insight or conceptual leap, and the Flip is\nthe resulting counterproposal. HypoGen uniquely integrates an explicit\nChain-of-Reasoning component that reflects the intellectual process from Bit to\nFlip. We demonstrate that framing hypothesis generation as conditional language\nmodelling, with the model fine-tuned on Bit-Flip-Spark and the\nChain-of-Reasoning (and where, at inference, we only provide the Bit), leads to\nimprovements in the overall quality of the hypotheses. Our evaluation employs\nautomated metrics and LLM judge rankings for overall quality assessment. We\nshow that by fine-tuning on our HypoGen dataset we improve the novelty,\nfeasibility, and overall quality of the generated hypotheses. The HypoGen\ndataset is publicly available at\nhuggingface.co/datasets/UniverseTBD/hypogen-dr1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating novel and creative scientific hypotheses is a cornerstone in\nachieving Artificial General Intelligence. Large language and reasoning models\nhave the potential to aid in the systematic creation, selection, and validation\nof scientifically informed hypotheses. However, current foundation models often\nstruggle to produce scientific ideas that are both novel and feasible. One\nreason is the lack of a dedicated dataset that frames Scientific Hypothesis\nGeneration (SHG) as a Natural Language Generation (NLG) task. In this paper, we\nintroduce HypoGen, the first dataset of approximately 5500 structured\nproblem-hypothesis pairs extracted from top-tier computer science conferences\nstructured with a Bit-Flip-Spark schema, where the Bit is the conventional\nassumption, the Spark is the key insight or conceptual leap, and the Flip is\nthe resulting counterproposal. HypoGen uniquely integrates an explicit\nChain-of-Reasoning component that reflects the intellectual process from Bit to\nFlip. We demonstrate that framing hypothesis generation as conditional language\nmodelling, with the model fine-tuned on Bit-Flip-Spark and the\nChain-of-Reasoning (and where, at inference, we only provide the Bit), leads to\nimprovements in the overall quality of the hypotheses. Our evaluation employs\nautomated metrics and LLM judge rankings for overall quality assessment. We\nshow that by fine-tuning on our HypoGen dataset we improve the novelty,\nfeasibility, and overall quality of the generated hypotheses. The HypoGen\ndataset is publicly available at\nhuggingface.co/datasets/UniverseTBD/hypogen-dr1."
                },
                "authors": [
                    {
                        "name": "Charles O'Neill"
                    },
                    {
                        "name": "Tirthankar Ghosal"
                    },
                    {
                        "name": "Roberta Răileanu"
                    },
                    {
                        "name": "Mike Walmsley"
                    },
                    {
                        "name": "Thang Bui"
                    },
                    {
                        "name": "Kevin Schawinski"
                    },
                    {
                        "name": "Ioana Ciucă"
                    }
                ],
                "author_detail": {
                    "name": "Ioana Ciucă"
                },
                "author": "Ioana Ciucă",
                "arxiv_comment": "9 pages, 2 figures. Comments welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12972v1",
                "updated": "2025-04-17T14:24:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    14,
                    24,
                    51,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T14:24:51Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    14,
                    24,
                    51,
                    3,
                    107,
                    0
                ],
                "title": "Estimating Optimal Context Length for Hybrid Retrieval-augmented\n  Multi-document Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating Optimal Context Length for Hybrid Retrieval-augmented\n  Multi-document Summarization"
                },
                "summary": "Recent advances in long-context reasoning abilities of language models led to\ninteresting applications in large-scale multi-document summarization. However,\nprior work has shown that these long-context models are not effective at their\nclaimed context windows. To this end, retrieval-augmented systems provide an\nefficient and effective alternative. However, their performance can be highly\nsensitive to the choice of retrieval context length. In this work, we present a\nhybrid method that combines retrieval-augmented systems with long-context\nwindows supported by recent language models. Our method first estimates the\noptimal retrieval length as a function of the retriever, summarizer, and\ndataset. On a randomly sampled subset of the dataset, we use a panel of LLMs to\ngenerate a pool of silver references. We use these silver references to\nestimate the optimal context length for a given RAG system configuration. Our\nresults on the multi-document summarization task showcase the effectiveness of\nour method across model classes and sizes. We compare against length estimates\nfrom strong long-context benchmarks such as RULER and HELMET. Our analysis also\nhighlights the effectiveness of our estimation method for very long-context LMs\nand its generalization to new classes of LMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in long-context reasoning abilities of language models led to\ninteresting applications in large-scale multi-document summarization. However,\nprior work has shown that these long-context models are not effective at their\nclaimed context windows. To this end, retrieval-augmented systems provide an\nefficient and effective alternative. However, their performance can be highly\nsensitive to the choice of retrieval context length. In this work, we present a\nhybrid method that combines retrieval-augmented systems with long-context\nwindows supported by recent language models. Our method first estimates the\noptimal retrieval length as a function of the retriever, summarizer, and\ndataset. On a randomly sampled subset of the dataset, we use a panel of LLMs to\ngenerate a pool of silver references. We use these silver references to\nestimate the optimal context length for a given RAG system configuration. Our\nresults on the multi-document summarization task showcase the effectiveness of\nour method across model classes and sizes. We compare against length estimates\nfrom strong long-context benchmarks such as RULER and HELMET. Our analysis also\nhighlights the effectiveness of our estimation method for very long-context LMs\nand its generalization to new classes of LMs."
                },
                "authors": [
                    {
                        "name": "Adithya Pratapa"
                    },
                    {
                        "name": "Teruko Mitamura"
                    }
                ],
                "author_detail": {
                    "name": "Teruko Mitamura"
                },
                "author": "Teruko Mitamura",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.00354v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.00354v2",
                "updated": "2025-04-17T14:20:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    14,
                    20,
                    44,
                    3,
                    107,
                    0
                ],
                "published": "2023-08-01T07:51:01Z",
                "published_parsed": [
                    2023,
                    8,
                    1,
                    7,
                    51,
                    1,
                    1,
                    213,
                    0
                ],
                "title": "Multidimensional scaling informed by $F$-statistic: Visualizing grouped\n  microbiome data with inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multidimensional scaling informed by $F$-statistic: Visualizing grouped\n  microbiome data with inference"
                },
                "summary": "Multidimensional scaling (MDS) is a dimensionality reduction technique for\nmicrobial ecology data analysis that represents the multivariate structure\nwhile preserving pairwise distances between samples. While its improvement has\nenhanced the ability to reveal data patterns by sample groups, these MDS-based\nmethods require prior assumptions for inference, limiting their application in\ngeneral microbiome analysis. In this study, we introduce a new MDS-based\nordination, $F$-informed MDS, which configures the data distribution based on\nthe $F$-statistic, the ratio of dispersion between groups sharing common and\ndifferent characteristics. Using simulated compositional datasets, we\ndemonstrate that the proposed method is robust to hyperparameter selection\nwhile maintaining statistical significance throughout the ordination process.\nVarious quality metrics for evaluating dimensionality reduction confirm that\n$F$-informed MDS is comparable to state-of-the-art methods in preserving both\nlocal and global data structures. Its application to a diatom-associated\nbacterial community suggests the role of this new method in interpreting the\ncommunity response to the host. Our approach offers a well-founded refinement\nof MDS that aligns with statistical test results, which can be beneficial for\nbroader compositional data analyses in microbiology and ecology. This new\nvisualization tool can be incorporated into standard microbiome data analyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multidimensional scaling (MDS) is a dimensionality reduction technique for\nmicrobial ecology data analysis that represents the multivariate structure\nwhile preserving pairwise distances between samples. While its improvement has\nenhanced the ability to reveal data patterns by sample groups, these MDS-based\nmethods require prior assumptions for inference, limiting their application in\ngeneral microbiome analysis. In this study, we introduce a new MDS-based\nordination, $F$-informed MDS, which configures the data distribution based on\nthe $F$-statistic, the ratio of dispersion between groups sharing common and\ndifferent characteristics. Using simulated compositional datasets, we\ndemonstrate that the proposed method is robust to hyperparameter selection\nwhile maintaining statistical significance throughout the ordination process.\nVarious quality metrics for evaluating dimensionality reduction confirm that\n$F$-informed MDS is comparable to state-of-the-art methods in preserving both\nlocal and global data structures. Its application to a diatom-associated\nbacterial community suggests the role of this new method in interpreting the\ncommunity response to the host. Our approach offers a well-founded refinement\nof MDS that aligns with statistical test results, which can be beneficial for\nbroader compositional data analyses in microbiology and ecology. This new\nvisualization tool can be incorporated into standard microbiome data analyses."
                },
                "authors": [
                    {
                        "name": "Hyungseok Kim"
                    },
                    {
                        "name": "Soobin Kim"
                    },
                    {
                        "name": "Jeffrey A. Kimbrel"
                    },
                    {
                        "name": "Megan M. Morris"
                    },
                    {
                        "name": "Xavier Mayali"
                    },
                    {
                        "name": "Cullen R. Buie"
                    }
                ],
                "author_detail": {
                    "name": "Cullen R. Buie"
                },
                "author": "Cullen R. Buie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.00354v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.00354v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12961v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12961v1",
                "updated": "2025-04-17T14:07:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    14,
                    7,
                    11,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T14:07:11Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    14,
                    7,
                    11,
                    3,
                    107,
                    0
                ],
                "title": "QLLM: Do We Really Need a Mixing Network for Credit Assignment in\n  Multi-Agent Reinforcement Learning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QLLM: Do We Really Need a Mixing Network for Credit Assignment in\n  Multi-Agent Reinforcement Learning?"
                },
                "summary": "Credit assignment has remained a fundamental challenge in multi-agent\nreinforcement learning (MARL). Previous studies have primarily addressed this\nissue through value decomposition methods under the centralized training with\ndecentralized execution paradigm, where neural networks are utilized to\napproximate the nonlinear relationship between individual Q-values and the\nglobal Q-value. Although these approaches have achieved considerable success in\nvarious benchmark tasks, they still suffer from several limitations, including\nimprecise attribution of contributions, limited interpretability, and poor\nscalability in high-dimensional state spaces. To address these challenges, we\npropose a novel algorithm, \\textbf{QLLM}, which facilitates the automatic\nconstruction of credit assignment functions using large language models (LLMs).\nSpecifically, the concept of \\textbf{TFCAF} is introduced, wherein the credit\nallocation process is represented as a direct and expressive nonlinear\nfunctional formulation. A custom-designed \\textit{coder-evaluator} framework is\nfurther employed to guide the generation, verification, and refinement of\nexecutable code by LLMs, significantly mitigating issues such as hallucination\nand shallow reasoning during inference. Extensive experiments conducted on\nseveral standard MARL benchmarks demonstrate that the proposed method\nconsistently outperforms existing state-of-the-art baselines. Moreover, QLLM\nexhibits strong generalization capability and maintains compatibility with a\nwide range of MARL algorithms that utilize mixing networks, positioning it as a\npromising and versatile solution for complex multi-agent scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Credit assignment has remained a fundamental challenge in multi-agent\nreinforcement learning (MARL). Previous studies have primarily addressed this\nissue through value decomposition methods under the centralized training with\ndecentralized execution paradigm, where neural networks are utilized to\napproximate the nonlinear relationship between individual Q-values and the\nglobal Q-value. Although these approaches have achieved considerable success in\nvarious benchmark tasks, they still suffer from several limitations, including\nimprecise attribution of contributions, limited interpretability, and poor\nscalability in high-dimensional state spaces. To address these challenges, we\npropose a novel algorithm, \\textbf{QLLM}, which facilitates the automatic\nconstruction of credit assignment functions using large language models (LLMs).\nSpecifically, the concept of \\textbf{TFCAF} is introduced, wherein the credit\nallocation process is represented as a direct and expressive nonlinear\nfunctional formulation. A custom-designed \\textit{coder-evaluator} framework is\nfurther employed to guide the generation, verification, and refinement of\nexecutable code by LLMs, significantly mitigating issues such as hallucination\nand shallow reasoning during inference. Extensive experiments conducted on\nseveral standard MARL benchmarks demonstrate that the proposed method\nconsistently outperforms existing state-of-the-art baselines. Moreover, QLLM\nexhibits strong generalization capability and maintains compatibility with a\nwide range of MARL algorithms that utilize mixing networks, positioning it as a\npromising and versatile solution for complex multi-agent scenarios."
                },
                "authors": [
                    {
                        "name": "Zhouyang Jiang"
                    },
                    {
                        "name": "Bin Zhang"
                    },
                    {
                        "name": "Airong Wei"
                    },
                    {
                        "name": "Zhiwei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiwei Xu"
                },
                "author": "Zhiwei Xu",
                "arxiv_comment": "9 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12961v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12961v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12951v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12951v1",
                "updated": "2025-04-17T13:52:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    13,
                    52,
                    48,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T13:52:48Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    13,
                    52,
                    48,
                    3,
                    107,
                    0
                ],
                "title": "Are Retrials All You Need? Enhancing Large Language Model Reasoning\n  Without Verbalized Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Retrials All You Need? Enhancing Large Language Model Reasoning\n  Without Verbalized Feedback"
                },
                "summary": "Recent advancements in large language models (LLMs) have catalyzed the\ndevelopment of general-purpose autonomous agents, demonstrating remarkable\nperformance in complex reasoning tasks across various domains. This surge has\nspurred the evolution of a plethora of prompt-based reasoning frameworks. A\nrecent focus has been on iterative reasoning strategies that refine outputs\nthrough self-evaluation and verbalized feedback. However, these strategies\nrequire additional computational complexity to enable models to recognize and\ncorrect their mistakes, leading to a significant increase in their cost. In\nthis work, we introduce the concept of ``retrials without feedback'', an\nembarrassingly simple yet powerful mechanism for enhancing reasoning frameworks\nby allowing LLMs to retry problem-solving attempts upon identifying incorrect\nanswers. Unlike conventional iterative refinement methods, our method does not\nrequire explicit self-reflection or verbalized feedback, simplifying the\nrefinement process. Our findings indicate that simpler retrial-based approaches\noften outperform more sophisticated reasoning frameworks, suggesting that the\nbenefits of complex methods may not always justify their computational costs.\nBy challenging the prevailing assumption that more intricate reasoning\nstrategies inherently lead to better performance, our work offers new insights\ninto how simpler, more efficient approaches can achieve optimal results. So,\nare retrials all you need?",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have catalyzed the\ndevelopment of general-purpose autonomous agents, demonstrating remarkable\nperformance in complex reasoning tasks across various domains. This surge has\nspurred the evolution of a plethora of prompt-based reasoning frameworks. A\nrecent focus has been on iterative reasoning strategies that refine outputs\nthrough self-evaluation and verbalized feedback. However, these strategies\nrequire additional computational complexity to enable models to recognize and\ncorrect their mistakes, leading to a significant increase in their cost. In\nthis work, we introduce the concept of ``retrials without feedback'', an\nembarrassingly simple yet powerful mechanism for enhancing reasoning frameworks\nby allowing LLMs to retry problem-solving attempts upon identifying incorrect\nanswers. Unlike conventional iterative refinement methods, our method does not\nrequire explicit self-reflection or verbalized feedback, simplifying the\nrefinement process. Our findings indicate that simpler retrial-based approaches\noften outperform more sophisticated reasoning frameworks, suggesting that the\nbenefits of complex methods may not always justify their computational costs.\nBy challenging the prevailing assumption that more intricate reasoning\nstrategies inherently lead to better performance, our work offers new insights\ninto how simpler, more efficient approaches can achieve optimal results. So,\nare retrials all you need?"
                },
                "authors": [
                    {
                        "name": "Nearchos Potamitis"
                    },
                    {
                        "name": "Akhil Arora"
                    }
                ],
                "author_detail": {
                    "name": "Akhil Arora"
                },
                "author": "Akhil Arora",
                "arxiv_comment": "8 pages, 16 figures, 1 table. arXiv admin note: text overlap with\n  arXiv:2405.06691",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12951v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12951v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12052v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12052v2",
                "updated": "2025-04-17T13:49:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    13,
                    49,
                    27,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-16T13:06:24Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    13,
                    6,
                    24,
                    2,
                    106,
                    0
                ],
                "title": "Bayesian dynamic borrowing considering semantic similarity between\n  outcomes for disproportionality analysis in FAERS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian dynamic borrowing considering semantic similarity between\n  outcomes for disproportionality analysis in FAERS"
                },
                "summary": "We present a Bayesian dynamic borrowing (BDB) approach to enhance the\nquantitative identification of adverse events (AEs) in spontaneous reporting\nsystems (SRSs). The method embeds a robust meta-analytic predictive (MAP) prior\nwithin a Bayesian hierarchical model and incorporates semantic similarity\nmeasures (SSMs) to enable weighted information sharing from MedDRA Preferred\nTerms (PTs) that are clinically similar to the target PT. This continuous\nsimilarity-based borrowing addresses limitation of rigid hierarchical grouping\nin current disproportionality analysis (DPA).\n  Using data from the FDA Adverse Event Reporting System (FAERS) between 2015\nand 2019, we evaluate this approach - termed IC SSM - against standard\nInformation Component (IC) analysis and IC with borrowing at the MedDRA\nhigh-level group term (HLGT) level. A novel references set (PVLens), derived\nfrom FDA product label updates, enabled prospective evaluation of method\nperformance in identifying AEs prior to official labeling.\n  The IC SSM approach demonstrated improved sensitivity compared to both\ntraditional IC and HLGT-based borrowing, with minor trade-offs in F1 scores and\nYouden's index. IC SSM consistently identified more true positives and detected\nsignals over 5 months sooner than traditional IC. Despite a marginally lower\naggregate Youden's index, IC SSM showed higher performance in the early\npost-marketing period, providing more stable and relevant estimates than\nHLGT-based borrowing and traditional IC.\n  These findings support the use of SSM-informed Bayesian borrowing as a\nscalable and context-aware enhancement to traditional DPA methods. Future\nresearch should validate this approach across other datasets and explore\nadditional similarity metrics and Bayesian inference strategies using\ncase-level data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a Bayesian dynamic borrowing (BDB) approach to enhance the\nquantitative identification of adverse events (AEs) in spontaneous reporting\nsystems (SRSs). The method embeds a robust meta-analytic predictive (MAP) prior\nwithin a Bayesian hierarchical model and incorporates semantic similarity\nmeasures (SSMs) to enable weighted information sharing from MedDRA Preferred\nTerms (PTs) that are clinically similar to the target PT. This continuous\nsimilarity-based borrowing addresses limitation of rigid hierarchical grouping\nin current disproportionality analysis (DPA).\n  Using data from the FDA Adverse Event Reporting System (FAERS) between 2015\nand 2019, we evaluate this approach - termed IC SSM - against standard\nInformation Component (IC) analysis and IC with borrowing at the MedDRA\nhigh-level group term (HLGT) level. A novel references set (PVLens), derived\nfrom FDA product label updates, enabled prospective evaluation of method\nperformance in identifying AEs prior to official labeling.\n  The IC SSM approach demonstrated improved sensitivity compared to both\ntraditional IC and HLGT-based borrowing, with minor trade-offs in F1 scores and\nYouden's index. IC SSM consistently identified more true positives and detected\nsignals over 5 months sooner than traditional IC. Despite a marginally lower\naggregate Youden's index, IC SSM showed higher performance in the early\npost-marketing period, providing more stable and relevant estimates than\nHLGT-based borrowing and traditional IC.\n  These findings support the use of SSM-informed Bayesian borrowing as a\nscalable and context-aware enhancement to traditional DPA methods. Future\nresearch should validate this approach across other datasets and explore\nadditional similarity metrics and Bayesian inference strategies using\ncase-level data."
                },
                "authors": [
                    {
                        "name": "François Haguinet"
                    },
                    {
                        "name": "Jeffery L Painter"
                    },
                    {
                        "name": "Gregory E Powell"
                    },
                    {
                        "name": "Andrea Callegaro"
                    },
                    {
                        "name": "Andrew Bate"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Bate"
                },
                "author": "Andrew Bate",
                "arxiv_comment": "30 pages, 7 figures, 5 supplementary figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12052v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12052v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.4; G.3; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12943v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12943v2",
                "updated": "2025-04-18T04:23:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    4,
                    23,
                    0,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-17T13:43:13Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    13,
                    43,
                    13,
                    3,
                    107,
                    0
                ],
                "title": "Customizing Emotional Support: How Do Individuals Construct and Interact\n  With LLM-Powered Chatbots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Customizing Emotional Support: How Do Individuals Construct and Interact\n  With LLM-Powered Chatbots"
                },
                "summary": "Personalized support is essential to fulfill individuals' emotional needs and\nsustain their mental well-being. Large language models (LLMs), with great\ncustomization flexibility, hold promises to enable individuals to create their\nown emotional support agents. In this work, we developed ChatLab, where users\ncould construct LLM-powered chatbots with additional interaction features\nincluding voices and avatars. Using a Research through Design approach, we\nconducted a week-long field study followed by interviews and design activities\n(N = 22), which uncovered how participants created diverse chatbot personas for\nemotional reliance, confronting stressors, connecting to intellectual\ndiscourse, reflecting mirrored selves, etc. We found that participants actively\nenriched the personas they constructed, shaping the dynamics between themselves\nand the chatbot to foster open and honest conversations. They also suggested\nother customizable features, such as integrating online activities and\nadjustable memory settings. Based on these findings, we discuss opportunities\nfor enhancing personalized emotional support through emerging AI technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized support is essential to fulfill individuals' emotional needs and\nsustain their mental well-being. Large language models (LLMs), with great\ncustomization flexibility, hold promises to enable individuals to create their\nown emotional support agents. In this work, we developed ChatLab, where users\ncould construct LLM-powered chatbots with additional interaction features\nincluding voices and avatars. Using a Research through Design approach, we\nconducted a week-long field study followed by interviews and design activities\n(N = 22), which uncovered how participants created diverse chatbot personas for\nemotional reliance, confronting stressors, connecting to intellectual\ndiscourse, reflecting mirrored selves, etc. We found that participants actively\nenriched the personas they constructed, shaping the dynamics between themselves\nand the chatbot to foster open and honest conversations. They also suggested\nother customizable features, such as integrating online activities and\nadjustable memory settings. Based on these findings, we discuss opportunities\nfor enhancing personalized emotional support through emerging AI technologies."
                },
                "authors": [
                    {
                        "name": "Xi Zheng"
                    },
                    {
                        "name": "Zhuoyang Li"
                    },
                    {
                        "name": "Xinning Gui"
                    },
                    {
                        "name": "Yuhan Luo"
                    }
                ],
                "author_detail": {
                    "name": "Yuhan Luo"
                },
                "author": "Yuhan Luo",
                "arxiv_comment": "20 pages, 3 figures, 3 tables. Accepted to CHI 2025, ACM Conference\n  on Human Factors in Computing Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12943v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12943v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12931v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12931v1",
                "updated": "2025-04-17T13:28:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    13,
                    28,
                    1,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T13:28:01Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    13,
                    28,
                    1,
                    3,
                    107,
                    0
                ],
                "title": "Explainable AI in Usable Privacy and Security: Challenges and\n  Opportunities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable AI in Usable Privacy and Security: Challenges and\n  Opportunities"
                },
                "summary": "Large Language Models (LLMs) are increasingly being used for automated\nevaluations and explaining them. However, concerns about explanation quality,\nconsistency, and hallucinations remain open research challenges, particularly\nin high-stakes contexts like privacy and security, where user trust and\ndecision-making are at stake. In this paper, we investigate these issues in the\ncontext of PRISMe, an interactive privacy policy assessment tool that leverages\nLLMs to evaluate and explain website privacy policies. Based on a prior user\nstudy with 22 participants, we identify key concerns regarding LLM judgment\ntransparency, consistency, and faithfulness, as well as variations in user\npreferences for explanation detail and engagement. We discuss potential\nstrategies to mitigate these concerns, including structured evaluation\ncriteria, uncertainty estimation, and retrieval-augmented generation (RAG). We\nidentify a need for adaptive explanation strategies tailored to different user\nprofiles for LLM-as-a-judge. Our goal is to showcase the application area of\nusable privacy and security to be promising for Human-Centered Explainable AI\n(HCXAI) to make an impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly being used for automated\nevaluations and explaining them. However, concerns about explanation quality,\nconsistency, and hallucinations remain open research challenges, particularly\nin high-stakes contexts like privacy and security, where user trust and\ndecision-making are at stake. In this paper, we investigate these issues in the\ncontext of PRISMe, an interactive privacy policy assessment tool that leverages\nLLMs to evaluate and explain website privacy policies. Based on a prior user\nstudy with 22 participants, we identify key concerns regarding LLM judgment\ntransparency, consistency, and faithfulness, as well as variations in user\npreferences for explanation detail and engagement. We discuss potential\nstrategies to mitigate these concerns, including structured evaluation\ncriteria, uncertainty estimation, and retrieval-augmented generation (RAG). We\nidentify a need for adaptive explanation strategies tailored to different user\nprofiles for LLM-as-a-judge. Our goal is to showcase the application area of\nusable privacy and security to be promising for Human-Centered Explainable AI\n(HCXAI) to make an impact."
                },
                "authors": [
                    {
                        "name": "Vincent Freiberger"
                    },
                    {
                        "name": "Arthur Fleig"
                    },
                    {
                        "name": "Erik Buchmann"
                    }
                ],
                "author_detail": {
                    "name": "Erik Buchmann"
                },
                "author": "Erik Buchmann",
                "arxiv_comment": "Presented at the 5th CHI Workshop on Human-Centered Explainable AI\n  (HCXAI)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12931v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12931v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12915v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12915v1",
                "updated": "2025-04-17T13:05:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    13,
                    5,
                    14,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T13:05:14Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    13,
                    5,
                    14,
                    3,
                    107,
                    0
                ],
                "title": "ConExion: Concept Extraction with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConExion: Concept Extraction with Large Language Models"
                },
                "summary": "In this paper, an approach for concept extraction from documents using\npre-trained large language models (LLMs) is presented. Compared with\nconventional methods that extract keyphrases summarizing the important\ninformation discussed in a document, our approach tackles a more challenging\ntask of extracting all present concepts related to the specific domain, not\njust the important ones. Through comprehensive evaluations of two widely used\nbenchmark datasets, we demonstrate that our method improves the F1 score\ncompared to state-of-the-art techniques. Additionally, we explore the potential\nof using prompts within these models for unsupervised concept extraction. The\nextracted concepts are intended to support domain coverage evaluation of\nontologies and facilitate ontology learning, highlighting the effectiveness of\nLLMs in concept extraction tasks. Our source code and datasets are publicly\navailable at https://github.com/ISE-FIZKarlsruhe/concept_extraction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, an approach for concept extraction from documents using\npre-trained large language models (LLMs) is presented. Compared with\nconventional methods that extract keyphrases summarizing the important\ninformation discussed in a document, our approach tackles a more challenging\ntask of extracting all present concepts related to the specific domain, not\njust the important ones. Through comprehensive evaluations of two widely used\nbenchmark datasets, we demonstrate that our method improves the F1 score\ncompared to state-of-the-art techniques. Additionally, we explore the potential\nof using prompts within these models for unsupervised concept extraction. The\nextracted concepts are intended to support domain coverage evaluation of\nontologies and facilitate ontology learning, highlighting the effectiveness of\nLLMs in concept extraction tasks. Our source code and datasets are publicly\navailable at https://github.com/ISE-FIZKarlsruhe/concept_extraction."
                },
                "authors": [
                    {
                        "name": "Ebrahim Norouzi"
                    },
                    {
                        "name": "Sven Hertling"
                    },
                    {
                        "name": "Harald Sack"
                    }
                ],
                "author_detail": {
                    "name": "Harald Sack"
                },
                "author": "Harald Sack",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12915v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12915v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12913v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12913v1",
                "updated": "2025-04-17T13:02:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    13,
                    2,
                    44,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T13:02:44Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    13,
                    2,
                    44,
                    3,
                    107,
                    0
                ],
                "title": "MAIN: Mutual Alignment Is Necessary for instruction tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAIN: Mutual Alignment Is Necessary for instruction tuning"
                },
                "summary": "Instruction tuning has enabled large language models (LLMs) to achieve\nremarkable performance, but its success heavily depends on the availability of\nlarge-scale, high-quality instruction-response pairs. However, current methods\nfor scaling up data generation often overlook a crucial aspect: the alignment\nbetween instructions and responses. We hypothesize that high-quality\ninstruction-response pairs are not defined by the individual quality of each\ncomponent, but by the extent of their alignment with each other. To address\nthis, we propose a Mutual Alignment Framework (MAIN) that ensures coherence\nbetween the instruction and response through mutual constraints. Experiments\ndemonstrate that models such as LLaMA and Mistral, fine-tuned within this\nframework, outperform traditional methods across multiple benchmarks. This\napproach underscores the critical role of instruction-response alignment in\nenabling scalable and high-quality instruction tuning for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning has enabled large language models (LLMs) to achieve\nremarkable performance, but its success heavily depends on the availability of\nlarge-scale, high-quality instruction-response pairs. However, current methods\nfor scaling up data generation often overlook a crucial aspect: the alignment\nbetween instructions and responses. We hypothesize that high-quality\ninstruction-response pairs are not defined by the individual quality of each\ncomponent, but by the extent of their alignment with each other. To address\nthis, we propose a Mutual Alignment Framework (MAIN) that ensures coherence\nbetween the instruction and response through mutual constraints. Experiments\ndemonstrate that models such as LLaMA and Mistral, fine-tuned within this\nframework, outperform traditional methods across multiple benchmarks. This\napproach underscores the critical role of instruction-response alignment in\nenabling scalable and high-quality instruction tuning for LLMs."
                },
                "authors": [
                    {
                        "name": "Fanyi Yang"
                    },
                    {
                        "name": "Jianfeng Liu"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Haoyu Liu"
                    },
                    {
                        "name": "Xixin Cao"
                    },
                    {
                        "name": "Yuefeng Zhan"
                    },
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Weiwei Deng"
                    },
                    {
                        "name": "Feng Sun"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12913v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12913v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12911v1",
                "updated": "2025-04-17T13:01:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    13,
                    1,
                    38,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T13:01:38Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    13,
                    1,
                    38,
                    3,
                    107,
                    0
                ],
                "title": "Benchmarking Multi-National Value Alignment for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Multi-National Value Alignment for Large Language Models"
                },
                "summary": "Do Large Language Models (LLMs) hold positions that conflict with your\ncountry's values? Occasionally they do! However, existing works primarily focus\non ethical reviews, failing to capture the diversity of national values, which\nencompass broader policy, legal, and moral considerations. Furthermore, current\nbenchmarks that rely on spectrum tests using manually designed questionnaires\nare not easily scalable.\n  To address these limitations, we introduce NaVAB, a comprehensive benchmark\nto evaluate the alignment of LLMs with the values of five major nations: China,\nthe United States, the United Kingdom, France, and Germany. NaVAB implements a\nnational value extraction pipeline to efficiently construct value assessment\ndatasets. Specifically, we propose a modeling procedure with instruction\ntagging to process raw data sources, a screening process to filter\nvalue-related topics and a generation process with a Conflict Reduction\nmechanism to filter non-conflicting values.We conduct extensive experiments on\nvarious LLMs across countries, and the results provide insights into assisting\nin the identification of misaligned scenarios. Moreover, we demonstrate that\nNaVAB can be combined with alignment techniques to effectively reduce value\nconcerns by aligning LLMs' values with the target country.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models (LLMs) hold positions that conflict with your\ncountry's values? Occasionally they do! However, existing works primarily focus\non ethical reviews, failing to capture the diversity of national values, which\nencompass broader policy, legal, and moral considerations. Furthermore, current\nbenchmarks that rely on spectrum tests using manually designed questionnaires\nare not easily scalable.\n  To address these limitations, we introduce NaVAB, a comprehensive benchmark\nto evaluate the alignment of LLMs with the values of five major nations: China,\nthe United States, the United Kingdom, France, and Germany. NaVAB implements a\nnational value extraction pipeline to efficiently construct value assessment\ndatasets. Specifically, we propose a modeling procedure with instruction\ntagging to process raw data sources, a screening process to filter\nvalue-related topics and a generation process with a Conflict Reduction\nmechanism to filter non-conflicting values.We conduct extensive experiments on\nvarious LLMs across countries, and the results provide insights into assisting\nin the identification of misaligned scenarios. Moreover, we demonstrate that\nNaVAB can be combined with alignment techniques to effectively reduce value\nconcerns by aligning LLMs' values with the target country."
                },
                "authors": [
                    {
                        "name": "Chengyi Ju"
                    },
                    {
                        "name": "Weijie Shi"
                    },
                    {
                        "name": "Chengzhong Liu"
                    },
                    {
                        "name": "Jiaming Ji"
                    },
                    {
                        "name": "Jipeng Zhang"
                    },
                    {
                        "name": "Ruiyuan Zhang"
                    },
                    {
                        "name": "Jia Zhu"
                    },
                    {
                        "name": "Jiajie Xu"
                    },
                    {
                        "name": "Yaodong Yang"
                    },
                    {
                        "name": "Sirui Han"
                    },
                    {
                        "name": "Yike Guo"
                    }
                ],
                "author_detail": {
                    "name": "Yike Guo"
                },
                "author": "Yike Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12905v1",
                "updated": "2025-04-17T12:52:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    12,
                    52,
                    8,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T12:52:08Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    12,
                    52,
                    8,
                    3,
                    107,
                    0
                ],
                "title": "Second-order Optimization of Gaussian Splats with Importance Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Second-order Optimization of Gaussian Splats with Importance Sampling"
                },
                "summary": "3D Gaussian Splatting (3DGS) is widely used for novel view synthesis due to\nits high rendering quality and fast inference time. However, 3DGS predominantly\nrelies on first-order optimizers such as Adam, which leads to long training\ntimes. To address this limitation, we propose a novel second-order optimization\nstrategy based on Levenberg-Marquardt (LM) and Conjugate Gradient (CG), which\nwe specifically tailor towards Gaussian Splatting. Our key insight is that the\nJacobian in 3DGS exhibits significant sparsity since each Gaussian affects only\na limited number of pixels. We exploit this sparsity by proposing a matrix-free\nand GPU-parallelized LM optimization. To further improve its efficiency, we\npropose sampling strategies for both the camera views and loss function and,\nconsequently, the normal equation, significantly reducing the computational\ncomplexity. In addition, we increase the convergence rate of the second-order\napproximation by introducing an effective heuristic to determine the learning\nrate that avoids the expensive computation cost of line search methods. As a\nresult, our method achieves a $3\\times$ speedup over standard LM and\noutperforms Adam by $~6\\times$ when the Gaussian count is low while remaining\ncompetitive for moderate counts. Project Page:\nhttps://vcai.mpi-inf.mpg.de/projects/LM-IS",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Gaussian Splatting (3DGS) is widely used for novel view synthesis due to\nits high rendering quality and fast inference time. However, 3DGS predominantly\nrelies on first-order optimizers such as Adam, which leads to long training\ntimes. To address this limitation, we propose a novel second-order optimization\nstrategy based on Levenberg-Marquardt (LM) and Conjugate Gradient (CG), which\nwe specifically tailor towards Gaussian Splatting. Our key insight is that the\nJacobian in 3DGS exhibits significant sparsity since each Gaussian affects only\na limited number of pixels. We exploit this sparsity by proposing a matrix-free\nand GPU-parallelized LM optimization. To further improve its efficiency, we\npropose sampling strategies for both the camera views and loss function and,\nconsequently, the normal equation, significantly reducing the computational\ncomplexity. In addition, we increase the convergence rate of the second-order\napproximation by introducing an effective heuristic to determine the learning\nrate that avoids the expensive computation cost of line search methods. As a\nresult, our method achieves a $3\\times$ speedup over standard LM and\noutperforms Adam by $~6\\times$ when the Gaussian count is low while remaining\ncompetitive for moderate counts. Project Page:\nhttps://vcai.mpi-inf.mpg.de/projects/LM-IS"
                },
                "authors": [
                    {
                        "name": "Hamza Pehlivan"
                    },
                    {
                        "name": "Andrea Boscolo Camiletto"
                    },
                    {
                        "name": "Lin Geng Foo"
                    },
                    {
                        "name": "Marc Habermann"
                    },
                    {
                        "name": "Christian Theobalt"
                    }
                ],
                "author_detail": {
                    "name": "Christian Theobalt"
                },
                "author": "Christian Theobalt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12898v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12898v1",
                "updated": "2025-04-17T12:39:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    12,
                    39,
                    25,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T12:39:25Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    12,
                    39,
                    25,
                    3,
                    107,
                    0
                ],
                "title": "Information Gain-Guided Causal Intervention for Autonomous Debiasing\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information Gain-Guided Causal Intervention for Autonomous Debiasing\n  Large Language Models"
                },
                "summary": "Despite significant progress, recent studies indicate that current large\nlanguage models (LLMs) may still capture dataset biases and utilize them during\ninference, leading to the poor generalizability of LLMs. However, due to the\ndiversity of dataset biases and the insufficient nature of bias suppression\nbased on in-context learning, the effectiveness of previous prior\nknowledge-based debiasing methods and in-context learning based automatic\ndebiasing methods is limited. To address these challenges, we explore the\ncombination of causal mechanisms with information theory and propose an\ninformation gain-guided causal intervention debiasing (IGCIDB) framework. This\nframework first utilizes an information gain-guided causal intervention method\nto automatically and autonomously balance the distribution of\ninstruction-tuning dataset. Subsequently, it employs a standard supervised\nfine-tuning process to train LLMs on the debiased dataset. Experimental results\nshow that IGCIDB can effectively debias LLM to improve its generalizability\nacross different tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant progress, recent studies indicate that current large\nlanguage models (LLMs) may still capture dataset biases and utilize them during\ninference, leading to the poor generalizability of LLMs. However, due to the\ndiversity of dataset biases and the insufficient nature of bias suppression\nbased on in-context learning, the effectiveness of previous prior\nknowledge-based debiasing methods and in-context learning based automatic\ndebiasing methods is limited. To address these challenges, we explore the\ncombination of causal mechanisms with information theory and propose an\ninformation gain-guided causal intervention debiasing (IGCIDB) framework. This\nframework first utilizes an information gain-guided causal intervention method\nto automatically and autonomously balance the distribution of\ninstruction-tuning dataset. Subsequently, it employs a standard supervised\nfine-tuning process to train LLMs on the debiased dataset. Experimental results\nshow that IGCIDB can effectively debias LLM to improve its generalizability\nacross different tasks."
                },
                "authors": [
                    {
                        "name": "Zhouhao Sun"
                    },
                    {
                        "name": "Xiao Ding"
                    },
                    {
                        "name": "Li Du"
                    },
                    {
                        "name": "Yunpeng Xu"
                    },
                    {
                        "name": "Yixuan Ma"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Bing Qin"
                    },
                    {
                        "name": "Ting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ting Liu"
                },
                "author": "Ting Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12898v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12898v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06778v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06778v3",
                "updated": "2025-04-17T12:36:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    12,
                    36,
                    56,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-09T10:58:54Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    10,
                    58,
                    54,
                    2,
                    99,
                    0
                ],
                "title": "CAFA: a Controllable Automatic Foley Artist",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAFA: a Controllable Automatic Foley Artist"
                },
                "summary": "Foley is a key element in video production, refers to the process of adding\nan audio signal to a silent video while ensuring semantic and temporal\nalignment. In recent years, the rise of personalized content creation and\nadvancements in automatic video-to-audio models have increased the demand for\ngreater user control in the process. One possible approach is to incorporate\ntext to guide audio generation. While supported by existing methods, challenges\nremain in ensuring compatibility between modalities, particularly when the text\nintroduces additional information or contradicts the sounds naturally inferred\nfrom the visuals. In this work, we introduce CAFA (Controllable Automatic Foley\nArtist) a video-and-text-to-audio model that generates semantically and\ntemporally aligned audio for a given video, guided by text input. CAFA is built\nupon a text-to-audio model and integrates video information through a modality\nadapter mechanism. By incorporating text, users can refine semantic details and\nintroduce creative variations, guiding the audio synthesis beyond the expected\nvideo contextual cues. Experiments show that besides its superior quality in\nterms of semantic alignment and audio-visual synchronization the proposed\nmethod enable high textual controllability as demonstrated in subjective and\nobjective evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foley is a key element in video production, refers to the process of adding\nan audio signal to a silent video while ensuring semantic and temporal\nalignment. In recent years, the rise of personalized content creation and\nadvancements in automatic video-to-audio models have increased the demand for\ngreater user control in the process. One possible approach is to incorporate\ntext to guide audio generation. While supported by existing methods, challenges\nremain in ensuring compatibility between modalities, particularly when the text\nintroduces additional information or contradicts the sounds naturally inferred\nfrom the visuals. In this work, we introduce CAFA (Controllable Automatic Foley\nArtist) a video-and-text-to-audio model that generates semantically and\ntemporally aligned audio for a given video, guided by text input. CAFA is built\nupon a text-to-audio model and integrates video information through a modality\nadapter mechanism. By incorporating text, users can refine semantic details and\nintroduce creative variations, guiding the audio synthesis beyond the expected\nvideo contextual cues. Experiments show that besides its superior quality in\nterms of semantic alignment and audio-visual synchronization the proposed\nmethod enable high textual controllability as demonstrated in subjective and\nobjective evaluations."
                },
                "authors": [
                    {
                        "name": "Roi Benita"
                    },
                    {
                        "name": "Michael Finkelson"
                    },
                    {
                        "name": "Tavi Halperin"
                    },
                    {
                        "name": "Gleb Sterkin"
                    },
                    {
                        "name": "Yossi Adi"
                    }
                ],
                "author_detail": {
                    "name": "Yossi Adi"
                },
                "author": "Yossi Adi",
                "arxiv_comment": "Renamed paper to \"CAFA: a Controllable Automatic Foley Artist\" from\n  \"Controllable Automatic Foley Artist\". Updated link to demo page",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06778v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06778v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10090v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10090v2",
                "updated": "2025-04-17T12:33:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    12,
                    33,
                    59,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-14T10:53:44Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    10,
                    53,
                    44,
                    0,
                    104,
                    0
                ],
                "title": "CameraBench: Benchmarking Visual Reasoning in MLLMs via Photography",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CameraBench: Benchmarking Visual Reasoning in MLLMs via Photography"
                },
                "summary": "Large language models (LLMs) and multimodal large language models (MLLMs)\nhave significantly advanced artificial intelligence. However, visual reasoning,\nreasoning involving both visual and textual inputs, remains underexplored.\nRecent advancements, including the reasoning models like OpenAI o1 and Gemini\n2.0 Flash Thinking, which incorporate image inputs, have opened this\ncapability. In this ongoing work, we focus specifically on photography-related\ntasks because a photo is a visual snapshot of the physical world where the\nunderlying physics (i.e., illumination, blur extent, etc.) interplay with the\ncamera parameters. Successfully reasoning from the visual information of a\nphoto to identify these numerical camera settings requires the MLLMs to have a\ndeeper understanding of the underlying physics for precise visual\ncomprehension, representing a challenging and intelligent capability essential\nfor practical applications like photography assistant agents. We aim to\nevaluate MLLMs on their ability to distinguish visual differences related to\nnumerical camera settings, extending a methodology previously proposed for\nvision-language models (VLMs). Our preliminary results demonstrate the\nimportance of visual reasoning in photography-related tasks. Moreover, these\nresults show that no single MLLM consistently dominates across all evaluation\ntasks, demonstrating ongoing challenges and opportunities in developing MLLMs\nwith better visual reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) and multimodal large language models (MLLMs)\nhave significantly advanced artificial intelligence. However, visual reasoning,\nreasoning involving both visual and textual inputs, remains underexplored.\nRecent advancements, including the reasoning models like OpenAI o1 and Gemini\n2.0 Flash Thinking, which incorporate image inputs, have opened this\ncapability. In this ongoing work, we focus specifically on photography-related\ntasks because a photo is a visual snapshot of the physical world where the\nunderlying physics (i.e., illumination, blur extent, etc.) interplay with the\ncamera parameters. Successfully reasoning from the visual information of a\nphoto to identify these numerical camera settings requires the MLLMs to have a\ndeeper understanding of the underlying physics for precise visual\ncomprehension, representing a challenging and intelligent capability essential\nfor practical applications like photography assistant agents. We aim to\nevaluate MLLMs on their ability to distinguish visual differences related to\nnumerical camera settings, extending a methodology previously proposed for\nvision-language models (VLMs). Our preliminary results demonstrate the\nimportance of visual reasoning in photography-related tasks. Moreover, these\nresults show that no single MLLM consistently dominates across all evaluation\ntasks, demonstrating ongoing challenges and opportunities in developing MLLMs\nwith better visual reasoning."
                },
                "authors": [
                    {
                        "name": "I-Sheng Fang"
                    },
                    {
                        "name": "Jun-Cheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jun-Cheng Chen"
                },
                "author": "Jun-Cheng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10090v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10090v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12229v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12229v3",
                "updated": "2025-04-17T11:50:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    11,
                    50,
                    59,
                    3,
                    107,
                    0
                ],
                "published": "2024-10-16T04:44:34Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    4,
                    44,
                    34,
                    2,
                    290,
                    0
                ],
                "title": "Comprehending Knowledge Graphs with Large Language Models for\n  Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comprehending Knowledge Graphs with Large Language Models for\n  Recommender Systems"
                },
                "summary": "In recent years, the introduction of knowledge graphs (KGs) has significantly\nadvanced recommender systems by facilitating the discovery of potential\nassociations between items. However, existing methods still face several\nlimitations. First, most KGs suffer from missing facts or limited scopes.\nSecond, existing methods convert textual information in KGs into IDs, resulting\nin the loss of natural semantic connections between different items. Third,\nexisting methods struggle to capture high-order connections in the global KG.\nTo address these limitations, we propose a novel method called CoLaKG, which\nleverages large language models (LLMs) to improve KG-based recommendations. The\nextensive knowledge and remarkable reasoning capabilities of LLMs enable our\nmethod to supplement missing facts in KGs, and their powerful text\nunderstanding abilities allow for better utilization of semantic information.\nSpecifically, CoLaKG extracts useful information from KGs at both local and\nglobal levels. By employing the item-centered subgraph extraction and prompt\nengineering, it can accurately understand the local information. In addition,\nthrough the semantic-based retrieval module, each item is enriched by related\nitems from the entire knowledge graph, effectively harnessing global\ninformation. Furthermore, the local and global information are effectively\nintegrated into the recommendation model through a representation fusion module\nand a retrieval-augmented representation learning module, respectively.\nExtensive experiments on four real-world datasets demonstrate the superiority\nof our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the introduction of knowledge graphs (KGs) has significantly\nadvanced recommender systems by facilitating the discovery of potential\nassociations between items. However, existing methods still face several\nlimitations. First, most KGs suffer from missing facts or limited scopes.\nSecond, existing methods convert textual information in KGs into IDs, resulting\nin the loss of natural semantic connections between different items. Third,\nexisting methods struggle to capture high-order connections in the global KG.\nTo address these limitations, we propose a novel method called CoLaKG, which\nleverages large language models (LLMs) to improve KG-based recommendations. The\nextensive knowledge and remarkable reasoning capabilities of LLMs enable our\nmethod to supplement missing facts in KGs, and their powerful text\nunderstanding abilities allow for better utilization of semantic information.\nSpecifically, CoLaKG extracts useful information from KGs at both local and\nglobal levels. By employing the item-centered subgraph extraction and prompt\nengineering, it can accurately understand the local information. In addition,\nthrough the semantic-based retrieval module, each item is enriched by related\nitems from the entire knowledge graph, effectively harnessing global\ninformation. Furthermore, the local and global information are effectively\nintegrated into the recommendation model through a representation fusion module\nand a retrieval-augmented representation learning module, respectively.\nExtensive experiments on four real-world datasets demonstrate the superiority\nof our method."
                },
                "authors": [
                    {
                        "name": "Ziqiang Cui"
                    },
                    {
                        "name": "Yunpeng Weng"
                    },
                    {
                        "name": "Xing Tang"
                    },
                    {
                        "name": "Fuyuan Lyu"
                    },
                    {
                        "name": "Dugang Liu"
                    },
                    {
                        "name": "Xiuqiang He"
                    },
                    {
                        "name": "Chen Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ma"
                },
                "author": "Chen Ma",
                "arxiv_doi": "10.1145/3726302.3729932",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3729932",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.12229v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12229v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted as a full paper by SIGIR'25",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12867v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12867v2",
                "updated": "2025-04-18T08:18:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    8,
                    18,
                    11,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-17T11:50:04Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    11,
                    50,
                    4,
                    3,
                    107,
                    0
                ],
                "title": "EmoVoice: LLM-based Emotional Text-To-Speech Model with Freestyle Text\n  Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EmoVoice: LLM-based Emotional Text-To-Speech Model with Freestyle Text\n  Prompting"
                },
                "summary": "Human speech goes beyond the mere transfer of information; it is a profound\nexchange of emotions and a connection between individuals. While Text-to-Speech\n(TTS) models have made huge progress, they still face challenges in controlling\nthe emotional expression in the generated speech. In this work, we propose\nEmoVoice, a novel emotion-controllable TTS model that exploits large language\nmodels (LLMs) to enable fine-grained freestyle natural language emotion\ncontrol, and a phoneme boost variant design that makes the model output phoneme\ntokens and audio tokens in parallel to enhance content consistency, inspired by\nchain-of-thought (CoT) and chain-of-modality (CoM) techniques. Besides, we\nintroduce EmoVoice-DB, a high-quality 40-hour English emotion dataset featuring\nexpressive speech and fine-grained emotion labels with natural language\ndescriptions. EmoVoice achieves state-of-the-art performance on the English\nEmoVoice-DB test set using only synthetic training data, and on the Chinese\nSecap test set using our in-house data. We further investigate the reliability\nof existing emotion evaluation metrics and their alignment with human\nperceptual preferences, and explore using SOTA multimodal LLMs GPT-4o-audio and\nGemini to assess emotional speech. Demo samples are available at\nhttps://anonymous.4open.science/r/EmoVoice-DF55. Dataset, code, and checkpoints\nwill be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human speech goes beyond the mere transfer of information; it is a profound\nexchange of emotions and a connection between individuals. While Text-to-Speech\n(TTS) models have made huge progress, they still face challenges in controlling\nthe emotional expression in the generated speech. In this work, we propose\nEmoVoice, a novel emotion-controllable TTS model that exploits large language\nmodels (LLMs) to enable fine-grained freestyle natural language emotion\ncontrol, and a phoneme boost variant design that makes the model output phoneme\ntokens and audio tokens in parallel to enhance content consistency, inspired by\nchain-of-thought (CoT) and chain-of-modality (CoM) techniques. Besides, we\nintroduce EmoVoice-DB, a high-quality 40-hour English emotion dataset featuring\nexpressive speech and fine-grained emotion labels with natural language\ndescriptions. EmoVoice achieves state-of-the-art performance on the English\nEmoVoice-DB test set using only synthetic training data, and on the Chinese\nSecap test set using our in-house data. We further investigate the reliability\nof existing emotion evaluation metrics and their alignment with human\nperceptual preferences, and explore using SOTA multimodal LLMs GPT-4o-audio and\nGemini to assess emotional speech. Demo samples are available at\nhttps://anonymous.4open.science/r/EmoVoice-DF55. Dataset, code, and checkpoints\nwill be released."
                },
                "authors": [
                    {
                        "name": "Guanrou Yang"
                    },
                    {
                        "name": "Chen Yang"
                    },
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Ziyang Ma"
                    },
                    {
                        "name": "Wenxi Chen"
                    },
                    {
                        "name": "Wen Wang"
                    },
                    {
                        "name": "Tianrui Wang"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Zhikang Niu"
                    },
                    {
                        "name": "Wenrui Liu"
                    },
                    {
                        "name": "Fan Yu"
                    },
                    {
                        "name": "Zhihao Du"
                    },
                    {
                        "name": "Zhifu Gao"
                    },
                    {
                        "name": "ShiLiang Zhang"
                    },
                    {
                        "name": "Xie Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xie Chen"
                },
                "author": "Xie Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12867v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12867v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12865v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12865v1",
                "updated": "2025-04-17T11:46:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    11,
                    46,
                    17,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T11:46:17Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    11,
                    46,
                    17,
                    3,
                    107,
                    0
                ],
                "title": "DashChat: Interactive Authoring of Industrial Dashboard Design\n  Prototypes through Conversation with LLM-Powered Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DashChat: Interactive Authoring of Industrial Dashboard Design\n  Prototypes through Conversation with LLM-Powered Agents"
                },
                "summary": "Industrial dashboards, commonly deployed by organizations such as enterprises\nand governments, are increasingly crucial in data communication and\ndecision-making support across various domains. Designing an industrial\ndashboard prototype is particularly challenging due to its visual complexity,\nwhich can include data visualization, layout configuration, embellishments, and\nanimations. Additionally, in real-world industrial settings, designers often\nencounter numerous constraints. For instance, when companies negotiate\ncollaborations with clients and determine design plans, they typically need to\ndemo design prototypes and iterate on them based on mock data quickly. Such a\ntask is very common and crucial during the ideation stage, as it not only helps\nsave developmental costs but also avoids data-related issues such as lengthy\ndata handover periods. However, existing authoring tools of dashboards are\nmostly not tailored to such prototyping needs, and motivated by these gaps, we\npropose DashChat, an interactive system that leverages large language models\n(LLMs) to generate industrial dashboard design prototypes from natural\nlanguage. We collaborated closely with designers from the industry and derived\nthe requirements based on their practical experience. First, by analyzing 114\nhigh-quality industrial dashboards, we summarized their common design patterns\nand inject the identified ones into LLMs as reference. Next, we built a\nmulti-agent pipeline powered by LLMs to understand textual requirements from\nusers and generate practical, aesthetic prototypes. Besides, functionally\ndistinct, parallel-operating agents are created to enable efficient generation.\nThen, we developed a user-friendly interface that supports text-based\ninteraction for generating and modifying prototypes. Two user studies\ndemonstrated that our system is both effective and efficient in supporting\ndesign prototyping.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Industrial dashboards, commonly deployed by organizations such as enterprises\nand governments, are increasingly crucial in data communication and\ndecision-making support across various domains. Designing an industrial\ndashboard prototype is particularly challenging due to its visual complexity,\nwhich can include data visualization, layout configuration, embellishments, and\nanimations. Additionally, in real-world industrial settings, designers often\nencounter numerous constraints. For instance, when companies negotiate\ncollaborations with clients and determine design plans, they typically need to\ndemo design prototypes and iterate on them based on mock data quickly. Such a\ntask is very common and crucial during the ideation stage, as it not only helps\nsave developmental costs but also avoids data-related issues such as lengthy\ndata handover periods. However, existing authoring tools of dashboards are\nmostly not tailored to such prototyping needs, and motivated by these gaps, we\npropose DashChat, an interactive system that leverages large language models\n(LLMs) to generate industrial dashboard design prototypes from natural\nlanguage. We collaborated closely with designers from the industry and derived\nthe requirements based on their practical experience. First, by analyzing 114\nhigh-quality industrial dashboards, we summarized their common design patterns\nand inject the identified ones into LLMs as reference. Next, we built a\nmulti-agent pipeline powered by LLMs to understand textual requirements from\nusers and generate practical, aesthetic prototypes. Besides, functionally\ndistinct, parallel-operating agents are created to enable efficient generation.\nThen, we developed a user-friendly interface that supports text-based\ninteraction for generating and modifying prototypes. Two user studies\ndemonstrated that our system is both effective and efficient in supporting\ndesign prototyping."
                },
                "authors": [
                    {
                        "name": "S. Shen"
                    },
                    {
                        "name": "Z. Lin"
                    },
                    {
                        "name": "W. Liu"
                    },
                    {
                        "name": "C. Xin"
                    },
                    {
                        "name": "W. Dai"
                    },
                    {
                        "name": "S. Chen"
                    },
                    {
                        "name": "X. Wen"
                    },
                    {
                        "name": "X. Lan"
                    }
                ],
                "author_detail": {
                    "name": "X. Lan"
                },
                "author": "X. Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12865v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20268v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20268v2",
                "updated": "2025-04-17T11:32:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    11,
                    32,
                    53,
                    3,
                    107,
                    0
                ],
                "published": "2025-02-27T16:55:18Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    55,
                    18,
                    3,
                    58,
                    0
                ],
                "title": "Large Language Models as Attribution Regularizers for Efficient Model\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models as Attribution Regularizers for Efficient Model\n  Training"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\ndiverse domains. However, effectively leveraging their vast knowledge for\ntraining smaller downstream models remains an open challenge, especially in\ndomains like tabular data learning, where simpler models are often preferred\ndue to interpretability and efficiency.\n  In this paper, we introduce a novel yet straightforward method for\nincorporating LLM-generated global task feature attributions into the training\nprocess of smaller networks. Specifically, we propose an attribution-matching\nregularization term that aligns the training dynamics of the smaller model with\nthe insights provided by the LLM. By doing so, our approach yields superior\nperformance in few-shot learning scenarios. Notably, our method requires only\nblack-box API access to the LLM, making it easy to integrate into existing\ntraining pipelines with minimal computational overhead.\n  Furthermore, we demonstrate how this method can be used to address common\nissues in real-world datasets, such as skewness and bias. By integrating\nhigh-level knowledge from LLMs, our approach improves generalization, even when\ntraining data is limited or imbalanced. We validate its effectiveness through\nextensive experiments across multiple tasks, demonstrating improved learning\nefficiency and model robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable performance across\ndiverse domains. However, effectively leveraging their vast knowledge for\ntraining smaller downstream models remains an open challenge, especially in\ndomains like tabular data learning, where simpler models are often preferred\ndue to interpretability and efficiency.\n  In this paper, we introduce a novel yet straightforward method for\nincorporating LLM-generated global task feature attributions into the training\nprocess of smaller networks. Specifically, we propose an attribution-matching\nregularization term that aligns the training dynamics of the smaller model with\nthe insights provided by the LLM. By doing so, our approach yields superior\nperformance in few-shot learning scenarios. Notably, our method requires only\nblack-box API access to the LLM, making it easy to integrate into existing\ntraining pipelines with minimal computational overhead.\n  Furthermore, we demonstrate how this method can be used to address common\nissues in real-world datasets, such as skewness and bias. By integrating\nhigh-level knowledge from LLMs, our approach improves generalization, even when\ntraining data is limited or imbalanced. We validate its effectiveness through\nextensive experiments across multiple tasks, demonstrating improved learning\nefficiency and model robustness."
                },
                "authors": [
                    {
                        "name": "Davor Vukadin"
                    },
                    {
                        "name": "Marin Šilić"
                    },
                    {
                        "name": "Goran Delač"
                    }
                ],
                "author_detail": {
                    "name": "Goran Delač"
                },
                "author": "Goran Delač",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20268v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20268v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00046v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00046v2",
                "updated": "2025-04-17T11:29:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    11,
                    29,
                    6,
                    3,
                    107,
                    0
                ],
                "published": "2025-03-30T22:53:52Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    22,
                    53,
                    52,
                    6,
                    89,
                    0
                ],
                "title": "Multi-Stakeholder Disaster Insights from Social Media Using Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Stakeholder Disaster Insights from Social Media Using Large\n  Language Models"
                },
                "summary": "In recent years, social media has emerged as a primary channel for users to\npromptly share feedback and issues during disasters and emergencies, playing a\nkey role in crisis management. While significant progress has been made in\ncollecting and analyzing social media content, there remains a pressing need to\nenhance the automation, aggregation, and customization of this data to deliver\nactionable insights tailored to diverse stakeholders, including the press,\npolice, EMS, and firefighters. This effort is essential for improving the\ncoordination of activities such as relief efforts, resource distribution, and\nmedia communication. This paper presents a methodology that leverages the\ncapabilities of LLMs to enhance disaster response and management. Our approach\ncombines classification techniques with generative AI to bridge the gap between\nraw user feedback and stakeholder-specific reports. Social media posts shared\nduring catastrophic events are analyzed with a focus on user-reported issues,\nservice interruptions, and encountered challenges. We employ full-spectrum\nLLMs, using analytical models like BERT for precise, multi-dimensional\nclassification of content type, sentiment, emotion, geolocation, and topic.\nGenerative models such as ChatGPT are then used to produce human-readable,\ninformative reports tailored to distinct audiences, synthesizing insights\nderived from detailed classifications. We compare standard approaches, which\nanalyze posts directly using prompts in ChatGPT, to our advanced method, which\nincorporates multi-dimensional classification, sub-event selection, and\ntailored report generation. Our methodology demonstrates superior performance\nin both quantitative metrics, such as text coherence scores and latent\nrepresentations, and qualitative assessments by automated tools and field\nexperts, delivering precise insights for diverse disaster response\nstakeholders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, social media has emerged as a primary channel for users to\npromptly share feedback and issues during disasters and emergencies, playing a\nkey role in crisis management. While significant progress has been made in\ncollecting and analyzing social media content, there remains a pressing need to\nenhance the automation, aggregation, and customization of this data to deliver\nactionable insights tailored to diverse stakeholders, including the press,\npolice, EMS, and firefighters. This effort is essential for improving the\ncoordination of activities such as relief efforts, resource distribution, and\nmedia communication. This paper presents a methodology that leverages the\ncapabilities of LLMs to enhance disaster response and management. Our approach\ncombines classification techniques with generative AI to bridge the gap between\nraw user feedback and stakeholder-specific reports. Social media posts shared\nduring catastrophic events are analyzed with a focus on user-reported issues,\nservice interruptions, and encountered challenges. We employ full-spectrum\nLLMs, using analytical models like BERT for precise, multi-dimensional\nclassification of content type, sentiment, emotion, geolocation, and topic.\nGenerative models such as ChatGPT are then used to produce human-readable,\ninformative reports tailored to distinct audiences, synthesizing insights\nderived from detailed classifications. We compare standard approaches, which\nanalyze posts directly using prompts in ChatGPT, to our advanced method, which\nincorporates multi-dimensional classification, sub-event selection, and\ntailored report generation. Our methodology demonstrates superior performance\nin both quantitative metrics, such as text coherence scores and latent\nrepresentations, and qualitative assessments by automated tools and field\nexperts, delivering precise insights for diverse disaster response\nstakeholders."
                },
                "authors": [
                    {
                        "name": "Loris Belcastro"
                    },
                    {
                        "name": "Cristian Cosentino"
                    },
                    {
                        "name": "Fabrizio Marozzo"
                    },
                    {
                        "name": "Merve Gündüz-Cüre"
                    },
                    {
                        "name": "Sule Öztürk-Birim"
                    }
                ],
                "author_detail": {
                    "name": "Sule Öztürk-Birim"
                },
                "author": "Sule Öztürk-Birim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00046v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00046v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03090v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03090v2",
                "updated": "2025-04-17T11:27:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    11,
                    27,
                    57,
                    3,
                    107,
                    0
                ],
                "published": "2024-07-03T13:30:51Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    13,
                    30,
                    51,
                    2,
                    185,
                    0
                ],
                "title": "Simulating the anharmonic phonon spectrum in critical systems:\n  self-consistent phonons and temperature-dependent effective potential methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating the anharmonic phonon spectrum in critical systems:\n  self-consistent phonons and temperature-dependent effective potential methods"
                },
                "summary": "Understanding and simulating the thermodynamic and dynamical properties of\nmaterials affected by strong ionic anharmonicity is a central challenge in\nmaterial science. Much interest is in material displaying critical displacive\nbehaviour, such as near a ferroelectric transition, charge-density waves, or in\ngeneral displacive second-order transitions. In these cases, molecular dynamics\nsuffer from a critical slowdown and emergent long-range fluctuations of the\norder parameter. Two prominent methods have emerged to solve this issue:\nSelf-consistent renormalization of the phonons like the Self-Consistent\nHarmonic Approximation (SCHA) and Self-Consistent Phonons (SCP), and methods\nthat fit the potential energy landscape from short molecular dynamics\ntrajectories, like the Temperature-Dependent Effective Potential (TDEP).\nDespite their widespread use, the limitations of these methods are often\noverlooked in the proximity of critical points.\n  Here, we establish a guiding rule set for the accuracy of each method on\ncritical quantities: free energy for computing the phase diagrams, static\ncorrelation functions for inferring phase stability and critical behaviours,\nand dynamic correlation functions for vibrational spectra and thermal\ntransport. Also, a new TDEP implementation is introduced to fix the calculation\nof dynamical spectra, restoring the correct perturbative limit violated by the\nstandard TDEP approach.\n  Results are benchmarked both against an exact one-dimensional anharmonic\npotential and two prototypical anharmonic crystals: the ferroelectric PbTe and\nthe metal-halide perovskite CsSnI3.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and simulating the thermodynamic and dynamical properties of\nmaterials affected by strong ionic anharmonicity is a central challenge in\nmaterial science. Much interest is in material displaying critical displacive\nbehaviour, such as near a ferroelectric transition, charge-density waves, or in\ngeneral displacive second-order transitions. In these cases, molecular dynamics\nsuffer from a critical slowdown and emergent long-range fluctuations of the\norder parameter. Two prominent methods have emerged to solve this issue:\nSelf-consistent renormalization of the phonons like the Self-Consistent\nHarmonic Approximation (SCHA) and Self-Consistent Phonons (SCP), and methods\nthat fit the potential energy landscape from short molecular dynamics\ntrajectories, like the Temperature-Dependent Effective Potential (TDEP).\nDespite their widespread use, the limitations of these methods are often\noverlooked in the proximity of critical points.\n  Here, we establish a guiding rule set for the accuracy of each method on\ncritical quantities: free energy for computing the phase diagrams, static\ncorrelation functions for inferring phase stability and critical behaviours,\nand dynamic correlation functions for vibrational spectra and thermal\ntransport. Also, a new TDEP implementation is introduced to fix the calculation\nof dynamical spectra, restoring the correct perturbative limit violated by the\nstandard TDEP approach.\n  Results are benchmarked both against an exact one-dimensional anharmonic\npotential and two prototypical anharmonic crystals: the ferroelectric PbTe and\nthe metal-halide perovskite CsSnI3."
                },
                "authors": [
                    {
                        "name": "Lorenzo Monacelli"
                    }
                ],
                "author_detail": {
                    "name": "Lorenzo Monacelli"
                },
                "author": "Lorenzo Monacelli",
                "arxiv_comment": "25 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03090v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03090v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07066v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07066v2",
                "updated": "2025-04-17T11:18:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    11,
                    18,
                    59,
                    3,
                    107,
                    0
                ],
                "published": "2024-06-11T08:50:55Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    8,
                    50,
                    55,
                    1,
                    163,
                    0
                ],
                "title": "Inferring the dependence graph density of binary graphical models in\n  high dimension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring the dependence graph density of binary graphical models in\n  high dimension"
                },
                "summary": "We consider a system of binary interacting chains describing the dynamics of\na group of $N$ components that, at each time unit, either send some signal to\nthe others or remain silent otherwise. The interactions among the chains are\nencoded by a directed Erd\\\"os-R\\'enyi random graph with unknown parameter $ p\n\\in (0, 1) .$ Moreover, the system is structured within two populations\n(excitatory chains versus inhibitory ones) which are coupled via a mean field\ninteraction on the underlying Erd\\\"os-R\\'enyi graph. In this paper, we address\nthe question of inferring the connectivity parameter $p$ based only on the\nobservation of the interacting chains over $T$ time units. In our main result,\nwe show that the connectivity parameter $p$ can be estimated with rate\n$N^{-1/2}+N^{1/2}/T+(\\log(T)/T)^{1/2}$ through an easy-to-compute estimator.\nOur analysis relies on a precise study of the spatio-temporal decay of\ncorrelations of the interacting chains. This is done through the study of\ncoalescing random walks defining a backward regeneration representation of the\nsystem. Interestingly, we also show that this backward regeneration\nrepresentation allows us to perfectly sample the system of interacting chains\n(conditionally on each realization of the underlying Erd\\\"os-R\\'enyi graph)\nfrom its stationary distribution. These probabilistic results have an interest\nin its own.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a system of binary interacting chains describing the dynamics of\na group of $N$ components that, at each time unit, either send some signal to\nthe others or remain silent otherwise. The interactions among the chains are\nencoded by a directed Erd\\\"os-R\\'enyi random graph with unknown parameter $ p\n\\in (0, 1) .$ Moreover, the system is structured within two populations\n(excitatory chains versus inhibitory ones) which are coupled via a mean field\ninteraction on the underlying Erd\\\"os-R\\'enyi graph. In this paper, we address\nthe question of inferring the connectivity parameter $p$ based only on the\nobservation of the interacting chains over $T$ time units. In our main result,\nwe show that the connectivity parameter $p$ can be estimated with rate\n$N^{-1/2}+N^{1/2}/T+(\\log(T)/T)^{1/2}$ through an easy-to-compute estimator.\nOur analysis relies on a precise study of the spatio-temporal decay of\ncorrelations of the interacting chains. This is done through the study of\ncoalescing random walks defining a backward regeneration representation of the\nsystem. Interestingly, we also show that this backward regeneration\nrepresentation allows us to perfectly sample the system of interacting chains\n(conditionally on each realization of the underlying Erd\\\"os-R\\'enyi graph)\nfrom its stationary distribution. These probabilistic results have an interest\nin its own."
                },
                "authors": [
                    {
                        "name": "Julien Chevallier"
                    },
                    {
                        "name": "Eva Löcherbach"
                    },
                    {
                        "name": "Guilherme Ost"
                    }
                ],
                "author_detail": {
                    "name": "Guilherme Ost"
                },
                "author": "Guilherme Ost",
                "arxiv_comment": "85 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07066v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07066v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62M05, 60J10, 60K35, 62F12",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.11458v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.11458v2",
                "updated": "2025-04-17T11:16:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    11,
                    16,
                    7,
                    3,
                    107,
                    0
                ],
                "published": "2023-08-22T14:08:05Z",
                "published_parsed": [
                    2023,
                    8,
                    22,
                    14,
                    8,
                    5,
                    1,
                    234,
                    0
                ],
                "title": "Towards a unified approach to formal risk of bias assessments for causal\n  and descriptive inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a unified approach to formal risk of bias assessments for causal\n  and descriptive inference"
                },
                "summary": "Statistics is sometimes described as the science of reasoning under\nuncertainty. Statistical models provide one view of this uncertainty, but what\nis frequently neglected is the invisible portion of uncertainty: that assumed\nnot to exist once a model has been fitted to some data. Systematic errors, i.e.\nbias, in data relative to some model and inferential goal can seriously\nundermine research conclusions, and qualitative and quantitative techniques\nhave been created across several disciplines to quantify and generally appraise\nsuch potential biases. Perhaps best known are so-called risk of bias assessment\ninstruments used to investigate the quality of randomised controlled trials in\nmedical research. However, the logic of assessing the risks caused by various\ntypes of systematic error to statistical arguments applies far more widely.\nThis logic applies even when statistical adjustment strategies for potential\nbiases are used, as these frequently make assumptions (e.g. data missing at\nrandom) that can never be guaranteed. Mounting concern about such situations\ncan be seen in the increasing calls for greater consideration of biases caused\nby nonprobability sampling in descriptive inference (e.g. in survey sampling),\nand the statistical generalisability of in-sample causal effect estimates in\ncausal inference. These both relate to the consideration of model-based and\nwider uncertainty when presenting research conclusions from models. Given that\nmodel-based adjustments are never perfect, we argue that qualitative risk of\nbias reporting frameworks for both descriptive and causal inferential arguments\nshould be further developed and made mandatory by journals and funders. It is\nonly through clear statements of the limits to statistical arguments that\nconsumers of research can fully judge their value for any specific application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistics is sometimes described as the science of reasoning under\nuncertainty. Statistical models provide one view of this uncertainty, but what\nis frequently neglected is the invisible portion of uncertainty: that assumed\nnot to exist once a model has been fitted to some data. Systematic errors, i.e.\nbias, in data relative to some model and inferential goal can seriously\nundermine research conclusions, and qualitative and quantitative techniques\nhave been created across several disciplines to quantify and generally appraise\nsuch potential biases. Perhaps best known are so-called risk of bias assessment\ninstruments used to investigate the quality of randomised controlled trials in\nmedical research. However, the logic of assessing the risks caused by various\ntypes of systematic error to statistical arguments applies far more widely.\nThis logic applies even when statistical adjustment strategies for potential\nbiases are used, as these frequently make assumptions (e.g. data missing at\nrandom) that can never be guaranteed. Mounting concern about such situations\ncan be seen in the increasing calls for greater consideration of biases caused\nby nonprobability sampling in descriptive inference (e.g. in survey sampling),\nand the statistical generalisability of in-sample causal effect estimates in\ncausal inference. These both relate to the consideration of model-based and\nwider uncertainty when presenting research conclusions from models. Given that\nmodel-based adjustments are never perfect, we argue that qualitative risk of\nbias reporting frameworks for both descriptive and causal inferential arguments\nshould be further developed and made mandatory by journals and funders. It is\nonly through clear statements of the limits to statistical arguments that\nconsumers of research can fully judge their value for any specific application."
                },
                "authors": [
                    {
                        "name": "Oliver L. Pescott"
                    },
                    {
                        "name": "Robin J. Boyd"
                    },
                    {
                        "name": "Gary D. Powney"
                    },
                    {
                        "name": "Gavin B. Stewart"
                    }
                ],
                "author_detail": {
                    "name": "Gavin B. Stewart"
                },
                "author": "Gavin B. Stewart",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.11458v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.11458v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.12599v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.12599v7",
                "updated": "2025-04-17T11:14:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    11,
                    14,
                    20,
                    3,
                    107,
                    0
                ],
                "published": "2023-05-21T23:16:26Z",
                "published_parsed": [
                    2023,
                    5,
                    21,
                    23,
                    16,
                    26,
                    6,
                    141,
                    0
                ],
                "title": "Abstract Meaning Representation-Based Logic-Driven Data Augmentation for\n  Logical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abstract Meaning Representation-Based Logic-Driven Data Augmentation for\n  Logical Reasoning"
                },
                "summary": "Combining large language models with logical reasoning enhances their\ncapacity to address problems in a robust and reliable manner. Nevertheless, the\nintricate nature of logical reasoning poses challenges when gathering reliable\ndata from the web to build comprehensive training datasets, subsequently\naffecting performance on downstream tasks. To address this, we introduce a\nnovel logic-driven data augmentation approach, AMR-LDA. AMR-LDA converts the\noriginal text into an Abstract Meaning Representation (AMR) graph, a structured\nsemantic representation that encapsulates the logical structure of the\nsentence, upon which operations are performed to generate logically modified\nAMR graphs. The modified AMR graphs are subsequently converted back into text\nto create augmented data. Notably, our methodology is architecture-agnostic and\nenhances both generative large language models, such as GPT-3.5 and GPT-4,\nthrough prompt augmentation, and discriminative large language models through\ncontrastive learning with logic-driven data augmentation. Empirical evidence\nunderscores the efficacy of our proposed method with improvement in performance\nacross seven downstream tasks, such as reading comprehension requiring logical\nreasoning, textual entailment, and natural language inference. Furthermore, our\nmethod leads on the ReClor leaderboard at\nhttps://eval.ai/web/challenges/challenge-page/503/leaderboard/1347. The source\ncode and data are publicly available at\nhttps://github.com/Strong-AI-Lab/Logical-Equivalence-driven-AMR-Data-Augmentation-for-Representation-Learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining large language models with logical reasoning enhances their\ncapacity to address problems in a robust and reliable manner. Nevertheless, the\nintricate nature of logical reasoning poses challenges when gathering reliable\ndata from the web to build comprehensive training datasets, subsequently\naffecting performance on downstream tasks. To address this, we introduce a\nnovel logic-driven data augmentation approach, AMR-LDA. AMR-LDA converts the\noriginal text into an Abstract Meaning Representation (AMR) graph, a structured\nsemantic representation that encapsulates the logical structure of the\nsentence, upon which operations are performed to generate logically modified\nAMR graphs. The modified AMR graphs are subsequently converted back into text\nto create augmented data. Notably, our methodology is architecture-agnostic and\nenhances both generative large language models, such as GPT-3.5 and GPT-4,\nthrough prompt augmentation, and discriminative large language models through\ncontrastive learning with logic-driven data augmentation. Empirical evidence\nunderscores the efficacy of our proposed method with improvement in performance\nacross seven downstream tasks, such as reading comprehension requiring logical\nreasoning, textual entailment, and natural language inference. Furthermore, our\nmethod leads on the ReClor leaderboard at\nhttps://eval.ai/web/challenges/challenge-page/503/leaderboard/1347. The source\ncode and data are publicly available at\nhttps://github.com/Strong-AI-Lab/Logical-Equivalence-driven-AMR-Data-Augmentation-for-Representation-Learning."
                },
                "authors": [
                    {
                        "name": "Qiming Bao"
                    },
                    {
                        "name": "Alex Yuxuan Peng"
                    },
                    {
                        "name": "Zhenyun Deng"
                    },
                    {
                        "name": "Wanjun Zhong"
                    },
                    {
                        "name": "Gael Gendron"
                    },
                    {
                        "name": "Timothy Pistotti"
                    },
                    {
                        "name": "Neset Tan"
                    },
                    {
                        "name": "Nathan Young"
                    },
                    {
                        "name": "Yang Chen"
                    },
                    {
                        "name": "Yonghua Zhu"
                    },
                    {
                        "name": "Paul Denny"
                    },
                    {
                        "name": "Michael Witbrock"
                    },
                    {
                        "name": "Jiamou Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jiamou Liu"
                },
                "author": "Jiamou Liu",
                "arxiv_comment": "21 pages, 8 figures, the Findings of ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.12599v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.12599v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2207.14000v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2207.14000v4",
                "updated": "2025-04-17T11:11:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    11,
                    11,
                    51,
                    3,
                    107,
                    0
                ],
                "published": "2022-07-28T10:44:46Z",
                "published_parsed": [
                    2022,
                    7,
                    28,
                    10,
                    44,
                    46,
                    3,
                    209,
                    0
                ],
                "title": "Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study\n  on Out-of-Distribution Generalisation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study\n  on Out-of-Distribution Generalisation"
                },
                "summary": "Combining deep learning with symbolic logic reasoning aims to capitalize on\nthe success of both fields and is drawing increasing attention. Inspired by\nDeepLogic, an end-to-end model trained to perform inference on logic programs,\nwe introduce IMA-GloVe-GA, an iterative neural inference network for multi-step\nreasoning expressed in natural language. In our model, reasoning is performed\nusing an iterative memory neural network based on RNN with a gated attention\nmechanism. We evaluate IMA-GloVe-GA on three datasets: PARARULES, CONCEPTRULES\nV1 and CONCEPTRULES V2. Experimental results show DeepLogic with gated\nattention can achieve higher test accuracy than DeepLogic and other RNN\nbaseline models. Our model achieves better out-of-distribution generalisation\nthan RoBERTa-Large when the rules have been shuffled. Furthermore, to address\nthe issue of unbalanced distribution of reasoning depths in the current\nmulti-step reasoning datasets, we develop PARARULE-Plus, a large dataset with\nmore examples that require deeper reasoning steps. Experimental results show\nthat the addition of PARARULE-Plus can increase the model's performance on\nexamples requiring deeper reasoning depths. The source code and data are\navailable at\nhttps://github.com/Strong-AI-Lab/Multi-Step-Deductive-Reasoning-Over-Natural-Language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining deep learning with symbolic logic reasoning aims to capitalize on\nthe success of both fields and is drawing increasing attention. Inspired by\nDeepLogic, an end-to-end model trained to perform inference on logic programs,\nwe introduce IMA-GloVe-GA, an iterative neural inference network for multi-step\nreasoning expressed in natural language. In our model, reasoning is performed\nusing an iterative memory neural network based on RNN with a gated attention\nmechanism. We evaluate IMA-GloVe-GA on three datasets: PARARULES, CONCEPTRULES\nV1 and CONCEPTRULES V2. Experimental results show DeepLogic with gated\nattention can achieve higher test accuracy than DeepLogic and other RNN\nbaseline models. Our model achieves better out-of-distribution generalisation\nthan RoBERTa-Large when the rules have been shuffled. Furthermore, to address\nthe issue of unbalanced distribution of reasoning depths in the current\nmulti-step reasoning datasets, we develop PARARULE-Plus, a large dataset with\nmore examples that require deeper reasoning steps. Experimental results show\nthat the addition of PARARULE-Plus can increase the model's performance on\nexamples requiring deeper reasoning depths. The source code and data are\navailable at\nhttps://github.com/Strong-AI-Lab/Multi-Step-Deductive-Reasoning-Over-Natural-Language."
                },
                "authors": [
                    {
                        "name": "Qiming Bao"
                    },
                    {
                        "name": "Alex Yuxuan Peng"
                    },
                    {
                        "name": "Tim Hartill"
                    },
                    {
                        "name": "Neset Tan"
                    },
                    {
                        "name": "Zhenyun Deng"
                    },
                    {
                        "name": "Michael Witbrock"
                    },
                    {
                        "name": "Jiamou Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jiamou Liu"
                },
                "author": "Jiamou Liu",
                "arxiv_comment": "10 pages, 3 figures, The 2nd International Joint Conference on\n  Learning & Reasoning and 16th International Workshop on Neural-Symbolic\n  Learning and Reasoning (IJCLR-NeSy 2022)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2207.14000v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2207.14000v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12845v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12845v1",
                "updated": "2025-04-17T11:02:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    11,
                    2,
                    35,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T11:02:35Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    11,
                    2,
                    35,
                    3,
                    107,
                    0
                ],
                "title": "Can LLMs reason over extended multilingual contexts? Towards\n  long-context evaluation beyond retrieval and haystacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs reason over extended multilingual contexts? Towards\n  long-context evaluation beyond retrieval and haystacks"
                },
                "summary": "Existing multilingual long-context benchmarks, often based on the popular\nneedle-in-a-haystack test, primarily evaluate a model's ability to locate\nspecific information buried within irrelevant texts. However, such a\nretrieval-centric approach is myopic and inherently limited, as successful\nrecall alone does not indicate a model's capacity to reason over extended\ncontexts. Moreover, these benchmarks are susceptible to data leakage,\nshort-circuiting, and risk making the evaluation a priori identifiable. To\naddress these limitations, we introduce MLRBench, a new synthetic benchmark for\nmultilingual long-context reasoning. Unlike existing benchmarks, MLRBench goes\nbeyond surface-level retrieval by including tasks that assess multi-hop\ninference, aggregation, and epistemic reasoning. Spanning seven languages,\nMLRBench is designed to be parallel, resistant to leakage, and scalable to\narbitrary context lengths. Our extensive experiments with an open-weight large\nlanguage model (LLM) reveal a pronounced gap between high- and low-resource\nlanguages, particularly for tasks requiring the model to aggregate multiple\nfacts or predict the absence of information. We also find that, in multilingual\nsettings, LLMs effectively utilize less than 30% of their claimed context\nlength. Although off-the-shelf Retrieval Augmented Generation helps alleviate\nthis to a certain extent, it does not solve the long-context problem. We\nopen-source MLRBench to enable future research in improved evaluation and\ntraining of multilingual LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing multilingual long-context benchmarks, often based on the popular\nneedle-in-a-haystack test, primarily evaluate a model's ability to locate\nspecific information buried within irrelevant texts. However, such a\nretrieval-centric approach is myopic and inherently limited, as successful\nrecall alone does not indicate a model's capacity to reason over extended\ncontexts. Moreover, these benchmarks are susceptible to data leakage,\nshort-circuiting, and risk making the evaluation a priori identifiable. To\naddress these limitations, we introduce MLRBench, a new synthetic benchmark for\nmultilingual long-context reasoning. Unlike existing benchmarks, MLRBench goes\nbeyond surface-level retrieval by including tasks that assess multi-hop\ninference, aggregation, and epistemic reasoning. Spanning seven languages,\nMLRBench is designed to be parallel, resistant to leakage, and scalable to\narbitrary context lengths. Our extensive experiments with an open-weight large\nlanguage model (LLM) reveal a pronounced gap between high- and low-resource\nlanguages, particularly for tasks requiring the model to aggregate multiple\nfacts or predict the absence of information. We also find that, in multilingual\nsettings, LLMs effectively utilize less than 30% of their claimed context\nlength. Although off-the-shelf Retrieval Augmented Generation helps alleviate\nthis to a certain extent, it does not solve the long-context problem. We\nopen-source MLRBench to enable future research in improved evaluation and\ntraining of multilingual LLMs."
                },
                "authors": [
                    {
                        "name": "Amey Hengle"
                    },
                    {
                        "name": "Prasoon Bajpai"
                    },
                    {
                        "name": "Soham Dan"
                    },
                    {
                        "name": "Tanmoy Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Chakraborty"
                },
                "author": "Tanmoy Chakraborty",
                "arxiv_comment": "33 Pages in Total - 23 (Main Manuscript) + 10 (Appendix)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12845v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12845v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12820v1",
                "updated": "2025-04-17T10:25:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    10,
                    25,
                    11,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T10:25:11Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    10,
                    25,
                    11,
                    3,
                    107,
                    0
                ],
                "title": "Skewness as a Probe of Gravity: Real and Redshift Space Counts-In-Cells",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skewness as a Probe of Gravity: Real and Redshift Space Counts-In-Cells"
                },
                "summary": "We study the counts-in-cells reduced skewness $s_3$ for dark matter, halo,\nand galaxy distributions in both real and redshift space, using the ELEPHANT\n($\\textit{Extended LEnsing PHysics with ANalytical ray Tracing}$) suite of\n$N$-body simulations. We compare General Relativity (GR) with two extended (EG)\ngravity models: $f(R)$ gravity with chameleon screening and the normal-branch\nDvali-Gabadadze-Porrati (nDGP) model with Vainshtein screening. We quantify the\nsuppression of $s_3$ by redshift-space distortions (RSD), finding that while\nsmall-scale skewness is strongly reduced, the $F5$ model retains a $\\sim 4\\%$\ndeviation from GR in galaxy samples, corresponding to a $2\\sigma$ significance.\nWe show that the ratio $s_3^{\\mathrm{RSD}}/s_3^{\\mathrm{real}}$ is\napproximately independent of the gravity model across tracers and redshifts.\nOur results demonstrate that real-space predictions can help reliably infer\nredshift-space skewness in both GR and extended gravity, providing a new tool\nfor testing gravity with current and forthcoming galaxy redshift surveys.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the counts-in-cells reduced skewness $s_3$ for dark matter, halo,\nand galaxy distributions in both real and redshift space, using the ELEPHANT\n($\\textit{Extended LEnsing PHysics with ANalytical ray Tracing}$) suite of\n$N$-body simulations. We compare General Relativity (GR) with two extended (EG)\ngravity models: $f(R)$ gravity with chameleon screening and the normal-branch\nDvali-Gabadadze-Porrati (nDGP) model with Vainshtein screening. We quantify the\nsuppression of $s_3$ by redshift-space distortions (RSD), finding that while\nsmall-scale skewness is strongly reduced, the $F5$ model retains a $\\sim 4\\%$\ndeviation from GR in galaxy samples, corresponding to a $2\\sigma$ significance.\nWe show that the ratio $s_3^{\\mathrm{RSD}}/s_3^{\\mathrm{real}}$ is\napproximately independent of the gravity model across tracers and redshifts.\nOur results demonstrate that real-space predictions can help reliably infer\nredshift-space skewness in both GR and extended gravity, providing a new tool\nfor testing gravity with current and forthcoming galaxy redshift surveys."
                },
                "authors": [
                    {
                        "name": "Paweł Drozda"
                    },
                    {
                        "name": "Wojciech A. Hellwing"
                    },
                    {
                        "name": "Maciej Bilicki"
                    }
                ],
                "author_detail": {
                    "name": "Maciej Bilicki"
                },
                "author": "Maciej Bilicki",
                "arxiv_comment": "14 pages, 9 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12805v1",
                "updated": "2025-04-17T10:10:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    10,
                    10,
                    25,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T10:10:25Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    10,
                    10,
                    25,
                    3,
                    107,
                    0
                ],
                "title": "Assesing LLMs in Art Contexts: Critique Generation and Theory of Mind\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assesing LLMs in Art Contexts: Critique Generation and Theory of Mind\n  Evaluation"
                },
                "summary": "This study explored how large language models (LLMs) perform in two areas\nrelated to art: writing critiques of artworks and reasoning about mental states\n(Theory of Mind, or ToM) in art-related situations. For the critique generation\npart, we built a system that combines Noel Carroll's evaluative framework with\na broad selection of art criticism theories. The model was prompted to first\nwrite a full-length critique and then shorter, more coherent versions using a\nstep-by-step prompting process. These AI-generated critiques were then compared\nwith those written by human experts in a Turing test-style evaluation. In many\ncases, human subjects had difficulty telling which was which, and the results\nsuggest that LLMs can produce critiques that are not only plausible in style\nbut also rich in interpretation, as long as they are carefully guided. In the\nsecond part, we introduced new simple ToM tasks based on situations involving\ninterpretation, emotion, and moral tension, which can appear in the context of\nart. These go beyond standard false-belief tests and allow for more complex,\nsocially embedded forms of reasoning. We tested 41 recent LLMs and found that\ntheir performance varied across tasks and models. In particular, tasks that\ninvolved affective or ambiguous situations tended to reveal clearer\ndifferences. Taken together, these results help clarify how LLMs respond to\ncomplex interpretative challenges, revealing both their cognitive limitations\nand potential. While our findings do not directly contradict the so-called\nGenerative AI Paradox--the idea that LLMs can produce expert-like output\nwithout genuine understanding--they suggest that, depending on how LLMs are\ninstructed, such as through carefully designed prompts, these models may begin\nto show behaviors that resemble understanding more closely than we might\nassume.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explored how large language models (LLMs) perform in two areas\nrelated to art: writing critiques of artworks and reasoning about mental states\n(Theory of Mind, or ToM) in art-related situations. For the critique generation\npart, we built a system that combines Noel Carroll's evaluative framework with\na broad selection of art criticism theories. The model was prompted to first\nwrite a full-length critique and then shorter, more coherent versions using a\nstep-by-step prompting process. These AI-generated critiques were then compared\nwith those written by human experts in a Turing test-style evaluation. In many\ncases, human subjects had difficulty telling which was which, and the results\nsuggest that LLMs can produce critiques that are not only plausible in style\nbut also rich in interpretation, as long as they are carefully guided. In the\nsecond part, we introduced new simple ToM tasks based on situations involving\ninterpretation, emotion, and moral tension, which can appear in the context of\nart. These go beyond standard false-belief tests and allow for more complex,\nsocially embedded forms of reasoning. We tested 41 recent LLMs and found that\ntheir performance varied across tasks and models. In particular, tasks that\ninvolved affective or ambiguous situations tended to reveal clearer\ndifferences. Taken together, these results help clarify how LLMs respond to\ncomplex interpretative challenges, revealing both their cognitive limitations\nand potential. While our findings do not directly contradict the so-called\nGenerative AI Paradox--the idea that LLMs can produce expert-like output\nwithout genuine understanding--they suggest that, depending on how LLMs are\ninstructed, such as through carefully designed prompts, these models may begin\nto show behaviors that resemble understanding more closely than we might\nassume."
                },
                "authors": [
                    {
                        "name": "Takaya Arita"
                    },
                    {
                        "name": "Wenxian Zheng"
                    },
                    {
                        "name": "Reiji Suzuki"
                    },
                    {
                        "name": "Fuminori Akiba"
                    }
                ],
                "author_detail": {
                    "name": "Fuminori Akiba"
                },
                "author": "Fuminori Akiba",
                "arxiv_comment": "30 pages, 13 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12799v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12799v1",
                "updated": "2025-04-17T10:00:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    10,
                    0,
                    9,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T10:00:09Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    10,
                    0,
                    9,
                    3,
                    107,
                    0
                ],
                "title": "TSGS: Improving Gaussian Splatting for Transparent Surface\n  Reconstruction via Normal and De-lighting Priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TSGS: Improving Gaussian Splatting for Transparent Surface\n  Reconstruction via Normal and De-lighting Priors"
                },
                "summary": "Reconstructing transparent surfaces is essential for tasks such as robotic\nmanipulation in labs, yet it poses a significant challenge for 3D\nreconstruction techniques like 3D Gaussian Splatting (3DGS). These methods\noften encounter a transparency-depth dilemma, where the pursuit of\nphotorealistic rendering through standard $\\alpha$-blending undermines\ngeometric precision, resulting in considerable depth estimation errors for\ntransparent materials. To address this issue, we introduce Transparent Surface\nGaussian Splatting (TSGS), a new framework that separates geometry learning\nfrom appearance refinement. In the geometry learning stage, TSGS focuses on\ngeometry by using specular-suppressed inputs to accurately represent surfaces.\nIn the second stage, TSGS improves visual fidelity through anisotropic specular\nmodeling, crucially maintaining the established opacity to ensure geometric\naccuracy. To enhance depth inference, TSGS employs a first-surface depth\nextraction method. This technique uses a sliding window over $\\alpha$-blending\nweights to pinpoint the most likely surface location and calculates a robust\nweighted average depth. To evaluate the transparent surface reconstruction task\nunder realistic conditions, we collect a TransLab dataset that includes complex\ntransparent laboratory glassware. Extensive experiments on TransLab show that\nTSGS achieves accurate geometric reconstruction and realistic rendering of\ntransparent objects simultaneously within the efficient 3DGS framework.\nSpecifically, TSGS significantly surpasses current leading methods, achieving a\n37.3% reduction in chamfer distance and an 8.0% improvement in F1 score\ncompared to the top baseline. The code and dataset will be released at\nhttps://longxiang-ai.github.io/TSGS/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstructing transparent surfaces is essential for tasks such as robotic\nmanipulation in labs, yet it poses a significant challenge for 3D\nreconstruction techniques like 3D Gaussian Splatting (3DGS). These methods\noften encounter a transparency-depth dilemma, where the pursuit of\nphotorealistic rendering through standard $\\alpha$-blending undermines\ngeometric precision, resulting in considerable depth estimation errors for\ntransparent materials. To address this issue, we introduce Transparent Surface\nGaussian Splatting (TSGS), a new framework that separates geometry learning\nfrom appearance refinement. In the geometry learning stage, TSGS focuses on\ngeometry by using specular-suppressed inputs to accurately represent surfaces.\nIn the second stage, TSGS improves visual fidelity through anisotropic specular\nmodeling, crucially maintaining the established opacity to ensure geometric\naccuracy. To enhance depth inference, TSGS employs a first-surface depth\nextraction method. This technique uses a sliding window over $\\alpha$-blending\nweights to pinpoint the most likely surface location and calculates a robust\nweighted average depth. To evaluate the transparent surface reconstruction task\nunder realistic conditions, we collect a TransLab dataset that includes complex\ntransparent laboratory glassware. Extensive experiments on TransLab show that\nTSGS achieves accurate geometric reconstruction and realistic rendering of\ntransparent objects simultaneously within the efficient 3DGS framework.\nSpecifically, TSGS significantly surpasses current leading methods, achieving a\n37.3% reduction in chamfer distance and an 8.0% improvement in F1 score\ncompared to the top baseline. The code and dataset will be released at\nhttps://longxiang-ai.github.io/TSGS/."
                },
                "authors": [
                    {
                        "name": "Mingwei Li"
                    },
                    {
                        "name": "Pu Pang"
                    },
                    {
                        "name": "Hehe Fan"
                    },
                    {
                        "name": "Hua Huang"
                    },
                    {
                        "name": "Yi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Yang"
                },
                "author": "Yi Yang",
                "arxiv_comment": "Project page: https://longxiang-ai.github.io/TSGS/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12799v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12799v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12795v1",
                "updated": "2025-04-17T09:56:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    9,
                    56,
                    35,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T09:56:35Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    9,
                    56,
                    35,
                    3,
                    107,
                    0
                ],
                "title": "EarthGPT-X: Enabling MLLMs to Flexibly and Comprehensively Understand\n  Multi-Source Remote Sensing Imagery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EarthGPT-X: Enabling MLLMs to Flexibly and Comprehensively Understand\n  Multi-Source Remote Sensing Imagery"
                },
                "summary": "Recent advances in the visual-language area have developed natural\nmulti-modal large language models (MLLMs) for spatial reasoning through visual\nprompting. However, due to remote sensing (RS) imagery containing abundant\ngeospatial information that differs from natural images, it is challenging to\neffectively adapt natural spatial models to the RS domain. Moreover, current RS\nMLLMs are limited in overly narrow interpretation levels and interaction\nmanner, hindering their applicability in real-world scenarios. To address those\nchallenges, a spatial MLLM named EarthGPT-X is proposed, enabling a\ncomprehensive understanding of multi-source RS imagery, such as optical,\nsynthetic aperture radar (SAR), and infrared. EarthGPT-X offers zoom-in and\nzoom-out insight, and possesses flexible multi-grained interactive abilities.\nMoreover, EarthGPT-X unifies two types of critical spatial tasks (i.e.,\nreferring and grounding) into a visual prompting framework. To achieve these\nversatile capabilities, several key strategies are developed. The first is the\nmulti-modal content integration method, which enhances the interplay between\nimages, visual prompts, and text instructions. Subsequently, a cross-domain\none-stage fusion training strategy is proposed, utilizing the large language\nmodel (LLM) as a unified interface for multi-source multi-task learning.\nFurthermore, by incorporating a pixel perception module, the referring and\ngrounding tasks are seamlessly unified within a single framework. In addition,\nthe experiments conducted demonstrate the superiority of the proposed\nEarthGPT-X in multi-grained tasks and its impressive flexibility in multi-modal\ninteraction, revealing significant advancements of MLLM in the RS field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in the visual-language area have developed natural\nmulti-modal large language models (MLLMs) for spatial reasoning through visual\nprompting. However, due to remote sensing (RS) imagery containing abundant\ngeospatial information that differs from natural images, it is challenging to\neffectively adapt natural spatial models to the RS domain. Moreover, current RS\nMLLMs are limited in overly narrow interpretation levels and interaction\nmanner, hindering their applicability in real-world scenarios. To address those\nchallenges, a spatial MLLM named EarthGPT-X is proposed, enabling a\ncomprehensive understanding of multi-source RS imagery, such as optical,\nsynthetic aperture radar (SAR), and infrared. EarthGPT-X offers zoom-in and\nzoom-out insight, and possesses flexible multi-grained interactive abilities.\nMoreover, EarthGPT-X unifies two types of critical spatial tasks (i.e.,\nreferring and grounding) into a visual prompting framework. To achieve these\nversatile capabilities, several key strategies are developed. The first is the\nmulti-modal content integration method, which enhances the interplay between\nimages, visual prompts, and text instructions. Subsequently, a cross-domain\none-stage fusion training strategy is proposed, utilizing the large language\nmodel (LLM) as a unified interface for multi-source multi-task learning.\nFurthermore, by incorporating a pixel perception module, the referring and\ngrounding tasks are seamlessly unified within a single framework. In addition,\nthe experiments conducted demonstrate the superiority of the proposed\nEarthGPT-X in multi-grained tasks and its impressive flexibility in multi-modal\ninteraction, revealing significant advancements of MLLM in the RS field."
                },
                "authors": [
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Miaoxin Cai"
                    },
                    {
                        "name": "Yaqian Ning"
                    },
                    {
                        "name": "Tong Zhang"
                    },
                    {
                        "name": "Yin Zhuang"
                    },
                    {
                        "name": "He Chen"
                    },
                    {
                        "name": "Jun Li"
                    },
                    {
                        "name": "Xuerui Mao"
                    }
                ],
                "author_detail": {
                    "name": "Xuerui Mao"
                },
                "author": "Xuerui Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12794v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12794v1",
                "updated": "2025-04-17T09:55:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    9,
                    55,
                    3,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T09:55:03Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    9,
                    55,
                    3,
                    3,
                    107,
                    0
                ],
                "title": "Supporting Urban Low-Altitude Economy: Channel Gain Map Inference Based\n  on 3D Conditional GAN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supporting Urban Low-Altitude Economy: Channel Gain Map Inference Based\n  on 3D Conditional GAN"
                },
                "summary": "The advancement of advanced air mobility (AAM) in recent years has given rise\nto the concept of low-altitude economy (LAE). However, the diverse flight\nactivities associated with the emerging LAE applications in urban scenarios\nconfront complex physical environments, which urgently necessitates ubiquitous\nand reliable communication to guarantee the operation safety of the\nlow-altitude aircraft. As one of promising technologies for the sixth\ngeneration (6G) mobile networks, channel knowledge map (CKM) enables the\nenvironment-aware communication by constructing a site-specific dataset,\nthereby providing a priori on-site information for the aircraft to obtain the\nchannel state information (CSI) at arbitrary locations with much reduced online\noverhead. Diverse base station (BS) deployments in the three-dimensional (3D)\nurban low-altitude environment require efficient 3D CKM construction to capture\nspatial channel characteristics with less overhead. Towards this end, this\npaper proposes a 3D channel gain map (CGM) inference method based on a 3D\nconditional generative adversarial network (3D-CGAN). Specifically, we first\nanalyze the potential deployment types of BSs in urban low-altitude scenario,\nand investigate the CGM representation with the corresponding 3D channel gain\nmodel. The framework of the proposed 3D-CGAN is then discussed, which is\ntrained by a dataset consisting of existing CGMs. Consequently, the trained\n3D-CGAN is capable of inferring the corresponding CGM only based on the BS\ncoordinate without additional measurement. The simulation results demonstrate\nthat the CGMs inferred by the proposed 3D-CGAN outperform those of the\nbenchmark schemes, which can accurately reflect the radio propagation condition\nin 3D environment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement of advanced air mobility (AAM) in recent years has given rise\nto the concept of low-altitude economy (LAE). However, the diverse flight\nactivities associated with the emerging LAE applications in urban scenarios\nconfront complex physical environments, which urgently necessitates ubiquitous\nand reliable communication to guarantee the operation safety of the\nlow-altitude aircraft. As one of promising technologies for the sixth\ngeneration (6G) mobile networks, channel knowledge map (CKM) enables the\nenvironment-aware communication by constructing a site-specific dataset,\nthereby providing a priori on-site information for the aircraft to obtain the\nchannel state information (CSI) at arbitrary locations with much reduced online\noverhead. Diverse base station (BS) deployments in the three-dimensional (3D)\nurban low-altitude environment require efficient 3D CKM construction to capture\nspatial channel characteristics with less overhead. Towards this end, this\npaper proposes a 3D channel gain map (CGM) inference method based on a 3D\nconditional generative adversarial network (3D-CGAN). Specifically, we first\nanalyze the potential deployment types of BSs in urban low-altitude scenario,\nand investigate the CGM representation with the corresponding 3D channel gain\nmodel. The framework of the proposed 3D-CGAN is then discussed, which is\ntrained by a dataset consisting of existing CGMs. Consequently, the trained\n3D-CGAN is capable of inferring the corresponding CGM only based on the BS\ncoordinate without additional measurement. The simulation results demonstrate\nthat the CGMs inferred by the proposed 3D-CGAN outperform those of the\nbenchmark schemes, which can accurately reflect the radio propagation condition\nin 3D environment."
                },
                "authors": [
                    {
                        "name": "Yonghao Wang"
                    },
                    {
                        "name": "Ruoguang Li"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Jiaqi Chen"
                    },
                    {
                        "name": "Yong Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Yong Zeng"
                },
                "author": "Yong Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12794v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12794v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06562v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06562v2",
                "updated": "2025-04-17T09:49:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    9,
                    49,
                    43,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-09T03:51:53Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    3,
                    51,
                    53,
                    2,
                    99,
                    0
                ],
                "title": "FuseRL: Dense Preference Optimization for Heterogeneous Model Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FuseRL: Dense Preference Optimization for Heterogeneous Model Fusion"
                },
                "summary": "Heterogeneous model fusion enhances the performance of LLMs by integrating\nthe knowledge and capabilities of multiple structurally diverse models.\nHowever, existing approaches often rely solely on selecting the best output for\neach prompt from source models, which underutilizes their full potential due to\nlimited source knowledge and results in sparse optimization signals. To address\nthis limitation, we propose FuseRL, a novel two-stage framework comprising\nFuseSFT and FusePO to maximize the utilization of source LLMs. FuseSFT\nestablishes a robust initialization by integrating the strengths of\nheterogeneous source models through weighted supervised fine-tuning (SFT) on\ndiverse outputs for each prompt. FusePO optimizes weighted preferences based on\nthe outputs of multiple source models to enable superior alignment performance.\nExtensive experiments demonstrate the effectiveness of our framework across\nvarious preference alignment methods, including RLOO, DPO, and SimPO. Using\nLlama-3.1-8B-Instruct as the target model, our approach achieves\nstate-of-the-art performance among 8B LLMs on the AlpacaEval-2 and Arena-Hard\nbenchmarks. Further analysis suggests that FuseSFT regularizes the training\nprocess to reduce overfitting, while FusePO introduces dense and diverse\nsignals for preference optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous model fusion enhances the performance of LLMs by integrating\nthe knowledge and capabilities of multiple structurally diverse models.\nHowever, existing approaches often rely solely on selecting the best output for\neach prompt from source models, which underutilizes their full potential due to\nlimited source knowledge and results in sparse optimization signals. To address\nthis limitation, we propose FuseRL, a novel two-stage framework comprising\nFuseSFT and FusePO to maximize the utilization of source LLMs. FuseSFT\nestablishes a robust initialization by integrating the strengths of\nheterogeneous source models through weighted supervised fine-tuning (SFT) on\ndiverse outputs for each prompt. FusePO optimizes weighted preferences based on\nthe outputs of multiple source models to enable superior alignment performance.\nExtensive experiments demonstrate the effectiveness of our framework across\nvarious preference alignment methods, including RLOO, DPO, and SimPO. Using\nLlama-3.1-8B-Instruct as the target model, our approach achieves\nstate-of-the-art performance among 8B LLMs on the AlpacaEval-2 and Arena-Hard\nbenchmarks. Further analysis suggests that FuseSFT regularizes the training\nprocess to reduce overfitting, while FusePO introduces dense and diverse\nsignals for preference optimization."
                },
                "authors": [
                    {
                        "name": "Longguang Zhong"
                    },
                    {
                        "name": "Fanqi Wan"
                    },
                    {
                        "name": "Ziyi Yang"
                    },
                    {
                        "name": "Guosheng Liang"
                    },
                    {
                        "name": "Tianyuan Shi"
                    },
                    {
                        "name": "Xiaojun Quan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojun Quan"
                },
                "author": "Xiaojun Quan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06562v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06562v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12773v1",
                "updated": "2025-04-17T09:13:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    9,
                    13,
                    46,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T09:13:46Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    9,
                    13,
                    46,
                    3,
                    107,
                    0
                ],
                "title": "Enhancing the Geometric Problem-Solving Ability of Multimodal LLMs via\n  Symbolic-Neural Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing the Geometric Problem-Solving Ability of Multimodal LLMs via\n  Symbolic-Neural Integration"
                },
                "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have achieved\nremarkable progress in general domains and demonstrated promise in multimodal\nmathematical reasoning. However, applying MLLMs to geometry problem solving\n(GPS) remains challenging due to lack of accurate step-by-step solution data\nand severe hallucinations during reasoning. In this paper, we propose GeoGen, a\npipeline that can automatically generates step-wise reasoning paths for\ngeometry diagrams. By leveraging the precise symbolic reasoning,\n\\textbf{GeoGen} produces large-scale, high-quality question-answer pairs. To\nfurther enhance the logical reasoning ability of MLLMs, we train\n\\textbf{GeoLogic}, a Large Language Model (LLM) using synthetic data generated\nby GeoGen. Serving as a bridge between natural language and symbolic systems,\nGeoLogic enables symbolic tools to help verifying MLLM outputs, making the\nreasoning process more rigorous and alleviating hallucinations. Experimental\nresults show that our approach consistently improves the performance of MLLMs,\nachieving remarkable results on benchmarks for geometric reasoning tasks. This\nimprovement stems from our integration of the strengths of LLMs and symbolic\nsystems, which enables a more reliable and interpretable approach for the GPS\ntask. Codes are available at https://github.com/ycpNotFound/GeoGen.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Multimodal Large Language Models (MLLMs) have achieved\nremarkable progress in general domains and demonstrated promise in multimodal\nmathematical reasoning. However, applying MLLMs to geometry problem solving\n(GPS) remains challenging due to lack of accurate step-by-step solution data\nand severe hallucinations during reasoning. In this paper, we propose GeoGen, a\npipeline that can automatically generates step-wise reasoning paths for\ngeometry diagrams. By leveraging the precise symbolic reasoning,\n\\textbf{GeoGen} produces large-scale, high-quality question-answer pairs. To\nfurther enhance the logical reasoning ability of MLLMs, we train\n\\textbf{GeoLogic}, a Large Language Model (LLM) using synthetic data generated\nby GeoGen. Serving as a bridge between natural language and symbolic systems,\nGeoLogic enables symbolic tools to help verifying MLLM outputs, making the\nreasoning process more rigorous and alleviating hallucinations. Experimental\nresults show that our approach consistently improves the performance of MLLMs,\nachieving remarkable results on benchmarks for geometric reasoning tasks. This\nimprovement stems from our integration of the strengths of LLMs and symbolic\nsystems, which enables a more reliable and interpretable approach for the GPS\ntask. Codes are available at https://github.com/ycpNotFound/GeoGen."
                },
                "authors": [
                    {
                        "name": "Yicheng Pan"
                    },
                    {
                        "name": "Zhenrong Zhang"
                    },
                    {
                        "name": "Pengfei Hu"
                    },
                    {
                        "name": "Jiefeng Ma"
                    },
                    {
                        "name": "Jun Du"
                    },
                    {
                        "name": "Jianshu Zhang"
                    },
                    {
                        "name": "Quan Liu"
                    },
                    {
                        "name": "Jianqing Gao"
                    },
                    {
                        "name": "Feng Ma"
                    }
                ],
                "author_detail": {
                    "name": "Feng Ma"
                },
                "author": "Feng Ma",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12764v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12764v1",
                "updated": "2025-04-17T09:01:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    9,
                    1,
                    16,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T09:01:16Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    9,
                    1,
                    16,
                    3,
                    107,
                    0
                ],
                "title": "GraphOmni: A Comprehensive and Extendable Benchmark Framework for Large\n  Language Models on Graph-theoretic Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphOmni: A Comprehensive and Extendable Benchmark Framework for Large\n  Language Models on Graph-theoretic Tasks"
                },
                "summary": "In this paper, we presented GraphOmni, a comprehensive benchmark framework\nfor systematically evaluating the graph reasoning capabilities of LLMs. By\nanalyzing critical dimensions, including graph types, serialization formats,\nand prompt schemes, we provided extensive insights into the strengths and\nlimitations of current LLMs. Our empirical findings emphasize that no single\nserialization or prompting strategy consistently outperforms others. Motivated\nby these insights, we propose a reinforcement learning-based approach that\ndynamically selects the best serialization-prompt pairings, resulting in\nsignificant accuracy improvements. GraphOmni's modular and extensible design\nestablishes a robust foundation for future research, facilitating advancements\ntoward general-purpose graph reasoning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we presented GraphOmni, a comprehensive benchmark framework\nfor systematically evaluating the graph reasoning capabilities of LLMs. By\nanalyzing critical dimensions, including graph types, serialization formats,\nand prompt schemes, we provided extensive insights into the strengths and\nlimitations of current LLMs. Our empirical findings emphasize that no single\nserialization or prompting strategy consistently outperforms others. Motivated\nby these insights, we propose a reinforcement learning-based approach that\ndynamically selects the best serialization-prompt pairings, resulting in\nsignificant accuracy improvements. GraphOmni's modular and extensible design\nestablishes a robust foundation for future research, facilitating advancements\ntoward general-purpose graph reasoning models."
                },
                "authors": [
                    {
                        "name": "Hao Xu"
                    },
                    {
                        "name": "Xiangru Jian"
                    },
                    {
                        "name": "Xinjian Zhao"
                    },
                    {
                        "name": "Wei Pang"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Suyuchen Wang"
                    },
                    {
                        "name": "Qixin Zhang"
                    },
                    {
                        "name": "Joao Monteiro"
                    },
                    {
                        "name": "Qiuzhuang Sun"
                    },
                    {
                        "name": "Tianshu Yu"
                    }
                ],
                "author_detail": {
                    "name": "Tianshu Yu"
                },
                "author": "Tianshu Yu",
                "arxiv_comment": "82 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12764v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12764v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.04719v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.04719v4",
                "updated": "2025-04-17T09:01:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    9,
                    1,
                    12,
                    3,
                    107,
                    0
                ],
                "published": "2024-04-06T19:50:37Z",
                "published_parsed": [
                    2024,
                    4,
                    6,
                    19,
                    50,
                    37,
                    5,
                    97,
                    0
                ],
                "title": "Change Point Detection in Dynamic Graphs with Decoder-only Latent Space\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Change Point Detection in Dynamic Graphs with Decoder-only Latent Space\n  Model"
                },
                "summary": "This manuscript studies the unsupervised change point detection problem in\ntime series of graphs using a decoder-only latent space model. The proposed\nframework consists of learnable prior distributions for low-dimensional graph\nrepresentations and of a decoder that bridges the observed graphs and latent\nrepresentations. The prior distributions of the latent spaces are learned from\nthe observed data as empirical Bayes to assist change point detection.\nSpecifically, the model parameters are estimated via maximum approximate\nlikelihood, with a Group Fused Lasso regularization imposed on the prior\nparameters. The augmented Lagrangian is solved via Alternating Direction Method\nof Multipliers, and Langevin Dynamics are recruited for posterior inference.\nSimulation studies show good performance of the latent space model in\nsupporting change point detection and real data experiments yield change points\nthat align with significant events.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This manuscript studies the unsupervised change point detection problem in\ntime series of graphs using a decoder-only latent space model. The proposed\nframework consists of learnable prior distributions for low-dimensional graph\nrepresentations and of a decoder that bridges the observed graphs and latent\nrepresentations. The prior distributions of the latent spaces are learned from\nthe observed data as empirical Bayes to assist change point detection.\nSpecifically, the model parameters are estimated via maximum approximate\nlikelihood, with a Group Fused Lasso regularization imposed on the prior\nparameters. The augmented Lagrangian is solved via Alternating Direction Method\nof Multipliers, and Langevin Dynamics are recruited for posterior inference.\nSimulation studies show good performance of the latent space model in\nsupporting change point detection and real data experiments yield change points\nthat align with significant events."
                },
                "authors": [
                    {
                        "name": "Yik Lun Kei"
                    },
                    {
                        "name": "Jialiang Li"
                    },
                    {
                        "name": "Hangjian Li"
                    },
                    {
                        "name": "Yanzhen Chen"
                    },
                    {
                        "name": "Oscar Hernan Madrid Padilla"
                    }
                ],
                "author_detail": {
                    "name": "Oscar Hernan Madrid Padilla"
                },
                "author": "Oscar Hernan Madrid Padilla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.04719v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.04719v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12760v1",
                "updated": "2025-04-17T08:56:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    8,
                    56,
                    1,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T08:56:01Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    8,
                    56,
                    1,
                    3,
                    107,
                    0
                ],
                "title": "Analyzing multi-center randomized trials with covariate adjustment while\n  accounting for clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing multi-center randomized trials with covariate adjustment while\n  accounting for clustering"
                },
                "summary": "Augmented inverse probability weighting (AIPW) and G-computation with\ncanonical generalized linear models have become increasingly popular for\nestimating the average treatment effect in randomized experiments. These\nestimators leverage outcome prediction models to adjust for imbalances in\nbaseline covariates across treatment arms, improving statistical power compared\nto unadjusted analyses, while maintaining control over Type I error rates, even\nwhen the models are misspecified. Practical application of such estimators\noften overlooks the clustering present in multi-center clinical trials. Even\nwhen prediction models account for center effects, this neglect can degrade the\ncoverage of confidence intervals, reduce the efficiency of the estimators, and\ncomplicate the interpretation of the corresponding estimands. These issues are\nparticularly pronounced for estimators of counterfactual means, though less\nsevere for those of the average treatment effect, as demonstrated through Monte\nCarlo simulations and supported by theoretical insights. To address these\nchallenges, we develop efficient estimators of counterfactual means and the\naverage treatment effect in a random center. These extract information from\nbaseline covariates by relying on outcome prediction models, but remain\nunbiased in large samples when these models are misspecified. We also introduce\nan accompanying inference framework inspired by random-effects meta-analysis\nand relevant for settings where data from many small centers are being\nanalyzed. Adjusting for center effects yields substantial gains in efficiency,\nespecially when treatment effect heterogeneity across centers is large. Monte\nCarlo simulations and application to the WASH Benefits Bangladesh study\ndemonstrate adequate performance of the proposed methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmented inverse probability weighting (AIPW) and G-computation with\ncanonical generalized linear models have become increasingly popular for\nestimating the average treatment effect in randomized experiments. These\nestimators leverage outcome prediction models to adjust for imbalances in\nbaseline covariates across treatment arms, improving statistical power compared\nto unadjusted analyses, while maintaining control over Type I error rates, even\nwhen the models are misspecified. Practical application of such estimators\noften overlooks the clustering present in multi-center clinical trials. Even\nwhen prediction models account for center effects, this neglect can degrade the\ncoverage of confidence intervals, reduce the efficiency of the estimators, and\ncomplicate the interpretation of the corresponding estimands. These issues are\nparticularly pronounced for estimators of counterfactual means, though less\nsevere for those of the average treatment effect, as demonstrated through Monte\nCarlo simulations and supported by theoretical insights. To address these\nchallenges, we develop efficient estimators of counterfactual means and the\naverage treatment effect in a random center. These extract information from\nbaseline covariates by relying on outcome prediction models, but remain\nunbiased in large samples when these models are misspecified. We also introduce\nan accompanying inference framework inspired by random-effects meta-analysis\nand relevant for settings where data from many small centers are being\nanalyzed. Adjusting for center effects yields substantial gains in efficiency,\nespecially when treatment effect heterogeneity across centers is large. Monte\nCarlo simulations and application to the WASH Benefits Bangladesh study\ndemonstrate adequate performance of the proposed methods."
                },
                "authors": [
                    {
                        "name": "Muluneh Alene"
                    },
                    {
                        "name": "Stijn Vansteelandt"
                    },
                    {
                        "name": "Kelly Van Lancker"
                    }
                ],
                "author_detail": {
                    "name": "Kelly Van Lancker"
                },
                "author": "Kelly Van Lancker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12758v1",
                "updated": "2025-04-17T08:53:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    8,
                    53,
                    30,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T08:53:30Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    8,
                    53,
                    30,
                    3,
                    107,
                    0
                ],
                "title": "Universal Approximation with XL MIMO Systems: OTA Classification via\n  Trainable Analog Combining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universal Approximation with XL MIMO Systems: OTA Classification via\n  Trainable Analog Combining"
                },
                "summary": "In this paper, we demonstrate that an eXtremely Large (XL) Multiple-Input\nMultiple-Output (MIMO) wireless system with appropriate analog combining\ncomponents exhibits the properties of a universal function approximator,\nsimilar to a feedforward neural network. By treating the XL MIMO channel\ncoefficients as the random nodes of a hidden layer, and the receiver's analog\ncombiner as a trainable output layer, we cast the end-to-end system to the\nExtreme Learning Machine (ELM) framework, leading to a novel formulation for\nOver-The-Air (OTA) edge inference without requiring traditional digital\nprocessing nor pre-processing at the transmitter. Through theoretical analysis\nand numerical evaluation, we showcase that XL-MIMO-ELM enables\nnear-instantaneous training and efficient classification, suggesting the\nparadigm shift of beyond massive MIMO systems as neural networks alongside\ntheir profound communications role. Compared to deep learning approaches and\nconventional ELMs, the proposed framework achieves on par performance with\norders of magnitude lower complexity, making it highly attractive for ultra low\npower wireless devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we demonstrate that an eXtremely Large (XL) Multiple-Input\nMultiple-Output (MIMO) wireless system with appropriate analog combining\ncomponents exhibits the properties of a universal function approximator,\nsimilar to a feedforward neural network. By treating the XL MIMO channel\ncoefficients as the random nodes of a hidden layer, and the receiver's analog\ncombiner as a trainable output layer, we cast the end-to-end system to the\nExtreme Learning Machine (ELM) framework, leading to a novel formulation for\nOver-The-Air (OTA) edge inference without requiring traditional digital\nprocessing nor pre-processing at the transmitter. Through theoretical analysis\nand numerical evaluation, we showcase that XL-MIMO-ELM enables\nnear-instantaneous training and efficient classification, suggesting the\nparadigm shift of beyond massive MIMO systems as neural networks alongside\ntheir profound communications role. Compared to deep learning approaches and\nconventional ELMs, the proposed framework achieves on par performance with\norders of magnitude lower complexity, making it highly attractive for ultra low\npower wireless devices."
                },
                "authors": [
                    {
                        "name": "Kyriakos Stylianopoulos"
                    },
                    {
                        "name": "George C. Alexandropoulos"
                    }
                ],
                "author_detail": {
                    "name": "George C. Alexandropoulos"
                },
                "author": "George C. Alexandropoulos",
                "arxiv_comment": "Submitted to IEEE SPAWC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06235v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06235v2",
                "updated": "2025-04-17T08:52:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    8,
                    52,
                    3,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-08T17:32:56Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    32,
                    56,
                    1,
                    98,
                    0
                ],
                "title": "Decentralized Federated Domain Generalization with Style Sharing: A\n  Formal Modeling and Convergence Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Federated Domain Generalization with Style Sharing: A\n  Formal Modeling and Convergence Analysis"
                },
                "summary": "Much of the federated learning (FL) literature focuses on settings where\nlocal dataset statistics remain the same between training and testing time.\nRecent advances in domain generalization (DG) aim to use data from source\n(training) domains to train a model that generalizes well to data from unseen\ntarget (testing) domains. In this paper, we are motivated by two major gaps in\nexisting work on FL and DG: (1) the lack of formal mathematical analysis of DG\nobjectives and training processes; and (2) DG research in FL being limited to\nthe conventional star-topology architecture. Addressing the second gap, we\ndevelop $\\textit{Decentralized Federated Domain Generalization with Style\nSharing}$ ($\\texttt{StyleDDG}$), a fully decentralized DG algorithm designed to\nallow devices in a peer-to-peer network to achieve DG based on sharing style\ninformation inferred from their datasets. Additionally, we fill the first gap\nby providing the first systematic approach to mathematically analyzing\nstyle-based DG training optimization. We cast existing centralized DG\nalgorithms within our framework, and employ their formalisms to model\n$\\texttt{StyleDDG}$. Based on this, we obtain analytical conditions under which\na sub-linear convergence rate of $\\texttt{StyleDDG}$ can be obtained. Through\nexperiments on two popular DG datasets, we demonstrate that $\\texttt{StyleDDG}$\ncan obtain significant improvements in accuracy across target domains with\nminimal added communication overhead compared to decentralized gradient methods\nthat do not employ style sharing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Much of the federated learning (FL) literature focuses on settings where\nlocal dataset statistics remain the same between training and testing time.\nRecent advances in domain generalization (DG) aim to use data from source\n(training) domains to train a model that generalizes well to data from unseen\ntarget (testing) domains. In this paper, we are motivated by two major gaps in\nexisting work on FL and DG: (1) the lack of formal mathematical analysis of DG\nobjectives and training processes; and (2) DG research in FL being limited to\nthe conventional star-topology architecture. Addressing the second gap, we\ndevelop $\\textit{Decentralized Federated Domain Generalization with Style\nSharing}$ ($\\texttt{StyleDDG}$), a fully decentralized DG algorithm designed to\nallow devices in a peer-to-peer network to achieve DG based on sharing style\ninformation inferred from their datasets. Additionally, we fill the first gap\nby providing the first systematic approach to mathematically analyzing\nstyle-based DG training optimization. We cast existing centralized DG\nalgorithms within our framework, and employ their formalisms to model\n$\\texttt{StyleDDG}$. Based on this, we obtain analytical conditions under which\na sub-linear convergence rate of $\\texttt{StyleDDG}$ can be obtained. Through\nexperiments on two popular DG datasets, we demonstrate that $\\texttt{StyleDDG}$\ncan obtain significant improvements in accuracy across target domains with\nminimal added communication overhead compared to decentralized gradient methods\nthat do not employ style sharing."
                },
                "authors": [
                    {
                        "name": "Shahryar Zehtabi"
                    },
                    {
                        "name": "Dong-Jun Han"
                    },
                    {
                        "name": "Seyyedali Hosseinalipour"
                    },
                    {
                        "name": "Christopher G. Brinton"
                    }
                ],
                "author_detail": {
                    "name": "Christopher G. Brinton"
                },
                "author": "Christopher G. Brinton",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06235v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06235v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12755v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12755v1",
                "updated": "2025-04-17T08:48:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    8,
                    48,
                    23,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T08:48:23Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    8,
                    48,
                    23,
                    3,
                    107,
                    0
                ],
                "title": "Trajectory Adaptation using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trajectory Adaptation using Large Language Models"
                },
                "summary": "Adapting robot trajectories based on human instructions as per new situations\nis essential for achieving more intuitive and scalable human-robot\ninteractions. This work proposes a flexible language-based framework to adapt\ngeneric robotic trajectories produced by off-the-shelf motion planners like\nRRT, A-star, etc, or learned from human demonstrations. We utilize pre-trained\nLLMs to adapt trajectory waypoints by generating code as a policy for dense\nrobot manipulation, enabling more complex and flexible instructions than\ncurrent methods. This approach allows us to incorporate a broader range of\ncommands, including numerical inputs. Compared to state-of-the-art\nfeature-based sequence-to-sequence models which require training, our method\ndoes not require task-specific training and offers greater interpretability and\nmore effective feedback mechanisms. We validate our approach through simulation\nexperiments on the robotic manipulator, aerial vehicle, and ground robot in the\nPybullet and Gazebo simulation environments, demonstrating that LLMs can\nsuccessfully adapt trajectories to complex human instructions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting robot trajectories based on human instructions as per new situations\nis essential for achieving more intuitive and scalable human-robot\ninteractions. This work proposes a flexible language-based framework to adapt\ngeneric robotic trajectories produced by off-the-shelf motion planners like\nRRT, A-star, etc, or learned from human demonstrations. We utilize pre-trained\nLLMs to adapt trajectory waypoints by generating code as a policy for dense\nrobot manipulation, enabling more complex and flexible instructions than\ncurrent methods. This approach allows us to incorporate a broader range of\ncommands, including numerical inputs. Compared to state-of-the-art\nfeature-based sequence-to-sequence models which require training, our method\ndoes not require task-specific training and offers greater interpretability and\nmore effective feedback mechanisms. We validate our approach through simulation\nexperiments on the robotic manipulator, aerial vehicle, and ground robot in the\nPybullet and Gazebo simulation environments, demonstrating that LLMs can\nsuccessfully adapt trajectories to complex human instructions."
                },
                "authors": [
                    {
                        "name": "Anurag Maurya"
                    },
                    {
                        "name": "Tashmoy Ghosh"
                    },
                    {
                        "name": "Ravi Prakash"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Prakash"
                },
                "author": "Ravi Prakash",
                "arxiv_comment": "Accepted to CoRL LangRob workshop 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12755v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12755v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12750v1",
                "updated": "2025-04-17T08:44:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    8,
                    44,
                    29,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T08:44:29Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    8,
                    44,
                    29,
                    3,
                    107,
                    0
                ],
                "title": "Spatial Functional Deep Neural Network Model: A New Prediction Algorithm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial Functional Deep Neural Network Model: A New Prediction Algorithm"
                },
                "summary": "Accurate prediction of spatially dependent functional data is critical for\nvarious engineering and scientific applications. In this study, a spatial\nfunctional deep neural network model was developed with a novel non-linear\nmodeling framework that seamlessly integrates spatial dependencies and\nfunctional predictors using deep learning techniques. The proposed model\nextends classical scalar-on-function regression by incorporating a spatial\nautoregressive component while leveraging functional deep neural networks to\ncapture complex non-linear relationships. To ensure a robust estimation, the\nmethodology employs an adaptive estimation approach, where the spatial\ndependence parameter was first inferred via maximum likelihood estimation,\nfollowed by non-linear functional regression using deep learning. The\neffectiveness of the proposed model was evaluated through extensive Monte Carlo\nsimulations and an application to Brazilian COVID-19 data, where the goal was\nto predict the average daily number of deaths. Comparative analysis with\nmaximum likelihood-based spatial functional linear regression and functional\ndeep neural network models demonstrates that the proposed algorithm\nsignificantly improves predictive performance. The results for the Brazilian\nCOVID-19 data showed that while all models achieved similar mean squared error\nvalues over the training modeling phase, the proposed model achieved the lowest\nmean squared prediction error in the testing phase, indicating superior\ngeneralization ability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate prediction of spatially dependent functional data is critical for\nvarious engineering and scientific applications. In this study, a spatial\nfunctional deep neural network model was developed with a novel non-linear\nmodeling framework that seamlessly integrates spatial dependencies and\nfunctional predictors using deep learning techniques. The proposed model\nextends classical scalar-on-function regression by incorporating a spatial\nautoregressive component while leveraging functional deep neural networks to\ncapture complex non-linear relationships. To ensure a robust estimation, the\nmethodology employs an adaptive estimation approach, where the spatial\ndependence parameter was first inferred via maximum likelihood estimation,\nfollowed by non-linear functional regression using deep learning. The\neffectiveness of the proposed model was evaluated through extensive Monte Carlo\nsimulations and an application to Brazilian COVID-19 data, where the goal was\nto predict the average daily number of deaths. Comparative analysis with\nmaximum likelihood-based spatial functional linear regression and functional\ndeep neural network models demonstrates that the proposed algorithm\nsignificantly improves predictive performance. The results for the Brazilian\nCOVID-19 data showed that while all models achieved similar mean squared error\nvalues over the training modeling phase, the proposed model achieved the lowest\nmean squared prediction error in the testing phase, indicating superior\ngeneralization ability."
                },
                "authors": [
                    {
                        "name": "Merve Basaran"
                    },
                    {
                        "name": "Ufuk Beyaztas"
                    },
                    {
                        "name": "Han Lin Shang"
                    },
                    {
                        "name": "Zaher Mundher Yaseen"
                    }
                ],
                "author_detail": {
                    "name": "Zaher Mundher Yaseen"
                },
                "author": "Zaher Mundher Yaseen",
                "arxiv_comment": "33 pages, 7 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62R10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11901v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11901v2",
                "updated": "2025-04-17T08:41:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    8,
                    41,
                    44,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-16T09:26:04Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    9,
                    26,
                    4,
                    2,
                    106,
                    0
                ],
                "title": "Causality-enhanced Decision-Making for Autonomous Mobile Robots in\n  Dynamic Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causality-enhanced Decision-Making for Autonomous Mobile Robots in\n  Dynamic Environments"
                },
                "summary": "The growing integration of robots in shared environments -- such as\nwarehouses, shopping centres, and hospitals -- demands a deep understanding of\nthe underlying dynamics and human behaviours, including how, when, and where\nindividuals engage in various activities and interactions. This knowledge goes\nbeyond simple correlation studies and requires a more comprehensive causal\nanalysis. By leveraging causal inference to model cause-and-effect\nrelationships, we can better anticipate critical environmental factors and\nenable autonomous robots to plan and execute tasks more effectively. To this\nend, we propose a novel causality-based decision-making framework that reasons\nover a learned causal model to predict battery usage and human obstructions,\nunderstanding how these factors could influence robot task execution. Such\nreasoning framework assists the robot in deciding when and how to complete a\ngiven task. To achieve this, we developed also PeopleFlow, a new Gazebo-based\nsimulator designed to model context-sensitive human-robot spatial interactions\nin shared workspaces. PeopleFlow features realistic human and robot\ntrajectories influenced by contextual factors such as time, environment layout,\nand robot state, and can simulate a large number of agents. While the simulator\nis general-purpose, in this paper we focus on a warehouse-like environment as a\ncase study, where we conduct an extensive evaluation benchmarking our causal\napproach against a non-causal baseline. Our findings demonstrate the efficacy\nof the proposed solutions, highlighting how causal reasoning enables autonomous\nrobots to operate more efficiently and safely in dynamic environments shared\nwith humans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing integration of robots in shared environments -- such as\nwarehouses, shopping centres, and hospitals -- demands a deep understanding of\nthe underlying dynamics and human behaviours, including how, when, and where\nindividuals engage in various activities and interactions. This knowledge goes\nbeyond simple correlation studies and requires a more comprehensive causal\nanalysis. By leveraging causal inference to model cause-and-effect\nrelationships, we can better anticipate critical environmental factors and\nenable autonomous robots to plan and execute tasks more effectively. To this\nend, we propose a novel causality-based decision-making framework that reasons\nover a learned causal model to predict battery usage and human obstructions,\nunderstanding how these factors could influence robot task execution. Such\nreasoning framework assists the robot in deciding when and how to complete a\ngiven task. To achieve this, we developed also PeopleFlow, a new Gazebo-based\nsimulator designed to model context-sensitive human-robot spatial interactions\nin shared workspaces. PeopleFlow features realistic human and robot\ntrajectories influenced by contextual factors such as time, environment layout,\nand robot state, and can simulate a large number of agents. While the simulator\nis general-purpose, in this paper we focus on a warehouse-like environment as a\ncase study, where we conduct an extensive evaluation benchmarking our causal\napproach against a non-causal baseline. Our findings demonstrate the efficacy\nof the proposed solutions, highlighting how causal reasoning enables autonomous\nrobots to operate more efficiently and safely in dynamic environments shared\nwith humans."
                },
                "authors": [
                    {
                        "name": "Luca Castri"
                    },
                    {
                        "name": "Gloria Beraldo"
                    },
                    {
                        "name": "Nicola Bellotto"
                    }
                ],
                "author_detail": {
                    "name": "Nicola Bellotto"
                },
                "author": "Nicola Bellotto",
                "arxiv_comment": "Causal Discovery and Inference - Robot Autonomy - Human-Robot Spatial\n  Interaction - Decision-Making",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11901v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11901v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.10305v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.10305v4",
                "updated": "2025-04-17T08:34:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    8,
                    34,
                    42,
                    3,
                    107,
                    0
                ],
                "published": "2023-09-19T04:13:22Z",
                "published_parsed": [
                    2023,
                    9,
                    19,
                    4,
                    13,
                    22,
                    1,
                    262,
                    0
                ],
                "title": "Baichuan 2: Open Large-scale Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Baichuan 2: Open Large-scale Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable performance on a\nvariety of natural language tasks based on just a few examples of natural\nlanguage instructions, reducing the need for extensive feature engineering.\nHowever, most powerful LLMs are closed-source or limited in their capability\nfor languages other than English. In this technical report, we present Baichuan\n2, a series of large-scale multilingual language models containing 7 billion\nand 13 billion parameters, trained from scratch, on 2.6 trillion tokens.\nBaichuan 2 matches or outperforms other open-source models of similar size on\npublic benchmarks like MMLU, CMMLU, GSM8K, and HumanEval. Furthermore, Baichuan\n2 excels in vertical domains such as medicine and law. We will release all\npre-training model checkpoints to benefit the research community in better\nunderstanding the training dynamics of Baichuan 2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance on a\nvariety of natural language tasks based on just a few examples of natural\nlanguage instructions, reducing the need for extensive feature engineering.\nHowever, most powerful LLMs are closed-source or limited in their capability\nfor languages other than English. In this technical report, we present Baichuan\n2, a series of large-scale multilingual language models containing 7 billion\nand 13 billion parameters, trained from scratch, on 2.6 trillion tokens.\nBaichuan 2 matches or outperforms other open-source models of similar size on\npublic benchmarks like MMLU, CMMLU, GSM8K, and HumanEval. Furthermore, Baichuan\n2 excels in vertical domains such as medicine and law. We will release all\npre-training model checkpoints to benefit the research community in better\nunderstanding the training dynamics of Baichuan 2."
                },
                "authors": [
                    {
                        "name": "Aiyuan Yang"
                    },
                    {
                        "name": "Bin Xiao"
                    },
                    {
                        "name": "Bingning Wang"
                    },
                    {
                        "name": "Borong Zhang"
                    },
                    {
                        "name": "Ce Bian"
                    },
                    {
                        "name": "Chao Yin"
                    },
                    {
                        "name": "Chenxu Lv"
                    },
                    {
                        "name": "Da Pan"
                    },
                    {
                        "name": "Dian Wang"
                    },
                    {
                        "name": "Dong Yan"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Fei Deng"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Guangwei Ai"
                    },
                    {
                        "name": "Guosheng Dong"
                    },
                    {
                        "name": "Haizhou Zhao"
                    },
                    {
                        "name": "Hang Xu"
                    },
                    {
                        "name": "Haoze Sun"
                    },
                    {
                        "name": "Hongda Zhang"
                    },
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Jiaming Ji"
                    },
                    {
                        "name": "Jian Xie"
                    },
                    {
                        "name": "JunTao Dai"
                    },
                    {
                        "name": "Kun Fang"
                    },
                    {
                        "name": "Lei Su"
                    },
                    {
                        "name": "Liang Song"
                    },
                    {
                        "name": "Lifeng Liu"
                    },
                    {
                        "name": "Liyun Ru"
                    },
                    {
                        "name": "Luyao Ma"
                    },
                    {
                        "name": "Mang Wang"
                    },
                    {
                        "name": "Mickel Liu"
                    },
                    {
                        "name": "MingAn Lin"
                    },
                    {
                        "name": "Nuolan Nie"
                    },
                    {
                        "name": "Peidong Guo"
                    },
                    {
                        "name": "Ruiyang Sun"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Tianpeng Li"
                    },
                    {
                        "name": "Tianyu Li"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Weipeng Chen"
                    },
                    {
                        "name": "Xiangrong Zeng"
                    },
                    {
                        "name": "Xiaochuan Wang"
                    },
                    {
                        "name": "Xiaoxi Chen"
                    },
                    {
                        "name": "Xin Men"
                    },
                    {
                        "name": "Xin Yu"
                    },
                    {
                        "name": "Xuehai Pan"
                    },
                    {
                        "name": "Yanjun Shen"
                    },
                    {
                        "name": "Yiding Wang"
                    },
                    {
                        "name": "Yiyu Li"
                    },
                    {
                        "name": "Youxin Jiang"
                    },
                    {
                        "name": "Yuchen Gao"
                    },
                    {
                        "name": "Yupeng Zhang"
                    },
                    {
                        "name": "Zenan Zhou"
                    },
                    {
                        "name": "Zhiying Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiying Wu"
                },
                "author": "Zhiying Wu",
                "arxiv_comment": "Baichuan 2 technical report. Github:\n  https://github.com/baichuan-inc/Baichuan2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.10305v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.10305v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12739v1",
                "updated": "2025-04-17T08:29:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    8,
                    29,
                    0,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T08:29:00Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    8,
                    29,
                    0,
                    3,
                    107,
                    0
                ],
                "title": "Mask Image Watermarking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mask Image Watermarking"
                },
                "summary": "We present MaskMark, a simple, efficient and flexible framework for image\nwatermarking. MaskMark has two variants: MaskMark-D, which supports global\nwatermark embedding, watermark localization, and local watermark extraction for\napplications such as tamper detection, and MaskMark-ED, which focuses on local\nwatermark embedding and extraction with enhanced robustness in small regions,\nenabling localized image protection. Built upon the classical Encoder-\nDistortion-Decoder training paradigm, MaskMark-D introduces a simple masking\nmechanism during the decoding stage to support both global and local watermark\nextraction. A mask is applied to the watermarked image before extraction,\nallowing the decoder to focus on selected regions and learn local extraction. A\nlocalization module is also integrated into the decoder to identify watermark\nregions during inference, reducing interference from irrelevant content and\nimproving accuracy. MaskMark-ED extends this design by incorporating the mask\ninto the encoding stage as well, guiding the encoder to embed the watermark in\ndesignated local regions for enhanced robustness. Comprehensive experiments\nshow that MaskMark achieves state-of-the-art performance in global watermark\nextraction, local watermark extraction, watermark localization, and\nmulti-watermark embedding. It outperforms all existing baselines, including the\nrecent leading model WAM for local watermarking, while preserving high visual\nquality of the watermarked images. MaskMark is also flexible, by adjusting the\ndistortion layer, it can adapt to different robustness requirements with just a\nfew steps of fine-tuning. Moreover, our approach is efficient and easy to\noptimize, requiring only 20 hours on a single A6000 GPU with just 1/15 the\ncomputational cost of WAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present MaskMark, a simple, efficient and flexible framework for image\nwatermarking. MaskMark has two variants: MaskMark-D, which supports global\nwatermark embedding, watermark localization, and local watermark extraction for\napplications such as tamper detection, and MaskMark-ED, which focuses on local\nwatermark embedding and extraction with enhanced robustness in small regions,\nenabling localized image protection. Built upon the classical Encoder-\nDistortion-Decoder training paradigm, MaskMark-D introduces a simple masking\nmechanism during the decoding stage to support both global and local watermark\nextraction. A mask is applied to the watermarked image before extraction,\nallowing the decoder to focus on selected regions and learn local extraction. A\nlocalization module is also integrated into the decoder to identify watermark\nregions during inference, reducing interference from irrelevant content and\nimproving accuracy. MaskMark-ED extends this design by incorporating the mask\ninto the encoding stage as well, guiding the encoder to embed the watermark in\ndesignated local regions for enhanced robustness. Comprehensive experiments\nshow that MaskMark achieves state-of-the-art performance in global watermark\nextraction, local watermark extraction, watermark localization, and\nmulti-watermark embedding. It outperforms all existing baselines, including the\nrecent leading model WAM for local watermarking, while preserving high visual\nquality of the watermarked images. MaskMark is also flexible, by adjusting the\ndistortion layer, it can adapt to different robustness requirements with just a\nfew steps of fine-tuning. Moreover, our approach is efficient and easy to\noptimize, requiring only 20 hours on a single A6000 GPU with just 1/15 the\ncomputational cost of WAM."
                },
                "authors": [
                    {
                        "name": "Runyi Hu"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Shiqian Zhao"
                    },
                    {
                        "name": "Nils Lukas"
                    },
                    {
                        "name": "Jiwei Li"
                    },
                    {
                        "name": "Qing Guo"
                    },
                    {
                        "name": "Han Qiu"
                    },
                    {
                        "name": "Tianwei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tianwei Zhang"
                },
                "author": "Tianwei Zhang",
                "arxiv_comment": "23 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12738v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12738v1",
                "updated": "2025-04-17T08:28:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    8,
                    28,
                    9,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T08:28:09Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    8,
                    28,
                    9,
                    3,
                    107,
                    0
                ],
                "title": "Macroscopic states and operations: a generalized resource theory of\n  coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Macroscopic states and operations: a generalized resource theory of\n  coherence"
                },
                "summary": "To understand the emergence of macroscopic irreversibility from microscopic\nreversible dynamics, the idea of coarse-graining plays a fundamental role. In\nthis work, we focus on the concept of macroscopic states, i.e. coarse\nrepresentations of microscopic details, defined as states that can be inferred\nsolely from the outcomes of macroscopic measurements. Building on the theories\nof quantum statistical sufficiency and quantum Bayesian retrodiction, we\ncharacterize macroscopic states through several equivalent formulations,\nranging from algebraic to explicitly constructive. We introduce a hierarchy of\nmacroscopicity-non-decreasing operations and develop a resource theory of\nmicroscopicity that unifies and generalizes existing resource theories of\ncoherence, athermality, purity, and asymmetry. Finally, we introduce the\nconcept of inferential reference frames and reinterpret macroscopic entropy as\na measure of inferential asymmetry, i.e., irretrodictability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To understand the emergence of macroscopic irreversibility from microscopic\nreversible dynamics, the idea of coarse-graining plays a fundamental role. In\nthis work, we focus on the concept of macroscopic states, i.e. coarse\nrepresentations of microscopic details, defined as states that can be inferred\nsolely from the outcomes of macroscopic measurements. Building on the theories\nof quantum statistical sufficiency and quantum Bayesian retrodiction, we\ncharacterize macroscopic states through several equivalent formulations,\nranging from algebraic to explicitly constructive. We introduce a hierarchy of\nmacroscopicity-non-decreasing operations and develop a resource theory of\nmicroscopicity that unifies and generalizes existing resource theories of\ncoherence, athermality, purity, and asymmetry. Finally, we introduce the\nconcept of inferential reference frames and reinterpret macroscopic entropy as\na measure of inferential asymmetry, i.e., irretrodictability."
                },
                "authors": [
                    {
                        "name": "Teruaki Nagasawa"
                    },
                    {
                        "name": "Eyuri Wakakuwa"
                    },
                    {
                        "name": "Kohtaro Kato"
                    },
                    {
                        "name": "Francesco Buscemi"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Buscemi"
                },
                "author": "Francesco Buscemi",
                "arxiv_comment": "18 pages, no figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12738v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12738v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.MP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12737v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12737v1",
                "updated": "2025-04-17T08:27:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    8,
                    27,
                    2,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T08:27:02Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    8,
                    27,
                    2,
                    3,
                    107,
                    0
                ],
                "title": "Chinese-Vicuna: A Chinese Instruction-following Llama-based Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chinese-Vicuna: A Chinese Instruction-following Llama-based Model"
                },
                "summary": "Chinese-Vicuna is an open-source, resource-efficient language model designed\nto bridge the gap in Chinese instruction-following capabilities by fine-tuning\nMeta's LLaMA architecture using Low-Rank Adaptation (LoRA). Targeting\nlow-resource environments, it enables cost-effective deployment on consumer\nGPUs (e.g., RTX-2080Ti for 7B models) and supports domain-specific adaptation\nin fields like healthcare and law. By integrating hybrid datasets (BELLE and\nGuanaco) and 4-bit quantization (QLoRA), the model achieves competitive\nperformance in tasks such as translation, code generation, and domain-specific\nQ\\&A. The project provides a comprehensive toolkit for model conversion, CPU\ninference, and multi-turn dialogue interfaces, emphasizing accessibility for\nresearchers and developers. Evaluations indicate competitive performance across\nmedical tasks, multi-turn dialogue coherence, and real-time legal updates.\nChinese-Vicuna's modular design, open-source ecosystem, and community-driven\nenhancements position it as a versatile foundation for Chinese LLM\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chinese-Vicuna is an open-source, resource-efficient language model designed\nto bridge the gap in Chinese instruction-following capabilities by fine-tuning\nMeta's LLaMA architecture using Low-Rank Adaptation (LoRA). Targeting\nlow-resource environments, it enables cost-effective deployment on consumer\nGPUs (e.g., RTX-2080Ti for 7B models) and supports domain-specific adaptation\nin fields like healthcare and law. By integrating hybrid datasets (BELLE and\nGuanaco) and 4-bit quantization (QLoRA), the model achieves competitive\nperformance in tasks such as translation, code generation, and domain-specific\nQ\\&A. The project provides a comprehensive toolkit for model conversion, CPU\ninference, and multi-turn dialogue interfaces, emphasizing accessibility for\nresearchers and developers. Evaluations indicate competitive performance across\nmedical tasks, multi-turn dialogue coherence, and real-time legal updates.\nChinese-Vicuna's modular design, open-source ecosystem, and community-driven\nenhancements position it as a versatile foundation for Chinese LLM\napplications."
                },
                "authors": [
                    {
                        "name": "Chenghao Fan"
                    },
                    {
                        "name": "Zhenyi Lu"
                    },
                    {
                        "name": "Jie Tian"
                    }
                ],
                "author_detail": {
                    "name": "Jie Tian"
                },
                "author": "Jie Tian",
                "arxiv_comment": "Chinese-Vicuna Technique Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12737v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12737v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14266v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14266v3",
                "updated": "2025-04-17T08:21:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    8,
                    21,
                    11,
                    3,
                    107,
                    0
                ],
                "published": "2024-09-21T23:37:20Z",
                "published_parsed": [
                    2024,
                    9,
                    21,
                    23,
                    37,
                    20,
                    5,
                    265,
                    0
                ],
                "title": "Differentiating frictionally locked asperities from kinematically\n  coupled zones",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentiating frictionally locked asperities from kinematically\n  coupled zones"
                },
                "summary": "Seismogenic areas on plate-boundary faults resist slipping until earthquakes\nbegin. Therefore, slip deficit, also called coupling, which represents delay\nrelative to rigid-body motion, is an interseismic proxy of seismic potential.\nHowever, when a part of a frictional interface sticks together (locked), its\nsliding surroundings are braked and slowed (coupled), so coupled zones are\noverestimates of locked zones. Several indicators collectively termed\nmechanical coupling have been proposed to capture locked zones, but their\nrelationship with true frictional locking is unclear. This study investigates\nthe frictional physics that locked and unlocked zones should observe,\nelucidating the physical foundation of inference on frictionally locked\nsegments, known as asperities in fault mechanics. Definitions of locking in\nvarious friction laws are shown to have a unified expression. (I) In any\nfriction law, locking means zero slip rate (pre-yield), and unlocking means\nstress at strength (post-yield). (II) Intersesismically, while locking keeps\ndenoting a stationary state with constant slip, unlocking becomes synonymous\nwith a quasi-steady state of constant stress. We use this result to develop\nslip-deficit inversions that incorporate physical constraints of\nlocking-unlocking, estimating locked zones as distributed circular asperities\nover unlocked interfaces. Our inversion of geodetic data detects five primary\nasperities in the Nankai subduction zone in southwestern Japan. Detected\nasperities spatially correlate with seafloor topography. Their locations are\nalso consistent with slip zones of historical megathrust earthquakes but mostly\nnon-overlapping with slow-earthquake occurrence zones at depth, supporting the\nhypothesis that the areas hosting slow earthquakes are normally in long\nspatiotemporal scales coupled but unlocked.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seismogenic areas on plate-boundary faults resist slipping until earthquakes\nbegin. Therefore, slip deficit, also called coupling, which represents delay\nrelative to rigid-body motion, is an interseismic proxy of seismic potential.\nHowever, when a part of a frictional interface sticks together (locked), its\nsliding surroundings are braked and slowed (coupled), so coupled zones are\noverestimates of locked zones. Several indicators collectively termed\nmechanical coupling have been proposed to capture locked zones, but their\nrelationship with true frictional locking is unclear. This study investigates\nthe frictional physics that locked and unlocked zones should observe,\nelucidating the physical foundation of inference on frictionally locked\nsegments, known as asperities in fault mechanics. Definitions of locking in\nvarious friction laws are shown to have a unified expression. (I) In any\nfriction law, locking means zero slip rate (pre-yield), and unlocking means\nstress at strength (post-yield). (II) Intersesismically, while locking keeps\ndenoting a stationary state with constant slip, unlocking becomes synonymous\nwith a quasi-steady state of constant stress. We use this result to develop\nslip-deficit inversions that incorporate physical constraints of\nlocking-unlocking, estimating locked zones as distributed circular asperities\nover unlocked interfaces. Our inversion of geodetic data detects five primary\nasperities in the Nankai subduction zone in southwestern Japan. Detected\nasperities spatially correlate with seafloor topography. Their locations are\nalso consistent with slip zones of historical megathrust earthquakes but mostly\nnon-overlapping with slow-earthquake occurrence zones at depth, supporting the\nhypothesis that the areas hosting slow earthquakes are normally in long\nspatiotemporal scales coupled but unlocked."
                },
                "authors": [
                    {
                        "name": "Dye SK Sato"
                    },
                    {
                        "name": "Takane Hori"
                    },
                    {
                        "name": "Yukitoshi Fukahata"
                    }
                ],
                "author_detail": {
                    "name": "Yukitoshi Fukahata"
                },
                "author": "Yukitoshi Fukahata",
                "arxiv_comment": "64 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14266v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14266v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12734v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12734v1",
                "updated": "2025-04-17T08:18:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    8,
                    18,
                    9,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T08:18:09Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    8,
                    18,
                    9,
                    3,
                    107,
                    0
                ],
                "title": "Pandora: A Code-Driven Large Language Model Agent for Unified Reasoning\n  Across Diverse Structured Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pandora: A Code-Driven Large Language Model Agent for Unified Reasoning\n  Across Diverse Structured Knowledge"
                },
                "summary": "Unified Structured Knowledge Reasoning (USKR) aims to answer natural language\nquestions (NLQs) by using structured sources such as tables, databases, and\nknowledge graphs in a unified way. Existing USKR methods either rely on\nemploying task-specific strategies or custom-defined representations, which\nstruggle to leverage the knowledge transfer between different SKR tasks or\nalign with the prior of LLMs, thereby limiting their performance. This paper\nproposes a novel USKR framework named \\textsc{Pandora}, which takes advantage\nof \\textsc{Python}'s \\textsc{Pandas} API to construct a unified knowledge\nrepresentation for alignment with LLM pre-training. It employs an LLM to\ngenerate textual reasoning steps and executable Python code for each question.\nDemonstrations are drawn from a memory of training examples that cover various\nSKR tasks, facilitating knowledge transfer. Extensive experiments on four\nbenchmarks involving three SKR tasks demonstrate that \\textsc{Pandora}\noutperforms existing unified frameworks and competes effectively with\ntask-specific methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified Structured Knowledge Reasoning (USKR) aims to answer natural language\nquestions (NLQs) by using structured sources such as tables, databases, and\nknowledge graphs in a unified way. Existing USKR methods either rely on\nemploying task-specific strategies or custom-defined representations, which\nstruggle to leverage the knowledge transfer between different SKR tasks or\nalign with the prior of LLMs, thereby limiting their performance. This paper\nproposes a novel USKR framework named \\textsc{Pandora}, which takes advantage\nof \\textsc{Python}'s \\textsc{Pandas} API to construct a unified knowledge\nrepresentation for alignment with LLM pre-training. It employs an LLM to\ngenerate textual reasoning steps and executable Python code for each question.\nDemonstrations are drawn from a memory of training examples that cover various\nSKR tasks, facilitating knowledge transfer. Extensive experiments on four\nbenchmarks involving three SKR tasks demonstrate that \\textsc{Pandora}\noutperforms existing unified frameworks and competes effectively with\ntask-specific methods."
                },
                "authors": [
                    {
                        "name": "Yongrui Chen"
                    },
                    {
                        "name": "Junhao He"
                    },
                    {
                        "name": "Linbo Fu"
                    },
                    {
                        "name": "Shenyu Zhang"
                    },
                    {
                        "name": "Rihui Jin"
                    },
                    {
                        "name": "Xinbang Dai"
                    },
                    {
                        "name": "Jiaqi Li"
                    },
                    {
                        "name": "Dehai Min"
                    },
                    {
                        "name": "Nan Hu"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Guilin Qi"
                    },
                    {
                        "name": "Yi Huang"
                    },
                    {
                        "name": "Tongtong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Tongtong Wu"
                },
                "author": "Tongtong Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12734v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12734v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12732v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12732v1",
                "updated": "2025-04-17T08:14:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    8,
                    14,
                    45,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T08:14:45Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    8,
                    14,
                    45,
                    3,
                    107,
                    0
                ],
                "title": "Validating LLM-Generated Relevance Labels for Educational Resource\n  Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Validating LLM-Generated Relevance Labels for Educational Resource\n  Search"
                },
                "summary": "Manual relevance judgements in Information Retrieval are costly and require\nexpertise, driving interest in using Large Language Models (LLMs) for automatic\nassessment. While LLMs have shown promise in general web search scenarios,\ntheir effectiveness for evaluating domain-specific search results, such as\neducational resources, remains unexplored. To investigate different ways of\nincluding domain-specific criteria in LLM prompts for relevance judgement, we\ncollected and released a dataset of 401 human relevance judgements from a user\nstudy involving teaching professionals performing search tasks related to\nlesson planning. We compared three approaches to structuring these prompts: a\nsimple two-aspect evaluation baseline from prior work on using LLMs as\nrelevance judges, a comprehensive 12-dimensional rubric derived from\neducational literature, and criteria directly informed by the study\nparticipants. Using domain-specific frameworks, LLMs achieved strong agreement\nwith human judgements (Cohen's $\\kappa$ up to 0.650), significantly\noutperforming the baseline approach. The participant-derived framework proved\nparticularly robust, with GPT-3.5 achieving $\\kappa$ scores of 0.639 and 0.613\nfor 10-dimension and 5-dimension versions respectively. System-level evaluation\nshowed that LLM judgements reliably identified top-performing retrieval\napproaches (RBO scores 0.71-0.76) while maintaining reasonable discrimination\nbetween systems (RBO 0.52-0.56). These findings suggest that LLMs can\neffectively evaluate educational resources when prompted with domain-specific\ncriteria, though performance varies with framework complexity and input\nstructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Manual relevance judgements in Information Retrieval are costly and require\nexpertise, driving interest in using Large Language Models (LLMs) for automatic\nassessment. While LLMs have shown promise in general web search scenarios,\ntheir effectiveness for evaluating domain-specific search results, such as\neducational resources, remains unexplored. To investigate different ways of\nincluding domain-specific criteria in LLM prompts for relevance judgement, we\ncollected and released a dataset of 401 human relevance judgements from a user\nstudy involving teaching professionals performing search tasks related to\nlesson planning. We compared three approaches to structuring these prompts: a\nsimple two-aspect evaluation baseline from prior work on using LLMs as\nrelevance judges, a comprehensive 12-dimensional rubric derived from\neducational literature, and criteria directly informed by the study\nparticipants. Using domain-specific frameworks, LLMs achieved strong agreement\nwith human judgements (Cohen's $\\kappa$ up to 0.650), significantly\noutperforming the baseline approach. The participant-derived framework proved\nparticularly robust, with GPT-3.5 achieving $\\kappa$ scores of 0.639 and 0.613\nfor 10-dimension and 5-dimension versions respectively. System-level evaluation\nshowed that LLM judgements reliably identified top-performing retrieval\napproaches (RBO scores 0.71-0.76) while maintaining reasonable discrimination\nbetween systems (RBO 0.52-0.56). These findings suggest that LLMs can\neffectively evaluate educational resources when prompted with domain-specific\ncriteria, though performance varies with framework complexity and input\nstructure."
                },
                "authors": [
                    {
                        "name": "Ratan J. Sebastian"
                    },
                    {
                        "name": "Anett Hoppe"
                    }
                ],
                "author_detail": {
                    "name": "Anett Hoppe"
                },
                "author": "Anett Hoppe",
                "arxiv_comment": "Presented in the LLM4Eval Workshop Co-located with WSDM '25 in\n  Hannover, Germany",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12732v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12732v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13054v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13054v2",
                "updated": "2025-04-17T07:48:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    7,
                    48,
                    47,
                    3,
                    107,
                    0
                ],
                "published": "2024-10-16T21:32:31Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    21,
                    32,
                    31,
                    2,
                    290,
                    0
                ],
                "title": "Systems with Switching Causal Relations: A Meta-Causal Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systems with Switching Causal Relations: A Meta-Causal Perspective"
                },
                "summary": "Most work on causality in machine learning assumes that causal relationships\nare driven by a constant underlying process. However, the flexibility of\nagents' actions or tipping points in the environmental process can change the\nqualitative dynamics of the system. As a result, new causal relationships may\nemerge, while existing ones change or disappear, resulting in an altered causal\ngraph. To analyze these qualitative changes on the causal graph, we propose the\nconcept of meta-causal states, which groups classical causal models into\nclusters based on equivalent qualitative behavior and consolidates specific\nmechanism parameterizations. We demonstrate how meta-causal states can be\ninferred from observed agent behavior, and discuss potential methods for\ndisentangling these states from unlabeled data. Finally, we direct our analysis\ntowards the application of a dynamical system, showing that meta-causal states\ncan also emerge from inherent system dynamics, and thus constitute more than a\ncontext-dependent framework in which mechanisms emerge only as a result of\nexternal factors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most work on causality in machine learning assumes that causal relationships\nare driven by a constant underlying process. However, the flexibility of\nagents' actions or tipping points in the environmental process can change the\nqualitative dynamics of the system. As a result, new causal relationships may\nemerge, while existing ones change or disappear, resulting in an altered causal\ngraph. To analyze these qualitative changes on the causal graph, we propose the\nconcept of meta-causal states, which groups classical causal models into\nclusters based on equivalent qualitative behavior and consolidates specific\nmechanism parameterizations. We demonstrate how meta-causal states can be\ninferred from observed agent behavior, and discuss potential methods for\ndisentangling these states from unlabeled data. Finally, we direct our analysis\ntowards the application of a dynamical system, showing that meta-causal states\ncan also emerge from inherent system dynamics, and thus constitute more than a\ncontext-dependent framework in which mechanisms emerge only as a result of\nexternal factors."
                },
                "authors": [
                    {
                        "name": "Moritz Willig"
                    },
                    {
                        "name": "Tim Nelson Tobiasch"
                    },
                    {
                        "name": "Florian Peter Busch"
                    },
                    {
                        "name": "Jonas Seng"
                    },
                    {
                        "name": "Devendra Singh Dhami"
                    },
                    {
                        "name": "Kristian Kersting"
                    }
                ],
                "author_detail": {
                    "name": "Kristian Kersting"
                },
                "author": "Kristian Kersting",
                "arxiv_comment": "21 pages, 3 figures, 4 tables, ICLR 2025 Camera Ready Version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13054v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13054v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.20285v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.20285v3",
                "updated": "2025-04-17T07:47:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    7,
                    47,
                    26,
                    3,
                    107,
                    0
                ],
                "published": "2023-10-31T08:58:16Z",
                "published_parsed": [
                    2023,
                    10,
                    31,
                    8,
                    58,
                    16,
                    1,
                    304,
                    0
                ],
                "title": "Accelerating Non-Conjugate Gaussian Processes By Trading Off Computation\n  For Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Non-Conjugate Gaussian Processes By Trading Off Computation\n  For Uncertainty"
                },
                "summary": "Non-conjugate Gaussian processes (NCGPs) define a flexible probabilistic\nframework to model categorical, ordinal and continuous data, and are widely\nused in practice. However, exact inference in NCGPs is prohibitively expensive\nfor large datasets, thus requiring approximations in practice. The\napproximation error adversely impacts the reliability of the model and is not\naccounted for in the uncertainty of the prediction. We introduce a family of\niterative methods that explicitly model this error. They are uniquely suited to\nparallel modern computing hardware, efficiently recycle computations, and\ncompress information to reduce both the time and memory requirements for NCGPs.\nAs we demonstrate on large-scale classification problems, our method\nsignificantly accelerates posterior inference compared to competitive baselines\nby trading off reduced computation for increased uncertainty.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-conjugate Gaussian processes (NCGPs) define a flexible probabilistic\nframework to model categorical, ordinal and continuous data, and are widely\nused in practice. However, exact inference in NCGPs is prohibitively expensive\nfor large datasets, thus requiring approximations in practice. The\napproximation error adversely impacts the reliability of the model and is not\naccounted for in the uncertainty of the prediction. We introduce a family of\niterative methods that explicitly model this error. They are uniquely suited to\nparallel modern computing hardware, efficiently recycle computations, and\ncompress information to reduce both the time and memory requirements for NCGPs.\nAs we demonstrate on large-scale classification problems, our method\nsignificantly accelerates posterior inference compared to competitive baselines\nby trading off reduced computation for increased uncertainty."
                },
                "authors": [
                    {
                        "name": "Lukas Tatzel"
                    },
                    {
                        "name": "Jonathan Wenger"
                    },
                    {
                        "name": "Frank Schneider"
                    },
                    {
                        "name": "Philipp Hennig"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Hennig"
                },
                "author": "Philipp Hennig",
                "arxiv_comment": "Main text: 15 pages, 7 figures; Supplements: 15 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.20285v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.20285v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24047v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24047v2",
                "updated": "2025-04-17T07:26:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    7,
                    26,
                    34,
                    3,
                    107,
                    0
                ],
                "published": "2025-03-31T13:11:28Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    13,
                    11,
                    28,
                    0,
                    90,
                    0
                ],
                "title": "Towards Scientific Intelligence: A Survey of LLM-based Scientific Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Scientific Intelligence: A Survey of LLM-based Scientific Agents"
                },
                "summary": "As scientific research becomes increasingly complex, innovative tools are\nneeded to manage vast data, facilitate interdisciplinary collaboration, and\naccelerate discovery. Large language models (LLMs) are now evolving into\nLLM-based scientific agents that automate critical tasks, ranging from\nhypothesis generation and experiment design to data analysis and simulation.\nUnlike general-purpose LLMs, these specialized agents integrate domain-specific\nknowledge, advanced tool sets, and robust validation mechanisms, enabling them\nto handle complex data types, ensure reproducibility, and drive scientific\nbreakthroughs. This survey provides a focused review of the architectures,\ndesign, benchmarks, applications, and ethical considerations surrounding\nLLM-based scientific agents. We highlight why they differ from general agents\nand the ways in which they advance research across various scientific fields.\nBy examining their development and challenges, this survey offers a\ncomprehensive roadmap for researchers and practitioners to harness these agents\nfor more efficient, reliable, and ethically sound scientific discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As scientific research becomes increasingly complex, innovative tools are\nneeded to manage vast data, facilitate interdisciplinary collaboration, and\naccelerate discovery. Large language models (LLMs) are now evolving into\nLLM-based scientific agents that automate critical tasks, ranging from\nhypothesis generation and experiment design to data analysis and simulation.\nUnlike general-purpose LLMs, these specialized agents integrate domain-specific\nknowledge, advanced tool sets, and robust validation mechanisms, enabling them\nto handle complex data types, ensure reproducibility, and drive scientific\nbreakthroughs. This survey provides a focused review of the architectures,\ndesign, benchmarks, applications, and ethical considerations surrounding\nLLM-based scientific agents. We highlight why they differ from general agents\nand the ways in which they advance research across various scientific fields.\nBy examining their development and challenges, this survey offers a\ncomprehensive roadmap for researchers and practitioners to harness these agents\nfor more efficient, reliable, and ethically sound scientific discovery."
                },
                "authors": [
                    {
                        "name": "Shuo Ren"
                    },
                    {
                        "name": "Pu Jian"
                    },
                    {
                        "name": "Zhenjiang Ren"
                    },
                    {
                        "name": "Chunlin Leng"
                    },
                    {
                        "name": "Can Xie"
                    },
                    {
                        "name": "Jiajun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Zhang"
                },
                "author": "Jiajun Zhang",
                "arxiv_comment": "34 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24047v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24047v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03728v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03728v2",
                "updated": "2025-04-17T07:11:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    7,
                    11,
                    34,
                    3,
                    107,
                    0
                ],
                "published": "2024-11-06T07:46:34Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    46,
                    34,
                    2,
                    311,
                    0
                ],
                "title": "Efficient Fourier Filtering Network with Contrastive Learning for\n  UAV-based Unaligned Bi-modal Salient Object Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Fourier Filtering Network with Contrastive Learning for\n  UAV-based Unaligned Bi-modal Salient Object Detection"
                },
                "summary": "Unmanned aerial vehicle (UAV)-based bi-modal salient object detection (BSOD)\naims to segment salient objects in a scene utilizing complementary cues in\nunaligned RGB and thermal image pairs. However, the high computational expense\nof existing UAV-based BSOD models limits their applicability to real-world UAV\ndevices. To address this problem, we propose an efficient Fourier filter\nnetwork with contrastive learning that achieves both real-time and accurate\nperformance. Specifically, we first design a semantic contrastive alignment\nloss to align the two modalities at the semantic level, which facilitates\nmutual refinement in a parameter-free way. Second, inspired by the fast Fourier\ntransform that obtains global relevance in linear complexity, we propose\nsynchronized alignment fusion, which aligns and fuses bi-modal features in the\nchannel and spatial dimensions by a hierarchical filtering mechanism. Our\nproposed model, AlignSal, reduces the number of parameters by 70.0%, decreases\nthe floating point operations by 49.4%, and increases the inference speed by\n152.5% compared to the cutting-edge BSOD model (i.e., MROS). Extensive\nexperiments on the UAV RGB-T 2400 and seven bi-modal dense prediction datasets\ndemonstrate that AlignSal achieves both real-time inference speed and better\nperformance and generalizability compared to nineteen state-of-the-art models\nacross most evaluation metrics. In addition, our ablation studies further\nverify AlignSal's potential in boosting the performance of existing aligned\nBSOD models on UAV-based unaligned data. The code is available at:\nhttps://github.com/JoshuaLPF/AlignSal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmanned aerial vehicle (UAV)-based bi-modal salient object detection (BSOD)\naims to segment salient objects in a scene utilizing complementary cues in\nunaligned RGB and thermal image pairs. However, the high computational expense\nof existing UAV-based BSOD models limits their applicability to real-world UAV\ndevices. To address this problem, we propose an efficient Fourier filter\nnetwork with contrastive learning that achieves both real-time and accurate\nperformance. Specifically, we first design a semantic contrastive alignment\nloss to align the two modalities at the semantic level, which facilitates\nmutual refinement in a parameter-free way. Second, inspired by the fast Fourier\ntransform that obtains global relevance in linear complexity, we propose\nsynchronized alignment fusion, which aligns and fuses bi-modal features in the\nchannel and spatial dimensions by a hierarchical filtering mechanism. Our\nproposed model, AlignSal, reduces the number of parameters by 70.0%, decreases\nthe floating point operations by 49.4%, and increases the inference speed by\n152.5% compared to the cutting-edge BSOD model (i.e., MROS). Extensive\nexperiments on the UAV RGB-T 2400 and seven bi-modal dense prediction datasets\ndemonstrate that AlignSal achieves both real-time inference speed and better\nperformance and generalizability compared to nineteen state-of-the-art models\nacross most evaluation metrics. In addition, our ablation studies further\nverify AlignSal's potential in boosting the performance of existing aligned\nBSOD models on UAV-based unaligned data. The code is available at:\nhttps://github.com/JoshuaLPF/AlignSal."
                },
                "authors": [
                    {
                        "name": "Pengfei Lyu"
                    },
                    {
                        "name": "Pak-Hei Yeung"
                    },
                    {
                        "name": "Xiaosheng Yu"
                    },
                    {
                        "name": "Xiufei Cheng"
                    },
                    {
                        "name": "Chengdong Wu"
                    },
                    {
                        "name": "Jagath C. Rajapakse"
                    }
                ],
                "author_detail": {
                    "name": "Jagath C. Rajapakse"
                },
                "author": "Jagath C. Rajapakse",
                "arxiv_doi": "10.1109/TGRS.2025.3562562",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TGRS.2025.3562562",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.03728v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03728v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by TGRS 2025",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01113v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01113v2",
                "updated": "2025-04-17T06:57:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    6,
                    57,
                    50,
                    3,
                    107,
                    0
                ],
                "published": "2024-12-02T04:35:54Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    4,
                    35,
                    54,
                    0,
                    337,
                    0
                ],
                "title": "Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in\n  Multi-Step Arithmetic Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in\n  Multi-Step Arithmetic Reasoning"
                },
                "summary": "This study investigates the internal reasoning process of language models\nduring arithmetic multi-step reasoning, motivated by the question of when they\ninternally form their answers during reasoning. Particularly, we inspect\nwhether the answer is determined before or after chain-of-thought (CoT) begins\nto determine whether models follow a post-hoc Think-to-Talk mode or a\nstep-by-step Talk-to-Think mode of explanation. Through causal probing\nexperiments in controlled arithmetic reasoning tasks, we found systematic\ninternal reasoning patterns across models in our case study; for example,\nsingle-step subproblems are solved before CoT begins, and more complicated\nmulti-step calculations are performed during CoT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the internal reasoning process of language models\nduring arithmetic multi-step reasoning, motivated by the question of when they\ninternally form their answers during reasoning. Particularly, we inspect\nwhether the answer is determined before or after chain-of-thought (CoT) begins\nto determine whether models follow a post-hoc Think-to-Talk mode or a\nstep-by-step Talk-to-Think mode of explanation. Through causal probing\nexperiments in controlled arithmetic reasoning tasks, we found systematic\ninternal reasoning patterns across models in our case study; for example,\nsingle-step subproblems are solved before CoT begins, and more complicated\nmulti-step calculations are performed during CoT."
                },
                "authors": [
                    {
                        "name": "Keito Kudo"
                    },
                    {
                        "name": "Yoichi Aoki"
                    },
                    {
                        "name": "Tatsuki Kuribayashi"
                    },
                    {
                        "name": "Shusaku Sone"
                    },
                    {
                        "name": "Masaya Taniguchi"
                    },
                    {
                        "name": "Ana Brassard"
                    },
                    {
                        "name": "Keisuke Sakaguchi"
                    },
                    {
                        "name": "Kentaro Inui"
                    }
                ],
                "author_detail": {
                    "name": "Kentaro Inui"
                },
                "author": "Kentaro Inui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01113v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01113v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12699v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12699v1",
                "updated": "2025-04-17T06:57:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    6,
                    57,
                    20,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T06:57:20Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    6,
                    57,
                    20,
                    3,
                    107,
                    0
                ],
                "title": "Unsupervised Cross-Domain 3D Human Pose Estimation via\n  Pseudo-Label-Guided Global Transforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised Cross-Domain 3D Human Pose Estimation via\n  Pseudo-Label-Guided Global Transforms"
                },
                "summary": "Existing 3D human pose estimation methods often suffer in performance, when\napplied to cross-scenario inference, due to domain shifts in characteristics\nsuch as camera viewpoint, position, posture, and body size. Among these\nfactors, camera viewpoints and locations {have been shown} to contribute\nsignificantly to the domain gap by influencing the global positions of human\nposes. To address this, we propose a novel framework that explicitly conducts\nglobal transformations between pose positions in the camera coordinate systems\nof source and target domains. We start with a Pseudo-Label Generation Module\nthat is applied to the 2D poses of the target dataset to generate pseudo-3D\nposes. Then, a Global Transformation Module leverages a human-centered\ncoordinate system as a novel bridging mechanism to seamlessly align the\npositional orientations of poses across disparate domains, ensuring consistent\nspatial referencing. To further enhance generalization, a Pose Augmentor is\nincorporated to address variations in human posture and body size. This process\nis iterative, allowing refined pseudo-labels to progressively improve guidance\nfor domain adaptation. Our method is evaluated on various cross-dataset\nbenchmarks, including Human3.6M, MPI-INF-3DHP, and 3DPW. The proposed method\noutperforms state-of-the-art approaches and even outperforms the target-trained\nmodel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing 3D human pose estimation methods often suffer in performance, when\napplied to cross-scenario inference, due to domain shifts in characteristics\nsuch as camera viewpoint, position, posture, and body size. Among these\nfactors, camera viewpoints and locations {have been shown} to contribute\nsignificantly to the domain gap by influencing the global positions of human\nposes. To address this, we propose a novel framework that explicitly conducts\nglobal transformations between pose positions in the camera coordinate systems\nof source and target domains. We start with a Pseudo-Label Generation Module\nthat is applied to the 2D poses of the target dataset to generate pseudo-3D\nposes. Then, a Global Transformation Module leverages a human-centered\ncoordinate system as a novel bridging mechanism to seamlessly align the\npositional orientations of poses across disparate domains, ensuring consistent\nspatial referencing. To further enhance generalization, a Pose Augmentor is\nincorporated to address variations in human posture and body size. This process\nis iterative, allowing refined pseudo-labels to progressively improve guidance\nfor domain adaptation. Our method is evaluated on various cross-dataset\nbenchmarks, including Human3.6M, MPI-INF-3DHP, and 3DPW. The proposed method\noutperforms state-of-the-art approaches and even outperforms the target-trained\nmodel."
                },
                "authors": [
                    {
                        "name": "Jingjing Liu"
                    },
                    {
                        "name": "Zhiyong Wang"
                    },
                    {
                        "name": "Xinyu Fan"
                    },
                    {
                        "name": "Amirhossein Dadashzadeh"
                    },
                    {
                        "name": "Honghai Liu"
                    },
                    {
                        "name": "Majid Mirmehdi"
                    }
                ],
                "author_detail": {
                    "name": "Majid Mirmehdi"
                },
                "author": "Majid Mirmehdi",
                "arxiv_comment": "11 pages, 6 figures, including appendix. This work has been submitted\n  to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12699v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12699v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12691v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12691v1",
                "updated": "2025-04-17T06:34:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    6,
                    34,
                    45,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T06:34:45Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    6,
                    34,
                    45,
                    3,
                    107,
                    0
                ],
                "title": "Why and How LLMs Hallucinate: Connecting the Dots with Subsequence\n  Associations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why and How LLMs Hallucinate: Connecting the Dots with Subsequence\n  Associations"
                },
                "summary": "Large language models (LLMs) frequently generate hallucinations-content that\ndeviates from factual accuracy or provided context-posing challenges for\ndiagnosis due to the complex interplay of underlying causes. This paper\nintroduces a subsequence association framework to systematically trace and\nunderstand hallucinations. Our key insight is that hallucinations arise when\ndominant hallucinatory associations outweigh faithful ones. Through theoretical\nand empirical analyses, we demonstrate that decoder-only transformers\neffectively function as subsequence embedding models, with linear layers\nencoding input-output associations. We propose a tracing algorithm that\nidentifies causal subsequences by analyzing hallucination probabilities across\nrandomized input contexts. Experiments show our method outperforms standard\nattribution techniques in identifying hallucination causes and aligns with\nevidence from the model's training corpus. This work provides a unified\nperspective on hallucinations and a robust framework for their tracing and\nanalysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) frequently generate hallucinations-content that\ndeviates from factual accuracy or provided context-posing challenges for\ndiagnosis due to the complex interplay of underlying causes. This paper\nintroduces a subsequence association framework to systematically trace and\nunderstand hallucinations. Our key insight is that hallucinations arise when\ndominant hallucinatory associations outweigh faithful ones. Through theoretical\nand empirical analyses, we demonstrate that decoder-only transformers\neffectively function as subsequence embedding models, with linear layers\nencoding input-output associations. We propose a tracing algorithm that\nidentifies causal subsequences by analyzing hallucination probabilities across\nrandomized input contexts. Experiments show our method outperforms standard\nattribution techniques in identifying hallucination causes and aligns with\nevidence from the model's training corpus. This work provides a unified\nperspective on hallucinations and a robust framework for their tracing and\nanalysis."
                },
                "authors": [
                    {
                        "name": "Yiyou Sun"
                    },
                    {
                        "name": "Yu Gai"
                    },
                    {
                        "name": "Lijie Chen"
                    },
                    {
                        "name": "Abhilasha Ravichander"
                    },
                    {
                        "name": "Yejin Choi"
                    },
                    {
                        "name": "Dawn Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawn Song"
                },
                "author": "Dawn Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12691v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12691v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12687v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12687v1",
                "updated": "2025-04-17T06:29:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    6,
                    29,
                    28,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T06:29:28Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    6,
                    29,
                    28,
                    3,
                    107,
                    0
                ],
                "title": "Data-efficient LLM Fine-tuning for Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-efficient LLM Fine-tuning for Code Generation"
                },
                "summary": "Large language models (LLMs) have demonstrated significant potential in code\ngeneration tasks. However, there remains a performance gap between open-source\nand closed-source models. To address this gap, existing approaches typically\ngenerate large amounts of synthetic data for fine-tuning, which often leads to\ninefficient training. In this work, we propose a data selection strategy in\norder to improve the effectiveness and efficiency of training for code-based\nLLMs. By prioritizing data complexity and ensuring that the sampled subset\naligns with the distribution of the original dataset, our sampling strategy\neffectively selects high-quality data. Additionally, we optimize the\ntokenization process through a \"dynamic pack\" technique, which minimizes\npadding tokens and reduces computational resource consumption. Experimental\nresults show that when training on 40% of the OSS-Instruct dataset, the\nDeepSeek-Coder-Base-6.7B model achieves an average performance of 66.9%,\nsurpassing the 66.1% performance with the full dataset. Moreover, training time\nis reduced from 47 minutes to 34 minutes, and the peak GPU memory decreases\nfrom 61.47 GB to 42.72 GB during a single epoch. Similar improvements are\nobserved with the CodeLlama-Python-7B model on the Evol-Instruct dataset. By\noptimizing both data selection and tokenization, our approach not only improves\nmodel performance but also improves training efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated significant potential in code\ngeneration tasks. However, there remains a performance gap between open-source\nand closed-source models. To address this gap, existing approaches typically\ngenerate large amounts of synthetic data for fine-tuning, which often leads to\ninefficient training. In this work, we propose a data selection strategy in\norder to improve the effectiveness and efficiency of training for code-based\nLLMs. By prioritizing data complexity and ensuring that the sampled subset\naligns with the distribution of the original dataset, our sampling strategy\neffectively selects high-quality data. Additionally, we optimize the\ntokenization process through a \"dynamic pack\" technique, which minimizes\npadding tokens and reduces computational resource consumption. Experimental\nresults show that when training on 40% of the OSS-Instruct dataset, the\nDeepSeek-Coder-Base-6.7B model achieves an average performance of 66.9%,\nsurpassing the 66.1% performance with the full dataset. Moreover, training time\nis reduced from 47 minutes to 34 minutes, and the peak GPU memory decreases\nfrom 61.47 GB to 42.72 GB during a single epoch. Similar improvements are\nobserved with the CodeLlama-Python-7B model on the Evol-Instruct dataset. By\noptimizing both data selection and tokenization, our approach not only improves\nmodel performance but also improves training efficiency."
                },
                "authors": [
                    {
                        "name": "Weijie Lv"
                    },
                    {
                        "name": "Xuan Xia"
                    },
                    {
                        "name": "Sheng-Jun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Sheng-Jun Huang"
                },
                "author": "Sheng-Jun Huang",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2408.02193",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12687v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12687v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11793v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11793v2",
                "updated": "2025-04-17T06:24:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    6,
                    24,
                    14,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-16T05:59:29Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    5,
                    59,
                    29,
                    2,
                    106,
                    0
                ],
                "title": "Selective Attention Federated Learning: Improving Privacy and Efficiency\n  for Clinical Text Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selective Attention Federated Learning: Improving Privacy and Efficiency\n  for Clinical Text Classification"
                },
                "summary": "Federated Learning (FL) faces major challenges regarding communication\noverhead and model privacy when training large language models (LLMs),\nespecially in healthcare applications. To address these, we introduce Selective\nAttention Federated Learning (SAFL), a novel approach that dynamically\nfine-tunes only those transformer layers identified as attention-critical. By\nemploying attention patterns to determine layer importance, SAFL significantly\nreduces communication bandwidth and enhances differential privacy resilience.\nEvaluations on clinical NLP benchmarks (i2b2 Clinical Concept Extraction and\nMIMIC-III discharge summaries) demonstrate that SAFL achieves competitive\nperformance with centralized models while substantially improving communication\nefficiency and privacy preservation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) faces major challenges regarding communication\noverhead and model privacy when training large language models (LLMs),\nespecially in healthcare applications. To address these, we introduce Selective\nAttention Federated Learning (SAFL), a novel approach that dynamically\nfine-tunes only those transformer layers identified as attention-critical. By\nemploying attention patterns to determine layer importance, SAFL significantly\nreduces communication bandwidth and enhances differential privacy resilience.\nEvaluations on clinical NLP benchmarks (i2b2 Clinical Concept Extraction and\nMIMIC-III discharge summaries) demonstrate that SAFL achieves competitive\nperformance with centralized models while substantially improving communication\nefficiency and privacy preservation."
                },
                "authors": [
                    {
                        "name": "Yue Li"
                    },
                    {
                        "name": "Lihong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lihong Zhang"
                },
                "author": "Lihong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11793v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11793v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.13178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13178v1",
                "updated": "2025-04-17T17:59:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    59,
                    54,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T17:59:54Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    59,
                    54,
                    3,
                    107,
                    0
                ],
                "title": "Aligning Constraint Generation with Design Intent in Parametric CAD",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Constraint Generation with Design Intent in Parametric CAD"
                },
                "summary": "We adapt alignment techniques from reasoning LLMs to the task of generating\nengineering sketch constraints found in computer-aided design (CAD) models.\nEngineering sketches consist of geometric primitives (e.g. points, lines)\nconnected by constraints (e.g. perpendicular, tangent) that define the\nrelationships between them. For a design to be easily editable, the constraints\nmust effectively capture design intent, ensuring the geometry updates\npredictably when parameters change. Although current approaches can generate\nCAD designs, an open challenge remains to align model outputs with design\nintent, we label this problem `design alignment'. A critical first step towards\naligning generative CAD models is to generate constraints which fully-constrain\nall geometric primitives, without over-constraining or distorting sketch\ngeometry. Using alignment techniques to train an existing constraint generation\nmodel with feedback from a constraint solver, we are able to fully-constrain\n93% of sketches compared to 34% when using a na\\\"ive supervised fine-tuning\n(SFT) baseline and only 8.9% without alignment. Our approach can be applied to\nany existing constraint generation model and sets the stage for further\nresearch bridging alignment strategies between the language and design domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We adapt alignment techniques from reasoning LLMs to the task of generating\nengineering sketch constraints found in computer-aided design (CAD) models.\nEngineering sketches consist of geometric primitives (e.g. points, lines)\nconnected by constraints (e.g. perpendicular, tangent) that define the\nrelationships between them. For a design to be easily editable, the constraints\nmust effectively capture design intent, ensuring the geometry updates\npredictably when parameters change. Although current approaches can generate\nCAD designs, an open challenge remains to align model outputs with design\nintent, we label this problem `design alignment'. A critical first step towards\naligning generative CAD models is to generate constraints which fully-constrain\nall geometric primitives, without over-constraining or distorting sketch\ngeometry. Using alignment techniques to train an existing constraint generation\nmodel with feedback from a constraint solver, we are able to fully-constrain\n93% of sketches compared to 34% when using a na\\\"ive supervised fine-tuning\n(SFT) baseline and only 8.9% without alignment. Our approach can be applied to\nany existing constraint generation model and sets the stage for further\nresearch bridging alignment strategies between the language and design domains."
                },
                "authors": [
                    {
                        "name": "Evan Casey"
                    },
                    {
                        "name": "Tianyu Zhang"
                    },
                    {
                        "name": "Shu Ishida"
                    },
                    {
                        "name": "John Roger Thompson"
                    },
                    {
                        "name": "Amir Khasahmadi"
                    },
                    {
                        "name": "Joseph George Lambourne"
                    },
                    {
                        "name": "Pradeep Kumar Jayaraman"
                    },
                    {
                        "name": "Karl D. D. Willis"
                    }
                ],
                "author_detail": {
                    "name": "Karl D. D. Willis"
                },
                "author": "Karl D. D. Willis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13171v1",
                "updated": "2025-04-17T17:59:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    59,
                    25,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T17:59:25Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    59,
                    25,
                    3,
                    107,
                    0
                ],
                "title": "Sleep-time Compute: Beyond Inference Scaling at Test-time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sleep-time Compute: Beyond Inference Scaling at Test-time"
                },
                "summary": "Scaling test-time compute has emerged as a key ingredient for enabling large\nlanguage models (LLMs) to solve difficult problems, but comes with high latency\nand inference cost. We introduce sleep-time compute, which allows models to\n\"think\" offline about contexts before queries are presented: by anticipating\nwhat queries users might ask and pre-computing useful quantities, we can\nsignificantly reduce the compute requirements at test-time. To demonstrate the\nefficacy of our method, we create modified versions of two reasoning tasks -\nStateful GSM-Symbolic and Stateful AIME. We find that sleep-time compute can\nreduce the amount of test-time compute needed to achieve the same accuracy by ~\n5x on Stateful GSM-Symbolic and Stateful AIME and that by scaling sleep-time\ncompute we can further increase accuracy by up to 13% on Stateful GSM-Symbolic\nand 18% on Stateful AIME. Furthermore, we introduce Multi-Query GSM-Symbolic,\nwhich extends GSM-Symbolic by including multiple related queries per context.\nBy amortizing sleep-time compute across related queries about the same context\nusing Multi-Query GSM-Symbolic, we can decrease the average cost per query by\n2.5x. We then conduct additional analysis to understand when sleep-time compute\nis most effective, finding the predictability of the user query to be well\ncorrelated with the efficacy of sleep-time compute. Finally, we conduct a\ncase-study of applying sleep-time compute to a realistic agentic SWE task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling test-time compute has emerged as a key ingredient for enabling large\nlanguage models (LLMs) to solve difficult problems, but comes with high latency\nand inference cost. We introduce sleep-time compute, which allows models to\n\"think\" offline about contexts before queries are presented: by anticipating\nwhat queries users might ask and pre-computing useful quantities, we can\nsignificantly reduce the compute requirements at test-time. To demonstrate the\nefficacy of our method, we create modified versions of two reasoning tasks -\nStateful GSM-Symbolic and Stateful AIME. We find that sleep-time compute can\nreduce the amount of test-time compute needed to achieve the same accuracy by ~\n5x on Stateful GSM-Symbolic and Stateful AIME and that by scaling sleep-time\ncompute we can further increase accuracy by up to 13% on Stateful GSM-Symbolic\nand 18% on Stateful AIME. Furthermore, we introduce Multi-Query GSM-Symbolic,\nwhich extends GSM-Symbolic by including multiple related queries per context.\nBy amortizing sleep-time compute across related queries about the same context\nusing Multi-Query GSM-Symbolic, we can decrease the average cost per query by\n2.5x. We then conduct additional analysis to understand when sleep-time compute\nis most effective, finding the predictability of the user query to be well\ncorrelated with the efficacy of sleep-time compute. Finally, we conduct a\ncase-study of applying sleep-time compute to a realistic agentic SWE task."
                },
                "authors": [
                    {
                        "name": "Kevin Lin"
                    },
                    {
                        "name": "Charlie Snell"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Charles Packer"
                    },
                    {
                        "name": "Sarah Wooders"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E. Gonzalez"
                },
                "author": "Joseph E. Gonzalez",
                "arxiv_comment": "Code and data released at:\n  https://github.com/letta-ai/sleep-time-compute",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13145v1",
                "updated": "2025-04-17T17:53:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    53,
                    54,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T17:53:54Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    53,
                    54,
                    3,
                    107,
                    0
                ],
                "title": "Exploring Expert Failures Improves LLM Agent Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Expert Failures Improves LLM Agent Tuning"
                },
                "summary": "Large Language Models (LLMs) have shown tremendous potential as agents,\nexcelling at tasks that require multiple rounds of reasoning and interactions.\nRejection Sampling Fine-Tuning (RFT) has emerged as an effective method for\nfinetuning LLMs as agents: it first imitates expert-generated successful\ntrajectories and further improves agentic skills through iterative fine-tuning\non successful, self-generated trajectories. However, since the expert (e.g.,\nGPT-4) succeeds primarily on simpler subtasks and RFT inherently favors simpler\nscenarios, many complex subtasks remain unsolved and persistently\nout-of-distribution (OOD). Upon investigating these challenging subtasks, we\ndiscovered that previously failed expert trajectories can often provide\nvaluable guidance, e.g., plans and key actions, that can significantly improve\nagent exploration efficiency and acquisition of critical skills. Motivated by\nthese observations, we propose Exploring Expert Failures (EEF), which\nidentifies beneficial actions from failed expert trajectories and integrates\nthem into the training dataset. Potentially harmful actions are meticulously\nexcluded to prevent contamination of the model learning process. By leveraging\nthe beneficial actions in expert failures, EEF successfully solves some\npreviously unsolvable subtasks and improves agent tuning performance.\nRemarkably, our approach achieved a 62\\% win rate in WebShop, outperforming RFT\n(53. 6\\%) and GPT-4 (35. 6\\%), and to the best of our knowledge, setting a new\nstate-of-the-art as the first method to surpass a score of 0.81 in WebShop and\nexceed 81 in SciWorld.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown tremendous potential as agents,\nexcelling at tasks that require multiple rounds of reasoning and interactions.\nRejection Sampling Fine-Tuning (RFT) has emerged as an effective method for\nfinetuning LLMs as agents: it first imitates expert-generated successful\ntrajectories and further improves agentic skills through iterative fine-tuning\non successful, self-generated trajectories. However, since the expert (e.g.,\nGPT-4) succeeds primarily on simpler subtasks and RFT inherently favors simpler\nscenarios, many complex subtasks remain unsolved and persistently\nout-of-distribution (OOD). Upon investigating these challenging subtasks, we\ndiscovered that previously failed expert trajectories can often provide\nvaluable guidance, e.g., plans and key actions, that can significantly improve\nagent exploration efficiency and acquisition of critical skills. Motivated by\nthese observations, we propose Exploring Expert Failures (EEF), which\nidentifies beneficial actions from failed expert trajectories and integrates\nthem into the training dataset. Potentially harmful actions are meticulously\nexcluded to prevent contamination of the model learning process. By leveraging\nthe beneficial actions in expert failures, EEF successfully solves some\npreviously unsolvable subtasks and improves agent tuning performance.\nRemarkably, our approach achieved a 62\\% win rate in WebShop, outperforming RFT\n(53. 6\\%) and GPT-4 (35. 6\\%), and to the best of our knowledge, setting a new\nstate-of-the-art as the first method to surpass a score of 0.81 in WebShop and\nexceed 81 in SciWorld."
                },
                "authors": [
                    {
                        "name": "Li-Cheng Lan"
                    },
                    {
                        "name": "Andrew Bai"
                    },
                    {
                        "name": "Minhao Cheng"
                    },
                    {
                        "name": "Ruochen Wang"
                    },
                    {
                        "name": "Cho-Jui Hsieh"
                    },
                    {
                        "name": "Tianyi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Zhou"
                },
                "author": "Tianyi Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13141v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13141v1",
                "updated": "2025-04-17T17:50:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    50,
                    44,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T17:50:44Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    50,
                    44,
                    3,
                    107,
                    0
                ],
                "title": "Complexity at Scale: A Quantitative Analysis of an Alibaba Microservice\n  Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Complexity at Scale: A Quantitative Analysis of an Alibaba Microservice\n  Deployment"
                },
                "summary": "Microservice architectures are increasingly prevalent in organisations\nproviding online applications. Recent studies have begun to explore the\ncharacteristics of real-world large-scale microservice deployments; however,\ntheir operational complexities, and the degree to which this complexities are\nconsistent across different deployments, remains under-explored. In this paper,\nwe analyse a microservice dataset released by Alibaba along three dimensions of\ncomplexity: scale, heterogeneity, and dynamicity. We find that large-scale\ndeployments can consist of tens of thousands of microservices, that support an\neven broader array of front-end functionality. Moreover, our analysis shows\nwide-spread long-tailed distributions of characteristics between microservices,\nsuch as share of workload and dependencies, highlighting inequality across the\ndeployment. This diversity is also reflected in call graphs, where we find that\nwhilst front-end services produce dominant call graphs, rarer non-dominant call\ngraphs are prevalent and could involve dissimilar microservice calls. We also\nfind that runtime dependencies between microservices deviate from the static\nview of system dependencies, and that the deployment undergoes daily changes to\nmicroservices. We discuss the implications of our findings for state-of-the-art\nresearch in microservice management and research testbed realism, and compare\nour results to previous descriptions of large-scale microservice deployments to\nbegin to build an understanding of their commonalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microservice architectures are increasingly prevalent in organisations\nproviding online applications. Recent studies have begun to explore the\ncharacteristics of real-world large-scale microservice deployments; however,\ntheir operational complexities, and the degree to which this complexities are\nconsistent across different deployments, remains under-explored. In this paper,\nwe analyse a microservice dataset released by Alibaba along three dimensions of\ncomplexity: scale, heterogeneity, and dynamicity. We find that large-scale\ndeployments can consist of tens of thousands of microservices, that support an\neven broader array of front-end functionality. Moreover, our analysis shows\nwide-spread long-tailed distributions of characteristics between microservices,\nsuch as share of workload and dependencies, highlighting inequality across the\ndeployment. This diversity is also reflected in call graphs, where we find that\nwhilst front-end services produce dominant call graphs, rarer non-dominant call\ngraphs are prevalent and could involve dissimilar microservice calls. We also\nfind that runtime dependencies between microservices deviate from the static\nview of system dependencies, and that the deployment undergoes daily changes to\nmicroservices. We discuss the implications of our findings for state-of-the-art\nresearch in microservice management and research testbed realism, and compare\nour results to previous descriptions of large-scale microservice deployments to\nbegin to build an understanding of their commonalities."
                },
                "authors": [
                    {
                        "name": "Giles Winchester"
                    },
                    {
                        "name": "George Parisis"
                    },
                    {
                        "name": "Luc Berthouze"
                    }
                ],
                "author_detail": {
                    "name": "Luc Berthouze"
                },
                "author": "Luc Berthouze",
                "arxiv_comment": "19 pages, 24 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13141v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13141v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17070v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17070v3",
                "updated": "2025-04-17T17:48:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    48,
                    53,
                    3,
                    107,
                    0
                ],
                "published": "2025-01-28T16:55:39Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    16,
                    55,
                    39,
                    1,
                    28,
                    0
                ],
                "title": "Contextual Agent Security: A Policy for Every Purpose",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextual Agent Security: A Policy for Every Purpose"
                },
                "summary": "Judging an action's safety requires knowledge of the context in which the\naction takes place. To human agents who act in various contexts, this may seem\nobvious: performing an action such as email deletion may or may not be\nappropriate depending on the email's content, the goal (e.g., to erase\nsensitive emails or to clean up trash), and the type of email address (e.g.,\nwork or personal). Unlike people, computational systems have often had only\nlimited agency in limited contexts. Thus, manually crafted policies and user\nconfirmation (e.g., smartphone app permissions or network access control\nlists), while imperfect, have sufficed to restrict harmful actions. However,\nwith the upcoming deployment of generalist agents that support a multitude of\ntasks (e.g., an automated personal assistant), we argue that we must rethink\nsecurity designs to adapt to the scale of contexts and capabilities of these\nsystems. As a first step, this paper explores contextual security in the domain\nof agents and proposes contextual agent security (Conseca), a framework to\ngenerate just-in-time, contextual, and human-verifiable security policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Judging an action's safety requires knowledge of the context in which the\naction takes place. To human agents who act in various contexts, this may seem\nobvious: performing an action such as email deletion may or may not be\nappropriate depending on the email's content, the goal (e.g., to erase\nsensitive emails or to clean up trash), and the type of email address (e.g.,\nwork or personal). Unlike people, computational systems have often had only\nlimited agency in limited contexts. Thus, manually crafted policies and user\nconfirmation (e.g., smartphone app permissions or network access control\nlists), while imperfect, have sufficed to restrict harmful actions. However,\nwith the upcoming deployment of generalist agents that support a multitude of\ntasks (e.g., an automated personal assistant), we argue that we must rethink\nsecurity designs to adapt to the scale of contexts and capabilities of these\nsystems. As a first step, this paper explores contextual security in the domain\nof agents and proposes contextual agent security (Conseca), a framework to\ngenerate just-in-time, contextual, and human-verifiable security policies."
                },
                "authors": [
                    {
                        "name": "Lillian Tsai"
                    },
                    {
                        "name": "Eugene Bagdasarian"
                    }
                ],
                "author_detail": {
                    "name": "Eugene Bagdasarian"
                },
                "author": "Eugene Bagdasarian",
                "arxiv_doi": "10.1145/3713082.3730378",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3713082.3730378",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.17070v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17070v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Workshop in Hot Topics in Operating Systems (HotOS) 2025",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13134v1",
                "updated": "2025-04-17T17:47:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    47,
                    15,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T17:47:15Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    47,
                    15,
                    3,
                    107,
                    0
                ],
                "title": "Energy-Based Reward Models for Robust Language Model Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy-Based Reward Models for Robust Language Model Alignment"
                },
                "summary": "Reward models (RMs) are essential for aligning Large Language Models (LLMs)\nwith human preferences. However, they often struggle with capturing complex\nhuman preferences and generalizing to unseen data. To address these challenges,\nwe introduce Energy-Based Reward Model (EBRM), a lightweight post-hoc\nrefinement framework that enhances RM robustness and generalization. EBRM\nmodels the reward distribution explicitly, capturing uncertainty in human\npreferences and mitigating the impact of noisy or misaligned annotations. It\nachieves this through conflict-aware data filtering, label-noise-aware\ncontrastive training, and hybrid initialization. Notably, EBRM enhances RMs\nwithout retraining, making it computationally efficient and adaptable across\ndifferent models and tasks. Empirical evaluations on RM benchmarks demonstrate\nsignificant improvements in both robustness and generalization, achieving up to\na 5.97% improvement in safety-critical alignment tasks compared to standard\nRMs. Furthermore, reinforcement learning experiments confirm that our refined\nrewards enhance alignment quality, effectively delaying reward hacking. These\nresults demonstrate our approach as a scalable and effective enhancement for\nexisting RMs and alignment pipelines. The code is available at EBRM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward models (RMs) are essential for aligning Large Language Models (LLMs)\nwith human preferences. However, they often struggle with capturing complex\nhuman preferences and generalizing to unseen data. To address these challenges,\nwe introduce Energy-Based Reward Model (EBRM), a lightweight post-hoc\nrefinement framework that enhances RM robustness and generalization. EBRM\nmodels the reward distribution explicitly, capturing uncertainty in human\npreferences and mitigating the impact of noisy or misaligned annotations. It\nachieves this through conflict-aware data filtering, label-noise-aware\ncontrastive training, and hybrid initialization. Notably, EBRM enhances RMs\nwithout retraining, making it computationally efficient and adaptable across\ndifferent models and tasks. Empirical evaluations on RM benchmarks demonstrate\nsignificant improvements in both robustness and generalization, achieving up to\na 5.97% improvement in safety-critical alignment tasks compared to standard\nRMs. Furthermore, reinforcement learning experiments confirm that our refined\nrewards enhance alignment quality, effectively delaying reward hacking. These\nresults demonstrate our approach as a scalable and effective enhancement for\nexisting RMs and alignment pipelines. The code is available at EBRM."
                },
                "authors": [
                    {
                        "name": "Anamika Lochab"
                    },
                    {
                        "name": "Ruqi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ruqi Zhang"
                },
                "author": "Ruqi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13125v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13125v1",
                "updated": "2025-04-17T17:42:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    42,
                    2,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T17:42:02Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    42,
                    2,
                    3,
                    107,
                    0
                ],
                "title": "LLMs Meet Finance: Fine-Tuning Foundation Models for the Open FinLLM\n  Leaderboard",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Meet Finance: Fine-Tuning Foundation Models for the Open FinLLM\n  Leaderboard"
                },
                "summary": "This paper investigates the application of large language models (LLMs) to\nfinancial tasks. We fine-tuned foundation models using the Open FinLLM\nLeaderboard as a benchmark. Building on Qwen2.5 and Deepseek-R1, we employed\ntechniques including supervised fine-tuning (SFT), direct preference\noptimization (DPO), and reinforcement learning (RL) to enhance their financial\ncapabilities. The fine-tuned models demonstrated substantial performance gains\nacross a wide range of financial tasks. Moreover, we measured the data scaling\nlaw in the financial domain. Our work demonstrates the potential of large\nlanguage models (LLMs) in financial applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the application of large language models (LLMs) to\nfinancial tasks. We fine-tuned foundation models using the Open FinLLM\nLeaderboard as a benchmark. Building on Qwen2.5 and Deepseek-R1, we employed\ntechniques including supervised fine-tuning (SFT), direct preference\noptimization (DPO), and reinforcement learning (RL) to enhance their financial\ncapabilities. The fine-tuned models demonstrated substantial performance gains\nacross a wide range of financial tasks. Moreover, we measured the data scaling\nlaw in the financial domain. Our work demonstrates the potential of large\nlanguage models (LLMs) in financial applications."
                },
                "authors": [
                    {
                        "name": "Varun Rao"
                    },
                    {
                        "name": "Youran Sun"
                    },
                    {
                        "name": "Mahendra Kumar"
                    },
                    {
                        "name": "Tejas Mutneja"
                    },
                    {
                        "name": "Agastya Mukherjee"
                    },
                    {
                        "name": "Haizhao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Haizhao Yang"
                },
                "author": "Haizhao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13125v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13125v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13122v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13122v1",
                "updated": "2025-04-17T17:39:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    39,
                    41,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T17:39:41Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    39,
                    41,
                    3,
                    107,
                    0
                ],
                "title": "VistaDPO: Video Hierarchical Spatial-Temporal Direct Preference\n  Optimization for Large Video Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VistaDPO: Video Hierarchical Spatial-Temporal Direct Preference\n  Optimization for Large Video Models"
                },
                "summary": "Large Video Models (LVMs) built upon Large Language Models (LLMs) have shown\npromise in video understanding but often suffer from misalignment with human\nintuition and video hallucination issues. To address these challenges, we\nintroduce VistaDPO, a novel framework for Video Hierarchical Spatial-Temporal\nDirect Preference Optimization. VistaDPO enhances text-video preference\nalignment across three hierarchical levels: i) Instance Level, aligning overall\nvideo content with responses; ii) Temporal Level, aligning video temporal\nsemantics with event descriptions; and iii) Perceptive Level, aligning spatial\nobjects with language tokens. Given the lack of datasets for fine-grained\nvideo-language preference alignment, we construct VistaDPO-7k, a dataset of\n7.2K QA pairs annotated with chosen and rejected responses, along with\nspatial-temporal grounding information such as timestamps, keyframes, and\nbounding boxes. Extensive experiments on benchmarks such as Video\nHallucination, Video QA, and Captioning performance tasks demonstrate that\nVistaDPO significantly improves the performance of existing LVMs, effectively\nmitigating video-language misalignment and hallucination. The code and data are\navailable at https://github.com/HaroldChen19/VistaDPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Video Models (LVMs) built upon Large Language Models (LLMs) have shown\npromise in video understanding but often suffer from misalignment with human\nintuition and video hallucination issues. To address these challenges, we\nintroduce VistaDPO, a novel framework for Video Hierarchical Spatial-Temporal\nDirect Preference Optimization. VistaDPO enhances text-video preference\nalignment across three hierarchical levels: i) Instance Level, aligning overall\nvideo content with responses; ii) Temporal Level, aligning video temporal\nsemantics with event descriptions; and iii) Perceptive Level, aligning spatial\nobjects with language tokens. Given the lack of datasets for fine-grained\nvideo-language preference alignment, we construct VistaDPO-7k, a dataset of\n7.2K QA pairs annotated with chosen and rejected responses, along with\nspatial-temporal grounding information such as timestamps, keyframes, and\nbounding boxes. Extensive experiments on benchmarks such as Video\nHallucination, Video QA, and Captioning performance tasks demonstrate that\nVistaDPO significantly improves the performance of existing LVMs, effectively\nmitigating video-language misalignment and hallucination. The code and data are\navailable at https://github.com/HaroldChen19/VistaDPO."
                },
                "authors": [
                    {
                        "name": "Haojian Huang"
                    },
                    {
                        "name": "Haodong Chen"
                    },
                    {
                        "name": "Shengqiong Wu"
                    },
                    {
                        "name": "Meng Luo"
                    },
                    {
                        "name": "Jinlan Fu"
                    },
                    {
                        "name": "Xinya Du"
                    },
                    {
                        "name": "Hanwang Zhang"
                    },
                    {
                        "name": "Hao Fei"
                    }
                ],
                "author_detail": {
                    "name": "Hao Fei"
                },
                "author": "Hao Fei",
                "arxiv_comment": "Code and Data: https://github.com/HaroldChen19/VistaDPO",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13122v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13122v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09081v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09081v2",
                "updated": "2025-04-17T17:34:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    34,
                    21,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-12T04:45:48Z",
                "published_parsed": [
                    2025,
                    4,
                    12,
                    4,
                    45,
                    48,
                    5,
                    102,
                    0
                ],
                "title": "SIFT-50M: A Large-Scale Multilingual Dataset for Speech Instruction\n  Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SIFT-50M: A Large-Scale Multilingual Dataset for Speech Instruction\n  Fine-Tuning"
                },
                "summary": "We introduce SIFT (Speech Instruction Fine-Tuning), a 50M-example dataset\ndesigned for instruction fine-tuning and pre-training of speech-text large\nlanguage models (LLMs). SIFT-50M is built from publicly available speech\ncorpora, which collectively contain 14K hours of speech, and leverages LLMs\nalong with off-the-shelf expert models. The dataset spans five languages,\nencompassing a diverse range of speech understanding as well as controllable\nspeech generation instructions. Using SIFT-50M, we train SIFT-LLM, which\noutperforms existing speech-text LLMs on instruction-following benchmarks while\nachieving competitive performance on foundational speech tasks. To support\nfurther research, we also introduce EvalSIFT, a benchmark dataset specifically\ndesigned to evaluate the instruction-following capabilities of speech-text\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SIFT (Speech Instruction Fine-Tuning), a 50M-example dataset\ndesigned for instruction fine-tuning and pre-training of speech-text large\nlanguage models (LLMs). SIFT-50M is built from publicly available speech\ncorpora, which collectively contain 14K hours of speech, and leverages LLMs\nalong with off-the-shelf expert models. The dataset spans five languages,\nencompassing a diverse range of speech understanding as well as controllable\nspeech generation instructions. Using SIFT-50M, we train SIFT-LLM, which\noutperforms existing speech-text LLMs on instruction-following benchmarks while\nachieving competitive performance on foundational speech tasks. To support\nfurther research, we also introduce EvalSIFT, a benchmark dataset specifically\ndesigned to evaluate the instruction-following capabilities of speech-text\nLLMs."
                },
                "authors": [
                    {
                        "name": "Prabhat Pandey"
                    },
                    {
                        "name": "Rupak Vignesh Swaminathan"
                    },
                    {
                        "name": "K V Vijay Girish"
                    },
                    {
                        "name": "Arunasish Sen"
                    },
                    {
                        "name": "Jian Xie"
                    },
                    {
                        "name": "Grant P. Strimel"
                    },
                    {
                        "name": "Andreas Schwarz"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Schwarz"
                },
                "author": "Andreas Schwarz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09081v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09081v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16063v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16063v4",
                "updated": "2025-04-17T17:28:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    28,
                    29,
                    3,
                    107,
                    0
                ],
                "published": "2024-02-25T11:24:41Z",
                "published_parsed": [
                    2024,
                    2,
                    25,
                    11,
                    24,
                    41,
                    6,
                    56,
                    0
                ],
                "title": "Citation-Enhanced Generation for LLM-based Chatbots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Citation-Enhanced Generation for LLM-based Chatbots"
                },
                "summary": "Large language models (LLMs) exhibit powerful general intelligence across\ndiverse scenarios, including their integration into chatbots. However, a vital\nchallenge of LLM-based chatbots is that they may produce hallucinated content\nin responses, which significantly limits their applicability. Various efforts\nhave been made to alleviate hallucination, such as retrieval augmented\ngeneration and reinforcement learning with human feedback, but most of them\nrequire additional training and data annotation. In this paper, we propose a\nnovel post-hoc Citation-Enhanced Generation (CEG) approach combined with\nretrieval argumentation. Unlike previous studies that focus on preventing\nhallucinations during generation, our method addresses this issue in a post-hoc\nway. It incorporates a retrieval module to search for supporting documents\nrelevant to the generated content, and employs a natural language\ninference-based citation generation module. Once the statements in the\ngenerated content lack of reference, our model can regenerate responses until\nall statements are supported by citations. Note that our method is a\ntraining-free plug-and-play plugin that is capable of various LLMs. Experiments\non various hallucination-related datasets show our framework outperforms\nstate-of-the-art methods in both hallucination detection and response\nregeneration on three benchmarks. Our codes and dataset will be publicly\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit powerful general intelligence across\ndiverse scenarios, including their integration into chatbots. However, a vital\nchallenge of LLM-based chatbots is that they may produce hallucinated content\nin responses, which significantly limits their applicability. Various efforts\nhave been made to alleviate hallucination, such as retrieval augmented\ngeneration and reinforcement learning with human feedback, but most of them\nrequire additional training and data annotation. In this paper, we propose a\nnovel post-hoc Citation-Enhanced Generation (CEG) approach combined with\nretrieval argumentation. Unlike previous studies that focus on preventing\nhallucinations during generation, our method addresses this issue in a post-hoc\nway. It incorporates a retrieval module to search for supporting documents\nrelevant to the generated content, and employs a natural language\ninference-based citation generation module. Once the statements in the\ngenerated content lack of reference, our model can regenerate responses until\nall statements are supported by citations. Note that our method is a\ntraining-free plug-and-play plugin that is capable of various LLMs. Experiments\non various hallucination-related datasets show our framework outperforms\nstate-of-the-art methods in both hallucination detection and response\nregeneration on three benchmarks. Our codes and dataset will be publicly\navailable."
                },
                "authors": [
                    {
                        "name": "Weitao Li"
                    },
                    {
                        "name": "Junkai Li"
                    },
                    {
                        "name": "Weizhi Ma"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_doi": "10.18653/v1/2024.acl-long.79",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2024.acl-long.79",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.16063v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16063v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proc. 62nd ACL Vol. 1 Long Papers, Bangkok Thailand, pp.\n  1451-1466, 2024",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13112v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13112v1",
                "updated": "2025-04-17T17:26:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    26,
                    29,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T17:26:29Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    26,
                    29,
                    3,
                    107,
                    0
                ],
                "title": "Hadamard product in deep learning: Introduction, Advances and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hadamard product in deep learning: Introduction, Advances and Challenges"
                },
                "summary": "While convolution and self-attention mechanisms have dominated architectural\ndesign in deep learning, this survey examines a fundamental yet understudied\nprimitive: the Hadamard product. Despite its widespread implementation across\nvarious applications, the Hadamard product has not been systematically analyzed\nas a core architectural primitive. We present the first comprehensive taxonomy\nof its applications in deep learning, identifying four principal domains:\nhigher-order correlation, multimodal data fusion, dynamic representation\nmodulation, and efficient pairwise operations. The Hadamard product's ability\nto model nonlinear interactions with linear computational complexity makes it\nparticularly valuable for resource-constrained deployments and edge computing\nscenarios. We demonstrate its natural applicability in multimodal fusion tasks,\nsuch as visual question answering, and its effectiveness in representation\nmasking for applications including image inpainting and pruning. This\nsystematic review not only consolidates existing knowledge about the Hadamard\nproduct's role in deep learning architectures but also establishes a foundation\nfor future architectural innovations. Our analysis reveals the Hadamard product\nas a versatile primitive that offers compelling trade-offs between\ncomputational efficiency and representational power, positioning it as a\ncrucial component in the deep learning toolkit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While convolution and self-attention mechanisms have dominated architectural\ndesign in deep learning, this survey examines a fundamental yet understudied\nprimitive: the Hadamard product. Despite its widespread implementation across\nvarious applications, the Hadamard product has not been systematically analyzed\nas a core architectural primitive. We present the first comprehensive taxonomy\nof its applications in deep learning, identifying four principal domains:\nhigher-order correlation, multimodal data fusion, dynamic representation\nmodulation, and efficient pairwise operations. The Hadamard product's ability\nto model nonlinear interactions with linear computational complexity makes it\nparticularly valuable for resource-constrained deployments and edge computing\nscenarios. We demonstrate its natural applicability in multimodal fusion tasks,\nsuch as visual question answering, and its effectiveness in representation\nmasking for applications including image inpainting and pruning. This\nsystematic review not only consolidates existing knowledge about the Hadamard\nproduct's role in deep learning architectures but also establishes a foundation\nfor future architectural innovations. Our analysis reveals the Hadamard product\nas a versatile primitive that offers compelling trade-offs between\ncomputational efficiency and representational power, positioning it as a\ncrucial component in the deep learning toolkit."
                },
                "authors": [
                    {
                        "name": "Grigorios G Chrysos"
                    },
                    {
                        "name": "Yongtao Wu"
                    },
                    {
                        "name": "Razvan Pascanu"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Volkan Cevher"
                    }
                ],
                "author_detail": {
                    "name": "Volkan Cevher"
                },
                "author": "Volkan Cevher",
                "arxiv_comment": "Accepted in IEEE T-PAMI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13112v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.11514v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.11514v3",
                "updated": "2025-04-17T17:18:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    18,
                    24,
                    3,
                    107,
                    0
                ],
                "published": "2024-05-19T10:54:03Z",
                "published_parsed": [
                    2024,
                    5,
                    19,
                    10,
                    54,
                    3,
                    6,
                    140,
                    0
                ],
                "title": "Towards Translating Real-World Code with LLMs: A Study of Translating to\n  Rust",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Translating Real-World Code with LLMs: A Study of Translating to\n  Rust"
                },
                "summary": "Large language models (LLMs) show promise in code translation - the task of\ntranslating code written in one programming language to another language - due\nto their ability to write code in most programming languages. However, LLM's\neffectiveness on translating real-world code remains largely unstudied. In this\nwork, we perform the first substantial study on LLM-based translation to Rust\nby assessing the ability of five state-of-the-art LLMs, GPT4, Claude 3, Claude\n2.1, Gemini Pro, and Mixtral. We conduct our study on code extracted from\nreal-world open source projects. To enable our study, we develop FLOURINE, an\nend-to-end code translation tool that uses differential fuzzing to check if a\nRust translation is I/O equivalent to the original source program, eliminating\nthe need for pre-existing test cases. As part of our investigation, we assess\nboth the LLM's ability to produce an initially successful translation, as well\nas their capacity to fix a previously generated buggy one. If the original and\nthe translated programs are not I/O equivalent, we apply a set of automated\nfeedback strategies, including feedback to the LLM with counterexamples. Our\nresults show that the most successful LLM can translate 47% of our benchmarks,\nand also provides insights into next steps for improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) show promise in code translation - the task of\ntranslating code written in one programming language to another language - due\nto their ability to write code in most programming languages. However, LLM's\neffectiveness on translating real-world code remains largely unstudied. In this\nwork, we perform the first substantial study on LLM-based translation to Rust\nby assessing the ability of five state-of-the-art LLMs, GPT4, Claude 3, Claude\n2.1, Gemini Pro, and Mixtral. We conduct our study on code extracted from\nreal-world open source projects. To enable our study, we develop FLOURINE, an\nend-to-end code translation tool that uses differential fuzzing to check if a\nRust translation is I/O equivalent to the original source program, eliminating\nthe need for pre-existing test cases. As part of our investigation, we assess\nboth the LLM's ability to produce an initially successful translation, as well\nas their capacity to fix a previously generated buggy one. If the original and\nthe translated programs are not I/O equivalent, we apply a set of automated\nfeedback strategies, including feedback to the LLM with counterexamples. Our\nresults show that the most successful LLM can translate 47% of our benchmarks,\nand also provides insights into next steps for improvements."
                },
                "authors": [
                    {
                        "name": "Hasan Ferit Eniser"
                    },
                    {
                        "name": "Hanliang Zhang"
                    },
                    {
                        "name": "Cristina David"
                    },
                    {
                        "name": "Meng Wang"
                    },
                    {
                        "name": "Maria Christakis"
                    },
                    {
                        "name": "Brandon Paulsen"
                    },
                    {
                        "name": "Joey Dodds"
                    },
                    {
                        "name": "Daniel Kroening"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Kroening"
                },
                "author": "Daniel Kroening",
                "arxiv_comment": "12 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.11514v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.11514v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09012v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09012v2",
                "updated": "2025-04-17T17:14:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    14,
                    9,
                    3,
                    107,
                    0
                ],
                "published": "2025-01-15T18:56:22Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    18,
                    56,
                    22,
                    2,
                    15,
                    0
                ],
                "title": "Multimodal LLMs Can Reason about Aesthetics in Zero-Shot",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal LLMs Can Reason about Aesthetics in Zero-Shot"
                },
                "summary": "The rapid progress of generative art has democratized the creation of\nvisually pleasing imagery. However, achieving genuine artistic impact - the\nkind that resonates with viewers on a deeper, more meaningful level - requires\na sophisticated aesthetic sensibility. This sensibility involves a\nmulti-faceted reasoning process extending beyond mere visual appeal, which is\noften overlooked by current computational models. This paper pioneers an\napproach to capture this complex process by investigating how the reasoning\ncapabilities of Multimodal LLMs (MLLMs) can be effectively elicited for\naesthetic judgment. Our analysis reveals a critical challenge: MLLMs exhibit a\ntendency towards hallucinations during aesthetic reasoning, characterized by\nsubjective opinions and unsubstantiated artistic interpretations. We further\ndemonstrate that these limitations can be overcome by employing an\nevidence-based, objective reasoning process, as substantiated by our proposed\nbaseline, ArtCoT. MLLMs prompted by this principle produce multi-faceted and\nin-depth aesthetic reasoning that aligns significantly better with human\njudgment. These findings have direct applications in areas such as AI art\ntutoring and as reward models for generative art. Ultimately, our work paves\nthe way for AI systems that can truly understand, appreciate, and generate\nartworks that align with the sensible human aesthetic standard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid progress of generative art has democratized the creation of\nvisually pleasing imagery. However, achieving genuine artistic impact - the\nkind that resonates with viewers on a deeper, more meaningful level - requires\na sophisticated aesthetic sensibility. This sensibility involves a\nmulti-faceted reasoning process extending beyond mere visual appeal, which is\noften overlooked by current computational models. This paper pioneers an\napproach to capture this complex process by investigating how the reasoning\ncapabilities of Multimodal LLMs (MLLMs) can be effectively elicited for\naesthetic judgment. Our analysis reveals a critical challenge: MLLMs exhibit a\ntendency towards hallucinations during aesthetic reasoning, characterized by\nsubjective opinions and unsubstantiated artistic interpretations. We further\ndemonstrate that these limitations can be overcome by employing an\nevidence-based, objective reasoning process, as substantiated by our proposed\nbaseline, ArtCoT. MLLMs prompted by this principle produce multi-faceted and\nin-depth aesthetic reasoning that aligns significantly better with human\njudgment. These findings have direct applications in areas such as AI art\ntutoring and as reward models for generative art. Ultimately, our work paves\nthe way for AI systems that can truly understand, appreciate, and generate\nartworks that align with the sensible human aesthetic standard."
                },
                "authors": [
                    {
                        "name": "Ruixiang Jiang"
                    },
                    {
                        "name": "Changwen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Changwen Chen"
                },
                "author": "Changwen Chen",
                "arxiv_comment": "WIP, Homepage https://github.com/songrise/MLLM4Art",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09012v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09012v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13099v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13099v1",
                "updated": "2025-04-17T17:08:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    8,
                    11,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T17:08:11Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    8,
                    11,
                    3,
                    107,
                    0
                ],
                "title": "RF-DETR Object Detection vs YOLOv12 : A Study of Transformer-based and\n  CNN-based Architectures for Single-Class and Multi-Class Greenfruit Detection\n  in Complex Orchard Environments Under Label Ambiguity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RF-DETR Object Detection vs YOLOv12 : A Study of Transformer-based and\n  CNN-based Architectures for Single-Class and Multi-Class Greenfruit Detection\n  in Complex Orchard Environments Under Label Ambiguity"
                },
                "summary": "This study conducts a detailed comparison of RF-DETR object detection base\nmodel and YOLOv12 object detection model configurations for detecting\ngreenfruits in a complex orchard environment marked by label ambiguity,\nocclusions, and background blending. A custom dataset was developed featuring\nboth single-class (greenfruit) and multi-class (occluded and non-occluded\ngreenfruits) annotations to assess model performance under dynamic real-world\nconditions. RF-DETR object detection model, utilizing a DINOv2 backbone and\ndeformable attention, excelled in global context modeling, effectively\nidentifying partially occluded or ambiguous greenfruits. In contrast, YOLOv12\nleveraged CNN-based attention for enhanced local feature extraction, optimizing\nit for computational efficiency and edge deployment. RF-DETR achieved the\nhighest mean Average Precision (mAP50) of 0.9464 in single-class detection,\nproving its superior ability to localize greenfruits in cluttered scenes.\nAlthough YOLOv12N recorded the highest mAP@50:95 of 0.7620, RF-DETR\nconsistently outperformed in complex spatial scenarios. For multi-class\ndetection, RF-DETR led with an mAP@50 of 0.8298, showing its capability to\ndifferentiate between occluded and non-occluded fruits, while YOLOv12L scored\nhighest in mAP@50:95 with 0.6622, indicating better classification in detailed\nocclusion contexts. Training dynamics analysis highlighted RF-DETR's swift\nconvergence, particularly in single-class settings where it plateaued within 10\nepochs, demonstrating the efficiency of transformer-based architectures in\nadapting to dynamic visual data. These findings validate RF-DETR's\neffectiveness for precision agricultural applications, with YOLOv12 suited for\nfast-response scenarios. >Index Terms: RF-DETR object detection, YOLOv12,\nYOLOv13, YOLOv14, YOLOv15, YOLOE, YOLO World, YOLO, You Only Look Once,\nRoboflow, Detection Transformers, CNNs",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study conducts a detailed comparison of RF-DETR object detection base\nmodel and YOLOv12 object detection model configurations for detecting\ngreenfruits in a complex orchard environment marked by label ambiguity,\nocclusions, and background blending. A custom dataset was developed featuring\nboth single-class (greenfruit) and multi-class (occluded and non-occluded\ngreenfruits) annotations to assess model performance under dynamic real-world\nconditions. RF-DETR object detection model, utilizing a DINOv2 backbone and\ndeformable attention, excelled in global context modeling, effectively\nidentifying partially occluded or ambiguous greenfruits. In contrast, YOLOv12\nleveraged CNN-based attention for enhanced local feature extraction, optimizing\nit for computational efficiency and edge deployment. RF-DETR achieved the\nhighest mean Average Precision (mAP50) of 0.9464 in single-class detection,\nproving its superior ability to localize greenfruits in cluttered scenes.\nAlthough YOLOv12N recorded the highest mAP@50:95 of 0.7620, RF-DETR\nconsistently outperformed in complex spatial scenarios. For multi-class\ndetection, RF-DETR led with an mAP@50 of 0.8298, showing its capability to\ndifferentiate between occluded and non-occluded fruits, while YOLOv12L scored\nhighest in mAP@50:95 with 0.6622, indicating better classification in detailed\nocclusion contexts. Training dynamics analysis highlighted RF-DETR's swift\nconvergence, particularly in single-class settings where it plateaued within 10\nepochs, demonstrating the efficiency of transformer-based architectures in\nadapting to dynamic visual data. These findings validate RF-DETR's\neffectiveness for precision agricultural applications, with YOLOv12 suited for\nfast-response scenarios. >Index Terms: RF-DETR object detection, YOLOv12,\nYOLOv13, YOLOv14, YOLOv15, YOLOE, YOLO World, YOLO, You Only Look Once,\nRoboflow, Detection Transformers, CNNs"
                },
                "authors": [
                    {
                        "name": "Ranjan Sapkota"
                    },
                    {
                        "name": "Rahul Harsha Cheppally"
                    },
                    {
                        "name": "Ajay Sharda"
                    },
                    {
                        "name": "Manoj Karkee"
                    }
                ],
                "author_detail": {
                    "name": "Manoj Karkee"
                },
                "author": "Manoj Karkee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13099v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13099v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22512v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22512v3",
                "updated": "2025-04-17T17:00:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    0,
                    56,
                    3,
                    107,
                    0
                ],
                "published": "2025-03-28T15:15:56Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    15,
                    15,
                    56,
                    4,
                    87,
                    0
                ],
                "title": "Unlocking LLM Repair Capabilities in Low-Resource Programming Languages\n  Through Cross-Language Translation and Multi-Agent Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking LLM Repair Capabilities in Low-Resource Programming Languages\n  Through Cross-Language Translation and Multi-Agent Refinement"
                },
                "summary": "Recent advances in leveraging LLMs for APR have demonstrated impressive\ncapabilities in fixing software defects. However, current LLM-based approaches\npredominantly focus on mainstream programming languages like Java and Python,\nneglecting less prevalent but emerging languages such as Rust due to expensive\ntraining resources, limited datasets, and insufficient community support. This\nnarrow focus creates a significant gap in repair capabilities across the\nprogramming language spectrum, where the full potential of LLMs for\ncomprehensive multilingual program repair remains largely unexplored. To\naddress this limitation, we introduce a novel cross-language program repair\napproach LANTERN that leverages LLMs' differential proficiency across languages\nthrough a multi-agent iterative repair paradigm. Our technique strategically\ntranslates defective code from languages where LLMs exhibit weaker repair\ncapabilities to languages where they demonstrate stronger performance, without\nrequiring additional training. A key innovation of our approach is an LLM-based\ndecision-making system that dynamically selects optimal target languages based\non bug characteristics and continuously incorporates feedback from previous\nrepair attempts. We evaluate our method on xCodeEval, a comprehensive\nmultilingual benchmark comprising 5,068 bugs across 11 programming languages.\nResults demonstrate significant enhancement in repair effectiveness,\nparticularly for underrepresented languages, with Rust showing a 22.09%\nimprovement in Pass@10 metrics. Our research provides the first empirical\nevidence that cross-language translation significantly expands the repair\ncapabilities of LLMs and effectively bridges the performance gap between\nprogramming languages with different levels of popularity, opening new avenues\nfor truly language-agnostic automated program repair.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in leveraging LLMs for APR have demonstrated impressive\ncapabilities in fixing software defects. However, current LLM-based approaches\npredominantly focus on mainstream programming languages like Java and Python,\nneglecting less prevalent but emerging languages such as Rust due to expensive\ntraining resources, limited datasets, and insufficient community support. This\nnarrow focus creates a significant gap in repair capabilities across the\nprogramming language spectrum, where the full potential of LLMs for\ncomprehensive multilingual program repair remains largely unexplored. To\naddress this limitation, we introduce a novel cross-language program repair\napproach LANTERN that leverages LLMs' differential proficiency across languages\nthrough a multi-agent iterative repair paradigm. Our technique strategically\ntranslates defective code from languages where LLMs exhibit weaker repair\ncapabilities to languages where they demonstrate stronger performance, without\nrequiring additional training. A key innovation of our approach is an LLM-based\ndecision-making system that dynamically selects optimal target languages based\non bug characteristics and continuously incorporates feedback from previous\nrepair attempts. We evaluate our method on xCodeEval, a comprehensive\nmultilingual benchmark comprising 5,068 bugs across 11 programming languages.\nResults demonstrate significant enhancement in repair effectiveness,\nparticularly for underrepresented languages, with Rust showing a 22.09%\nimprovement in Pass@10 metrics. Our research provides the first empirical\nevidence that cross-language translation significantly expands the repair\ncapabilities of LLMs and effectively bridges the performance gap between\nprogramming languages with different levels of popularity, opening new avenues\nfor truly language-agnostic automated program repair."
                },
                "authors": [
                    {
                        "name": "Wenqiang Luo"
                    },
                    {
                        "name": "Jacky Wai Keung"
                    },
                    {
                        "name": "Boyang Yang"
                    },
                    {
                        "name": "Jacques Klein"
                    },
                    {
                        "name": "Tegawende F. Bissyande"
                    },
                    {
                        "name": "Haoye Tian"
                    },
                    {
                        "name": "Bach Le"
                    }
                ],
                "author_detail": {
                    "name": "Bach Le"
                },
                "author": "Bach Le",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22512v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22512v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13092v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13092v1",
                "updated": "2025-04-17T16:59:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    16,
                    59,
                    4,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T16:59:04Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    16,
                    59,
                    4,
                    3,
                    107,
                    0
                ],
                "title": "EventVAD: Training-Free Event-Aware Video Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EventVAD: Training-Free Event-Aware Video Anomaly Detection"
                },
                "summary": "Video Anomaly Detection~(VAD) focuses on identifying anomalies within videos.\nSupervised methods require an amount of in-domain training data and often\nstruggle to generalize to unseen anomalies. In contrast, training-free methods\nleverage the intrinsic world knowledge of large language models (LLMs) to\ndetect anomalies but face challenges in localizing fine-grained visual\ntransitions and diverse events. Therefore, we propose EventVAD, an event-aware\nvideo anomaly detection framework that combines tailored dynamic graph\narchitectures and multimodal LLMs through temporal-event reasoning.\nSpecifically, EventVAD first employs dynamic spatiotemporal graph modeling with\ntime-decay constraints to capture event-aware video features. Then, it performs\nadaptive noise filtering and uses signal ratio thresholding to detect event\nboundaries via unsupervised statistical features. The statistical boundary\ndetection module reduces the complexity of processing long videos for MLLMs and\nimproves their temporal reasoning through event consistency. Finally, it\nutilizes a hierarchical prompting strategy to guide MLLMs in performing\nreasoning before determining final decisions. We conducted extensive\nexperiments on the UCF-Crime and XD-Violence datasets. The results demonstrate\nthat EventVAD with a 7B MLLM achieves state-of-the-art (SOTA) in training-free\nsettings, outperforming strong baselines that use 7B or larger MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Anomaly Detection~(VAD) focuses on identifying anomalies within videos.\nSupervised methods require an amount of in-domain training data and often\nstruggle to generalize to unseen anomalies. In contrast, training-free methods\nleverage the intrinsic world knowledge of large language models (LLMs) to\ndetect anomalies but face challenges in localizing fine-grained visual\ntransitions and diverse events. Therefore, we propose EventVAD, an event-aware\nvideo anomaly detection framework that combines tailored dynamic graph\narchitectures and multimodal LLMs through temporal-event reasoning.\nSpecifically, EventVAD first employs dynamic spatiotemporal graph modeling with\ntime-decay constraints to capture event-aware video features. Then, it performs\nadaptive noise filtering and uses signal ratio thresholding to detect event\nboundaries via unsupervised statistical features. The statistical boundary\ndetection module reduces the complexity of processing long videos for MLLMs and\nimproves their temporal reasoning through event consistency. Finally, it\nutilizes a hierarchical prompting strategy to guide MLLMs in performing\nreasoning before determining final decisions. We conducted extensive\nexperiments on the UCF-Crime and XD-Violence datasets. The results demonstrate\nthat EventVAD with a 7B MLLM achieves state-of-the-art (SOTA) in training-free\nsettings, outperforming strong baselines that use 7B or larger MLLMs."
                },
                "authors": [
                    {
                        "name": "Yihua Shao"
                    },
                    {
                        "name": "Haojin He"
                    },
                    {
                        "name": "Sijie Li"
                    },
                    {
                        "name": "Siyu Chen"
                    },
                    {
                        "name": "Xinwei Long"
                    },
                    {
                        "name": "Fanhu Zeng"
                    },
                    {
                        "name": "Yuxuan Fan"
                    },
                    {
                        "name": "Muyang Zhang"
                    },
                    {
                        "name": "Ziyang Yan"
                    },
                    {
                        "name": "Ao Ma"
                    },
                    {
                        "name": "Xiaochen Wang"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Shuyan Li"
                    }
                ],
                "author_detail": {
                    "name": "Shuyan Li"
                },
                "author": "Shuyan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13092v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09597v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09597v2",
                "updated": "2025-04-17T16:53:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    16,
                    53,
                    17,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-13T14:31:52Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    31,
                    52,
                    6,
                    103,
                    0
                ],
                "title": "Understanding LLM Behaviors via Compression: Data Generation, Knowledge\n  Acquisition and Scaling Laws",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding LLM Behaviors via Compression: Data Generation, Knowledge\n  Acquisition and Scaling Laws"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nnumerous tasks, yet principled explanations for their underlying mechanisms and\nseveral phenomena, such as scaling laws, hallucinations, and related behaviors,\nremain elusive. In this work, we revisit the classical relationship between\ncompression and prediction, grounded in Kolmogorov complexity and Shannon\ninformation theory, to provide deeper insights into LLM behaviors. By\nleveraging the Kolmogorov Structure Function and interpreting LLM compression\nas a two-part coding process, we offer a detailed view of how LLMs acquire and\nstore information across increasing model and data scales -- from pervasive\nsyntactic patterns to progressively rarer knowledge elements. Motivated by this\ntheoretical perspective and natural assumptions inspired by Heap's and Zipf's\nlaws, we introduce a simplified yet representative hierarchical data-generation\nframework called the Syntax-Knowledge model. Under the Bayesian setting, we\nshow that prediction and compression within this model naturally lead to\ndiverse learning and scaling behaviors of LLMs. In particular, our theoretical\nanalysis offers intuitive and principled explanations for both data and model\nscaling laws, the dynamics of knowledge acquisition during training and\nfine-tuning, factual knowledge hallucinations in LLMs. The experimental results\nvalidate our theoretical predictions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nnumerous tasks, yet principled explanations for their underlying mechanisms and\nseveral phenomena, such as scaling laws, hallucinations, and related behaviors,\nremain elusive. In this work, we revisit the classical relationship between\ncompression and prediction, grounded in Kolmogorov complexity and Shannon\ninformation theory, to provide deeper insights into LLM behaviors. By\nleveraging the Kolmogorov Structure Function and interpreting LLM compression\nas a two-part coding process, we offer a detailed view of how LLMs acquire and\nstore information across increasing model and data scales -- from pervasive\nsyntactic patterns to progressively rarer knowledge elements. Motivated by this\ntheoretical perspective and natural assumptions inspired by Heap's and Zipf's\nlaws, we introduce a simplified yet representative hierarchical data-generation\nframework called the Syntax-Knowledge model. Under the Bayesian setting, we\nshow that prediction and compression within this model naturally lead to\ndiverse learning and scaling behaviors of LLMs. In particular, our theoretical\nanalysis offers intuitive and principled explanations for both data and model\nscaling laws, the dynamics of knowledge acquisition during training and\nfine-tuning, factual knowledge hallucinations in LLMs. The experimental results\nvalidate our theoretical predictions."
                },
                "authors": [
                    {
                        "name": "Zhixuan Pan"
                    },
                    {
                        "name": "Shaowen Wang"
                    },
                    {
                        "name": "Jian Li"
                    }
                ],
                "author_detail": {
                    "name": "Jian Li"
                },
                "author": "Jian Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09597v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09597v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13079v1",
                "updated": "2025-04-17T16:46:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    16,
                    46,
                    11,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T16:46:11Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    16,
                    46,
                    11,
                    3,
                    107,
                    0
                ],
                "title": "Retrieval-Augmented Generation with Conflicting Evidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation with Conflicting Evidence"
                },
                "summary": "Large language model (LLM) agents are increasingly employing\nretrieval-augmented generation (RAG) to improve the factuality of their\nresponses. However, in practice, these systems often need to handle ambiguous\nuser queries and potentially conflicting information from multiple sources\nwhile also suppressing inaccurate information from noisy or irrelevant\ndocuments. Prior work has generally studied and addressed these challenges in\nisolation, considering only one aspect at a time, such as handling ambiguity or\nrobustness to noise and misinformation. We instead consider multiple factors\nsimultaneously, proposing (i) RAMDocs (Retrieval with Ambiguity and\nMisinformation in Documents), a new dataset that simulates complex and\nrealistic scenarios for conflicting evidence for a user query, including\nambiguity, misinformation, and noise; and (ii) MADAM-RAG, a multi-agent\napproach in which LLM agents debate over the merits of an answer over multiple\nrounds, allowing an aggregator to collate responses corresponding to\ndisambiguated entities while discarding misinformation and noise, thereby\nhandling diverse sources of conflict jointly. We demonstrate the effectiveness\nof MADAM-RAG using both closed and open-source models on AmbigDocs -- which\nrequires presenting all valid answers for ambiguous queries -- improving over\nstrong RAG baselines by up to 11.40% and on FaithEval -- which requires\nsuppressing misinformation -- where we improve by up to 15.80% (absolute) with\nLlama3.3-70B-Instruct. Furthermore, we find that RAMDocs poses a challenge for\nexisting RAG baselines (Llama3.3-70B-Instruct only obtains 32.60 exact match\nscore). While MADAM-RAG begins to address these conflicting factors, our\nanalysis indicates that a substantial gap remains especially when increasing\nthe level of imbalance in supporting evidence and misinformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) agents are increasingly employing\nretrieval-augmented generation (RAG) to improve the factuality of their\nresponses. However, in practice, these systems often need to handle ambiguous\nuser queries and potentially conflicting information from multiple sources\nwhile also suppressing inaccurate information from noisy or irrelevant\ndocuments. Prior work has generally studied and addressed these challenges in\nisolation, considering only one aspect at a time, such as handling ambiguity or\nrobustness to noise and misinformation. We instead consider multiple factors\nsimultaneously, proposing (i) RAMDocs (Retrieval with Ambiguity and\nMisinformation in Documents), a new dataset that simulates complex and\nrealistic scenarios for conflicting evidence for a user query, including\nambiguity, misinformation, and noise; and (ii) MADAM-RAG, a multi-agent\napproach in which LLM agents debate over the merits of an answer over multiple\nrounds, allowing an aggregator to collate responses corresponding to\ndisambiguated entities while discarding misinformation and noise, thereby\nhandling diverse sources of conflict jointly. We demonstrate the effectiveness\nof MADAM-RAG using both closed and open-source models on AmbigDocs -- which\nrequires presenting all valid answers for ambiguous queries -- improving over\nstrong RAG baselines by up to 11.40% and on FaithEval -- which requires\nsuppressing misinformation -- where we improve by up to 15.80% (absolute) with\nLlama3.3-70B-Instruct. Furthermore, we find that RAMDocs poses a challenge for\nexisting RAG baselines (Llama3.3-70B-Instruct only obtains 32.60 exact match\nscore). While MADAM-RAG begins to address these conflicting factors, our\nanalysis indicates that a substantial gap remains especially when increasing\nthe level of imbalance in supporting evidence and misinformation."
                },
                "authors": [
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Archiki Prasad"
                    },
                    {
                        "name": "Elias Stengel-Eskin"
                    },
                    {
                        "name": "Mohit Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Bansal"
                },
                "author": "Mohit Bansal",
                "arxiv_comment": "Our data and code is available at:\n  https://github.com/HanNight/RAMDocs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11536v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11536v2",
                "updated": "2025-04-17T16:46:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    16,
                    46,
                    7,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-15T18:10:22Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    18,
                    10,
                    22,
                    1,
                    105,
                    0
                ],
                "title": "ReTool: Reinforcement Learning for Strategic Tool Use in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReTool: Reinforcement Learning for Strategic Tool Use in LLMs"
                },
                "summary": "While reasoning models (e.g., DeepSeek R1) trained with reinforcement\nlearning (RL), excel in textual reasoning, they struggle in scenarios requiring\nstructured problem-solving, such as geometric reasoning, concise computation,\nor complex equation solving-areas where computational tools like code\ninterpreters (CI) demonstrate distinct advantages. To bridge this gap, we\npropose ReTool, which enhances long-form reasoning with tool-integrated\nlearning, including two key features: (1) dynamic interleaving of real-time\ncode execution within natural language reasoning processes, and (2) an\nautomated RL paradigm that allows policy rollouts with multi-turn real-time\ncode execution and teaches the model in learning when and how to invoke tools\nbased on outcome feedback. ReTool employs a systematic training framework,\nbeginning with synthetic cold-start data generation to produce code-augmented\nlong-form reasoning traces for fine-tuning base models. Subsequent RL training\nleverages task outcomes as rewards to iteratively refine the model's tool use\nstrategy, enabling autonomous discovery of optimal tool invocation patterns\nwithout human priors. Experiments on the challenging MATH Olympiad benchmark\nAIME demonstrate ReTool's superiority: Our 32B model achieves 67% accuracy with\n400 training steps, outperforming text-based RL baseline (40% accuracy, 1080\nsteps) in efficiency and performance. Remarkably, ReTool-32B attains 72.5%\naccuracy in extended settings, surpassing OpenAI's o1-preview by 27.9%. Further\nanalysis reveals emergent behaviors such as code self-correction, signaling an\n''aha moment'' in which the model autonomously masters adaptive tool use. These\nfindings highlight the promise of outcome-driven tool integration for advancing\ncomplex mathematical reasoning and offer new insights into hybrid\nneuro-symbolic systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While reasoning models (e.g., DeepSeek R1) trained with reinforcement\nlearning (RL), excel in textual reasoning, they struggle in scenarios requiring\nstructured problem-solving, such as geometric reasoning, concise computation,\nor complex equation solving-areas where computational tools like code\ninterpreters (CI) demonstrate distinct advantages. To bridge this gap, we\npropose ReTool, which enhances long-form reasoning with tool-integrated\nlearning, including two key features: (1) dynamic interleaving of real-time\ncode execution within natural language reasoning processes, and (2) an\nautomated RL paradigm that allows policy rollouts with multi-turn real-time\ncode execution and teaches the model in learning when and how to invoke tools\nbased on outcome feedback. ReTool employs a systematic training framework,\nbeginning with synthetic cold-start data generation to produce code-augmented\nlong-form reasoning traces for fine-tuning base models. Subsequent RL training\nleverages task outcomes as rewards to iteratively refine the model's tool use\nstrategy, enabling autonomous discovery of optimal tool invocation patterns\nwithout human priors. Experiments on the challenging MATH Olympiad benchmark\nAIME demonstrate ReTool's superiority: Our 32B model achieves 67% accuracy with\n400 training steps, outperforming text-based RL baseline (40% accuracy, 1080\nsteps) in efficiency and performance. Remarkably, ReTool-32B attains 72.5%\naccuracy in extended settings, surpassing OpenAI's o1-preview by 27.9%. Further\nanalysis reveals emergent behaviors such as code self-correction, signaling an\n''aha moment'' in which the model autonomously masters adaptive tool use. These\nfindings highlight the promise of outcome-driven tool integration for advancing\ncomplex mathematical reasoning and offer new insights into hybrid\nneuro-symbolic systems."
                },
                "authors": [
                    {
                        "name": "Jiazhan Feng"
                    },
                    {
                        "name": "Shijue Huang"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Yujia Qin"
                    },
                    {
                        "name": "Baoquan Zhong"
                    },
                    {
                        "name": "Chengquan Jiang"
                    },
                    {
                        "name": "Jinxin Chi"
                    },
                    {
                        "name": "Wanjun Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Wanjun Zhong"
                },
                "author": "Wanjun Zhong",
                "arxiv_comment": "fix typos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11536v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11536v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13074v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13074v2",
                "updated": "2025-04-18T09:46:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    9,
                    46,
                    32,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-17T16:37:27Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    16,
                    37,
                    27,
                    3,
                    107,
                    0
                ],
                "title": "SkyReels-V2: Infinite-length Film Generative Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SkyReels-V2: Infinite-length Film Generative Model"
                },
                "summary": "Recent advances in video generation have been driven by diffusion models and\nautoregressive frameworks, yet critical challenges persist in harmonizing\nprompt adherence, visual quality, motion dynamics, and duration: compromises in\nmotion dynamics to enhance temporal visual quality, constrained video duration\n(5-10 seconds) to prioritize resolution, and inadequate shot-aware generation\nstemming from general-purpose MLLMs' inability to interpret cinematic grammar,\nsuch as shot composition, actor expressions, and camera motions. These\nintertwined limitations hinder realistic long-form synthesis and professional\nfilm-style generation. To address these limitations, we propose SkyReels-V2, an\nInfinite-length Film Generative Model, that synergizes Multi-modal Large\nLanguage Model (MLLM), Multi-stage Pretraining, Reinforcement Learning, and\nDiffusion Forcing Framework. Firstly, we design a comprehensive structural\nrepresentation of video that combines the general descriptions by the\nMulti-modal LLM and the detailed shot language by sub-expert models. Aided with\nhuman annotation, we then train a unified Video Captioner, named\nSkyCaptioner-V1, to efficiently label the video data. Secondly, we establish\nprogressive-resolution pretraining for the fundamental video generation,\nfollowed by a four-stage post-training enhancement: Initial concept-balanced\nSupervised Fine-Tuning (SFT) improves baseline quality; Motion-specific\nReinforcement Learning (RL) training with human-annotated and synthetic\ndistortion data addresses dynamic artifacts; Our diffusion forcing framework\nwith non-decreasing noise schedules enables long-video synthesis in an\nefficient search space; Final high-quality SFT refines visual fidelity. All the\ncode and models are available at https://github.com/SkyworkAI/SkyReels-V2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in video generation have been driven by diffusion models and\nautoregressive frameworks, yet critical challenges persist in harmonizing\nprompt adherence, visual quality, motion dynamics, and duration: compromises in\nmotion dynamics to enhance temporal visual quality, constrained video duration\n(5-10 seconds) to prioritize resolution, and inadequate shot-aware generation\nstemming from general-purpose MLLMs' inability to interpret cinematic grammar,\nsuch as shot composition, actor expressions, and camera motions. These\nintertwined limitations hinder realistic long-form synthesis and professional\nfilm-style generation. To address these limitations, we propose SkyReels-V2, an\nInfinite-length Film Generative Model, that synergizes Multi-modal Large\nLanguage Model (MLLM), Multi-stage Pretraining, Reinforcement Learning, and\nDiffusion Forcing Framework. Firstly, we design a comprehensive structural\nrepresentation of video that combines the general descriptions by the\nMulti-modal LLM and the detailed shot language by sub-expert models. Aided with\nhuman annotation, we then train a unified Video Captioner, named\nSkyCaptioner-V1, to efficiently label the video data. Secondly, we establish\nprogressive-resolution pretraining for the fundamental video generation,\nfollowed by a four-stage post-training enhancement: Initial concept-balanced\nSupervised Fine-Tuning (SFT) improves baseline quality; Motion-specific\nReinforcement Learning (RL) training with human-annotated and synthetic\ndistortion data addresses dynamic artifacts; Our diffusion forcing framework\nwith non-decreasing noise schedules enables long-video synthesis in an\nefficient search space; Final high-quality SFT refines visual fidelity. All the\ncode and models are available at https://github.com/SkyworkAI/SkyReels-V2."
                },
                "authors": [
                    {
                        "name": "Guibin Chen"
                    },
                    {
                        "name": "Dixuan Lin"
                    },
                    {
                        "name": "Jiangping Yang"
                    },
                    {
                        "name": "Chunze Lin"
                    },
                    {
                        "name": "Juncheng Zhu"
                    },
                    {
                        "name": "Mingyuan Fan"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Sheng Chen"
                    },
                    {
                        "name": "Zheng Chen"
                    },
                    {
                        "name": "Chengchen Ma"
                    },
                    {
                        "name": "Weiming Xiong"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Nuo Pang"
                    },
                    {
                        "name": "Kang Kang"
                    },
                    {
                        "name": "Zhiheng Xu"
                    },
                    {
                        "name": "Yuzhe Jin"
                    },
                    {
                        "name": "Yupeng Liang"
                    },
                    {
                        "name": "Yubing Song"
                    },
                    {
                        "name": "Peng Zhao"
                    },
                    {
                        "name": "Boyuan Xu"
                    },
                    {
                        "name": "Di Qiu"
                    },
                    {
                        "name": "Debang Li"
                    },
                    {
                        "name": "Zhengcong Fei"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Yahui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yahui Zhou"
                },
                "author": "Yahui Zhou",
                "arxiv_comment": "31 pages,10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13074v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13074v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13068v1",
                "updated": "2025-04-17T16:29:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    16,
                    29,
                    8,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T16:29:08Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    16,
                    29,
                    8,
                    3,
                    107,
                    0
                ],
                "title": "Accuracy is Not Agreement: Expert-Aligned Evaluation of Crash Narrative\n  Classification Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accuracy is Not Agreement: Expert-Aligned Evaluation of Crash Narrative\n  Classification Models"
                },
                "summary": "This study explores the relationship between deep learning (DL) model\naccuracy and expert agreement in the classification of crash narratives. We\nevaluate five DL models -- including BERT variants, the Universal Sentence\nEncoder (USE), and a zero-shot classifier -- against expert-labeled data and\nnarrative text. The analysis is further extended to four large language models\n(LLMs): GPT-4, LLaMA 3, Qwen, and Claude. Our results reveal a counterintuitive\ntrend: models with higher technical accuracy often exhibit lower agreement with\ndomain experts, whereas LLMs demonstrate greater expert alignment despite\nrelatively lower accuracy scores. To quantify and interpret model-expert\nagreement, we employ Cohen's Kappa, Principal Component Analysis (PCA), and\nSHAP-based explainability techniques. Findings indicate that expert-aligned\nmodels tend to rely more on contextual and temporal language cues, rather than\nlocation-specific keywords. These results underscore that accuracy alone is\ninsufficient for evaluating models in safety-critical NLP applications. We\nadvocate for incorporating expert agreement as a complementary metric in model\nevaluation frameworks and highlight the promise of LLMs as interpretable,\nscalable tools for crash analysis pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the relationship between deep learning (DL) model\naccuracy and expert agreement in the classification of crash narratives. We\nevaluate five DL models -- including BERT variants, the Universal Sentence\nEncoder (USE), and a zero-shot classifier -- against expert-labeled data and\nnarrative text. The analysis is further extended to four large language models\n(LLMs): GPT-4, LLaMA 3, Qwen, and Claude. Our results reveal a counterintuitive\ntrend: models with higher technical accuracy often exhibit lower agreement with\ndomain experts, whereas LLMs demonstrate greater expert alignment despite\nrelatively lower accuracy scores. To quantify and interpret model-expert\nagreement, we employ Cohen's Kappa, Principal Component Analysis (PCA), and\nSHAP-based explainability techniques. Findings indicate that expert-aligned\nmodels tend to rely more on contextual and temporal language cues, rather than\nlocation-specific keywords. These results underscore that accuracy alone is\ninsufficient for evaluating models in safety-critical NLP applications. We\nadvocate for incorporating expert agreement as a complementary metric in model\nevaluation frameworks and highlight the promise of LLMs as interpretable,\nscalable tools for crash analysis pipelines."
                },
                "authors": [
                    {
                        "name": "Sudesh Ramesh Bhagat"
                    },
                    {
                        "name": "Ibne Farabi Shihab"
                    },
                    {
                        "name": "Anuj Sharma"
                    }
                ],
                "author_detail": {
                    "name": "Anuj Sharma"
                },
                "author": "Anuj Sharma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11543v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11543v2",
                "updated": "2025-04-17T16:28:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    16,
                    28,
                    46,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-15T18:22:55Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    18,
                    22,
                    55,
                    1,
                    105,
                    0
                ],
                "title": "REAL: Benchmarking Autonomous Agents on Deterministic Simulations of\n  Real Websites",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REAL: Benchmarking Autonomous Agents on Deterministic Simulations of\n  Real Websites"
                },
                "summary": "We introduce REAL, a benchmark and framework for multi-turn agent evaluations\non deterministic simulations of real-world websites. REAL comprises\nhigh-fidelity, deterministic replicas of 11 widely-used websites across domains\nsuch as e-commerce, travel, communication, and professional networking. We also\nrelease a benchmark consisting of 112 practical tasks that mirror everyday\ncomplex user interactions requiring both accurate information retrieval and\nstate-changing actions. All interactions occur within this fully controlled\nsetting, eliminating safety risks and enabling robust, reproducible evaluation\nof agent capability and reliability. Our novel evaluation framework combines\nprogrammatic checks of website state for action-based tasks with rubric-guided\nLLM-based judgments for information retrieval. The framework supports both\nopen-source and proprietary agent systems through a flexible evaluation harness\nthat accommodates black-box commands within browser environments, allowing\nresearch labs to test agentic systems without modification. Our empirical\nresults show that frontier language models achieve at most a 41% success rate\non REAL, highlighting critical gaps in autonomous web navigation and task\ncompletion capabilities. Our framework supports easy integration of new tasks,\nreproducible evaluation, and scalable post-training data generation, marking a\nsignificant step forward in evaluating and advancing agent capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce REAL, a benchmark and framework for multi-turn agent evaluations\non deterministic simulations of real-world websites. REAL comprises\nhigh-fidelity, deterministic replicas of 11 widely-used websites across domains\nsuch as e-commerce, travel, communication, and professional networking. We also\nrelease a benchmark consisting of 112 practical tasks that mirror everyday\ncomplex user interactions requiring both accurate information retrieval and\nstate-changing actions. All interactions occur within this fully controlled\nsetting, eliminating safety risks and enabling robust, reproducible evaluation\nof agent capability and reliability. Our novel evaluation framework combines\nprogrammatic checks of website state for action-based tasks with rubric-guided\nLLM-based judgments for information retrieval. The framework supports both\nopen-source and proprietary agent systems through a flexible evaluation harness\nthat accommodates black-box commands within browser environments, allowing\nresearch labs to test agentic systems without modification. Our empirical\nresults show that frontier language models achieve at most a 41% success rate\non REAL, highlighting critical gaps in autonomous web navigation and task\ncompletion capabilities. Our framework supports easy integration of new tasks,\nreproducible evaluation, and scalable post-training data generation, marking a\nsignificant step forward in evaluating and advancing agent capabilities."
                },
                "authors": [
                    {
                        "name": "Divyansh Garg"
                    },
                    {
                        "name": "Shaun VanWeelden"
                    },
                    {
                        "name": "Diego Caples"
                    },
                    {
                        "name": "Andis Draguns"
                    },
                    {
                        "name": "Nikil Ravi"
                    },
                    {
                        "name": "Pranav Putta"
                    },
                    {
                        "name": "Naman Garg"
                    },
                    {
                        "name": "Tomas Abraham"
                    },
                    {
                        "name": "Michael Lara"
                    },
                    {
                        "name": "Federico Lopez"
                    },
                    {
                        "name": "James Liu"
                    },
                    {
                        "name": "Atharva Gundawar"
                    },
                    {
                        "name": "Prannay Hebbar"
                    },
                    {
                        "name": "Youngchul Joo"
                    },
                    {
                        "name": "Jindong Gu"
                    },
                    {
                        "name": "Charles London"
                    },
                    {
                        "name": "Christian Schroeder de Witt"
                    },
                    {
                        "name": "Sumeet Motwani"
                    }
                ],
                "author_detail": {
                    "name": "Sumeet Motwani"
                },
                "author": "Sumeet Motwani",
                "arxiv_comment": "The websites, framework, and leaderboard are available at\n  https://realevals.xyz and https://github.com/agi-inc/REAL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11543v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11543v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13052v1",
                "updated": "2025-04-17T16:09:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    16,
                    9,
                    12,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T16:09:12Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    16,
                    9,
                    12,
                    3,
                    107,
                    0
                ],
                "title": "GraphAttack: Exploiting Representational Blindspots in LLM Safety\n  Mechanisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphAttack: Exploiting Representational Blindspots in LLM Safety\n  Mechanisms"
                },
                "summary": "Large Language Models (LLMs) have been equipped with safety mechanisms to\nprevent harmful outputs, but these guardrails can often be bypassed through\n\"jailbreak\" prompts. This paper introduces a novel graph-based approach to\nsystematically generate jailbreak prompts through semantic transformations. We\nrepresent malicious prompts as nodes in a graph structure with edges denoting\ndifferent transformations, leveraging Abstract Meaning Representation (AMR) and\nResource Description Framework (RDF) to parse user goals into semantic\ncomponents that can be manipulated to evade safety filters. We demonstrate a\nparticularly effective exploitation vector by instructing LLMs to generate code\nthat realizes the intent described in these semantic graphs, achieving success\nrates of up to 87% against leading commercial LLMs. Our analysis reveals that\ncontextual framing and abstraction are particularly effective at circumventing\nsafety measures, highlighting critical gaps in current safety alignment\ntechniques that focus primarily on surface-level patterns. These findings\nprovide insights for developing more robust safeguards against structured\nsemantic attacks. Our research contributes both a theoretical framework and\npractical methodology for systematically stress-testing LLM safety mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been equipped with safety mechanisms to\nprevent harmful outputs, but these guardrails can often be bypassed through\n\"jailbreak\" prompts. This paper introduces a novel graph-based approach to\nsystematically generate jailbreak prompts through semantic transformations. We\nrepresent malicious prompts as nodes in a graph structure with edges denoting\ndifferent transformations, leveraging Abstract Meaning Representation (AMR) and\nResource Description Framework (RDF) to parse user goals into semantic\ncomponents that can be manipulated to evade safety filters. We demonstrate a\nparticularly effective exploitation vector by instructing LLMs to generate code\nthat realizes the intent described in these semantic graphs, achieving success\nrates of up to 87% against leading commercial LLMs. Our analysis reveals that\ncontextual framing and abstraction are particularly effective at circumventing\nsafety measures, highlighting critical gaps in current safety alignment\ntechniques that focus primarily on surface-level patterns. These findings\nprovide insights for developing more robust safeguards against structured\nsemantic attacks. Our research contributes both a theoretical framework and\npractical methodology for systematically stress-testing LLM safety mechanisms."
                },
                "authors": [
                    {
                        "name": "Sinan He"
                    },
                    {
                        "name": "An Wang"
                    }
                ],
                "author_detail": {
                    "name": "An Wang"
                },
                "author": "An Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20973v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20973v2",
                "updated": "2025-04-17T16:07:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    16,
                    7,
                    44,
                    3,
                    107,
                    0
                ],
                "published": "2025-02-28T11:37:52Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    11,
                    37,
                    52,
                    4,
                    59,
                    0
                ],
                "title": "Arabizi vs LLMs: Can the Genie Understand the Language of Aladdin?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arabizi vs LLMs: Can the Genie Understand the Language of Aladdin?"
                },
                "summary": "In this era of rapid technological advancements, communication continues to\nevolve as new linguistic phenomena emerge. Among these is Arabizi, a hybrid\nform of Arabic that incorporates Latin characters and numbers to represent the\nspoken dialects of Arab communities. Arabizi is widely used on social media and\nallows people to communicate in an informal and dynamic way, but it poses\nsignificant challenges for machine translation due to its lack of formal\nstructure and deeply embedded cultural nuances. This case study arises from a\ngrowing need to translate Arabizi for gisting purposes. It evaluates the\ncapacity of different LLMs to decode and translate Arabizi, focusing on\nmultiple Arabic dialects that have rarely been studied up until now. Using a\ncombination of human evaluators and automatic metrics, this research project\ninvestigates the model's performance in translating Arabizi into both Modern\nStandard Arabic and English. Key questions explored include which dialects are\ntranslated most effectively and whether translations into English surpass those\ninto Arabic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this era of rapid technological advancements, communication continues to\nevolve as new linguistic phenomena emerge. Among these is Arabizi, a hybrid\nform of Arabic that incorporates Latin characters and numbers to represent the\nspoken dialects of Arab communities. Arabizi is widely used on social media and\nallows people to communicate in an informal and dynamic way, but it poses\nsignificant challenges for machine translation due to its lack of formal\nstructure and deeply embedded cultural nuances. This case study arises from a\ngrowing need to translate Arabizi for gisting purposes. It evaluates the\ncapacity of different LLMs to decode and translate Arabizi, focusing on\nmultiple Arabic dialects that have rarely been studied up until now. Using a\ncombination of human evaluators and automatic metrics, this research project\ninvestigates the model's performance in translating Arabizi into both Modern\nStandard Arabic and English. Key questions explored include which dialects are\ntranslated most effectively and whether translations into English surpass those\ninto Arabic."
                },
                "authors": [
                    {
                        "name": "Perla Al Almaoui"
                    },
                    {
                        "name": "Pierrette Bouillon"
                    },
                    {
                        "name": "Simon Hengchen"
                    }
                ],
                "author_detail": {
                    "name": "Simon Hengchen"
                },
                "author": "Simon Hengchen",
                "arxiv_comment": "Accepted to MT Summit 2025 (Track: Implementation and Case Studies)\n  https://mtsummit2025.unige.ch/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20973v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20973v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10445v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10445v4",
                "updated": "2025-04-17T16:05:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    16,
                    5,
                    20,
                    3,
                    107,
                    0
                ],
                "published": "2024-04-16T10:31:06Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    10,
                    31,
                    6,
                    1,
                    107,
                    0
                ],
                "title": "SparseDM: Toward Sparse Efficient Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseDM: Toward Sparse Efficient Diffusion Models"
                },
                "summary": "Diffusion models represent a powerful family of generative models widely used\nfor image and video generation. However, the time-consuming deployment, long\ninference time, and requirements on large memory hinder their applications on\nresource constrained devices. In this paper, we propose a method based on the\nimproved Straight-Through Estimator to improve the deployment efficiency of\ndiffusion models. Specifically, we add sparse masks to the Convolution and\nLinear layers in a pre-trained diffusion model, then transfer learn the sparse\nmodel during the fine-tuning stage and turn on the sparse masks during\ninference. Experimental results on a Transformer and UNet-based diffusion\nmodels demonstrate that our method reduces MACs by 50% while maintaining FID.\nSparse models are accelerated by approximately 1.2x on the GPU. Under other\nMACs conditions, the FID is also lower than 1 compared to other methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models represent a powerful family of generative models widely used\nfor image and video generation. However, the time-consuming deployment, long\ninference time, and requirements on large memory hinder their applications on\nresource constrained devices. In this paper, we propose a method based on the\nimproved Straight-Through Estimator to improve the deployment efficiency of\ndiffusion models. Specifically, we add sparse masks to the Convolution and\nLinear layers in a pre-trained diffusion model, then transfer learn the sparse\nmodel during the fine-tuning stage and turn on the sparse masks during\ninference. Experimental results on a Transformer and UNet-based diffusion\nmodels demonstrate that our method reduces MACs by 50% while maintaining FID.\nSparse models are accelerated by approximately 1.2x on the GPU. Under other\nMACs conditions, the FID is also lower than 1 compared to other methods."
                },
                "authors": [
                    {
                        "name": "Kafeng Wang"
                    },
                    {
                        "name": "Jianfei Chen"
                    },
                    {
                        "name": "He Li"
                    },
                    {
                        "name": "Zhenpeng Mi"
                    },
                    {
                        "name": "Jun Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhu"
                },
                "author": "Jun Zhu",
                "arxiv_comment": "This paper has been accepted by ICME 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10445v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10445v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00638v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00638v2",
                "updated": "2025-04-17T16:01:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    16,
                    1,
                    23,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-01T10:48:00Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    10,
                    48,
                    0,
                    1,
                    91,
                    0
                ],
                "title": "Impact of Data Duplication on Deep Neural Network-Based Image\n  Classifiers: Robust vs. Standard Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impact of Data Duplication on Deep Neural Network-Based Image\n  Classifiers: Robust vs. Standard Models"
                },
                "summary": "The accuracy and robustness of machine learning models against adversarial\nattacks are significantly influenced by factors such as training data quality,\nmodel architecture, the training process, and the deployment environment. In\nrecent years, duplicated data in training sets, especially in language models,\nhas attracted considerable attention. It has been shown that deduplication\nenhances both training performance and model accuracy in language models. While\nthe importance of data quality in training image classifier Deep Neural\nNetworks (DNNs) is widely recognized, the impact of duplicated images in the\ntraining set on model generalization and performance has received little\nattention.\n  In this paper, we address this gap and provide a comprehensive study on the\neffect of duplicates in image classification. Our analysis indicates that the\npresence of duplicated images in the training set not only negatively affects\nthe efficiency of model training but also may result in lower accuracy of the\nimage classifier. This negative impact of duplication on accuracy is\nparticularly evident when duplicated data is non-uniform across classes or when\nduplication, whether uniform or non-uniform, occurs in the training set of an\nadversarially trained model. Even when duplicated samples are selected in a\nuniform way, increasing the amount of duplication does not lead to a\nsignificant improvement in accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The accuracy and robustness of machine learning models against adversarial\nattacks are significantly influenced by factors such as training data quality,\nmodel architecture, the training process, and the deployment environment. In\nrecent years, duplicated data in training sets, especially in language models,\nhas attracted considerable attention. It has been shown that deduplication\nenhances both training performance and model accuracy in language models. While\nthe importance of data quality in training image classifier Deep Neural\nNetworks (DNNs) is widely recognized, the impact of duplicated images in the\ntraining set on model generalization and performance has received little\nattention.\n  In this paper, we address this gap and provide a comprehensive study on the\neffect of duplicates in image classification. Our analysis indicates that the\npresence of duplicated images in the training set not only negatively affects\nthe efficiency of model training but also may result in lower accuracy of the\nimage classifier. This negative impact of duplication on accuracy is\nparticularly evident when duplicated data is non-uniform across classes or when\nduplication, whether uniform or non-uniform, occurs in the training set of an\nadversarially trained model. Even when duplicated samples are selected in a\nuniform way, increasing the amount of duplication does not lead to a\nsignificant improvement in accuracy."
                },
                "authors": [
                    {
                        "name": "Alireza Aghabagherloo"
                    },
                    {
                        "name": "Aydin Abadi"
                    },
                    {
                        "name": "Sumanta Sarkar"
                    },
                    {
                        "name": "Vishnu Asutosh Dasu"
                    },
                    {
                        "name": "Bart Preneel"
                    }
                ],
                "author_detail": {
                    "name": "Bart Preneel"
                },
                "author": "Bart Preneel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00638v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00638v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13038v1",
                "updated": "2025-04-17T15:51:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    15,
                    51,
                    59,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T15:51:59Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    15,
                    51,
                    59,
                    3,
                    107,
                    0
                ],
                "title": "How Large Language Models Are Changing MOOC Essay Answers: A Comparison\n  of Pre- and Post-LLM Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Large Language Models Are Changing MOOC Essay Answers: A Comparison\n  of Pre- and Post-LLM Responses"
                },
                "summary": "The release of ChatGPT in late 2022 caused a flurry of activity and concern\nin the academic and educational communities. Some see the tool's ability to\ngenerate human-like text that passes at least cursory inspections for factual\naccuracy ``often enough'' a golden age of information retrieval and\ncomputer-assisted learning. Some, on the other hand, worry the tool may lead to\nunprecedented levels of academic dishonesty and cheating. In this work, we\nquantify some of the effects of the emergence of Large Language Models (LLMs)\non online education by analyzing a multi-year dataset of student essay\nresponses from a free university-level MOOC on AI ethics. Our dataset includes\nessays submitted both before and after ChatGPT's release. We find that the\nlaunch of ChatGPT coincided with significant changes in both the length and\nstyle of student essays, mirroring observations in other contexts such as\nacademic publishing. We also observe -- as expected based on related public\ndiscourse -- changes in prevalence of key content words related to AI and LLMs,\nbut not necessarily the general themes or topics discussed in the student\nessays as identified through (dynamic) topic modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The release of ChatGPT in late 2022 caused a flurry of activity and concern\nin the academic and educational communities. Some see the tool's ability to\ngenerate human-like text that passes at least cursory inspections for factual\naccuracy ``often enough'' a golden age of information retrieval and\ncomputer-assisted learning. Some, on the other hand, worry the tool may lead to\nunprecedented levels of academic dishonesty and cheating. In this work, we\nquantify some of the effects of the emergence of Large Language Models (LLMs)\non online education by analyzing a multi-year dataset of student essay\nresponses from a free university-level MOOC on AI ethics. Our dataset includes\nessays submitted both before and after ChatGPT's release. We find that the\nlaunch of ChatGPT coincided with significant changes in both the length and\nstyle of student essays, mirroring observations in other contexts such as\nacademic publishing. We also observe -- as expected based on related public\ndiscourse -- changes in prevalence of key content words related to AI and LLMs,\nbut not necessarily the general themes or topics discussed in the student\nessays as identified through (dynamic) topic modeling."
                },
                "authors": [
                    {
                        "name": "Leo Leppänen"
                    },
                    {
                        "name": "Lili Aunimo"
                    },
                    {
                        "name": "Arto Hellas"
                    },
                    {
                        "name": "Jukka K. Nurminen"
                    },
                    {
                        "name": "Linda Mannila"
                    }
                ],
                "author_detail": {
                    "name": "Linda Mannila"
                },
                "author": "Linda Mannila",
                "arxiv_comment": "10 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.3.1; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13032v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13032v1",
                "updated": "2025-04-17T15:41:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    15,
                    41,
                    39,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T15:41:39Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    15,
                    41,
                    39,
                    3,
                    107,
                    0
                ],
                "title": "InstructRAG: Leveraging Retrieval-Augmented Generation on Instruction\n  Graphs for LLM-Based Task Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstructRAG: Leveraging Retrieval-Augmented Generation on Instruction\n  Graphs for LLM-Based Task Planning"
                },
                "summary": "Recent advancements in large language models (LLMs) have enabled their use as\nagents for planning complex tasks. Existing methods typically rely on a\nthought-action-observation (TAO) process to enhance LLM performance, but these\napproaches are often constrained by the LLMs' limited knowledge of complex\ntasks. Retrieval-augmented generation (RAG) offers new opportunities by\nleveraging external databases to ground generation in retrieved information. In\nthis paper, we identify two key challenges (enlargability and transferability)\nin applying RAG to task planning. We propose InstructRAG, a novel solution\nwithin a multi-agent meta-reinforcement learning framework, to address these\nchallenges. InstructRAG includes a graph to organize past instruction paths\n(sequences of correct actions), an RL-Agent with Reinforcement Learning to\nexpand graph coverage for enlargability, and an ML-Agent with Meta-Learning to\nimprove task generalization for transferability. The two agents are trained\nend-to-end to optimize overall planning performance. Our experiments on four\nwidely used task planning datasets demonstrate that InstructRAG significantly\nenhances performance and adapts efficiently to new tasks, achieving up to a\n19.2% improvement over the best existing approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have enabled their use as\nagents for planning complex tasks. Existing methods typically rely on a\nthought-action-observation (TAO) process to enhance LLM performance, but these\napproaches are often constrained by the LLMs' limited knowledge of complex\ntasks. Retrieval-augmented generation (RAG) offers new opportunities by\nleveraging external databases to ground generation in retrieved information. In\nthis paper, we identify two key challenges (enlargability and transferability)\nin applying RAG to task planning. We propose InstructRAG, a novel solution\nwithin a multi-agent meta-reinforcement learning framework, to address these\nchallenges. InstructRAG includes a graph to organize past instruction paths\n(sequences of correct actions), an RL-Agent with Reinforcement Learning to\nexpand graph coverage for enlargability, and an ML-Agent with Meta-Learning to\nimprove task generalization for transferability. The two agents are trained\nend-to-end to optimize overall planning performance. Our experiments on four\nwidely used task planning datasets demonstrate that InstructRAG significantly\nenhances performance and adapts efficiently to new tasks, achieving up to a\n19.2% improvement over the best existing approach."
                },
                "authors": [
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Shu Xian Teo"
                    },
                    {
                        "name": "Jun Jie Chew"
                    },
                    {
                        "name": "Wei Shi"
                    }
                ],
                "author_detail": {
                    "name": "Wei Shi"
                },
                "author": "Wei Shi",
                "arxiv_comment": "This paper has been accepted by SIGIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13032v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13032v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13023v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13023v1",
                "updated": "2025-04-17T15:33:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    15,
                    33,
                    17,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T15:33:17Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    15,
                    33,
                    17,
                    3,
                    107,
                    0
                ],
                "title": "ChatEXAONEPath: An Expert-level Multimodal Large Language Model for\n  Histopathology Using Whole Slide Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatEXAONEPath: An Expert-level Multimodal Large Language Model for\n  Histopathology Using Whole Slide Images"
                },
                "summary": "Recent studies have made significant progress in developing large language\nmodels (LLMs) in the medical domain, which can answer expert-level questions\nand demonstrate the potential to assist clinicians in real-world clinical\nscenarios. Studies have also witnessed the importance of integrating various\nmodalities with the existing LLMs for a better understanding of complex\nclinical contexts, which are innately multi-faceted by nature. Although studies\nhave demonstrated the ability of multimodal LLMs in histopathology to answer\nquestions from given images, they lack in understanding of thorough clinical\ncontext due to the patch-level data with limited information from public\ndatasets. Thus, developing WSI-level MLLMs is significant in terms of the\nscalability and applicability of MLLMs in histopathology. In this study, we\nintroduce an expert-level MLLM for histopathology using WSIs, dubbed as\nChatEXAONEPath. We present a retrieval-based data generation pipeline using\n10,094 pairs of WSIs and histopathology reports from The Cancer Genome Atlas\n(TCGA). We also showcase an AI-based evaluation protocol for a comprehensive\nunderstanding of the medical context from given multimodal information and\nevaluate generated answers compared to the original histopathology reports. We\ndemonstrate the ability of diagnosing the given histopathology images using\nChatEXAONEPath with the acceptance rate of 62.9% from 1,134 pairs of WSIs and\nreports. Our proposed model can understand pan-cancer WSIs and clinical context\nfrom various cancer types. We argue that our proposed model has the potential\nto assist clinicians by comprehensively understanding complex morphology of\nWSIs for cancer diagnosis through the integration of multiple modalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have made significant progress in developing large language\nmodels (LLMs) in the medical domain, which can answer expert-level questions\nand demonstrate the potential to assist clinicians in real-world clinical\nscenarios. Studies have also witnessed the importance of integrating various\nmodalities with the existing LLMs for a better understanding of complex\nclinical contexts, which are innately multi-faceted by nature. Although studies\nhave demonstrated the ability of multimodal LLMs in histopathology to answer\nquestions from given images, they lack in understanding of thorough clinical\ncontext due to the patch-level data with limited information from public\ndatasets. Thus, developing WSI-level MLLMs is significant in terms of the\nscalability and applicability of MLLMs in histopathology. In this study, we\nintroduce an expert-level MLLM for histopathology using WSIs, dubbed as\nChatEXAONEPath. We present a retrieval-based data generation pipeline using\n10,094 pairs of WSIs and histopathology reports from The Cancer Genome Atlas\n(TCGA). We also showcase an AI-based evaluation protocol for a comprehensive\nunderstanding of the medical context from given multimodal information and\nevaluate generated answers compared to the original histopathology reports. We\ndemonstrate the ability of diagnosing the given histopathology images using\nChatEXAONEPath with the acceptance rate of 62.9% from 1,134 pairs of WSIs and\nreports. Our proposed model can understand pan-cancer WSIs and clinical context\nfrom various cancer types. We argue that our proposed model has the potential\nto assist clinicians by comprehensively understanding complex morphology of\nWSIs for cancer diagnosis through the integration of multiple modalities."
                },
                "authors": [
                    {
                        "name": "Sangwook Kim"
                    },
                    {
                        "name": "Soonyoung Lee"
                    },
                    {
                        "name": "Jongseong Jang"
                    }
                ],
                "author_detail": {
                    "name": "Jongseong Jang"
                },
                "author": "Jongseong Jang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13023v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13023v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13016v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13016v1",
                "updated": "2025-04-17T15:24:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    15,
                    24,
                    25,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T15:24:25Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    15,
                    24,
                    25,
                    3,
                    107,
                    0
                ],
                "title": "ORIS allocation to minimize the outage probability in a multi-user VLC\n  scenario",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ORIS allocation to minimize the outage probability in a multi-user VLC\n  scenario"
                },
                "summary": "Visible Light Communication (VLC) is a promising solution to address the\ngrowing demand for wireless data, leveraging the widespread use of\nlight-emitting diodes (LEDs) as transmitters. However, its deployment is\nchallenged by link blockages that cause connectivity outages. Optical\nreconfigurable intelligent surfaces (ORISs) have recently emerged as a solution\nto mitigate these disruptions. This work considers a multi-user VLC system and\ninvestigates the optimal association of ORISs to LEDs and users to minimize the\noutage probability while limiting the number of ORISs used. Numerical results\nfrom our proposed optimization algorithm demonstrate that using ORISs can\nreduce the outage probability by up to 85% compared to a no-ORIS scenario.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visible Light Communication (VLC) is a promising solution to address the\ngrowing demand for wireless data, leveraging the widespread use of\nlight-emitting diodes (LEDs) as transmitters. However, its deployment is\nchallenged by link blockages that cause connectivity outages. Optical\nreconfigurable intelligent surfaces (ORISs) have recently emerged as a solution\nto mitigate these disruptions. This work considers a multi-user VLC system and\ninvestigates the optimal association of ORISs to LEDs and users to minimize the\noutage probability while limiting the number of ORISs used. Numerical results\nfrom our proposed optimization algorithm demonstrate that using ORISs can\nreduce the outage probability by up to 85% compared to a no-ORIS scenario."
                },
                "authors": [
                    {
                        "name": "Borja Genoves Guzman"
                    },
                    {
                        "name": "Maïté Brandt-Pearce"
                    }
                ],
                "author_detail": {
                    "name": "Maïté Brandt-Pearce"
                },
                "author": "Maïté Brandt-Pearce",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13016v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13016v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11672v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11672v3",
                "updated": "2025-04-17T15:10:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    15,
                    10,
                    52,
                    3,
                    107,
                    0
                ],
                "published": "2024-04-17T18:13:16Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    18,
                    13,
                    16,
                    2,
                    108,
                    0
                ],
                "title": "MemLLM: Finetuning LLMs to Use An Explicit Read-Write Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemLLM: Finetuning LLMs to Use An Explicit Read-Write Memory"
                },
                "summary": "While current large language models (LLMs) perform well on many\nknowledge-related tasks, they are limited by relying on their parameters as an\nimplicit storage mechanism. As a result, they struggle with memorizing rare\nevents and with updating their memory as facts change over time. In addition,\nthe uninterpretable nature of parametric memory makes it challenging to prevent\nhallucination. Model editing and augmenting LLMs with parameters specialized\nfor memory are only partial solutions. In this paper, we introduce MemLLM, a\nnovel method of enhancing LLMs by integrating a structured and explicit\nread-and-write memory module. MemLLM tackles the aforementioned challenges by\nenabling dynamic interaction with the memory and improving the LLM's\ncapabilities in using stored knowledge. Our experiments indicate that MemLLM\nenhances the LLM's performance and interpretability, in language modeling in\ngeneral and knowledge-intensive tasks in particular. We see MemLLM as an\nimportant step towards making LLMs more grounded and factual through memory\naugmentation. The project repository is publicly available at\nhttps://github.com/amodaresi/MemLLM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While current large language models (LLMs) perform well on many\nknowledge-related tasks, they are limited by relying on their parameters as an\nimplicit storage mechanism. As a result, they struggle with memorizing rare\nevents and with updating their memory as facts change over time. In addition,\nthe uninterpretable nature of parametric memory makes it challenging to prevent\nhallucination. Model editing and augmenting LLMs with parameters specialized\nfor memory are only partial solutions. In this paper, we introduce MemLLM, a\nnovel method of enhancing LLMs by integrating a structured and explicit\nread-and-write memory module. MemLLM tackles the aforementioned challenges by\nenabling dynamic interaction with the memory and improving the LLM's\ncapabilities in using stored knowledge. Our experiments indicate that MemLLM\nenhances the LLM's performance and interpretability, in language modeling in\ngeneral and knowledge-intensive tasks in particular. We see MemLLM as an\nimportant step towards making LLMs more grounded and factual through memory\naugmentation. The project repository is publicly available at\nhttps://github.com/amodaresi/MemLLM"
                },
                "authors": [
                    {
                        "name": "Ali Modarressi"
                    },
                    {
                        "name": "Abdullatif Köksal"
                    },
                    {
                        "name": "Ayyoob Imani"
                    },
                    {
                        "name": "Mohsen Fayyaz"
                    },
                    {
                        "name": "Hinrich Schütze"
                    }
                ],
                "author_detail": {
                    "name": "Hinrich Schütze"
                },
                "author": "Hinrich Schütze",
                "arxiv_comment": "Published in Transactions on Machine Learning Research (TMLR)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11672v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11672v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12996v1",
                "updated": "2025-04-17T15:05:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    15,
                    5,
                    40,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T15:05:40Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    15,
                    5,
                    40,
                    3,
                    107,
                    0
                ],
                "title": "SHA256 at SemEval-2025 Task 4: Selective Amnesia -- Constrained\n  Unlearning for Large Language Models via Knowledge Isolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SHA256 at SemEval-2025 Task 4: Selective Amnesia -- Constrained\n  Unlearning for Large Language Models via Knowledge Isolation"
                },
                "summary": "Large language models (LLMs) frequently memorize sensitive information during\ntraining, posing risks when deploying publicly accessible models. Current\nmachine unlearning methods struggle to selectively remove specific data\nassociations without degrading overall model capabilities. This paper presents\nour solution to SemEval-2025 Task 4 on targeted unlearning, which introduces a\ntwo-stage methodology that combines causal mediation analysis with\nlayer-specific optimization. Through systematic causal tracing experiments on\nOLMo architectures (1B and 7B parameters), we identify the critical role of the\nfirst few transformer layers (layers 0-5) in storing subject-attribute\nassociations within MLP modules. Building on this insight, we develop a\nconstrained optimization approach that freezes upper layers while applying a\nnovel joint loss function to lower layers-simultaneously maximizing forget set\nloss via output token cross-entropy penalties and minimizing retain set\ndeviation through adaptive regularization. Our method achieves 2nd place in the\n1B model track, demonstrating strong task performance while maintaining 88% of\nbaseline MMLU accuracy. These results establish causal-informed layer\noptimization as a promising paradigm for efficient, precise unlearning in LLMs,\noffering a significant step forward in addressing data privacy concerns in AI\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) frequently memorize sensitive information during\ntraining, posing risks when deploying publicly accessible models. Current\nmachine unlearning methods struggle to selectively remove specific data\nassociations without degrading overall model capabilities. This paper presents\nour solution to SemEval-2025 Task 4 on targeted unlearning, which introduces a\ntwo-stage methodology that combines causal mediation analysis with\nlayer-specific optimization. Through systematic causal tracing experiments on\nOLMo architectures (1B and 7B parameters), we identify the critical role of the\nfirst few transformer layers (layers 0-5) in storing subject-attribute\nassociations within MLP modules. Building on this insight, we develop a\nconstrained optimization approach that freezes upper layers while applying a\nnovel joint loss function to lower layers-simultaneously maximizing forget set\nloss via output token cross-entropy penalties and minimizing retain set\ndeviation through adaptive regularization. Our method achieves 2nd place in the\n1B model track, demonstrating strong task performance while maintaining 88% of\nbaseline MMLU accuracy. These results establish causal-informed layer\noptimization as a promising paradigm for efficient, precise unlearning in LLMs,\noffering a significant step forward in addressing data privacy concerns in AI\nsystems."
                },
                "authors": [
                    {
                        "name": "Saransh Agrawal"
                    },
                    {
                        "name": "Kuan-Hao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kuan-Hao Huang"
                },
                "author": "Kuan-Hao Huang",
                "arxiv_comment": "8 pages, In Proceedings of The 19th International Workshop on\n  Semantic Evaluation (SemEval), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12991v1",
                "updated": "2025-04-17T14:59:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    14,
                    59,
                    29,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T14:59:29Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    14,
                    59,
                    29,
                    3,
                    107,
                    0
                ],
                "title": "Chain-of-Thought Prompting for Out-of-Distribution Samples: A\n  Latent-Variable Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought Prompting for Out-of-Distribution Samples: A\n  Latent-Variable Study"
                },
                "summary": "Chain-of-Thought (CoT) prompting has emerged as a powerful technique to\nimprove in-context learning (ICL) in large language models (LLMs) by breaking\ncomplex reasoning into intermediate steps. However, the ability of CoT to\ngeneralize under distribution shift remains poorly understood. In this work, we\nextend a latent-variable framework for CoT prompting and study its behavior on\ntwo prototypical out-of-distribution (OOD) scenarios: (i) the latent variables\nfor CoT steps are permuted into novel combinations, and (ii) the latent\nvariables uniformly scaled by a factor. Our experiments demonstrate that CoT\ninference generalizes effectively to OOD samples whose latent variables closely\nresemble those seen during training, but its performance degrades as this\nsimilarity decreases. These findings provide foundational insights into the\nstrengths and limitations of CoT prompting under OOD conditions and suggest\ndirections for developing more resilient reasoning strategies in future LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) prompting has emerged as a powerful technique to\nimprove in-context learning (ICL) in large language models (LLMs) by breaking\ncomplex reasoning into intermediate steps. However, the ability of CoT to\ngeneralize under distribution shift remains poorly understood. In this work, we\nextend a latent-variable framework for CoT prompting and study its behavior on\ntwo prototypical out-of-distribution (OOD) scenarios: (i) the latent variables\nfor CoT steps are permuted into novel combinations, and (ii) the latent\nvariables uniformly scaled by a factor. Our experiments demonstrate that CoT\ninference generalizes effectively to OOD samples whose latent variables closely\nresemble those seen during training, but its performance degrades as this\nsimilarity decreases. These findings provide foundational insights into the\nstrengths and limitations of CoT prompting under OOD conditions and suggest\ndirections for developing more resilient reasoning strategies in future LLMs."
                },
                "authors": [
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Fu-Chieh Chang"
                    },
                    {
                        "name": "Pei-Yuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Pei-Yuan Wu"
                },
                "author": "Pei-Yuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12984v1",
                "updated": "2025-04-17T14:45:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    14,
                    45,
                    3,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T14:45:03Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    14,
                    45,
                    3,
                    3,
                    107,
                    0
                ],
                "title": "A Virtual Machine for Arbitrary Low-Precision GPGPU Computation in LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Virtual Machine for Arbitrary Low-Precision GPGPU Computation in LLM\n  Serving"
                },
                "summary": "Serving Large Language Models (LLMs) is critical for AI-powered applications\nbut demands substantial computational resources, particularly in memory\nbandwidth and computational throughput. Low-precision computation has emerged\nas a key technique to improve efficiency while reducing resource consumption.\nExisting approaches for generating low-precision kernels are limited to weight\nbit widths that are powers of two and suffer from suboptimal performance due to\nhigh-level GPU programming abstractions. These abstractions restrict critical\noptimizations, such as fine-grained register management and optimized memory\naccess patterns, which are essential for efficient low-precision computations.\nIn this paper, we introduce a virtual machine (VM) designed for General-Purpose\nGPU (GPGPU) computing, enabling support for low-precision data types with\narbitrary bit widths while maintaining GPU programmability. The proposed VM\nfeatures a thread-block-level programming model, a hierarchical memory space, a\nnovel algebraic layout system, and extensive support for diverse low-precision\ndata types. VM programs are compiled into highly efficient GPU programs with\nautomatic vectorization and instruction selection. Extensive experiments\ndemonstrate that our VM efficiently supports a full spectrum of low-precision\ndata types, and outperforms state-of-the-art low-precision kernels on their\nsupported types. Compared to existing compilers like Triton and Ladder, as well\nas hand-optimized kernels such as QuantLLM and Marlin, our VM achieves\nperformance improvements of 1.75x, 2.61x, 1.29x and 1.03x, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Large Language Models (LLMs) is critical for AI-powered applications\nbut demands substantial computational resources, particularly in memory\nbandwidth and computational throughput. Low-precision computation has emerged\nas a key technique to improve efficiency while reducing resource consumption.\nExisting approaches for generating low-precision kernels are limited to weight\nbit widths that are powers of two and suffer from suboptimal performance due to\nhigh-level GPU programming abstractions. These abstractions restrict critical\noptimizations, such as fine-grained register management and optimized memory\naccess patterns, which are essential for efficient low-precision computations.\nIn this paper, we introduce a virtual machine (VM) designed for General-Purpose\nGPU (GPGPU) computing, enabling support for low-precision data types with\narbitrary bit widths while maintaining GPU programmability. The proposed VM\nfeatures a thread-block-level programming model, a hierarchical memory space, a\nnovel algebraic layout system, and extensive support for diverse low-precision\ndata types. VM programs are compiled into highly efficient GPU programs with\nautomatic vectorization and instruction selection. Extensive experiments\ndemonstrate that our VM efficiently supports a full spectrum of low-precision\ndata types, and outperforms state-of-the-art low-precision kernels on their\nsupported types. Compared to existing compilers like Triton and Ladder, as well\nas hand-optimized kernels such as QuantLLM and Marlin, our VM achieves\nperformance improvements of 1.75x, 2.61x, 1.29x and 1.03x, respectively."
                },
                "authors": [
                    {
                        "name": "Yaoyao Ding"
                    },
                    {
                        "name": "Bohan Hou"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Allan Lin"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Cody Yu Hao"
                    },
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Gennady Pekhimenko"
                    }
                ],
                "author_detail": {
                    "name": "Gennady Pekhimenko"
                },
                "author": "Gennady Pekhimenko",
                "arxiv_comment": "18 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17188v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17188v2",
                "updated": "2025-04-17T14:40:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    14,
                    40,
                    34,
                    3,
                    107,
                    0
                ],
                "published": "2024-10-22T17:09:28Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    17,
                    9,
                    28,
                    1,
                    296,
                    0
                ],
                "title": "Minimum-Violation Temporal Logic Planning for Heterogeneous Robots under\n  Robot Skill Failures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minimum-Violation Temporal Logic Planning for Heterogeneous Robots under\n  Robot Skill Failures"
                },
                "summary": "In this paper, we consider teams of robots with heterogeneous skills (e.g.,\nsensing and manipulation) tasked with collaborative missions described by\nLinear Temporal Logic (LTL) formulas. These LTL-encoded tasks require robots to\napply their skills to specific regions and objects in a temporal and logical\norder. While existing temporal logic planning algorithms can synthesize\ncorrect-by-construction plans, they typically lack reactivity to unexpected\nfailures of robot skills, which can compromise mission performance. This paper\naddresses this challenge by proposing a reactive LTL planning algorithm that\nadapts to unexpected failures during deployment. Specifically, the proposed\nalgorithm reassigns sub-tasks to robots based on their functioning skills and\nlocally revises team plans to accommodate these new assignments and ensure\nmission completion. The main novelty of the proposed algorithm is its ability\nto handle cases where mission completion becomes impossible due to limited\nfunctioning robots. Instead of reporting mission failure, the algorithm\nstrategically prioritizes the most crucial sub-tasks and locally revises the\nteam's plans, as per user-specified priorities, to minimize mission violations.\nWe provide theoretical conditions under which the proposed framework computes\nthe minimum-violation task reassignments and team plans. We provide numerical\nand hardware experiments to demonstrate the efficiency of the proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we consider teams of robots with heterogeneous skills (e.g.,\nsensing and manipulation) tasked with collaborative missions described by\nLinear Temporal Logic (LTL) formulas. These LTL-encoded tasks require robots to\napply their skills to specific regions and objects in a temporal and logical\norder. While existing temporal logic planning algorithms can synthesize\ncorrect-by-construction plans, they typically lack reactivity to unexpected\nfailures of robot skills, which can compromise mission performance. This paper\naddresses this challenge by proposing a reactive LTL planning algorithm that\nadapts to unexpected failures during deployment. Specifically, the proposed\nalgorithm reassigns sub-tasks to robots based on their functioning skills and\nlocally revises team plans to accommodate these new assignments and ensure\nmission completion. The main novelty of the proposed algorithm is its ability\nto handle cases where mission completion becomes impossible due to limited\nfunctioning robots. Instead of reporting mission failure, the algorithm\nstrategically prioritizes the most crucial sub-tasks and locally revises the\nteam's plans, as per user-specified priorities, to minimize mission violations.\nWe provide theoretical conditions under which the proposed framework computes\nthe minimum-violation task reassignments and team plans. We provide numerical\nand hardware experiments to demonstrate the efficiency of the proposed method."
                },
                "authors": [
                    {
                        "name": "Samarth Kalluraya"
                    },
                    {
                        "name": "Beichen Zhou"
                    },
                    {
                        "name": "Yiannis Kantaros"
                    }
                ],
                "author_detail": {
                    "name": "Yiannis Kantaros"
                },
                "author": "Yiannis Kantaros",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17188v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17188v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12982v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12982v1",
                "updated": "2025-04-17T14:40:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    14,
                    40,
                    31,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T14:40:31Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    14,
                    40,
                    31,
                    3,
                    107,
                    0
                ],
                "title": "Accommodate Knowledge Conflicts in Retrieval-augmented LLMs: Towards\n  Reliable Response Generation in the Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accommodate Knowledge Conflicts in Retrieval-augmented LLMs: Towards\n  Reliable Response Generation in the Wild"
                },
                "summary": "The proliferation of large language models (LLMs) has significantly advanced\ninformation retrieval systems, particularly in response generation (RG).\nUnfortunately, LLMs often face knowledge conflicts between internal memory and\nretrievaled external information, arising from misinformation, biases, or\noutdated knowledge. These conflicts undermine response reliability and\nintroduce uncertainty in decision-making. In this work, we analyze how LLMs\nnavigate knowledge conflicts from an information-theoretic perspective and\nreveal that when conflicting and supplementary information exhibit significant\ndifferences, LLMs confidently resolve their preferences. However, when the\ndistinction is ambiguous, LLMs experience heightened uncertainty. Based on this\ninsight, we propose Swin-VIB, a novel framework that integrates a pipeline of\nvariational information bottleneck models into adaptive augmentation of\nretrieved information and guiding LLM preference in response generation.\nExtensive experiments on single-choice, open-ended question-answering (QA), and\nretrieval augmented generation (RAG) validate our theoretical findings and\ndemonstrate the efficacy of Swin-VIB. Notably, our method improves\nsingle-choice task accuracy by at least 7.54\\% over competitive baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of large language models (LLMs) has significantly advanced\ninformation retrieval systems, particularly in response generation (RG).\nUnfortunately, LLMs often face knowledge conflicts between internal memory and\nretrievaled external information, arising from misinformation, biases, or\noutdated knowledge. These conflicts undermine response reliability and\nintroduce uncertainty in decision-making. In this work, we analyze how LLMs\nnavigate knowledge conflicts from an information-theoretic perspective and\nreveal that when conflicting and supplementary information exhibit significant\ndifferences, LLMs confidently resolve their preferences. However, when the\ndistinction is ambiguous, LLMs experience heightened uncertainty. Based on this\ninsight, we propose Swin-VIB, a novel framework that integrates a pipeline of\nvariational information bottleneck models into adaptive augmentation of\nretrieved information and guiding LLM preference in response generation.\nExtensive experiments on single-choice, open-ended question-answering (QA), and\nretrieval augmented generation (RAG) validate our theoretical findings and\ndemonstrate the efficacy of Swin-VIB. Notably, our method improves\nsingle-choice task accuracy by at least 7.54\\% over competitive baselines."
                },
                "authors": [
                    {
                        "name": "Jiatai Wang"
                    },
                    {
                        "name": "Zhiwei Xu"
                    },
                    {
                        "name": "Di Jin"
                    },
                    {
                        "name": "Xuewen Yang"
                    },
                    {
                        "name": "Tao Li"
                    }
                ],
                "author_detail": {
                    "name": "Tao Li"
                },
                "author": "Tao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12982v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12976v1",
                "updated": "2025-04-17T14:29:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    14,
                    29,
                    18,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T14:29:18Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    14,
                    29,
                    18,
                    3,
                    107,
                    0
                ],
                "title": "Sparks of Science: Hypothesis Generation Using Structured Paper Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparks of Science: Hypothesis Generation Using Structured Paper Data"
                },
                "summary": "Generating novel and creative scientific hypotheses is a cornerstone in\nachieving Artificial General Intelligence. Large language and reasoning models\nhave the potential to aid in the systematic creation, selection, and validation\nof scientifically informed hypotheses. However, current foundation models often\nstruggle to produce scientific ideas that are both novel and feasible. One\nreason is the lack of a dedicated dataset that frames Scientific Hypothesis\nGeneration (SHG) as a Natural Language Generation (NLG) task. In this paper, we\nintroduce HypoGen, the first dataset of approximately 5500 structured\nproblem-hypothesis pairs extracted from top-tier computer science conferences\nstructured with a Bit-Flip-Spark schema, where the Bit is the conventional\nassumption, the Spark is the key insight or conceptual leap, and the Flip is\nthe resulting counterproposal. HypoGen uniquely integrates an explicit\nChain-of-Reasoning component that reflects the intellectual process from Bit to\nFlip. We demonstrate that framing hypothesis generation as conditional language\nmodelling, with the model fine-tuned on Bit-Flip-Spark and the\nChain-of-Reasoning (and where, at inference, we only provide the Bit), leads to\nimprovements in the overall quality of the hypotheses. Our evaluation employs\nautomated metrics and LLM judge rankings for overall quality assessment. We\nshow that by fine-tuning on our HypoGen dataset we improve the novelty,\nfeasibility, and overall quality of the generated hypotheses. The HypoGen\ndataset is publicly available at\nhuggingface.co/datasets/UniverseTBD/hypogen-dr1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating novel and creative scientific hypotheses is a cornerstone in\nachieving Artificial General Intelligence. Large language and reasoning models\nhave the potential to aid in the systematic creation, selection, and validation\nof scientifically informed hypotheses. However, current foundation models often\nstruggle to produce scientific ideas that are both novel and feasible. One\nreason is the lack of a dedicated dataset that frames Scientific Hypothesis\nGeneration (SHG) as a Natural Language Generation (NLG) task. In this paper, we\nintroduce HypoGen, the first dataset of approximately 5500 structured\nproblem-hypothesis pairs extracted from top-tier computer science conferences\nstructured with a Bit-Flip-Spark schema, where the Bit is the conventional\nassumption, the Spark is the key insight or conceptual leap, and the Flip is\nthe resulting counterproposal. HypoGen uniquely integrates an explicit\nChain-of-Reasoning component that reflects the intellectual process from Bit to\nFlip. We demonstrate that framing hypothesis generation as conditional language\nmodelling, with the model fine-tuned on Bit-Flip-Spark and the\nChain-of-Reasoning (and where, at inference, we only provide the Bit), leads to\nimprovements in the overall quality of the hypotheses. Our evaluation employs\nautomated metrics and LLM judge rankings for overall quality assessment. We\nshow that by fine-tuning on our HypoGen dataset we improve the novelty,\nfeasibility, and overall quality of the generated hypotheses. The HypoGen\ndataset is publicly available at\nhuggingface.co/datasets/UniverseTBD/hypogen-dr1."
                },
                "authors": [
                    {
                        "name": "Charles O'Neill"
                    },
                    {
                        "name": "Tirthankar Ghosal"
                    },
                    {
                        "name": "Roberta Răileanu"
                    },
                    {
                        "name": "Mike Walmsley"
                    },
                    {
                        "name": "Thang Bui"
                    },
                    {
                        "name": "Kevin Schawinski"
                    },
                    {
                        "name": "Ioana Ciucă"
                    }
                ],
                "author_detail": {
                    "name": "Ioana Ciucă"
                },
                "author": "Ioana Ciucă",
                "arxiv_comment": "9 pages, 2 figures. Comments welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12972v1",
                "updated": "2025-04-17T14:24:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    14,
                    24,
                    51,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T14:24:51Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    14,
                    24,
                    51,
                    3,
                    107,
                    0
                ],
                "title": "Estimating Optimal Context Length for Hybrid Retrieval-augmented\n  Multi-document Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating Optimal Context Length for Hybrid Retrieval-augmented\n  Multi-document Summarization"
                },
                "summary": "Recent advances in long-context reasoning abilities of language models led to\ninteresting applications in large-scale multi-document summarization. However,\nprior work has shown that these long-context models are not effective at their\nclaimed context windows. To this end, retrieval-augmented systems provide an\nefficient and effective alternative. However, their performance can be highly\nsensitive to the choice of retrieval context length. In this work, we present a\nhybrid method that combines retrieval-augmented systems with long-context\nwindows supported by recent language models. Our method first estimates the\noptimal retrieval length as a function of the retriever, summarizer, and\ndataset. On a randomly sampled subset of the dataset, we use a panel of LLMs to\ngenerate a pool of silver references. We use these silver references to\nestimate the optimal context length for a given RAG system configuration. Our\nresults on the multi-document summarization task showcase the effectiveness of\nour method across model classes and sizes. We compare against length estimates\nfrom strong long-context benchmarks such as RULER and HELMET. Our analysis also\nhighlights the effectiveness of our estimation method for very long-context LMs\nand its generalization to new classes of LMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in long-context reasoning abilities of language models led to\ninteresting applications in large-scale multi-document summarization. However,\nprior work has shown that these long-context models are not effective at their\nclaimed context windows. To this end, retrieval-augmented systems provide an\nefficient and effective alternative. However, their performance can be highly\nsensitive to the choice of retrieval context length. In this work, we present a\nhybrid method that combines retrieval-augmented systems with long-context\nwindows supported by recent language models. Our method first estimates the\noptimal retrieval length as a function of the retriever, summarizer, and\ndataset. On a randomly sampled subset of the dataset, we use a panel of LLMs to\ngenerate a pool of silver references. We use these silver references to\nestimate the optimal context length for a given RAG system configuration. Our\nresults on the multi-document summarization task showcase the effectiveness of\nour method across model classes and sizes. We compare against length estimates\nfrom strong long-context benchmarks such as RULER and HELMET. Our analysis also\nhighlights the effectiveness of our estimation method for very long-context LMs\nand its generalization to new classes of LMs."
                },
                "authors": [
                    {
                        "name": "Adithya Pratapa"
                    },
                    {
                        "name": "Teruko Mitamura"
                    }
                ],
                "author_detail": {
                    "name": "Teruko Mitamura"
                },
                "author": "Teruko Mitamura",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11230v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11230v2",
                "updated": "2025-04-17T14:13:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    14,
                    13,
                    37,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-15T14:30:26Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    30,
                    26,
                    1,
                    105,
                    0
                ],
                "title": "CAP-Net: A Unified Network for 6D Pose and Size Estimation of\n  Categorical Articulated Parts from a Single RGB-D Image",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAP-Net: A Unified Network for 6D Pose and Size Estimation of\n  Categorical Articulated Parts from a Single RGB-D Image"
                },
                "summary": "This paper tackles category-level pose estimation of articulated objects in\nrobotic manipulation tasks and introduces a new benchmark dataset. While recent\nmethods estimate part poses and sizes at the category level, they often rely on\ngeometric cues and complex multi-stage pipelines that first segment parts from\nthe point cloud, followed by Normalized Part Coordinate Space (NPCS) estimation\nfor 6D poses. These approaches overlook dense semantic cues from RGB images,\nleading to suboptimal accuracy, particularly for objects with small parts. To\naddress these limitations, we propose a single-stage Network, CAP-Net, for\nestimating the 6D poses and sizes of Categorical Articulated Parts. This method\ncombines RGB-D features to generate instance segmentation and NPCS\nrepresentations for each part in an end-to-end manner. CAP-Net uses a unified\nnetwork to simultaneously predict point-wise class labels, centroid offsets,\nand NPCS maps. A clustering algorithm then groups points of the same predicted\nclass based on their estimated centroid distances to isolate each part.\nFinally, the NPCS region of each part is aligned with the point cloud to\nrecover its final pose and size. To bridge the sim-to-real domain gap, we\nintroduce the RGBD-Art dataset, the largest RGB-D articulated dataset to date,\nfeaturing photorealistic RGB images and depth noise simulated from real\nsensors. Experimental evaluations on the RGBD-Art dataset demonstrate that our\nmethod significantly outperforms the state-of-the-art approach. Real-world\ndeployments of our model in robotic tasks underscore its robustness and\nexceptional sim-to-real transfer capabilities, confirming its substantial\npractical utility. Our dataset, code and pre-trained models are available on\nthe project page.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper tackles category-level pose estimation of articulated objects in\nrobotic manipulation tasks and introduces a new benchmark dataset. While recent\nmethods estimate part poses and sizes at the category level, they often rely on\ngeometric cues and complex multi-stage pipelines that first segment parts from\nthe point cloud, followed by Normalized Part Coordinate Space (NPCS) estimation\nfor 6D poses. These approaches overlook dense semantic cues from RGB images,\nleading to suboptimal accuracy, particularly for objects with small parts. To\naddress these limitations, we propose a single-stage Network, CAP-Net, for\nestimating the 6D poses and sizes of Categorical Articulated Parts. This method\ncombines RGB-D features to generate instance segmentation and NPCS\nrepresentations for each part in an end-to-end manner. CAP-Net uses a unified\nnetwork to simultaneously predict point-wise class labels, centroid offsets,\nand NPCS maps. A clustering algorithm then groups points of the same predicted\nclass based on their estimated centroid distances to isolate each part.\nFinally, the NPCS region of each part is aligned with the point cloud to\nrecover its final pose and size. To bridge the sim-to-real domain gap, we\nintroduce the RGBD-Art dataset, the largest RGB-D articulated dataset to date,\nfeaturing photorealistic RGB images and depth noise simulated from real\nsensors. Experimental evaluations on the RGBD-Art dataset demonstrate that our\nmethod significantly outperforms the state-of-the-art approach. Real-world\ndeployments of our model in robotic tasks underscore its robustness and\nexceptional sim-to-real transfer capabilities, confirming its substantial\npractical utility. Our dataset, code and pre-trained models are available on\nthe project page."
                },
                "authors": [
                    {
                        "name": "Jingshun Huang"
                    },
                    {
                        "name": "Haitao Lin"
                    },
                    {
                        "name": "Tianyu Wang"
                    },
                    {
                        "name": "Yanwei Fu"
                    },
                    {
                        "name": "Xiangyang Xue"
                    },
                    {
                        "name": "Yi Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zhu"
                },
                "author": "Yi Zhu",
                "arxiv_comment": "To appear in CVPR 2025 (Highlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11230v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11230v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12961v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12961v1",
                "updated": "2025-04-17T14:07:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    14,
                    7,
                    11,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T14:07:11Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    14,
                    7,
                    11,
                    3,
                    107,
                    0
                ],
                "title": "QLLM: Do We Really Need a Mixing Network for Credit Assignment in\n  Multi-Agent Reinforcement Learning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QLLM: Do We Really Need a Mixing Network for Credit Assignment in\n  Multi-Agent Reinforcement Learning?"
                },
                "summary": "Credit assignment has remained a fundamental challenge in multi-agent\nreinforcement learning (MARL). Previous studies have primarily addressed this\nissue through value decomposition methods under the centralized training with\ndecentralized execution paradigm, where neural networks are utilized to\napproximate the nonlinear relationship between individual Q-values and the\nglobal Q-value. Although these approaches have achieved considerable success in\nvarious benchmark tasks, they still suffer from several limitations, including\nimprecise attribution of contributions, limited interpretability, and poor\nscalability in high-dimensional state spaces. To address these challenges, we\npropose a novel algorithm, \\textbf{QLLM}, which facilitates the automatic\nconstruction of credit assignment functions using large language models (LLMs).\nSpecifically, the concept of \\textbf{TFCAF} is introduced, wherein the credit\nallocation process is represented as a direct and expressive nonlinear\nfunctional formulation. A custom-designed \\textit{coder-evaluator} framework is\nfurther employed to guide the generation, verification, and refinement of\nexecutable code by LLMs, significantly mitigating issues such as hallucination\nand shallow reasoning during inference. Extensive experiments conducted on\nseveral standard MARL benchmarks demonstrate that the proposed method\nconsistently outperforms existing state-of-the-art baselines. Moreover, QLLM\nexhibits strong generalization capability and maintains compatibility with a\nwide range of MARL algorithms that utilize mixing networks, positioning it as a\npromising and versatile solution for complex multi-agent scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Credit assignment has remained a fundamental challenge in multi-agent\nreinforcement learning (MARL). Previous studies have primarily addressed this\nissue through value decomposition methods under the centralized training with\ndecentralized execution paradigm, where neural networks are utilized to\napproximate the nonlinear relationship between individual Q-values and the\nglobal Q-value. Although these approaches have achieved considerable success in\nvarious benchmark tasks, they still suffer from several limitations, including\nimprecise attribution of contributions, limited interpretability, and poor\nscalability in high-dimensional state spaces. To address these challenges, we\npropose a novel algorithm, \\textbf{QLLM}, which facilitates the automatic\nconstruction of credit assignment functions using large language models (LLMs).\nSpecifically, the concept of \\textbf{TFCAF} is introduced, wherein the credit\nallocation process is represented as a direct and expressive nonlinear\nfunctional formulation. A custom-designed \\textit{coder-evaluator} framework is\nfurther employed to guide the generation, verification, and refinement of\nexecutable code by LLMs, significantly mitigating issues such as hallucination\nand shallow reasoning during inference. Extensive experiments conducted on\nseveral standard MARL benchmarks demonstrate that the proposed method\nconsistently outperforms existing state-of-the-art baselines. Moreover, QLLM\nexhibits strong generalization capability and maintains compatibility with a\nwide range of MARL algorithms that utilize mixing networks, positioning it as a\npromising and versatile solution for complex multi-agent scenarios."
                },
                "authors": [
                    {
                        "name": "Zhouyang Jiang"
                    },
                    {
                        "name": "Bin Zhang"
                    },
                    {
                        "name": "Airong Wei"
                    },
                    {
                        "name": "Zhiwei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiwei Xu"
                },
                "author": "Zhiwei Xu",
                "arxiv_comment": "9 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12961v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12961v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12951v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12951v1",
                "updated": "2025-04-17T13:52:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    13,
                    52,
                    48,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T13:52:48Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    13,
                    52,
                    48,
                    3,
                    107,
                    0
                ],
                "title": "Are Retrials All You Need? Enhancing Large Language Model Reasoning\n  Without Verbalized Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Retrials All You Need? Enhancing Large Language Model Reasoning\n  Without Verbalized Feedback"
                },
                "summary": "Recent advancements in large language models (LLMs) have catalyzed the\ndevelopment of general-purpose autonomous agents, demonstrating remarkable\nperformance in complex reasoning tasks across various domains. This surge has\nspurred the evolution of a plethora of prompt-based reasoning frameworks. A\nrecent focus has been on iterative reasoning strategies that refine outputs\nthrough self-evaluation and verbalized feedback. However, these strategies\nrequire additional computational complexity to enable models to recognize and\ncorrect their mistakes, leading to a significant increase in their cost. In\nthis work, we introduce the concept of ``retrials without feedback'', an\nembarrassingly simple yet powerful mechanism for enhancing reasoning frameworks\nby allowing LLMs to retry problem-solving attempts upon identifying incorrect\nanswers. Unlike conventional iterative refinement methods, our method does not\nrequire explicit self-reflection or verbalized feedback, simplifying the\nrefinement process. Our findings indicate that simpler retrial-based approaches\noften outperform more sophisticated reasoning frameworks, suggesting that the\nbenefits of complex methods may not always justify their computational costs.\nBy challenging the prevailing assumption that more intricate reasoning\nstrategies inherently lead to better performance, our work offers new insights\ninto how simpler, more efficient approaches can achieve optimal results. So,\nare retrials all you need?",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have catalyzed the\ndevelopment of general-purpose autonomous agents, demonstrating remarkable\nperformance in complex reasoning tasks across various domains. This surge has\nspurred the evolution of a plethora of prompt-based reasoning frameworks. A\nrecent focus has been on iterative reasoning strategies that refine outputs\nthrough self-evaluation and verbalized feedback. However, these strategies\nrequire additional computational complexity to enable models to recognize and\ncorrect their mistakes, leading to a significant increase in their cost. In\nthis work, we introduce the concept of ``retrials without feedback'', an\nembarrassingly simple yet powerful mechanism for enhancing reasoning frameworks\nby allowing LLMs to retry problem-solving attempts upon identifying incorrect\nanswers. Unlike conventional iterative refinement methods, our method does not\nrequire explicit self-reflection or verbalized feedback, simplifying the\nrefinement process. Our findings indicate that simpler retrial-based approaches\noften outperform more sophisticated reasoning frameworks, suggesting that the\nbenefits of complex methods may not always justify their computational costs.\nBy challenging the prevailing assumption that more intricate reasoning\nstrategies inherently lead to better performance, our work offers new insights\ninto how simpler, more efficient approaches can achieve optimal results. So,\nare retrials all you need?"
                },
                "authors": [
                    {
                        "name": "Nearchos Potamitis"
                    },
                    {
                        "name": "Akhil Arora"
                    }
                ],
                "author_detail": {
                    "name": "Akhil Arora"
                },
                "author": "Akhil Arora",
                "arxiv_comment": "8 pages, 16 figures, 1 table. arXiv admin note: text overlap with\n  arXiv:2405.06691",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12951v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12951v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12943v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12943v2",
                "updated": "2025-04-18T04:23:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    4,
                    23,
                    0,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-17T13:43:13Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    13,
                    43,
                    13,
                    3,
                    107,
                    0
                ],
                "title": "Customizing Emotional Support: How Do Individuals Construct and Interact\n  With LLM-Powered Chatbots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Customizing Emotional Support: How Do Individuals Construct and Interact\n  With LLM-Powered Chatbots"
                },
                "summary": "Personalized support is essential to fulfill individuals' emotional needs and\nsustain their mental well-being. Large language models (LLMs), with great\ncustomization flexibility, hold promises to enable individuals to create their\nown emotional support agents. In this work, we developed ChatLab, where users\ncould construct LLM-powered chatbots with additional interaction features\nincluding voices and avatars. Using a Research through Design approach, we\nconducted a week-long field study followed by interviews and design activities\n(N = 22), which uncovered how participants created diverse chatbot personas for\nemotional reliance, confronting stressors, connecting to intellectual\ndiscourse, reflecting mirrored selves, etc. We found that participants actively\nenriched the personas they constructed, shaping the dynamics between themselves\nand the chatbot to foster open and honest conversations. They also suggested\nother customizable features, such as integrating online activities and\nadjustable memory settings. Based on these findings, we discuss opportunities\nfor enhancing personalized emotional support through emerging AI technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized support is essential to fulfill individuals' emotional needs and\nsustain their mental well-being. Large language models (LLMs), with great\ncustomization flexibility, hold promises to enable individuals to create their\nown emotional support agents. In this work, we developed ChatLab, where users\ncould construct LLM-powered chatbots with additional interaction features\nincluding voices and avatars. Using a Research through Design approach, we\nconducted a week-long field study followed by interviews and design activities\n(N = 22), which uncovered how participants created diverse chatbot personas for\nemotional reliance, confronting stressors, connecting to intellectual\ndiscourse, reflecting mirrored selves, etc. We found that participants actively\nenriched the personas they constructed, shaping the dynamics between themselves\nand the chatbot to foster open and honest conversations. They also suggested\nother customizable features, such as integrating online activities and\nadjustable memory settings. Based on these findings, we discuss opportunities\nfor enhancing personalized emotional support through emerging AI technologies."
                },
                "authors": [
                    {
                        "name": "Xi Zheng"
                    },
                    {
                        "name": "Zhuoyang Li"
                    },
                    {
                        "name": "Xinning Gui"
                    },
                    {
                        "name": "Yuhan Luo"
                    }
                ],
                "author_detail": {
                    "name": "Yuhan Luo"
                },
                "author": "Yuhan Luo",
                "arxiv_comment": "20 pages, 3 figures, 3 tables. Accepted to CHI 2025, ACM Conference\n  on Human Factors in Computing Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12943v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12943v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12931v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12931v1",
                "updated": "2025-04-17T13:28:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    13,
                    28,
                    1,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T13:28:01Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    13,
                    28,
                    1,
                    3,
                    107,
                    0
                ],
                "title": "Explainable AI in Usable Privacy and Security: Challenges and\n  Opportunities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable AI in Usable Privacy and Security: Challenges and\n  Opportunities"
                },
                "summary": "Large Language Models (LLMs) are increasingly being used for automated\nevaluations and explaining them. However, concerns about explanation quality,\nconsistency, and hallucinations remain open research challenges, particularly\nin high-stakes contexts like privacy and security, where user trust and\ndecision-making are at stake. In this paper, we investigate these issues in the\ncontext of PRISMe, an interactive privacy policy assessment tool that leverages\nLLMs to evaluate and explain website privacy policies. Based on a prior user\nstudy with 22 participants, we identify key concerns regarding LLM judgment\ntransparency, consistency, and faithfulness, as well as variations in user\npreferences for explanation detail and engagement. We discuss potential\nstrategies to mitigate these concerns, including structured evaluation\ncriteria, uncertainty estimation, and retrieval-augmented generation (RAG). We\nidentify a need for adaptive explanation strategies tailored to different user\nprofiles for LLM-as-a-judge. Our goal is to showcase the application area of\nusable privacy and security to be promising for Human-Centered Explainable AI\n(HCXAI) to make an impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly being used for automated\nevaluations and explaining them. However, concerns about explanation quality,\nconsistency, and hallucinations remain open research challenges, particularly\nin high-stakes contexts like privacy and security, where user trust and\ndecision-making are at stake. In this paper, we investigate these issues in the\ncontext of PRISMe, an interactive privacy policy assessment tool that leverages\nLLMs to evaluate and explain website privacy policies. Based on a prior user\nstudy with 22 participants, we identify key concerns regarding LLM judgment\ntransparency, consistency, and faithfulness, as well as variations in user\npreferences for explanation detail and engagement. We discuss potential\nstrategies to mitigate these concerns, including structured evaluation\ncriteria, uncertainty estimation, and retrieval-augmented generation (RAG). We\nidentify a need for adaptive explanation strategies tailored to different user\nprofiles for LLM-as-a-judge. Our goal is to showcase the application area of\nusable privacy and security to be promising for Human-Centered Explainable AI\n(HCXAI) to make an impact."
                },
                "authors": [
                    {
                        "name": "Vincent Freiberger"
                    },
                    {
                        "name": "Arthur Fleig"
                    },
                    {
                        "name": "Erik Buchmann"
                    }
                ],
                "author_detail": {
                    "name": "Erik Buchmann"
                },
                "author": "Erik Buchmann",
                "arxiv_comment": "Presented at the 5th CHI Workshop on Human-Centered Explainable AI\n  (HCXAI)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12931v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12931v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12915v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12915v1",
                "updated": "2025-04-17T13:05:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    13,
                    5,
                    14,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T13:05:14Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    13,
                    5,
                    14,
                    3,
                    107,
                    0
                ],
                "title": "ConExion: Concept Extraction with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConExion: Concept Extraction with Large Language Models"
                },
                "summary": "In this paper, an approach for concept extraction from documents using\npre-trained large language models (LLMs) is presented. Compared with\nconventional methods that extract keyphrases summarizing the important\ninformation discussed in a document, our approach tackles a more challenging\ntask of extracting all present concepts related to the specific domain, not\njust the important ones. Through comprehensive evaluations of two widely used\nbenchmark datasets, we demonstrate that our method improves the F1 score\ncompared to state-of-the-art techniques. Additionally, we explore the potential\nof using prompts within these models for unsupervised concept extraction. The\nextracted concepts are intended to support domain coverage evaluation of\nontologies and facilitate ontology learning, highlighting the effectiveness of\nLLMs in concept extraction tasks. Our source code and datasets are publicly\navailable at https://github.com/ISE-FIZKarlsruhe/concept_extraction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, an approach for concept extraction from documents using\npre-trained large language models (LLMs) is presented. Compared with\nconventional methods that extract keyphrases summarizing the important\ninformation discussed in a document, our approach tackles a more challenging\ntask of extracting all present concepts related to the specific domain, not\njust the important ones. Through comprehensive evaluations of two widely used\nbenchmark datasets, we demonstrate that our method improves the F1 score\ncompared to state-of-the-art techniques. Additionally, we explore the potential\nof using prompts within these models for unsupervised concept extraction. The\nextracted concepts are intended to support domain coverage evaluation of\nontologies and facilitate ontology learning, highlighting the effectiveness of\nLLMs in concept extraction tasks. Our source code and datasets are publicly\navailable at https://github.com/ISE-FIZKarlsruhe/concept_extraction."
                },
                "authors": [
                    {
                        "name": "Ebrahim Norouzi"
                    },
                    {
                        "name": "Sven Hertling"
                    },
                    {
                        "name": "Harald Sack"
                    }
                ],
                "author_detail": {
                    "name": "Harald Sack"
                },
                "author": "Harald Sack",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12915v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12915v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12913v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12913v1",
                "updated": "2025-04-17T13:02:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    13,
                    2,
                    44,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T13:02:44Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    13,
                    2,
                    44,
                    3,
                    107,
                    0
                ],
                "title": "MAIN: Mutual Alignment Is Necessary for instruction tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAIN: Mutual Alignment Is Necessary for instruction tuning"
                },
                "summary": "Instruction tuning has enabled large language models (LLMs) to achieve\nremarkable performance, but its success heavily depends on the availability of\nlarge-scale, high-quality instruction-response pairs. However, current methods\nfor scaling up data generation often overlook a crucial aspect: the alignment\nbetween instructions and responses. We hypothesize that high-quality\ninstruction-response pairs are not defined by the individual quality of each\ncomponent, but by the extent of their alignment with each other. To address\nthis, we propose a Mutual Alignment Framework (MAIN) that ensures coherence\nbetween the instruction and response through mutual constraints. Experiments\ndemonstrate that models such as LLaMA and Mistral, fine-tuned within this\nframework, outperform traditional methods across multiple benchmarks. This\napproach underscores the critical role of instruction-response alignment in\nenabling scalable and high-quality instruction tuning for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning has enabled large language models (LLMs) to achieve\nremarkable performance, but its success heavily depends on the availability of\nlarge-scale, high-quality instruction-response pairs. However, current methods\nfor scaling up data generation often overlook a crucial aspect: the alignment\nbetween instructions and responses. We hypothesize that high-quality\ninstruction-response pairs are not defined by the individual quality of each\ncomponent, but by the extent of their alignment with each other. To address\nthis, we propose a Mutual Alignment Framework (MAIN) that ensures coherence\nbetween the instruction and response through mutual constraints. Experiments\ndemonstrate that models such as LLaMA and Mistral, fine-tuned within this\nframework, outperform traditional methods across multiple benchmarks. This\napproach underscores the critical role of instruction-response alignment in\nenabling scalable and high-quality instruction tuning for LLMs."
                },
                "authors": [
                    {
                        "name": "Fanyi Yang"
                    },
                    {
                        "name": "Jianfeng Liu"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Haoyu Liu"
                    },
                    {
                        "name": "Xixin Cao"
                    },
                    {
                        "name": "Yuefeng Zhan"
                    },
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Weiwei Deng"
                    },
                    {
                        "name": "Feng Sun"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12913v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12913v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12911v1",
                "updated": "2025-04-17T13:01:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    13,
                    1,
                    38,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T13:01:38Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    13,
                    1,
                    38,
                    3,
                    107,
                    0
                ],
                "title": "Benchmarking Multi-National Value Alignment for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Multi-National Value Alignment for Large Language Models"
                },
                "summary": "Do Large Language Models (LLMs) hold positions that conflict with your\ncountry's values? Occasionally they do! However, existing works primarily focus\non ethical reviews, failing to capture the diversity of national values, which\nencompass broader policy, legal, and moral considerations. Furthermore, current\nbenchmarks that rely on spectrum tests using manually designed questionnaires\nare not easily scalable.\n  To address these limitations, we introduce NaVAB, a comprehensive benchmark\nto evaluate the alignment of LLMs with the values of five major nations: China,\nthe United States, the United Kingdom, France, and Germany. NaVAB implements a\nnational value extraction pipeline to efficiently construct value assessment\ndatasets. Specifically, we propose a modeling procedure with instruction\ntagging to process raw data sources, a screening process to filter\nvalue-related topics and a generation process with a Conflict Reduction\nmechanism to filter non-conflicting values.We conduct extensive experiments on\nvarious LLMs across countries, and the results provide insights into assisting\nin the identification of misaligned scenarios. Moreover, we demonstrate that\nNaVAB can be combined with alignment techniques to effectively reduce value\nconcerns by aligning LLMs' values with the target country.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models (LLMs) hold positions that conflict with your\ncountry's values? Occasionally they do! However, existing works primarily focus\non ethical reviews, failing to capture the diversity of national values, which\nencompass broader policy, legal, and moral considerations. Furthermore, current\nbenchmarks that rely on spectrum tests using manually designed questionnaires\nare not easily scalable.\n  To address these limitations, we introduce NaVAB, a comprehensive benchmark\nto evaluate the alignment of LLMs with the values of five major nations: China,\nthe United States, the United Kingdom, France, and Germany. NaVAB implements a\nnational value extraction pipeline to efficiently construct value assessment\ndatasets. Specifically, we propose a modeling procedure with instruction\ntagging to process raw data sources, a screening process to filter\nvalue-related topics and a generation process with a Conflict Reduction\nmechanism to filter non-conflicting values.We conduct extensive experiments on\nvarious LLMs across countries, and the results provide insights into assisting\nin the identification of misaligned scenarios. Moreover, we demonstrate that\nNaVAB can be combined with alignment techniques to effectively reduce value\nconcerns by aligning LLMs' values with the target country."
                },
                "authors": [
                    {
                        "name": "Chengyi Ju"
                    },
                    {
                        "name": "Weijie Shi"
                    },
                    {
                        "name": "Chengzhong Liu"
                    },
                    {
                        "name": "Jiaming Ji"
                    },
                    {
                        "name": "Jipeng Zhang"
                    },
                    {
                        "name": "Ruiyuan Zhang"
                    },
                    {
                        "name": "Jia Zhu"
                    },
                    {
                        "name": "Jiajie Xu"
                    },
                    {
                        "name": "Yaodong Yang"
                    },
                    {
                        "name": "Sirui Han"
                    },
                    {
                        "name": "Yike Guo"
                    }
                ],
                "author_detail": {
                    "name": "Yike Guo"
                },
                "author": "Yike Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12898v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12898v1",
                "updated": "2025-04-17T12:39:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    12,
                    39,
                    25,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T12:39:25Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    12,
                    39,
                    25,
                    3,
                    107,
                    0
                ],
                "title": "Information Gain-Guided Causal Intervention for Autonomous Debiasing\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information Gain-Guided Causal Intervention for Autonomous Debiasing\n  Large Language Models"
                },
                "summary": "Despite significant progress, recent studies indicate that current large\nlanguage models (LLMs) may still capture dataset biases and utilize them during\ninference, leading to the poor generalizability of LLMs. However, due to the\ndiversity of dataset biases and the insufficient nature of bias suppression\nbased on in-context learning, the effectiveness of previous prior\nknowledge-based debiasing methods and in-context learning based automatic\ndebiasing methods is limited. To address these challenges, we explore the\ncombination of causal mechanisms with information theory and propose an\ninformation gain-guided causal intervention debiasing (IGCIDB) framework. This\nframework first utilizes an information gain-guided causal intervention method\nto automatically and autonomously balance the distribution of\ninstruction-tuning dataset. Subsequently, it employs a standard supervised\nfine-tuning process to train LLMs on the debiased dataset. Experimental results\nshow that IGCIDB can effectively debias LLM to improve its generalizability\nacross different tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant progress, recent studies indicate that current large\nlanguage models (LLMs) may still capture dataset biases and utilize them during\ninference, leading to the poor generalizability of LLMs. However, due to the\ndiversity of dataset biases and the insufficient nature of bias suppression\nbased on in-context learning, the effectiveness of previous prior\nknowledge-based debiasing methods and in-context learning based automatic\ndebiasing methods is limited. To address these challenges, we explore the\ncombination of causal mechanisms with information theory and propose an\ninformation gain-guided causal intervention debiasing (IGCIDB) framework. This\nframework first utilizes an information gain-guided causal intervention method\nto automatically and autonomously balance the distribution of\ninstruction-tuning dataset. Subsequently, it employs a standard supervised\nfine-tuning process to train LLMs on the debiased dataset. Experimental results\nshow that IGCIDB can effectively debias LLM to improve its generalizability\nacross different tasks."
                },
                "authors": [
                    {
                        "name": "Zhouhao Sun"
                    },
                    {
                        "name": "Xiao Ding"
                    },
                    {
                        "name": "Li Du"
                    },
                    {
                        "name": "Yunpeng Xu"
                    },
                    {
                        "name": "Yixuan Ma"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Bing Qin"
                    },
                    {
                        "name": "Ting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ting Liu"
                },
                "author": "Ting Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12898v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12898v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10090v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10090v2",
                "updated": "2025-04-17T12:33:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    12,
                    33,
                    59,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-14T10:53:44Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    10,
                    53,
                    44,
                    0,
                    104,
                    0
                ],
                "title": "CameraBench: Benchmarking Visual Reasoning in MLLMs via Photography",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CameraBench: Benchmarking Visual Reasoning in MLLMs via Photography"
                },
                "summary": "Large language models (LLMs) and multimodal large language models (MLLMs)\nhave significantly advanced artificial intelligence. However, visual reasoning,\nreasoning involving both visual and textual inputs, remains underexplored.\nRecent advancements, including the reasoning models like OpenAI o1 and Gemini\n2.0 Flash Thinking, which incorporate image inputs, have opened this\ncapability. In this ongoing work, we focus specifically on photography-related\ntasks because a photo is a visual snapshot of the physical world where the\nunderlying physics (i.e., illumination, blur extent, etc.) interplay with the\ncamera parameters. Successfully reasoning from the visual information of a\nphoto to identify these numerical camera settings requires the MLLMs to have a\ndeeper understanding of the underlying physics for precise visual\ncomprehension, representing a challenging and intelligent capability essential\nfor practical applications like photography assistant agents. We aim to\nevaluate MLLMs on their ability to distinguish visual differences related to\nnumerical camera settings, extending a methodology previously proposed for\nvision-language models (VLMs). Our preliminary results demonstrate the\nimportance of visual reasoning in photography-related tasks. Moreover, these\nresults show that no single MLLM consistently dominates across all evaluation\ntasks, demonstrating ongoing challenges and opportunities in developing MLLMs\nwith better visual reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) and multimodal large language models (MLLMs)\nhave significantly advanced artificial intelligence. However, visual reasoning,\nreasoning involving both visual and textual inputs, remains underexplored.\nRecent advancements, including the reasoning models like OpenAI o1 and Gemini\n2.0 Flash Thinking, which incorporate image inputs, have opened this\ncapability. In this ongoing work, we focus specifically on photography-related\ntasks because a photo is a visual snapshot of the physical world where the\nunderlying physics (i.e., illumination, blur extent, etc.) interplay with the\ncamera parameters. Successfully reasoning from the visual information of a\nphoto to identify these numerical camera settings requires the MLLMs to have a\ndeeper understanding of the underlying physics for precise visual\ncomprehension, representing a challenging and intelligent capability essential\nfor practical applications like photography assistant agents. We aim to\nevaluate MLLMs on their ability to distinguish visual differences related to\nnumerical camera settings, extending a methodology previously proposed for\nvision-language models (VLMs). Our preliminary results demonstrate the\nimportance of visual reasoning in photography-related tasks. Moreover, these\nresults show that no single MLLM consistently dominates across all evaluation\ntasks, demonstrating ongoing challenges and opportunities in developing MLLMs\nwith better visual reasoning."
                },
                "authors": [
                    {
                        "name": "I-Sheng Fang"
                    },
                    {
                        "name": "Jun-Cheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jun-Cheng Chen"
                },
                "author": "Jun-Cheng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10090v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10090v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12229v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12229v3",
                "updated": "2025-04-17T11:50:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    11,
                    50,
                    59,
                    3,
                    107,
                    0
                ],
                "published": "2024-10-16T04:44:34Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    4,
                    44,
                    34,
                    2,
                    290,
                    0
                ],
                "title": "Comprehending Knowledge Graphs with Large Language Models for\n  Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comprehending Knowledge Graphs with Large Language Models for\n  Recommender Systems"
                },
                "summary": "In recent years, the introduction of knowledge graphs (KGs) has significantly\nadvanced recommender systems by facilitating the discovery of potential\nassociations between items. However, existing methods still face several\nlimitations. First, most KGs suffer from missing facts or limited scopes.\nSecond, existing methods convert textual information in KGs into IDs, resulting\nin the loss of natural semantic connections between different items. Third,\nexisting methods struggle to capture high-order connections in the global KG.\nTo address these limitations, we propose a novel method called CoLaKG, which\nleverages large language models (LLMs) to improve KG-based recommendations. The\nextensive knowledge and remarkable reasoning capabilities of LLMs enable our\nmethod to supplement missing facts in KGs, and their powerful text\nunderstanding abilities allow for better utilization of semantic information.\nSpecifically, CoLaKG extracts useful information from KGs at both local and\nglobal levels. By employing the item-centered subgraph extraction and prompt\nengineering, it can accurately understand the local information. In addition,\nthrough the semantic-based retrieval module, each item is enriched by related\nitems from the entire knowledge graph, effectively harnessing global\ninformation. Furthermore, the local and global information are effectively\nintegrated into the recommendation model through a representation fusion module\nand a retrieval-augmented representation learning module, respectively.\nExtensive experiments on four real-world datasets demonstrate the superiority\nof our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the introduction of knowledge graphs (KGs) has significantly\nadvanced recommender systems by facilitating the discovery of potential\nassociations between items. However, existing methods still face several\nlimitations. First, most KGs suffer from missing facts or limited scopes.\nSecond, existing methods convert textual information in KGs into IDs, resulting\nin the loss of natural semantic connections between different items. Third,\nexisting methods struggle to capture high-order connections in the global KG.\nTo address these limitations, we propose a novel method called CoLaKG, which\nleverages large language models (LLMs) to improve KG-based recommendations. The\nextensive knowledge and remarkable reasoning capabilities of LLMs enable our\nmethod to supplement missing facts in KGs, and their powerful text\nunderstanding abilities allow for better utilization of semantic information.\nSpecifically, CoLaKG extracts useful information from KGs at both local and\nglobal levels. By employing the item-centered subgraph extraction and prompt\nengineering, it can accurately understand the local information. In addition,\nthrough the semantic-based retrieval module, each item is enriched by related\nitems from the entire knowledge graph, effectively harnessing global\ninformation. Furthermore, the local and global information are effectively\nintegrated into the recommendation model through a representation fusion module\nand a retrieval-augmented representation learning module, respectively.\nExtensive experiments on four real-world datasets demonstrate the superiority\nof our method."
                },
                "authors": [
                    {
                        "name": "Ziqiang Cui"
                    },
                    {
                        "name": "Yunpeng Weng"
                    },
                    {
                        "name": "Xing Tang"
                    },
                    {
                        "name": "Fuyuan Lyu"
                    },
                    {
                        "name": "Dugang Liu"
                    },
                    {
                        "name": "Xiuqiang He"
                    },
                    {
                        "name": "Chen Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ma"
                },
                "author": "Chen Ma",
                "arxiv_doi": "10.1145/3726302.3729932",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3729932",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.12229v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12229v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted as a full paper by SIGIR'25",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12867v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12867v2",
                "updated": "2025-04-18T08:18:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    8,
                    18,
                    11,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-17T11:50:04Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    11,
                    50,
                    4,
                    3,
                    107,
                    0
                ],
                "title": "EmoVoice: LLM-based Emotional Text-To-Speech Model with Freestyle Text\n  Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EmoVoice: LLM-based Emotional Text-To-Speech Model with Freestyle Text\n  Prompting"
                },
                "summary": "Human speech goes beyond the mere transfer of information; it is a profound\nexchange of emotions and a connection between individuals. While Text-to-Speech\n(TTS) models have made huge progress, they still face challenges in controlling\nthe emotional expression in the generated speech. In this work, we propose\nEmoVoice, a novel emotion-controllable TTS model that exploits large language\nmodels (LLMs) to enable fine-grained freestyle natural language emotion\ncontrol, and a phoneme boost variant design that makes the model output phoneme\ntokens and audio tokens in parallel to enhance content consistency, inspired by\nchain-of-thought (CoT) and chain-of-modality (CoM) techniques. Besides, we\nintroduce EmoVoice-DB, a high-quality 40-hour English emotion dataset featuring\nexpressive speech and fine-grained emotion labels with natural language\ndescriptions. EmoVoice achieves state-of-the-art performance on the English\nEmoVoice-DB test set using only synthetic training data, and on the Chinese\nSecap test set using our in-house data. We further investigate the reliability\nof existing emotion evaluation metrics and their alignment with human\nperceptual preferences, and explore using SOTA multimodal LLMs GPT-4o-audio and\nGemini to assess emotional speech. Demo samples are available at\nhttps://anonymous.4open.science/r/EmoVoice-DF55. Dataset, code, and checkpoints\nwill be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human speech goes beyond the mere transfer of information; it is a profound\nexchange of emotions and a connection between individuals. While Text-to-Speech\n(TTS) models have made huge progress, they still face challenges in controlling\nthe emotional expression in the generated speech. In this work, we propose\nEmoVoice, a novel emotion-controllable TTS model that exploits large language\nmodels (LLMs) to enable fine-grained freestyle natural language emotion\ncontrol, and a phoneme boost variant design that makes the model output phoneme\ntokens and audio tokens in parallel to enhance content consistency, inspired by\nchain-of-thought (CoT) and chain-of-modality (CoM) techniques. Besides, we\nintroduce EmoVoice-DB, a high-quality 40-hour English emotion dataset featuring\nexpressive speech and fine-grained emotion labels with natural language\ndescriptions. EmoVoice achieves state-of-the-art performance on the English\nEmoVoice-DB test set using only synthetic training data, and on the Chinese\nSecap test set using our in-house data. We further investigate the reliability\nof existing emotion evaluation metrics and their alignment with human\nperceptual preferences, and explore using SOTA multimodal LLMs GPT-4o-audio and\nGemini to assess emotional speech. Demo samples are available at\nhttps://anonymous.4open.science/r/EmoVoice-DF55. Dataset, code, and checkpoints\nwill be released."
                },
                "authors": [
                    {
                        "name": "Guanrou Yang"
                    },
                    {
                        "name": "Chen Yang"
                    },
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Ziyang Ma"
                    },
                    {
                        "name": "Wenxi Chen"
                    },
                    {
                        "name": "Wen Wang"
                    },
                    {
                        "name": "Tianrui Wang"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Zhikang Niu"
                    },
                    {
                        "name": "Wenrui Liu"
                    },
                    {
                        "name": "Fan Yu"
                    },
                    {
                        "name": "Zhihao Du"
                    },
                    {
                        "name": "Zhifu Gao"
                    },
                    {
                        "name": "ShiLiang Zhang"
                    },
                    {
                        "name": "Xie Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xie Chen"
                },
                "author": "Xie Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12867v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12867v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12865v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12865v1",
                "updated": "2025-04-17T11:46:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    11,
                    46,
                    17,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T11:46:17Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    11,
                    46,
                    17,
                    3,
                    107,
                    0
                ],
                "title": "DashChat: Interactive Authoring of Industrial Dashboard Design\n  Prototypes through Conversation with LLM-Powered Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DashChat: Interactive Authoring of Industrial Dashboard Design\n  Prototypes through Conversation with LLM-Powered Agents"
                },
                "summary": "Industrial dashboards, commonly deployed by organizations such as enterprises\nand governments, are increasingly crucial in data communication and\ndecision-making support across various domains. Designing an industrial\ndashboard prototype is particularly challenging due to its visual complexity,\nwhich can include data visualization, layout configuration, embellishments, and\nanimations. Additionally, in real-world industrial settings, designers often\nencounter numerous constraints. For instance, when companies negotiate\ncollaborations with clients and determine design plans, they typically need to\ndemo design prototypes and iterate on them based on mock data quickly. Such a\ntask is very common and crucial during the ideation stage, as it not only helps\nsave developmental costs but also avoids data-related issues such as lengthy\ndata handover periods. However, existing authoring tools of dashboards are\nmostly not tailored to such prototyping needs, and motivated by these gaps, we\npropose DashChat, an interactive system that leverages large language models\n(LLMs) to generate industrial dashboard design prototypes from natural\nlanguage. We collaborated closely with designers from the industry and derived\nthe requirements based on their practical experience. First, by analyzing 114\nhigh-quality industrial dashboards, we summarized their common design patterns\nand inject the identified ones into LLMs as reference. Next, we built a\nmulti-agent pipeline powered by LLMs to understand textual requirements from\nusers and generate practical, aesthetic prototypes. Besides, functionally\ndistinct, parallel-operating agents are created to enable efficient generation.\nThen, we developed a user-friendly interface that supports text-based\ninteraction for generating and modifying prototypes. Two user studies\ndemonstrated that our system is both effective and efficient in supporting\ndesign prototyping.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Industrial dashboards, commonly deployed by organizations such as enterprises\nand governments, are increasingly crucial in data communication and\ndecision-making support across various domains. Designing an industrial\ndashboard prototype is particularly challenging due to its visual complexity,\nwhich can include data visualization, layout configuration, embellishments, and\nanimations. Additionally, in real-world industrial settings, designers often\nencounter numerous constraints. For instance, when companies negotiate\ncollaborations with clients and determine design plans, they typically need to\ndemo design prototypes and iterate on them based on mock data quickly. Such a\ntask is very common and crucial during the ideation stage, as it not only helps\nsave developmental costs but also avoids data-related issues such as lengthy\ndata handover periods. However, existing authoring tools of dashboards are\nmostly not tailored to such prototyping needs, and motivated by these gaps, we\npropose DashChat, an interactive system that leverages large language models\n(LLMs) to generate industrial dashboard design prototypes from natural\nlanguage. We collaborated closely with designers from the industry and derived\nthe requirements based on their practical experience. First, by analyzing 114\nhigh-quality industrial dashboards, we summarized their common design patterns\nand inject the identified ones into LLMs as reference. Next, we built a\nmulti-agent pipeline powered by LLMs to understand textual requirements from\nusers and generate practical, aesthetic prototypes. Besides, functionally\ndistinct, parallel-operating agents are created to enable efficient generation.\nThen, we developed a user-friendly interface that supports text-based\ninteraction for generating and modifying prototypes. Two user studies\ndemonstrated that our system is both effective and efficient in supporting\ndesign prototyping."
                },
                "authors": [
                    {
                        "name": "S. Shen"
                    },
                    {
                        "name": "Z. Lin"
                    },
                    {
                        "name": "W. Liu"
                    },
                    {
                        "name": "C. Xin"
                    },
                    {
                        "name": "W. Dai"
                    },
                    {
                        "name": "S. Chen"
                    },
                    {
                        "name": "X. Wen"
                    },
                    {
                        "name": "X. Lan"
                    }
                ],
                "author_detail": {
                    "name": "X. Lan"
                },
                "author": "X. Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12865v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20268v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20268v2",
                "updated": "2025-04-17T11:32:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    11,
                    32,
                    53,
                    3,
                    107,
                    0
                ],
                "published": "2025-02-27T16:55:18Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    55,
                    18,
                    3,
                    58,
                    0
                ],
                "title": "Large Language Models as Attribution Regularizers for Efficient Model\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models as Attribution Regularizers for Efficient Model\n  Training"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\ndiverse domains. However, effectively leveraging their vast knowledge for\ntraining smaller downstream models remains an open challenge, especially in\ndomains like tabular data learning, where simpler models are often preferred\ndue to interpretability and efficiency.\n  In this paper, we introduce a novel yet straightforward method for\nincorporating LLM-generated global task feature attributions into the training\nprocess of smaller networks. Specifically, we propose an attribution-matching\nregularization term that aligns the training dynamics of the smaller model with\nthe insights provided by the LLM. By doing so, our approach yields superior\nperformance in few-shot learning scenarios. Notably, our method requires only\nblack-box API access to the LLM, making it easy to integrate into existing\ntraining pipelines with minimal computational overhead.\n  Furthermore, we demonstrate how this method can be used to address common\nissues in real-world datasets, such as skewness and bias. By integrating\nhigh-level knowledge from LLMs, our approach improves generalization, even when\ntraining data is limited or imbalanced. We validate its effectiveness through\nextensive experiments across multiple tasks, demonstrating improved learning\nefficiency and model robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable performance across\ndiverse domains. However, effectively leveraging their vast knowledge for\ntraining smaller downstream models remains an open challenge, especially in\ndomains like tabular data learning, where simpler models are often preferred\ndue to interpretability and efficiency.\n  In this paper, we introduce a novel yet straightforward method for\nincorporating LLM-generated global task feature attributions into the training\nprocess of smaller networks. Specifically, we propose an attribution-matching\nregularization term that aligns the training dynamics of the smaller model with\nthe insights provided by the LLM. By doing so, our approach yields superior\nperformance in few-shot learning scenarios. Notably, our method requires only\nblack-box API access to the LLM, making it easy to integrate into existing\ntraining pipelines with minimal computational overhead.\n  Furthermore, we demonstrate how this method can be used to address common\nissues in real-world datasets, such as skewness and bias. By integrating\nhigh-level knowledge from LLMs, our approach improves generalization, even when\ntraining data is limited or imbalanced. We validate its effectiveness through\nextensive experiments across multiple tasks, demonstrating improved learning\nefficiency and model robustness."
                },
                "authors": [
                    {
                        "name": "Davor Vukadin"
                    },
                    {
                        "name": "Marin Šilić"
                    },
                    {
                        "name": "Goran Delač"
                    }
                ],
                "author_detail": {
                    "name": "Goran Delač"
                },
                "author": "Goran Delač",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20268v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20268v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00046v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00046v2",
                "updated": "2025-04-17T11:29:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    11,
                    29,
                    6,
                    3,
                    107,
                    0
                ],
                "published": "2025-03-30T22:53:52Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    22,
                    53,
                    52,
                    6,
                    89,
                    0
                ],
                "title": "Multi-Stakeholder Disaster Insights from Social Media Using Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Stakeholder Disaster Insights from Social Media Using Large\n  Language Models"
                },
                "summary": "In recent years, social media has emerged as a primary channel for users to\npromptly share feedback and issues during disasters and emergencies, playing a\nkey role in crisis management. While significant progress has been made in\ncollecting and analyzing social media content, there remains a pressing need to\nenhance the automation, aggregation, and customization of this data to deliver\nactionable insights tailored to diverse stakeholders, including the press,\npolice, EMS, and firefighters. This effort is essential for improving the\ncoordination of activities such as relief efforts, resource distribution, and\nmedia communication. This paper presents a methodology that leverages the\ncapabilities of LLMs to enhance disaster response and management. Our approach\ncombines classification techniques with generative AI to bridge the gap between\nraw user feedback and stakeholder-specific reports. Social media posts shared\nduring catastrophic events are analyzed with a focus on user-reported issues,\nservice interruptions, and encountered challenges. We employ full-spectrum\nLLMs, using analytical models like BERT for precise, multi-dimensional\nclassification of content type, sentiment, emotion, geolocation, and topic.\nGenerative models such as ChatGPT are then used to produce human-readable,\ninformative reports tailored to distinct audiences, synthesizing insights\nderived from detailed classifications. We compare standard approaches, which\nanalyze posts directly using prompts in ChatGPT, to our advanced method, which\nincorporates multi-dimensional classification, sub-event selection, and\ntailored report generation. Our methodology demonstrates superior performance\nin both quantitative metrics, such as text coherence scores and latent\nrepresentations, and qualitative assessments by automated tools and field\nexperts, delivering precise insights for diverse disaster response\nstakeholders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, social media has emerged as a primary channel for users to\npromptly share feedback and issues during disasters and emergencies, playing a\nkey role in crisis management. While significant progress has been made in\ncollecting and analyzing social media content, there remains a pressing need to\nenhance the automation, aggregation, and customization of this data to deliver\nactionable insights tailored to diverse stakeholders, including the press,\npolice, EMS, and firefighters. This effort is essential for improving the\ncoordination of activities such as relief efforts, resource distribution, and\nmedia communication. This paper presents a methodology that leverages the\ncapabilities of LLMs to enhance disaster response and management. Our approach\ncombines classification techniques with generative AI to bridge the gap between\nraw user feedback and stakeholder-specific reports. Social media posts shared\nduring catastrophic events are analyzed with a focus on user-reported issues,\nservice interruptions, and encountered challenges. We employ full-spectrum\nLLMs, using analytical models like BERT for precise, multi-dimensional\nclassification of content type, sentiment, emotion, geolocation, and topic.\nGenerative models such as ChatGPT are then used to produce human-readable,\ninformative reports tailored to distinct audiences, synthesizing insights\nderived from detailed classifications. We compare standard approaches, which\nanalyze posts directly using prompts in ChatGPT, to our advanced method, which\nincorporates multi-dimensional classification, sub-event selection, and\ntailored report generation. Our methodology demonstrates superior performance\nin both quantitative metrics, such as text coherence scores and latent\nrepresentations, and qualitative assessments by automated tools and field\nexperts, delivering precise insights for diverse disaster response\nstakeholders."
                },
                "authors": [
                    {
                        "name": "Loris Belcastro"
                    },
                    {
                        "name": "Cristian Cosentino"
                    },
                    {
                        "name": "Fabrizio Marozzo"
                    },
                    {
                        "name": "Merve Gündüz-Cüre"
                    },
                    {
                        "name": "Sule Öztürk-Birim"
                    }
                ],
                "author_detail": {
                    "name": "Sule Öztürk-Birim"
                },
                "author": "Sule Öztürk-Birim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00046v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00046v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12845v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12845v1",
                "updated": "2025-04-17T11:02:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    11,
                    2,
                    35,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T11:02:35Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    11,
                    2,
                    35,
                    3,
                    107,
                    0
                ],
                "title": "Can LLMs reason over extended multilingual contexts? Towards\n  long-context evaluation beyond retrieval and haystacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs reason over extended multilingual contexts? Towards\n  long-context evaluation beyond retrieval and haystacks"
                },
                "summary": "Existing multilingual long-context benchmarks, often based on the popular\nneedle-in-a-haystack test, primarily evaluate a model's ability to locate\nspecific information buried within irrelevant texts. However, such a\nretrieval-centric approach is myopic and inherently limited, as successful\nrecall alone does not indicate a model's capacity to reason over extended\ncontexts. Moreover, these benchmarks are susceptible to data leakage,\nshort-circuiting, and risk making the evaluation a priori identifiable. To\naddress these limitations, we introduce MLRBench, a new synthetic benchmark for\nmultilingual long-context reasoning. Unlike existing benchmarks, MLRBench goes\nbeyond surface-level retrieval by including tasks that assess multi-hop\ninference, aggregation, and epistemic reasoning. Spanning seven languages,\nMLRBench is designed to be parallel, resistant to leakage, and scalable to\narbitrary context lengths. Our extensive experiments with an open-weight large\nlanguage model (LLM) reveal a pronounced gap between high- and low-resource\nlanguages, particularly for tasks requiring the model to aggregate multiple\nfacts or predict the absence of information. We also find that, in multilingual\nsettings, LLMs effectively utilize less than 30% of their claimed context\nlength. Although off-the-shelf Retrieval Augmented Generation helps alleviate\nthis to a certain extent, it does not solve the long-context problem. We\nopen-source MLRBench to enable future research in improved evaluation and\ntraining of multilingual LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing multilingual long-context benchmarks, often based on the popular\nneedle-in-a-haystack test, primarily evaluate a model's ability to locate\nspecific information buried within irrelevant texts. However, such a\nretrieval-centric approach is myopic and inherently limited, as successful\nrecall alone does not indicate a model's capacity to reason over extended\ncontexts. Moreover, these benchmarks are susceptible to data leakage,\nshort-circuiting, and risk making the evaluation a priori identifiable. To\naddress these limitations, we introduce MLRBench, a new synthetic benchmark for\nmultilingual long-context reasoning. Unlike existing benchmarks, MLRBench goes\nbeyond surface-level retrieval by including tasks that assess multi-hop\ninference, aggregation, and epistemic reasoning. Spanning seven languages,\nMLRBench is designed to be parallel, resistant to leakage, and scalable to\narbitrary context lengths. Our extensive experiments with an open-weight large\nlanguage model (LLM) reveal a pronounced gap between high- and low-resource\nlanguages, particularly for tasks requiring the model to aggregate multiple\nfacts or predict the absence of information. We also find that, in multilingual\nsettings, LLMs effectively utilize less than 30% of their claimed context\nlength. Although off-the-shelf Retrieval Augmented Generation helps alleviate\nthis to a certain extent, it does not solve the long-context problem. We\nopen-source MLRBench to enable future research in improved evaluation and\ntraining of multilingual LLMs."
                },
                "authors": [
                    {
                        "name": "Amey Hengle"
                    },
                    {
                        "name": "Prasoon Bajpai"
                    },
                    {
                        "name": "Soham Dan"
                    },
                    {
                        "name": "Tanmoy Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Chakraborty"
                },
                "author": "Tanmoy Chakraborty",
                "arxiv_comment": "33 Pages in Total - 23 (Main Manuscript) + 10 (Appendix)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12845v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12845v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20245v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20245v4",
                "updated": "2025-04-17T10:56:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    10,
                    56,
                    59,
                    3,
                    107,
                    0
                ],
                "published": "2024-09-30T12:34:04Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    12,
                    34,
                    4,
                    0,
                    274,
                    0
                ],
                "title": "A Framework for Holistic KLD-based Waveform Design for\n  Multi-User-Multi-Target ISAC Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Holistic KLD-based Waveform Design for\n  Multi-User-Multi-Target ISAC Systems"
                },
                "summary": "This paper introduces a novel framework aimed at designing integrated\nwaveforms for robust integrated sensing and communication systems. The system\nmodel consists of a multiple-input multiple-output (MIMO) base station that\nsimultaneously serves communication user equipments and detects multiple\ntargets using a shared-antenna deployment scenario. By leveraging\nKullback-Leibler divergence to holistically characterise both communication and\nsensing subsystems, three optimisation problems are formulated: (i) radar\nwaveform KLD maximisation under communication constraints, (ii) communication\nwaveform KLD maximisation subject to radar KLD requirements, and (iii) an\nintegrated waveform KLD-based optimisation for ISAC that jointly balances both\nsubsystems. The first two problems are solved using a projected gradient method\nwith adaptive penalties for the radar waveforms and a gradient assisted\ninterior point method for the communication waveforms. The third, integrated\nwaveform optimisation approach adopts an alternating direction method of\nmultipliers framework to unify radar and communication waveform designs into a\nsingle integrated optimisation, thereby strengthening the trade-off between\nsensing and communication objectives and achieving higher overall performance\nthan the individual radar- or communication-only techniques. Unlike most\nexisting ISAC waveform designs that regard communication signals solely as\ninterference for sensing, the proposed framework utilises the holistic ISAC\nwaveform-that is, the superimposed communication and sensing signals to boost\ndetection performance in the radar subsystem. Simulation results show\nsignificant improvements in both radar detection and communication reliability\ncompared with conventional zero-forcing beamforming and identity covariance\nradar baselines, demonstrating the promise of KLD-based waveform designs for\nnext-generation ISAC networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel framework aimed at designing integrated\nwaveforms for robust integrated sensing and communication systems. The system\nmodel consists of a multiple-input multiple-output (MIMO) base station that\nsimultaneously serves communication user equipments and detects multiple\ntargets using a shared-antenna deployment scenario. By leveraging\nKullback-Leibler divergence to holistically characterise both communication and\nsensing subsystems, three optimisation problems are formulated: (i) radar\nwaveform KLD maximisation under communication constraints, (ii) communication\nwaveform KLD maximisation subject to radar KLD requirements, and (iii) an\nintegrated waveform KLD-based optimisation for ISAC that jointly balances both\nsubsystems. The first two problems are solved using a projected gradient method\nwith adaptive penalties for the radar waveforms and a gradient assisted\ninterior point method for the communication waveforms. The third, integrated\nwaveform optimisation approach adopts an alternating direction method of\nmultipliers framework to unify radar and communication waveform designs into a\nsingle integrated optimisation, thereby strengthening the trade-off between\nsensing and communication objectives and achieving higher overall performance\nthan the individual radar- or communication-only techniques. Unlike most\nexisting ISAC waveform designs that regard communication signals solely as\ninterference for sensing, the proposed framework utilises the holistic ISAC\nwaveform-that is, the superimposed communication and sensing signals to boost\ndetection performance in the radar subsystem. Simulation results show\nsignificant improvements in both radar detection and communication reliability\ncompared with conventional zero-forcing beamforming and identity covariance\nradar baselines, demonstrating the promise of KLD-based waveform designs for\nnext-generation ISAC networks."
                },
                "authors": [
                    {
                        "name": "Yousef Kloob"
                    },
                    {
                        "name": "Mohammad Al-Jarrah"
                    },
                    {
                        "name": "Emad Alsusa"
                    }
                ],
                "author_detail": {
                    "name": "Emad Alsusa"
                },
                "author": "Emad Alsusa",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20245v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20245v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12805v1",
                "updated": "2025-04-17T10:10:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    10,
                    10,
                    25,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T10:10:25Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    10,
                    10,
                    25,
                    3,
                    107,
                    0
                ],
                "title": "Assesing LLMs in Art Contexts: Critique Generation and Theory of Mind\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assesing LLMs in Art Contexts: Critique Generation and Theory of Mind\n  Evaluation"
                },
                "summary": "This study explored how large language models (LLMs) perform in two areas\nrelated to art: writing critiques of artworks and reasoning about mental states\n(Theory of Mind, or ToM) in art-related situations. For the critique generation\npart, we built a system that combines Noel Carroll's evaluative framework with\na broad selection of art criticism theories. The model was prompted to first\nwrite a full-length critique and then shorter, more coherent versions using a\nstep-by-step prompting process. These AI-generated critiques were then compared\nwith those written by human experts in a Turing test-style evaluation. In many\ncases, human subjects had difficulty telling which was which, and the results\nsuggest that LLMs can produce critiques that are not only plausible in style\nbut also rich in interpretation, as long as they are carefully guided. In the\nsecond part, we introduced new simple ToM tasks based on situations involving\ninterpretation, emotion, and moral tension, which can appear in the context of\nart. These go beyond standard false-belief tests and allow for more complex,\nsocially embedded forms of reasoning. We tested 41 recent LLMs and found that\ntheir performance varied across tasks and models. In particular, tasks that\ninvolved affective or ambiguous situations tended to reveal clearer\ndifferences. Taken together, these results help clarify how LLMs respond to\ncomplex interpretative challenges, revealing both their cognitive limitations\nand potential. While our findings do not directly contradict the so-called\nGenerative AI Paradox--the idea that LLMs can produce expert-like output\nwithout genuine understanding--they suggest that, depending on how LLMs are\ninstructed, such as through carefully designed prompts, these models may begin\nto show behaviors that resemble understanding more closely than we might\nassume.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explored how large language models (LLMs) perform in two areas\nrelated to art: writing critiques of artworks and reasoning about mental states\n(Theory of Mind, or ToM) in art-related situations. For the critique generation\npart, we built a system that combines Noel Carroll's evaluative framework with\na broad selection of art criticism theories. The model was prompted to first\nwrite a full-length critique and then shorter, more coherent versions using a\nstep-by-step prompting process. These AI-generated critiques were then compared\nwith those written by human experts in a Turing test-style evaluation. In many\ncases, human subjects had difficulty telling which was which, and the results\nsuggest that LLMs can produce critiques that are not only plausible in style\nbut also rich in interpretation, as long as they are carefully guided. In the\nsecond part, we introduced new simple ToM tasks based on situations involving\ninterpretation, emotion, and moral tension, which can appear in the context of\nart. These go beyond standard false-belief tests and allow for more complex,\nsocially embedded forms of reasoning. We tested 41 recent LLMs and found that\ntheir performance varied across tasks and models. In particular, tasks that\ninvolved affective or ambiguous situations tended to reveal clearer\ndifferences. Taken together, these results help clarify how LLMs respond to\ncomplex interpretative challenges, revealing both their cognitive limitations\nand potential. While our findings do not directly contradict the so-called\nGenerative AI Paradox--the idea that LLMs can produce expert-like output\nwithout genuine understanding--they suggest that, depending on how LLMs are\ninstructed, such as through carefully designed prompts, these models may begin\nto show behaviors that resemble understanding more closely than we might\nassume."
                },
                "authors": [
                    {
                        "name": "Takaya Arita"
                    },
                    {
                        "name": "Wenxian Zheng"
                    },
                    {
                        "name": "Reiji Suzuki"
                    },
                    {
                        "name": "Fuminori Akiba"
                    }
                ],
                "author_detail": {
                    "name": "Fuminori Akiba"
                },
                "author": "Fuminori Akiba",
                "arxiv_comment": "30 pages, 13 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12795v1",
                "updated": "2025-04-17T09:56:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    9,
                    56,
                    35,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T09:56:35Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    9,
                    56,
                    35,
                    3,
                    107,
                    0
                ],
                "title": "EarthGPT-X: Enabling MLLMs to Flexibly and Comprehensively Understand\n  Multi-Source Remote Sensing Imagery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EarthGPT-X: Enabling MLLMs to Flexibly and Comprehensively Understand\n  Multi-Source Remote Sensing Imagery"
                },
                "summary": "Recent advances in the visual-language area have developed natural\nmulti-modal large language models (MLLMs) for spatial reasoning through visual\nprompting. However, due to remote sensing (RS) imagery containing abundant\ngeospatial information that differs from natural images, it is challenging to\neffectively adapt natural spatial models to the RS domain. Moreover, current RS\nMLLMs are limited in overly narrow interpretation levels and interaction\nmanner, hindering their applicability in real-world scenarios. To address those\nchallenges, a spatial MLLM named EarthGPT-X is proposed, enabling a\ncomprehensive understanding of multi-source RS imagery, such as optical,\nsynthetic aperture radar (SAR), and infrared. EarthGPT-X offers zoom-in and\nzoom-out insight, and possesses flexible multi-grained interactive abilities.\nMoreover, EarthGPT-X unifies two types of critical spatial tasks (i.e.,\nreferring and grounding) into a visual prompting framework. To achieve these\nversatile capabilities, several key strategies are developed. The first is the\nmulti-modal content integration method, which enhances the interplay between\nimages, visual prompts, and text instructions. Subsequently, a cross-domain\none-stage fusion training strategy is proposed, utilizing the large language\nmodel (LLM) as a unified interface for multi-source multi-task learning.\nFurthermore, by incorporating a pixel perception module, the referring and\ngrounding tasks are seamlessly unified within a single framework. In addition,\nthe experiments conducted demonstrate the superiority of the proposed\nEarthGPT-X in multi-grained tasks and its impressive flexibility in multi-modal\ninteraction, revealing significant advancements of MLLM in the RS field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in the visual-language area have developed natural\nmulti-modal large language models (MLLMs) for spatial reasoning through visual\nprompting. However, due to remote sensing (RS) imagery containing abundant\ngeospatial information that differs from natural images, it is challenging to\neffectively adapt natural spatial models to the RS domain. Moreover, current RS\nMLLMs are limited in overly narrow interpretation levels and interaction\nmanner, hindering their applicability in real-world scenarios. To address those\nchallenges, a spatial MLLM named EarthGPT-X is proposed, enabling a\ncomprehensive understanding of multi-source RS imagery, such as optical,\nsynthetic aperture radar (SAR), and infrared. EarthGPT-X offers zoom-in and\nzoom-out insight, and possesses flexible multi-grained interactive abilities.\nMoreover, EarthGPT-X unifies two types of critical spatial tasks (i.e.,\nreferring and grounding) into a visual prompting framework. To achieve these\nversatile capabilities, several key strategies are developed. The first is the\nmulti-modal content integration method, which enhances the interplay between\nimages, visual prompts, and text instructions. Subsequently, a cross-domain\none-stage fusion training strategy is proposed, utilizing the large language\nmodel (LLM) as a unified interface for multi-source multi-task learning.\nFurthermore, by incorporating a pixel perception module, the referring and\ngrounding tasks are seamlessly unified within a single framework. In addition,\nthe experiments conducted demonstrate the superiority of the proposed\nEarthGPT-X in multi-grained tasks and its impressive flexibility in multi-modal\ninteraction, revealing significant advancements of MLLM in the RS field."
                },
                "authors": [
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Miaoxin Cai"
                    },
                    {
                        "name": "Yaqian Ning"
                    },
                    {
                        "name": "Tong Zhang"
                    },
                    {
                        "name": "Yin Zhuang"
                    },
                    {
                        "name": "He Chen"
                    },
                    {
                        "name": "Jun Li"
                    },
                    {
                        "name": "Xuerui Mao"
                    }
                ],
                "author_detail": {
                    "name": "Xuerui Mao"
                },
                "author": "Xuerui Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12794v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12794v1",
                "updated": "2025-04-17T09:55:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    9,
                    55,
                    3,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T09:55:03Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    9,
                    55,
                    3,
                    3,
                    107,
                    0
                ],
                "title": "Supporting Urban Low-Altitude Economy: Channel Gain Map Inference Based\n  on 3D Conditional GAN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supporting Urban Low-Altitude Economy: Channel Gain Map Inference Based\n  on 3D Conditional GAN"
                },
                "summary": "The advancement of advanced air mobility (AAM) in recent years has given rise\nto the concept of low-altitude economy (LAE). However, the diverse flight\nactivities associated with the emerging LAE applications in urban scenarios\nconfront complex physical environments, which urgently necessitates ubiquitous\nand reliable communication to guarantee the operation safety of the\nlow-altitude aircraft. As one of promising technologies for the sixth\ngeneration (6G) mobile networks, channel knowledge map (CKM) enables the\nenvironment-aware communication by constructing a site-specific dataset,\nthereby providing a priori on-site information for the aircraft to obtain the\nchannel state information (CSI) at arbitrary locations with much reduced online\noverhead. Diverse base station (BS) deployments in the three-dimensional (3D)\nurban low-altitude environment require efficient 3D CKM construction to capture\nspatial channel characteristics with less overhead. Towards this end, this\npaper proposes a 3D channel gain map (CGM) inference method based on a 3D\nconditional generative adversarial network (3D-CGAN). Specifically, we first\nanalyze the potential deployment types of BSs in urban low-altitude scenario,\nand investigate the CGM representation with the corresponding 3D channel gain\nmodel. The framework of the proposed 3D-CGAN is then discussed, which is\ntrained by a dataset consisting of existing CGMs. Consequently, the trained\n3D-CGAN is capable of inferring the corresponding CGM only based on the BS\ncoordinate without additional measurement. The simulation results demonstrate\nthat the CGMs inferred by the proposed 3D-CGAN outperform those of the\nbenchmark schemes, which can accurately reflect the radio propagation condition\nin 3D environment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement of advanced air mobility (AAM) in recent years has given rise\nto the concept of low-altitude economy (LAE). However, the diverse flight\nactivities associated with the emerging LAE applications in urban scenarios\nconfront complex physical environments, which urgently necessitates ubiquitous\nand reliable communication to guarantee the operation safety of the\nlow-altitude aircraft. As one of promising technologies for the sixth\ngeneration (6G) mobile networks, channel knowledge map (CKM) enables the\nenvironment-aware communication by constructing a site-specific dataset,\nthereby providing a priori on-site information for the aircraft to obtain the\nchannel state information (CSI) at arbitrary locations with much reduced online\noverhead. Diverse base station (BS) deployments in the three-dimensional (3D)\nurban low-altitude environment require efficient 3D CKM construction to capture\nspatial channel characteristics with less overhead. Towards this end, this\npaper proposes a 3D channel gain map (CGM) inference method based on a 3D\nconditional generative adversarial network (3D-CGAN). Specifically, we first\nanalyze the potential deployment types of BSs in urban low-altitude scenario,\nand investigate the CGM representation with the corresponding 3D channel gain\nmodel. The framework of the proposed 3D-CGAN is then discussed, which is\ntrained by a dataset consisting of existing CGMs. Consequently, the trained\n3D-CGAN is capable of inferring the corresponding CGM only based on the BS\ncoordinate without additional measurement. The simulation results demonstrate\nthat the CGMs inferred by the proposed 3D-CGAN outperform those of the\nbenchmark schemes, which can accurately reflect the radio propagation condition\nin 3D environment."
                },
                "authors": [
                    {
                        "name": "Yonghao Wang"
                    },
                    {
                        "name": "Ruoguang Li"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Jiaqi Chen"
                    },
                    {
                        "name": "Yong Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Yong Zeng"
                },
                "author": "Yong Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12794v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12794v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06562v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06562v2",
                "updated": "2025-04-17T09:49:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    9,
                    49,
                    43,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-09T03:51:53Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    3,
                    51,
                    53,
                    2,
                    99,
                    0
                ],
                "title": "FuseRL: Dense Preference Optimization for Heterogeneous Model Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FuseRL: Dense Preference Optimization for Heterogeneous Model Fusion"
                },
                "summary": "Heterogeneous model fusion enhances the performance of LLMs by integrating\nthe knowledge and capabilities of multiple structurally diverse models.\nHowever, existing approaches often rely solely on selecting the best output for\neach prompt from source models, which underutilizes their full potential due to\nlimited source knowledge and results in sparse optimization signals. To address\nthis limitation, we propose FuseRL, a novel two-stage framework comprising\nFuseSFT and FusePO to maximize the utilization of source LLMs. FuseSFT\nestablishes a robust initialization by integrating the strengths of\nheterogeneous source models through weighted supervised fine-tuning (SFT) on\ndiverse outputs for each prompt. FusePO optimizes weighted preferences based on\nthe outputs of multiple source models to enable superior alignment performance.\nExtensive experiments demonstrate the effectiveness of our framework across\nvarious preference alignment methods, including RLOO, DPO, and SimPO. Using\nLlama-3.1-8B-Instruct as the target model, our approach achieves\nstate-of-the-art performance among 8B LLMs on the AlpacaEval-2 and Arena-Hard\nbenchmarks. Further analysis suggests that FuseSFT regularizes the training\nprocess to reduce overfitting, while FusePO introduces dense and diverse\nsignals for preference optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous model fusion enhances the performance of LLMs by integrating\nthe knowledge and capabilities of multiple structurally diverse models.\nHowever, existing approaches often rely solely on selecting the best output for\neach prompt from source models, which underutilizes their full potential due to\nlimited source knowledge and results in sparse optimization signals. To address\nthis limitation, we propose FuseRL, a novel two-stage framework comprising\nFuseSFT and FusePO to maximize the utilization of source LLMs. FuseSFT\nestablishes a robust initialization by integrating the strengths of\nheterogeneous source models through weighted supervised fine-tuning (SFT) on\ndiverse outputs for each prompt. FusePO optimizes weighted preferences based on\nthe outputs of multiple source models to enable superior alignment performance.\nExtensive experiments demonstrate the effectiveness of our framework across\nvarious preference alignment methods, including RLOO, DPO, and SimPO. Using\nLlama-3.1-8B-Instruct as the target model, our approach achieves\nstate-of-the-art performance among 8B LLMs on the AlpacaEval-2 and Arena-Hard\nbenchmarks. Further analysis suggests that FuseSFT regularizes the training\nprocess to reduce overfitting, while FusePO introduces dense and diverse\nsignals for preference optimization."
                },
                "authors": [
                    {
                        "name": "Longguang Zhong"
                    },
                    {
                        "name": "Fanqi Wan"
                    },
                    {
                        "name": "Ziyi Yang"
                    },
                    {
                        "name": "Guosheng Liang"
                    },
                    {
                        "name": "Tianyuan Shi"
                    },
                    {
                        "name": "Xiaojun Quan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojun Quan"
                },
                "author": "Xiaojun Quan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06562v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06562v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12782v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12782v1",
                "updated": "2025-04-17T09:29:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    9,
                    29,
                    30,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T09:29:30Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    9,
                    29,
                    30,
                    3,
                    107,
                    0
                ],
                "title": "Set You Straight: Auto-Steering Denoising Trajectories to Sidestep\n  Unwanted Concepts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Set You Straight: Auto-Steering Denoising Trajectories to Sidestep\n  Unwanted Concepts"
                },
                "summary": "Ensuring the ethical deployment of text-to-image models requires effective\ntechniques to prevent the generation of harmful or inappropriate content. While\nconcept erasure methods offer a promising solution, existing finetuning-based\napproaches suffer from notable limitations. Anchor-free methods risk disrupting\nsampling trajectories, leading to visual artifacts, while anchor-based methods\nrely on the heuristic selection of anchor concepts. To overcome these\nshortcomings, we introduce a finetuning framework, dubbed ANT, which\nAutomatically guides deNoising Trajectories to avoid unwanted concepts. ANT is\nbuilt on a key insight: reversing the condition direction of classifier-free\nguidance during mid-to-late denoising stages enables precise content\nmodification without sacrificing early-stage structural integrity. This\ninspires a trajectory-aware objective that preserves the integrity of the\nearly-stage score function field, which steers samples toward the natural image\nmanifold, without relying on heuristic anchor concept selection. For\nsingle-concept erasure, we propose an augmentation-enhanced weight saliency map\nto precisely identify the critical parameters that most significantly\ncontribute to the unwanted concept, enabling more thorough and efficient\nerasure. For multi-concept erasure, our objective function offers a versatile\nplug-and-play solution that significantly boosts performance. Extensive\nexperiments demonstrate that ANT achieves state-of-the-art results in both\nsingle and multi-concept erasure, delivering high-quality, safe outputs without\ncompromising the generative fidelity. Code is available at\nhttps://github.com/lileyang1210/ANT",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring the ethical deployment of text-to-image models requires effective\ntechniques to prevent the generation of harmful or inappropriate content. While\nconcept erasure methods offer a promising solution, existing finetuning-based\napproaches suffer from notable limitations. Anchor-free methods risk disrupting\nsampling trajectories, leading to visual artifacts, while anchor-based methods\nrely on the heuristic selection of anchor concepts. To overcome these\nshortcomings, we introduce a finetuning framework, dubbed ANT, which\nAutomatically guides deNoising Trajectories to avoid unwanted concepts. ANT is\nbuilt on a key insight: reversing the condition direction of classifier-free\nguidance during mid-to-late denoising stages enables precise content\nmodification without sacrificing early-stage structural integrity. This\ninspires a trajectory-aware objective that preserves the integrity of the\nearly-stage score function field, which steers samples toward the natural image\nmanifold, without relying on heuristic anchor concept selection. For\nsingle-concept erasure, we propose an augmentation-enhanced weight saliency map\nto precisely identify the critical parameters that most significantly\ncontribute to the unwanted concept, enabling more thorough and efficient\nerasure. For multi-concept erasure, our objective function offers a versatile\nplug-and-play solution that significantly boosts performance. Extensive\nexperiments demonstrate that ANT achieves state-of-the-art results in both\nsingle and multi-concept erasure, delivering high-quality, safe outputs without\ncompromising the generative fidelity. Code is available at\nhttps://github.com/lileyang1210/ANT"
                },
                "authors": [
                    {
                        "name": "Leyang Li"
                    },
                    {
                        "name": "Shilin Lu"
                    },
                    {
                        "name": "Yan Ren"
                    },
                    {
                        "name": "Adams Wai-Kin Kong"
                    }
                ],
                "author_detail": {
                    "name": "Adams Wai-Kin Kong"
                },
                "author": "Adams Wai-Kin Kong",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12782v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12782v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12773v1",
                "updated": "2025-04-17T09:13:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    9,
                    13,
                    46,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T09:13:46Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    9,
                    13,
                    46,
                    3,
                    107,
                    0
                ],
                "title": "Enhancing the Geometric Problem-Solving Ability of Multimodal LLMs via\n  Symbolic-Neural Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing the Geometric Problem-Solving Ability of Multimodal LLMs via\n  Symbolic-Neural Integration"
                },
                "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have achieved\nremarkable progress in general domains and demonstrated promise in multimodal\nmathematical reasoning. However, applying MLLMs to geometry problem solving\n(GPS) remains challenging due to lack of accurate step-by-step solution data\nand severe hallucinations during reasoning. In this paper, we propose GeoGen, a\npipeline that can automatically generates step-wise reasoning paths for\ngeometry diagrams. By leveraging the precise symbolic reasoning,\n\\textbf{GeoGen} produces large-scale, high-quality question-answer pairs. To\nfurther enhance the logical reasoning ability of MLLMs, we train\n\\textbf{GeoLogic}, a Large Language Model (LLM) using synthetic data generated\nby GeoGen. Serving as a bridge between natural language and symbolic systems,\nGeoLogic enables symbolic tools to help verifying MLLM outputs, making the\nreasoning process more rigorous and alleviating hallucinations. Experimental\nresults show that our approach consistently improves the performance of MLLMs,\nachieving remarkable results on benchmarks for geometric reasoning tasks. This\nimprovement stems from our integration of the strengths of LLMs and symbolic\nsystems, which enables a more reliable and interpretable approach for the GPS\ntask. Codes are available at https://github.com/ycpNotFound/GeoGen.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Multimodal Large Language Models (MLLMs) have achieved\nremarkable progress in general domains and demonstrated promise in multimodal\nmathematical reasoning. However, applying MLLMs to geometry problem solving\n(GPS) remains challenging due to lack of accurate step-by-step solution data\nand severe hallucinations during reasoning. In this paper, we propose GeoGen, a\npipeline that can automatically generates step-wise reasoning paths for\ngeometry diagrams. By leveraging the precise symbolic reasoning,\n\\textbf{GeoGen} produces large-scale, high-quality question-answer pairs. To\nfurther enhance the logical reasoning ability of MLLMs, we train\n\\textbf{GeoLogic}, a Large Language Model (LLM) using synthetic data generated\nby GeoGen. Serving as a bridge between natural language and symbolic systems,\nGeoLogic enables symbolic tools to help verifying MLLM outputs, making the\nreasoning process more rigorous and alleviating hallucinations. Experimental\nresults show that our approach consistently improves the performance of MLLMs,\nachieving remarkable results on benchmarks for geometric reasoning tasks. This\nimprovement stems from our integration of the strengths of LLMs and symbolic\nsystems, which enables a more reliable and interpretable approach for the GPS\ntask. Codes are available at https://github.com/ycpNotFound/GeoGen."
                },
                "authors": [
                    {
                        "name": "Yicheng Pan"
                    },
                    {
                        "name": "Zhenrong Zhang"
                    },
                    {
                        "name": "Pengfei Hu"
                    },
                    {
                        "name": "Jiefeng Ma"
                    },
                    {
                        "name": "Jun Du"
                    },
                    {
                        "name": "Jianshu Zhang"
                    },
                    {
                        "name": "Quan Liu"
                    },
                    {
                        "name": "Jianqing Gao"
                    },
                    {
                        "name": "Feng Ma"
                    }
                ],
                "author_detail": {
                    "name": "Feng Ma"
                },
                "author": "Feng Ma",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12770v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12770v1",
                "updated": "2025-04-17T09:09:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    9,
                    9,
                    56,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T09:09:56Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    9,
                    9,
                    56,
                    3,
                    107,
                    0
                ],
                "title": "Comparative Analysis of POX and RYU SDN Controllers in Scalable Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparative Analysis of POX and RYU SDN Controllers in Scalable Networks"
                },
                "summary": "This paper explores the Quality of Service (QoS) performance of two widely\nused Software-Defined Networking (SDN) controllers, POX and Ryu, using Mininet\nfor network simulation. SDN, a transformative approach to network architecture,\nseparates the control and data planes, enabling centralized management,\nimproved agility, and cost-effective solutions. The study evaluates key QoS\nparameters, including throughput, delay, and jitter, to understand the\ncapabilities and limitations of the POX and Ryu controllers in handling traffic\nunder diverse network topologies. The research employs a systematic methodology\ninvolving the design of custom network topologies, implementation of OpenFlow\nrules, and analysis of controller behavior under simulated conditions. Results\nreveal that while POX offers simplicity and ease of use, making it suitable for\nsmaller-scale applications and experimentation, Ryu provides superior\nscalability and adaptability for more complex network environments. The\nfindings highlight the strengths and challenges of each controller, providing\nvaluable insights for organizations seeking to optimize SDN deployment. This\nstudy contributes to the growing body of knowledge on SDN technologies and\ntheir role in building scalable, efficient, and resilient network\ninfrastructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the Quality of Service (QoS) performance of two widely\nused Software-Defined Networking (SDN) controllers, POX and Ryu, using Mininet\nfor network simulation. SDN, a transformative approach to network architecture,\nseparates the control and data planes, enabling centralized management,\nimproved agility, and cost-effective solutions. The study evaluates key QoS\nparameters, including throughput, delay, and jitter, to understand the\ncapabilities and limitations of the POX and Ryu controllers in handling traffic\nunder diverse network topologies. The research employs a systematic methodology\ninvolving the design of custom network topologies, implementation of OpenFlow\nrules, and analysis of controller behavior under simulated conditions. Results\nreveal that while POX offers simplicity and ease of use, making it suitable for\nsmaller-scale applications and experimentation, Ryu provides superior\nscalability and adaptability for more complex network environments. The\nfindings highlight the strengths and challenges of each controller, providing\nvaluable insights for organizations seeking to optimize SDN deployment. This\nstudy contributes to the growing body of knowledge on SDN technologies and\ntheir role in building scalable, efficient, and resilient network\ninfrastructures."
                },
                "authors": [
                    {
                        "name": "Chandimal Jayawardena"
                    },
                    {
                        "name": "Jay Chen"
                    },
                    {
                        "name": "Amay Bhalla"
                    },
                    {
                        "name": "Lin Bu"
                    }
                ],
                "author_detail": {
                    "name": "Lin Bu"
                },
                "author": "Lin Bu",
                "arxiv_doi": "10.5121/ijcnc.2025.17203",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.5121/ijcnc.2025.17203",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.12770v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12770v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "17 pages",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12764v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12764v1",
                "updated": "2025-04-17T09:01:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    9,
                    1,
                    16,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T09:01:16Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    9,
                    1,
                    16,
                    3,
                    107,
                    0
                ],
                "title": "GraphOmni: A Comprehensive and Extendable Benchmark Framework for Large\n  Language Models on Graph-theoretic Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphOmni: A Comprehensive and Extendable Benchmark Framework for Large\n  Language Models on Graph-theoretic Tasks"
                },
                "summary": "In this paper, we presented GraphOmni, a comprehensive benchmark framework\nfor systematically evaluating the graph reasoning capabilities of LLMs. By\nanalyzing critical dimensions, including graph types, serialization formats,\nand prompt schemes, we provided extensive insights into the strengths and\nlimitations of current LLMs. Our empirical findings emphasize that no single\nserialization or prompting strategy consistently outperforms others. Motivated\nby these insights, we propose a reinforcement learning-based approach that\ndynamically selects the best serialization-prompt pairings, resulting in\nsignificant accuracy improvements. GraphOmni's modular and extensible design\nestablishes a robust foundation for future research, facilitating advancements\ntoward general-purpose graph reasoning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we presented GraphOmni, a comprehensive benchmark framework\nfor systematically evaluating the graph reasoning capabilities of LLMs. By\nanalyzing critical dimensions, including graph types, serialization formats,\nand prompt schemes, we provided extensive insights into the strengths and\nlimitations of current LLMs. Our empirical findings emphasize that no single\nserialization or prompting strategy consistently outperforms others. Motivated\nby these insights, we propose a reinforcement learning-based approach that\ndynamically selects the best serialization-prompt pairings, resulting in\nsignificant accuracy improvements. GraphOmni's modular and extensible design\nestablishes a robust foundation for future research, facilitating advancements\ntoward general-purpose graph reasoning models."
                },
                "authors": [
                    {
                        "name": "Hao Xu"
                    },
                    {
                        "name": "Xiangru Jian"
                    },
                    {
                        "name": "Xinjian Zhao"
                    },
                    {
                        "name": "Wei Pang"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Suyuchen Wang"
                    },
                    {
                        "name": "Qixin Zhang"
                    },
                    {
                        "name": "Joao Monteiro"
                    },
                    {
                        "name": "Qiuzhuang Sun"
                    },
                    {
                        "name": "Tianshu Yu"
                    }
                ],
                "author_detail": {
                    "name": "Tianshu Yu"
                },
                "author": "Tianshu Yu",
                "arxiv_comment": "82 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12764v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12764v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12755v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12755v1",
                "updated": "2025-04-17T08:48:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    8,
                    48,
                    23,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T08:48:23Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    8,
                    48,
                    23,
                    3,
                    107,
                    0
                ],
                "title": "Trajectory Adaptation using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trajectory Adaptation using Large Language Models"
                },
                "summary": "Adapting robot trajectories based on human instructions as per new situations\nis essential for achieving more intuitive and scalable human-robot\ninteractions. This work proposes a flexible language-based framework to adapt\ngeneric robotic trajectories produced by off-the-shelf motion planners like\nRRT, A-star, etc, or learned from human demonstrations. We utilize pre-trained\nLLMs to adapt trajectory waypoints by generating code as a policy for dense\nrobot manipulation, enabling more complex and flexible instructions than\ncurrent methods. This approach allows us to incorporate a broader range of\ncommands, including numerical inputs. Compared to state-of-the-art\nfeature-based sequence-to-sequence models which require training, our method\ndoes not require task-specific training and offers greater interpretability and\nmore effective feedback mechanisms. We validate our approach through simulation\nexperiments on the robotic manipulator, aerial vehicle, and ground robot in the\nPybullet and Gazebo simulation environments, demonstrating that LLMs can\nsuccessfully adapt trajectories to complex human instructions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting robot trajectories based on human instructions as per new situations\nis essential for achieving more intuitive and scalable human-robot\ninteractions. This work proposes a flexible language-based framework to adapt\ngeneric robotic trajectories produced by off-the-shelf motion planners like\nRRT, A-star, etc, or learned from human demonstrations. We utilize pre-trained\nLLMs to adapt trajectory waypoints by generating code as a policy for dense\nrobot manipulation, enabling more complex and flexible instructions than\ncurrent methods. This approach allows us to incorporate a broader range of\ncommands, including numerical inputs. Compared to state-of-the-art\nfeature-based sequence-to-sequence models which require training, our method\ndoes not require task-specific training and offers greater interpretability and\nmore effective feedback mechanisms. We validate our approach through simulation\nexperiments on the robotic manipulator, aerial vehicle, and ground robot in the\nPybullet and Gazebo simulation environments, demonstrating that LLMs can\nsuccessfully adapt trajectories to complex human instructions."
                },
                "authors": [
                    {
                        "name": "Anurag Maurya"
                    },
                    {
                        "name": "Tashmoy Ghosh"
                    },
                    {
                        "name": "Ravi Prakash"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Prakash"
                },
                "author": "Ravi Prakash",
                "arxiv_comment": "Accepted to CoRL LangRob workshop 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12755v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12755v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12749v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12749v1",
                "updated": "2025-04-17T08:41:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    8,
                    41,
                    23,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T08:41:23Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    8,
                    41,
                    23,
                    3,
                    107,
                    0
                ],
                "title": "LAD-Reasoner: Tiny Multimodal Models are Good Reasoners for Logical\n  Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAD-Reasoner: Tiny Multimodal Models are Good Reasoners for Logical\n  Anomaly Detection"
                },
                "summary": "Recent advances in industrial anomaly detection have highlighted the need for\ndeeper logical anomaly analysis, where unexpected relationships among objects,\ncounts, and spatial configurations must be identified and explained. Existing\napproaches often rely on large-scale external reasoning modules or elaborate\npipeline designs, hindering practical deployment and interpretability. To\naddress these limitations, we introduce a new task, Reasoning Logical Anomaly\nDetection (RLAD), which extends traditional anomaly detection by incorporating\nlogical reasoning. We propose a new framework, LAD-Reasoner, a customized tiny\nmultimodal language model built on Qwen2.5-VL 3B. Our approach leverages a\ntwo-stage training paradigm that first employs Supervised Fine-Tuning (SFT) for\nfine-grained visual understanding, followed by Group Relative Policy\nOptimization (GRPO) to refine logical anomaly detection and enforce coherent,\nhuman-readable reasoning. Crucially, reward signals are derived from both the\ndetection accuracy and the structural quality of the outputs, obviating the\nneed for building chain of thought (CoT) reasoning data. Experiments on the\nMVTec LOCO AD dataset show that LAD-Reasoner, though significantly smaller,\nmatches the performance of Qwen2.5-VL-72B in accuracy and F1 score, and further\nexcels in producing concise and interpretable rationales. This unified design\nreduces reliance on large models and complex pipelines, while offering\ntransparent and interpretable insights into logical anomaly detection. Code and\ndata will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in industrial anomaly detection have highlighted the need for\ndeeper logical anomaly analysis, where unexpected relationships among objects,\ncounts, and spatial configurations must be identified and explained. Existing\napproaches often rely on large-scale external reasoning modules or elaborate\npipeline designs, hindering practical deployment and interpretability. To\naddress these limitations, we introduce a new task, Reasoning Logical Anomaly\nDetection (RLAD), which extends traditional anomaly detection by incorporating\nlogical reasoning. We propose a new framework, LAD-Reasoner, a customized tiny\nmultimodal language model built on Qwen2.5-VL 3B. Our approach leverages a\ntwo-stage training paradigm that first employs Supervised Fine-Tuning (SFT) for\nfine-grained visual understanding, followed by Group Relative Policy\nOptimization (GRPO) to refine logical anomaly detection and enforce coherent,\nhuman-readable reasoning. Crucially, reward signals are derived from both the\ndetection accuracy and the structural quality of the outputs, obviating the\nneed for building chain of thought (CoT) reasoning data. Experiments on the\nMVTec LOCO AD dataset show that LAD-Reasoner, though significantly smaller,\nmatches the performance of Qwen2.5-VL-72B in accuracy and F1 score, and further\nexcels in producing concise and interpretable rationales. This unified design\nreduces reliance on large models and complex pipelines, while offering\ntransparent and interpretable insights into logical anomaly detection. Code and\ndata will be released."
                },
                "authors": [
                    {
                        "name": "Weijia Li"
                    },
                    {
                        "name": "Guanglei Chu"
                    },
                    {
                        "name": "Jiong Chen"
                    },
                    {
                        "name": "Guo-Sen Xie"
                    },
                    {
                        "name": "Caifeng Shan"
                    },
                    {
                        "name": "Fang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Fang Zhao"
                },
                "author": "Fang Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12749v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12749v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.10305v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.10305v4",
                "updated": "2025-04-17T08:34:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    8,
                    34,
                    42,
                    3,
                    107,
                    0
                ],
                "published": "2023-09-19T04:13:22Z",
                "published_parsed": [
                    2023,
                    9,
                    19,
                    4,
                    13,
                    22,
                    1,
                    262,
                    0
                ],
                "title": "Baichuan 2: Open Large-scale Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Baichuan 2: Open Large-scale Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable performance on a\nvariety of natural language tasks based on just a few examples of natural\nlanguage instructions, reducing the need for extensive feature engineering.\nHowever, most powerful LLMs are closed-source or limited in their capability\nfor languages other than English. In this technical report, we present Baichuan\n2, a series of large-scale multilingual language models containing 7 billion\nand 13 billion parameters, trained from scratch, on 2.6 trillion tokens.\nBaichuan 2 matches or outperforms other open-source models of similar size on\npublic benchmarks like MMLU, CMMLU, GSM8K, and HumanEval. Furthermore, Baichuan\n2 excels in vertical domains such as medicine and law. We will release all\npre-training model checkpoints to benefit the research community in better\nunderstanding the training dynamics of Baichuan 2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance on a\nvariety of natural language tasks based on just a few examples of natural\nlanguage instructions, reducing the need for extensive feature engineering.\nHowever, most powerful LLMs are closed-source or limited in their capability\nfor languages other than English. In this technical report, we present Baichuan\n2, a series of large-scale multilingual language models containing 7 billion\nand 13 billion parameters, trained from scratch, on 2.6 trillion tokens.\nBaichuan 2 matches or outperforms other open-source models of similar size on\npublic benchmarks like MMLU, CMMLU, GSM8K, and HumanEval. Furthermore, Baichuan\n2 excels in vertical domains such as medicine and law. We will release all\npre-training model checkpoints to benefit the research community in better\nunderstanding the training dynamics of Baichuan 2."
                },
                "authors": [
                    {
                        "name": "Aiyuan Yang"
                    },
                    {
                        "name": "Bin Xiao"
                    },
                    {
                        "name": "Bingning Wang"
                    },
                    {
                        "name": "Borong Zhang"
                    },
                    {
                        "name": "Ce Bian"
                    },
                    {
                        "name": "Chao Yin"
                    },
                    {
                        "name": "Chenxu Lv"
                    },
                    {
                        "name": "Da Pan"
                    },
                    {
                        "name": "Dian Wang"
                    },
                    {
                        "name": "Dong Yan"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Fei Deng"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Guangwei Ai"
                    },
                    {
                        "name": "Guosheng Dong"
                    },
                    {
                        "name": "Haizhou Zhao"
                    },
                    {
                        "name": "Hang Xu"
                    },
                    {
                        "name": "Haoze Sun"
                    },
                    {
                        "name": "Hongda Zhang"
                    },
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Jiaming Ji"
                    },
                    {
                        "name": "Jian Xie"
                    },
                    {
                        "name": "JunTao Dai"
                    },
                    {
                        "name": "Kun Fang"
                    },
                    {
                        "name": "Lei Su"
                    },
                    {
                        "name": "Liang Song"
                    },
                    {
                        "name": "Lifeng Liu"
                    },
                    {
                        "name": "Liyun Ru"
                    },
                    {
                        "name": "Luyao Ma"
                    },
                    {
                        "name": "Mang Wang"
                    },
                    {
                        "name": "Mickel Liu"
                    },
                    {
                        "name": "MingAn Lin"
                    },
                    {
                        "name": "Nuolan Nie"
                    },
                    {
                        "name": "Peidong Guo"
                    },
                    {
                        "name": "Ruiyang Sun"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Tianpeng Li"
                    },
                    {
                        "name": "Tianyu Li"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Weipeng Chen"
                    },
                    {
                        "name": "Xiangrong Zeng"
                    },
                    {
                        "name": "Xiaochuan Wang"
                    },
                    {
                        "name": "Xiaoxi Chen"
                    },
                    {
                        "name": "Xin Men"
                    },
                    {
                        "name": "Xin Yu"
                    },
                    {
                        "name": "Xuehai Pan"
                    },
                    {
                        "name": "Yanjun Shen"
                    },
                    {
                        "name": "Yiding Wang"
                    },
                    {
                        "name": "Yiyu Li"
                    },
                    {
                        "name": "Youxin Jiang"
                    },
                    {
                        "name": "Yuchen Gao"
                    },
                    {
                        "name": "Yupeng Zhang"
                    },
                    {
                        "name": "Zenan Zhou"
                    },
                    {
                        "name": "Zhiying Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiying Wu"
                },
                "author": "Zhiying Wu",
                "arxiv_comment": "Baichuan 2 technical report. Github:\n  https://github.com/baichuan-inc/Baichuan2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.10305v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.10305v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12737v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12737v1",
                "updated": "2025-04-17T08:27:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    8,
                    27,
                    2,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T08:27:02Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    8,
                    27,
                    2,
                    3,
                    107,
                    0
                ],
                "title": "Chinese-Vicuna: A Chinese Instruction-following Llama-based Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chinese-Vicuna: A Chinese Instruction-following Llama-based Model"
                },
                "summary": "Chinese-Vicuna is an open-source, resource-efficient language model designed\nto bridge the gap in Chinese instruction-following capabilities by fine-tuning\nMeta's LLaMA architecture using Low-Rank Adaptation (LoRA). Targeting\nlow-resource environments, it enables cost-effective deployment on consumer\nGPUs (e.g., RTX-2080Ti for 7B models) and supports domain-specific adaptation\nin fields like healthcare and law. By integrating hybrid datasets (BELLE and\nGuanaco) and 4-bit quantization (QLoRA), the model achieves competitive\nperformance in tasks such as translation, code generation, and domain-specific\nQ\\&A. The project provides a comprehensive toolkit for model conversion, CPU\ninference, and multi-turn dialogue interfaces, emphasizing accessibility for\nresearchers and developers. Evaluations indicate competitive performance across\nmedical tasks, multi-turn dialogue coherence, and real-time legal updates.\nChinese-Vicuna's modular design, open-source ecosystem, and community-driven\nenhancements position it as a versatile foundation for Chinese LLM\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chinese-Vicuna is an open-source, resource-efficient language model designed\nto bridge the gap in Chinese instruction-following capabilities by fine-tuning\nMeta's LLaMA architecture using Low-Rank Adaptation (LoRA). Targeting\nlow-resource environments, it enables cost-effective deployment on consumer\nGPUs (e.g., RTX-2080Ti for 7B models) and supports domain-specific adaptation\nin fields like healthcare and law. By integrating hybrid datasets (BELLE and\nGuanaco) and 4-bit quantization (QLoRA), the model achieves competitive\nperformance in tasks such as translation, code generation, and domain-specific\nQ\\&A. The project provides a comprehensive toolkit for model conversion, CPU\ninference, and multi-turn dialogue interfaces, emphasizing accessibility for\nresearchers and developers. Evaluations indicate competitive performance across\nmedical tasks, multi-turn dialogue coherence, and real-time legal updates.\nChinese-Vicuna's modular design, open-source ecosystem, and community-driven\nenhancements position it as a versatile foundation for Chinese LLM\napplications."
                },
                "authors": [
                    {
                        "name": "Chenghao Fan"
                    },
                    {
                        "name": "Zhenyi Lu"
                    },
                    {
                        "name": "Jie Tian"
                    }
                ],
                "author_detail": {
                    "name": "Jie Tian"
                },
                "author": "Jie Tian",
                "arxiv_comment": "Chinese-Vicuna Technique Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12737v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12737v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12736v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12736v1",
                "updated": "2025-04-17T08:24:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    8,
                    24,
                    32,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T08:24:32Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    8,
                    24,
                    32,
                    3,
                    107,
                    0
                ],
                "title": "Incorporating a Deep Neural Network into Moving Horizon Estimation for\n  Embedded Thermal Torque Derating of an Electric Machine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incorporating a Deep Neural Network into Moving Horizon Estimation for\n  Embedded Thermal Torque Derating of an Electric Machine"
                },
                "summary": "This study introduces a novel state estimation framework that incorporates\nDeep Neural Networks (DNNs) into Moving Horizon Estimation (MHE), shifting from\ntraditional physics-based models to rapidly developed data-driven techniques. A\nDNN model with Long Short-Term Memory (LSTM) nodes is trained on synthetic data\ngenerated by a high-fidelity thermal model of a Permanent Magnet Synchronous\nMachine (PMSM), which undergoes thermal derating as part of the torque control\nstrategy in a battery electric vehicle. The MHE is constructed by integrating\nthe trained DNN with a simplified driving dynamics model in a discrete-time\nformulation, incorporating the LSTM hidden and cell states in the state vector\nto retain system dynamics. The resulting optimal control problem (OCP) is\nformulated as a nonlinear program (NLP) and implemented using the acados\nframework. Model-in-the-loop (MiL) simulations demonstrate accurate temperature\nestimation, even under noisy sensor conditions or failures. Achieving threefold\nreal-time capability on embedded hardware confirms the feasibility of the\napproach for practical deployment. The primary focus of this study is to assess\nthe feasibility of the MHE framework using a DNN-based plant model instead of\nfocusing on quantitative comparisons of vehicle performance. Overall, this\nresearch highlights the potential of DNN-based MHE for real-time,\nsafety-critical applications by combining the strengths of model-based and\ndata-driven methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study introduces a novel state estimation framework that incorporates\nDeep Neural Networks (DNNs) into Moving Horizon Estimation (MHE), shifting from\ntraditional physics-based models to rapidly developed data-driven techniques. A\nDNN model with Long Short-Term Memory (LSTM) nodes is trained on synthetic data\ngenerated by a high-fidelity thermal model of a Permanent Magnet Synchronous\nMachine (PMSM), which undergoes thermal derating as part of the torque control\nstrategy in a battery electric vehicle. The MHE is constructed by integrating\nthe trained DNN with a simplified driving dynamics model in a discrete-time\nformulation, incorporating the LSTM hidden and cell states in the state vector\nto retain system dynamics. The resulting optimal control problem (OCP) is\nformulated as a nonlinear program (NLP) and implemented using the acados\nframework. Model-in-the-loop (MiL) simulations demonstrate accurate temperature\nestimation, even under noisy sensor conditions or failures. Achieving threefold\nreal-time capability on embedded hardware confirms the feasibility of the\napproach for practical deployment. The primary focus of this study is to assess\nthe feasibility of the MHE framework using a DNN-based plant model instead of\nfocusing on quantitative comparisons of vehicle performance. Overall, this\nresearch highlights the potential of DNN-based MHE for real-time,\nsafety-critical applications by combining the strengths of model-based and\ndata-driven methods."
                },
                "authors": [
                    {
                        "name": "Alexander Winkler"
                    },
                    {
                        "name": "Pranav Shah"
                    },
                    {
                        "name": "Katrin Baumgärtner"
                    },
                    {
                        "name": "Vasu Sharma"
                    },
                    {
                        "name": "David Gordon"
                    },
                    {
                        "name": "Jakob Andert"
                    }
                ],
                "author_detail": {
                    "name": "Jakob Andert"
                },
                "author": "Jakob Andert",
                "arxiv_comment": "17 pages, 13 figures, data publication incl. all scripts and data\n  available, submitted to Energies Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12736v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12736v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12734v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12734v1",
                "updated": "2025-04-17T08:18:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    8,
                    18,
                    9,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T08:18:09Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    8,
                    18,
                    9,
                    3,
                    107,
                    0
                ],
                "title": "Pandora: A Code-Driven Large Language Model Agent for Unified Reasoning\n  Across Diverse Structured Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pandora: A Code-Driven Large Language Model Agent for Unified Reasoning\n  Across Diverse Structured Knowledge"
                },
                "summary": "Unified Structured Knowledge Reasoning (USKR) aims to answer natural language\nquestions (NLQs) by using structured sources such as tables, databases, and\nknowledge graphs in a unified way. Existing USKR methods either rely on\nemploying task-specific strategies or custom-defined representations, which\nstruggle to leverage the knowledge transfer between different SKR tasks or\nalign with the prior of LLMs, thereby limiting their performance. This paper\nproposes a novel USKR framework named \\textsc{Pandora}, which takes advantage\nof \\textsc{Python}'s \\textsc{Pandas} API to construct a unified knowledge\nrepresentation for alignment with LLM pre-training. It employs an LLM to\ngenerate textual reasoning steps and executable Python code for each question.\nDemonstrations are drawn from a memory of training examples that cover various\nSKR tasks, facilitating knowledge transfer. Extensive experiments on four\nbenchmarks involving three SKR tasks demonstrate that \\textsc{Pandora}\noutperforms existing unified frameworks and competes effectively with\ntask-specific methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified Structured Knowledge Reasoning (USKR) aims to answer natural language\nquestions (NLQs) by using structured sources such as tables, databases, and\nknowledge graphs in a unified way. Existing USKR methods either rely on\nemploying task-specific strategies or custom-defined representations, which\nstruggle to leverage the knowledge transfer between different SKR tasks or\nalign with the prior of LLMs, thereby limiting their performance. This paper\nproposes a novel USKR framework named \\textsc{Pandora}, which takes advantage\nof \\textsc{Python}'s \\textsc{Pandas} API to construct a unified knowledge\nrepresentation for alignment with LLM pre-training. It employs an LLM to\ngenerate textual reasoning steps and executable Python code for each question.\nDemonstrations are drawn from a memory of training examples that cover various\nSKR tasks, facilitating knowledge transfer. Extensive experiments on four\nbenchmarks involving three SKR tasks demonstrate that \\textsc{Pandora}\noutperforms existing unified frameworks and competes effectively with\ntask-specific methods."
                },
                "authors": [
                    {
                        "name": "Yongrui Chen"
                    },
                    {
                        "name": "Junhao He"
                    },
                    {
                        "name": "Linbo Fu"
                    },
                    {
                        "name": "Shenyu Zhang"
                    },
                    {
                        "name": "Rihui Jin"
                    },
                    {
                        "name": "Xinbang Dai"
                    },
                    {
                        "name": "Jiaqi Li"
                    },
                    {
                        "name": "Dehai Min"
                    },
                    {
                        "name": "Nan Hu"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Guilin Qi"
                    },
                    {
                        "name": "Yi Huang"
                    },
                    {
                        "name": "Tongtong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Tongtong Wu"
                },
                "author": "Tongtong Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12734v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12734v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12732v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12732v1",
                "updated": "2025-04-17T08:14:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    8,
                    14,
                    45,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T08:14:45Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    8,
                    14,
                    45,
                    3,
                    107,
                    0
                ],
                "title": "Validating LLM-Generated Relevance Labels for Educational Resource\n  Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Validating LLM-Generated Relevance Labels for Educational Resource\n  Search"
                },
                "summary": "Manual relevance judgements in Information Retrieval are costly and require\nexpertise, driving interest in using Large Language Models (LLMs) for automatic\nassessment. While LLMs have shown promise in general web search scenarios,\ntheir effectiveness for evaluating domain-specific search results, such as\neducational resources, remains unexplored. To investigate different ways of\nincluding domain-specific criteria in LLM prompts for relevance judgement, we\ncollected and released a dataset of 401 human relevance judgements from a user\nstudy involving teaching professionals performing search tasks related to\nlesson planning. We compared three approaches to structuring these prompts: a\nsimple two-aspect evaluation baseline from prior work on using LLMs as\nrelevance judges, a comprehensive 12-dimensional rubric derived from\neducational literature, and criteria directly informed by the study\nparticipants. Using domain-specific frameworks, LLMs achieved strong agreement\nwith human judgements (Cohen's $\\kappa$ up to 0.650), significantly\noutperforming the baseline approach. The participant-derived framework proved\nparticularly robust, with GPT-3.5 achieving $\\kappa$ scores of 0.639 and 0.613\nfor 10-dimension and 5-dimension versions respectively. System-level evaluation\nshowed that LLM judgements reliably identified top-performing retrieval\napproaches (RBO scores 0.71-0.76) while maintaining reasonable discrimination\nbetween systems (RBO 0.52-0.56). These findings suggest that LLMs can\neffectively evaluate educational resources when prompted with domain-specific\ncriteria, though performance varies with framework complexity and input\nstructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Manual relevance judgements in Information Retrieval are costly and require\nexpertise, driving interest in using Large Language Models (LLMs) for automatic\nassessment. While LLMs have shown promise in general web search scenarios,\ntheir effectiveness for evaluating domain-specific search results, such as\neducational resources, remains unexplored. To investigate different ways of\nincluding domain-specific criteria in LLM prompts for relevance judgement, we\ncollected and released a dataset of 401 human relevance judgements from a user\nstudy involving teaching professionals performing search tasks related to\nlesson planning. We compared three approaches to structuring these prompts: a\nsimple two-aspect evaluation baseline from prior work on using LLMs as\nrelevance judges, a comprehensive 12-dimensional rubric derived from\neducational literature, and criteria directly informed by the study\nparticipants. Using domain-specific frameworks, LLMs achieved strong agreement\nwith human judgements (Cohen's $\\kappa$ up to 0.650), significantly\noutperforming the baseline approach. The participant-derived framework proved\nparticularly robust, with GPT-3.5 achieving $\\kappa$ scores of 0.639 and 0.613\nfor 10-dimension and 5-dimension versions respectively. System-level evaluation\nshowed that LLM judgements reliably identified top-performing retrieval\napproaches (RBO scores 0.71-0.76) while maintaining reasonable discrimination\nbetween systems (RBO 0.52-0.56). These findings suggest that LLMs can\neffectively evaluate educational resources when prompted with domain-specific\ncriteria, though performance varies with framework complexity and input\nstructure."
                },
                "authors": [
                    {
                        "name": "Ratan J. Sebastian"
                    },
                    {
                        "name": "Anett Hoppe"
                    }
                ],
                "author_detail": {
                    "name": "Anett Hoppe"
                },
                "author": "Anett Hoppe",
                "arxiv_comment": "Presented in the LLM4Eval Workshop Co-located with WSDM '25 in\n  Hannover, Germany",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12732v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12732v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12719v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12719v1",
                "updated": "2025-04-17T07:48:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    7,
                    48,
                    50,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T07:48:50Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    7,
                    48,
                    50,
                    3,
                    107,
                    0
                ],
                "title": "B*: Efficient and Optimal Base Placement for Fixed-Base Manipulators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "B*: Efficient and Optimal Base Placement for Fixed-Base Manipulators"
                },
                "summary": "B* is a novel optimization framework that addresses a critical challenge in\nfixed-base manipulator robotics: optimal base placement. Current methods rely\non pre-computed kinematics databases generated through sampling to search for\nsolutions. However, they face an inherent trade-off between solution optimality\nand computational efficiency when determining sampling resolution. To address\nthese limitations, B* unifies multiple objectives without database dependence.\nThe framework employs a two-layer hierarchical approach. The outer layer\nsystematically manages terminal constraints through progressive tightening,\nparticularly for base mobility, enabling feasible initialization and broad\nsolution exploration. The inner layer addresses non-convexities in each\nouter-layer subproblem through sequential local linearization, converting the\noriginal problem into tractable sequential linear programming (SLP). Testing\nacross multiple robot platforms demonstrates B*'s effectiveness. The framework\nachieves solution optimality five orders of magnitude better than\nsampling-based approaches while maintaining perfect success rates and reduced\ncomputational overhead. Operating directly in configuration space, B* enables\nsimultaneous path planning with customizable optimization criteria. B* serves\nas a crucial initialization tool that bridges the gap between theoretical\nmotion planning and practical deployment, where feasible trajectory existence\nis fundamental.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "B* is a novel optimization framework that addresses a critical challenge in\nfixed-base manipulator robotics: optimal base placement. Current methods rely\non pre-computed kinematics databases generated through sampling to search for\nsolutions. However, they face an inherent trade-off between solution optimality\nand computational efficiency when determining sampling resolution. To address\nthese limitations, B* unifies multiple objectives without database dependence.\nThe framework employs a two-layer hierarchical approach. The outer layer\nsystematically manages terminal constraints through progressive tightening,\nparticularly for base mobility, enabling feasible initialization and broad\nsolution exploration. The inner layer addresses non-convexities in each\nouter-layer subproblem through sequential local linearization, converting the\noriginal problem into tractable sequential linear programming (SLP). Testing\nacross multiple robot platforms demonstrates B*'s effectiveness. The framework\nachieves solution optimality five orders of magnitude better than\nsampling-based approaches while maintaining perfect success rates and reduced\ncomputational overhead. Operating directly in configuration space, B* enables\nsimultaneous path planning with customizable optimization criteria. B* serves\nas a crucial initialization tool that bridges the gap between theoretical\nmotion planning and practical deployment, where feasible trajectory existence\nis fundamental."
                },
                "authors": [
                    {
                        "name": "Zihang Zhao"
                    },
                    {
                        "name": "Leiyao Cui"
                    },
                    {
                        "name": "Sirui Xie"
                    },
                    {
                        "name": "Saiyao Zhang"
                    },
                    {
                        "name": "Zhi Han"
                    },
                    {
                        "name": "Lecheng Ruan"
                    },
                    {
                        "name": "Yixin Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yixin Zhu"
                },
                "author": "Yixin Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12719v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12719v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24047v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24047v2",
                "updated": "2025-04-17T07:26:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    7,
                    26,
                    34,
                    3,
                    107,
                    0
                ],
                "published": "2025-03-31T13:11:28Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    13,
                    11,
                    28,
                    0,
                    90,
                    0
                ],
                "title": "Towards Scientific Intelligence: A Survey of LLM-based Scientific Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Scientific Intelligence: A Survey of LLM-based Scientific Agents"
                },
                "summary": "As scientific research becomes increasingly complex, innovative tools are\nneeded to manage vast data, facilitate interdisciplinary collaboration, and\naccelerate discovery. Large language models (LLMs) are now evolving into\nLLM-based scientific agents that automate critical tasks, ranging from\nhypothesis generation and experiment design to data analysis and simulation.\nUnlike general-purpose LLMs, these specialized agents integrate domain-specific\nknowledge, advanced tool sets, and robust validation mechanisms, enabling them\nto handle complex data types, ensure reproducibility, and drive scientific\nbreakthroughs. This survey provides a focused review of the architectures,\ndesign, benchmarks, applications, and ethical considerations surrounding\nLLM-based scientific agents. We highlight why they differ from general agents\nand the ways in which they advance research across various scientific fields.\nBy examining their development and challenges, this survey offers a\ncomprehensive roadmap for researchers and practitioners to harness these agents\nfor more efficient, reliable, and ethically sound scientific discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As scientific research becomes increasingly complex, innovative tools are\nneeded to manage vast data, facilitate interdisciplinary collaboration, and\naccelerate discovery. Large language models (LLMs) are now evolving into\nLLM-based scientific agents that automate critical tasks, ranging from\nhypothesis generation and experiment design to data analysis and simulation.\nUnlike general-purpose LLMs, these specialized agents integrate domain-specific\nknowledge, advanced tool sets, and robust validation mechanisms, enabling them\nto handle complex data types, ensure reproducibility, and drive scientific\nbreakthroughs. This survey provides a focused review of the architectures,\ndesign, benchmarks, applications, and ethical considerations surrounding\nLLM-based scientific agents. We highlight why they differ from general agents\nand the ways in which they advance research across various scientific fields.\nBy examining their development and challenges, this survey offers a\ncomprehensive roadmap for researchers and practitioners to harness these agents\nfor more efficient, reliable, and ethically sound scientific discovery."
                },
                "authors": [
                    {
                        "name": "Shuo Ren"
                    },
                    {
                        "name": "Pu Jian"
                    },
                    {
                        "name": "Zhenjiang Ren"
                    },
                    {
                        "name": "Chunlin Leng"
                    },
                    {
                        "name": "Can Xie"
                    },
                    {
                        "name": "Jiajun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Zhang"
                },
                "author": "Jiajun Zhang",
                "arxiv_comment": "34 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24047v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24047v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01113v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01113v2",
                "updated": "2025-04-17T06:57:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    6,
                    57,
                    50,
                    3,
                    107,
                    0
                ],
                "published": "2024-12-02T04:35:54Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    4,
                    35,
                    54,
                    0,
                    337,
                    0
                ],
                "title": "Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in\n  Multi-Step Arithmetic Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in\n  Multi-Step Arithmetic Reasoning"
                },
                "summary": "This study investigates the internal reasoning process of language models\nduring arithmetic multi-step reasoning, motivated by the question of when they\ninternally form their answers during reasoning. Particularly, we inspect\nwhether the answer is determined before or after chain-of-thought (CoT) begins\nto determine whether models follow a post-hoc Think-to-Talk mode or a\nstep-by-step Talk-to-Think mode of explanation. Through causal probing\nexperiments in controlled arithmetic reasoning tasks, we found systematic\ninternal reasoning patterns across models in our case study; for example,\nsingle-step subproblems are solved before CoT begins, and more complicated\nmulti-step calculations are performed during CoT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the internal reasoning process of language models\nduring arithmetic multi-step reasoning, motivated by the question of when they\ninternally form their answers during reasoning. Particularly, we inspect\nwhether the answer is determined before or after chain-of-thought (CoT) begins\nto determine whether models follow a post-hoc Think-to-Talk mode or a\nstep-by-step Talk-to-Think mode of explanation. Through causal probing\nexperiments in controlled arithmetic reasoning tasks, we found systematic\ninternal reasoning patterns across models in our case study; for example,\nsingle-step subproblems are solved before CoT begins, and more complicated\nmulti-step calculations are performed during CoT."
                },
                "authors": [
                    {
                        "name": "Keito Kudo"
                    },
                    {
                        "name": "Yoichi Aoki"
                    },
                    {
                        "name": "Tatsuki Kuribayashi"
                    },
                    {
                        "name": "Shusaku Sone"
                    },
                    {
                        "name": "Masaya Taniguchi"
                    },
                    {
                        "name": "Ana Brassard"
                    },
                    {
                        "name": "Keisuke Sakaguchi"
                    },
                    {
                        "name": "Kentaro Inui"
                    }
                ],
                "author_detail": {
                    "name": "Kentaro Inui"
                },
                "author": "Kentaro Inui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01113v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01113v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12691v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12691v1",
                "updated": "2025-04-17T06:34:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    6,
                    34,
                    45,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T06:34:45Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    6,
                    34,
                    45,
                    3,
                    107,
                    0
                ],
                "title": "Why and How LLMs Hallucinate: Connecting the Dots with Subsequence\n  Associations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why and How LLMs Hallucinate: Connecting the Dots with Subsequence\n  Associations"
                },
                "summary": "Large language models (LLMs) frequently generate hallucinations-content that\ndeviates from factual accuracy or provided context-posing challenges for\ndiagnosis due to the complex interplay of underlying causes. This paper\nintroduces a subsequence association framework to systematically trace and\nunderstand hallucinations. Our key insight is that hallucinations arise when\ndominant hallucinatory associations outweigh faithful ones. Through theoretical\nand empirical analyses, we demonstrate that decoder-only transformers\neffectively function as subsequence embedding models, with linear layers\nencoding input-output associations. We propose a tracing algorithm that\nidentifies causal subsequences by analyzing hallucination probabilities across\nrandomized input contexts. Experiments show our method outperforms standard\nattribution techniques in identifying hallucination causes and aligns with\nevidence from the model's training corpus. This work provides a unified\nperspective on hallucinations and a robust framework for their tracing and\nanalysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) frequently generate hallucinations-content that\ndeviates from factual accuracy or provided context-posing challenges for\ndiagnosis due to the complex interplay of underlying causes. This paper\nintroduces a subsequence association framework to systematically trace and\nunderstand hallucinations. Our key insight is that hallucinations arise when\ndominant hallucinatory associations outweigh faithful ones. Through theoretical\nand empirical analyses, we demonstrate that decoder-only transformers\neffectively function as subsequence embedding models, with linear layers\nencoding input-output associations. We propose a tracing algorithm that\nidentifies causal subsequences by analyzing hallucination probabilities across\nrandomized input contexts. Experiments show our method outperforms standard\nattribution techniques in identifying hallucination causes and aligns with\nevidence from the model's training corpus. This work provides a unified\nperspective on hallucinations and a robust framework for their tracing and\nanalysis."
                },
                "authors": [
                    {
                        "name": "Yiyou Sun"
                    },
                    {
                        "name": "Yu Gai"
                    },
                    {
                        "name": "Lijie Chen"
                    },
                    {
                        "name": "Abhilasha Ravichander"
                    },
                    {
                        "name": "Yejin Choi"
                    },
                    {
                        "name": "Dawn Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawn Song"
                },
                "author": "Dawn Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12691v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12691v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12689v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12689v1",
                "updated": "2025-04-17T06:31:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    6,
                    31,
                    26,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T06:31:26Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    6,
                    31,
                    26,
                    3,
                    107,
                    0
                ],
                "title": "HSS-IAD: A Heterogeneous Same-Sort Industrial Anomaly Detection Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HSS-IAD: A Heterogeneous Same-Sort Industrial Anomaly Detection Dataset"
                },
                "summary": "Multi-class Unsupervised Anomaly Detection algorithms (MUAD) are receiving\nincreasing attention due to their relatively low deployment costs and improved\ntraining efficiency. However, the real-world effectiveness of MUAD methods is\nquestioned due to limitations in current Industrial Anomaly Detection (IAD)\ndatasets. These datasets contain numerous classes that are unlikely to be\nproduced by the same factory and fail to cover multiple structures or\nappearances. Additionally, the defects do not reflect real-world\ncharacteristics. Therefore, we introduce the Heterogeneous Same-Sort Industrial\nAnomaly Detection (HSS-IAD) dataset, which contains 8,580 images of\nmetallic-like industrial parts and precise anomaly annotations. These parts\nexhibit variations in structure and appearance, with subtle defects that\nclosely resemble the base materials. We also provide foreground images for\nsynthetic anomaly generation. Finally, we evaluate popular IAD methods on this\ndataset under multi-class and class-separated settings, demonstrating its\npotential to bridge the gap between existing datasets and real factory\nconditions. The dataset is available at\nhttps://github.com/Qiqigeww/HSS-IAD-Dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-class Unsupervised Anomaly Detection algorithms (MUAD) are receiving\nincreasing attention due to their relatively low deployment costs and improved\ntraining efficiency. However, the real-world effectiveness of MUAD methods is\nquestioned due to limitations in current Industrial Anomaly Detection (IAD)\ndatasets. These datasets contain numerous classes that are unlikely to be\nproduced by the same factory and fail to cover multiple structures or\nappearances. Additionally, the defects do not reflect real-world\ncharacteristics. Therefore, we introduce the Heterogeneous Same-Sort Industrial\nAnomaly Detection (HSS-IAD) dataset, which contains 8,580 images of\nmetallic-like industrial parts and precise anomaly annotations. These parts\nexhibit variations in structure and appearance, with subtle defects that\nclosely resemble the base materials. We also provide foreground images for\nsynthetic anomaly generation. Finally, we evaluate popular IAD methods on this\ndataset under multi-class and class-separated settings, demonstrating its\npotential to bridge the gap between existing datasets and real factory\nconditions. The dataset is available at\nhttps://github.com/Qiqigeww/HSS-IAD-Dataset."
                },
                "authors": [
                    {
                        "name": "Qishan Wang"
                    },
                    {
                        "name": "Shuyong Gao"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Jiawen Yu"
                    },
                    {
                        "name": "Xuan Tong"
                    },
                    {
                        "name": "You Li"
                    },
                    {
                        "name": "Wenqiang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wenqiang Zhang"
                },
                "author": "Wenqiang Zhang",
                "arxiv_comment": "Accepted to IEEE ICME 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12689v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12689v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12687v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12687v1",
                "updated": "2025-04-17T06:29:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    6,
                    29,
                    28,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T06:29:28Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    6,
                    29,
                    28,
                    3,
                    107,
                    0
                ],
                "title": "Data-efficient LLM Fine-tuning for Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-efficient LLM Fine-tuning for Code Generation"
                },
                "summary": "Large language models (LLMs) have demonstrated significant potential in code\ngeneration tasks. However, there remains a performance gap between open-source\nand closed-source models. To address this gap, existing approaches typically\ngenerate large amounts of synthetic data for fine-tuning, which often leads to\ninefficient training. In this work, we propose a data selection strategy in\norder to improve the effectiveness and efficiency of training for code-based\nLLMs. By prioritizing data complexity and ensuring that the sampled subset\naligns with the distribution of the original dataset, our sampling strategy\neffectively selects high-quality data. Additionally, we optimize the\ntokenization process through a \"dynamic pack\" technique, which minimizes\npadding tokens and reduces computational resource consumption. Experimental\nresults show that when training on 40% of the OSS-Instruct dataset, the\nDeepSeek-Coder-Base-6.7B model achieves an average performance of 66.9%,\nsurpassing the 66.1% performance with the full dataset. Moreover, training time\nis reduced from 47 minutes to 34 minutes, and the peak GPU memory decreases\nfrom 61.47 GB to 42.72 GB during a single epoch. Similar improvements are\nobserved with the CodeLlama-Python-7B model on the Evol-Instruct dataset. By\noptimizing both data selection and tokenization, our approach not only improves\nmodel performance but also improves training efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated significant potential in code\ngeneration tasks. However, there remains a performance gap between open-source\nand closed-source models. To address this gap, existing approaches typically\ngenerate large amounts of synthetic data for fine-tuning, which often leads to\ninefficient training. In this work, we propose a data selection strategy in\norder to improve the effectiveness and efficiency of training for code-based\nLLMs. By prioritizing data complexity and ensuring that the sampled subset\naligns with the distribution of the original dataset, our sampling strategy\neffectively selects high-quality data. Additionally, we optimize the\ntokenization process through a \"dynamic pack\" technique, which minimizes\npadding tokens and reduces computational resource consumption. Experimental\nresults show that when training on 40% of the OSS-Instruct dataset, the\nDeepSeek-Coder-Base-6.7B model achieves an average performance of 66.9%,\nsurpassing the 66.1% performance with the full dataset. Moreover, training time\nis reduced from 47 minutes to 34 minutes, and the peak GPU memory decreases\nfrom 61.47 GB to 42.72 GB during a single epoch. Similar improvements are\nobserved with the CodeLlama-Python-7B model on the Evol-Instruct dataset. By\noptimizing both data selection and tokenization, our approach not only improves\nmodel performance but also improves training efficiency."
                },
                "authors": [
                    {
                        "name": "Weijie Lv"
                    },
                    {
                        "name": "Xuan Xia"
                    },
                    {
                        "name": "Sheng-Jun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Sheng-Jun Huang"
                },
                "author": "Sheng-Jun Huang",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2408.02193",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12687v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12687v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11793v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11793v2",
                "updated": "2025-04-17T06:24:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    6,
                    24,
                    14,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-16T05:59:29Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    5,
                    59,
                    29,
                    2,
                    106,
                    0
                ],
                "title": "Selective Attention Federated Learning: Improving Privacy and Efficiency\n  for Clinical Text Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selective Attention Federated Learning: Improving Privacy and Efficiency\n  for Clinical Text Classification"
                },
                "summary": "Federated Learning (FL) faces major challenges regarding communication\noverhead and model privacy when training large language models (LLMs),\nespecially in healthcare applications. To address these, we introduce Selective\nAttention Federated Learning (SAFL), a novel approach that dynamically\nfine-tunes only those transformer layers identified as attention-critical. By\nemploying attention patterns to determine layer importance, SAFL significantly\nreduces communication bandwidth and enhances differential privacy resilience.\nEvaluations on clinical NLP benchmarks (i2b2 Clinical Concept Extraction and\nMIMIC-III discharge summaries) demonstrate that SAFL achieves competitive\nperformance with centralized models while substantially improving communication\nefficiency and privacy preservation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) faces major challenges regarding communication\noverhead and model privacy when training large language models (LLMs),\nespecially in healthcare applications. To address these, we introduce Selective\nAttention Federated Learning (SAFL), a novel approach that dynamically\nfine-tunes only those transformer layers identified as attention-critical. By\nemploying attention patterns to determine layer importance, SAFL significantly\nreduces communication bandwidth and enhances differential privacy resilience.\nEvaluations on clinical NLP benchmarks (i2b2 Clinical Concept Extraction and\nMIMIC-III discharge summaries) demonstrate that SAFL achieves competitive\nperformance with centralized models while substantially improving communication\nefficiency and privacy preservation."
                },
                "authors": [
                    {
                        "name": "Yue Li"
                    },
                    {
                        "name": "Lihong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lihong Zhang"
                },
                "author": "Lihong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11793v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11793v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12682v1",
                "updated": "2025-04-17T06:16:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    6,
                    16,
                    40,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T06:16:40Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    6,
                    16,
                    40,
                    3,
                    107,
                    0
                ],
                "title": "WebLists: Extracting Structured Information From Complex Interactive\n  Websites Using Executable LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WebLists: Extracting Structured Information From Complex Interactive\n  Websites Using Executable LLM Agents"
                },
                "summary": "Most recent web agent research has focused on navigation and transaction\ntasks, with little emphasis on extracting structured data at scale. We present\nWebLists, a benchmark of 200 data-extraction tasks across four common business\nand enterprise use-cases. Each task requires an agent to navigate to a webpage,\nconfigure it appropriately, and extract complete datasets with well-defined\nschemas. We show that both LLMs with search capabilities and SOTA web agents\nstruggle with these tasks, with a recall of 3% and 31%, respectively, despite\nhigher performance on question-answering tasks.\n  To address this challenge, we propose BardeenAgent, a novel framework that\nenables web agents to convert their execution into repeatable programs, and\nreplay them at scale across pages with similar structure. BardeenAgent is also\nthe first LLM agent to take advantage of the regular structure of HTML. In\nparticular BardeenAgent constructs a generalizable CSS selector to capture all\nrelevant items on the page, then fits the operations to extract the data.\n  On the WebLists benchmark, BardeenAgent achieves 66% recall overall, more\nthan doubling the performance of SOTA web agents, and reducing cost per output\nrow by 3x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most recent web agent research has focused on navigation and transaction\ntasks, with little emphasis on extracting structured data at scale. We present\nWebLists, a benchmark of 200 data-extraction tasks across four common business\nand enterprise use-cases. Each task requires an agent to navigate to a webpage,\nconfigure it appropriately, and extract complete datasets with well-defined\nschemas. We show that both LLMs with search capabilities and SOTA web agents\nstruggle with these tasks, with a recall of 3% and 31%, respectively, despite\nhigher performance on question-answering tasks.\n  To address this challenge, we propose BardeenAgent, a novel framework that\nenables web agents to convert their execution into repeatable programs, and\nreplay them at scale across pages with similar structure. BardeenAgent is also\nthe first LLM agent to take advantage of the regular structure of HTML. In\nparticular BardeenAgent constructs a generalizable CSS selector to capture all\nrelevant items on the page, then fits the operations to extract the data.\n  On the WebLists benchmark, BardeenAgent achieves 66% recall overall, more\nthan doubling the performance of SOTA web agents, and reducing cost per output\nrow by 3x."
                },
                "authors": [
                    {
                        "name": "Arth Bohra"
                    },
                    {
                        "name": "Manvel Saroyan"
                    },
                    {
                        "name": "Danil Melkozerov"
                    },
                    {
                        "name": "Vahe Karufanyan"
                    },
                    {
                        "name": "Gabriel Maher"
                    },
                    {
                        "name": "Pascal Weinberger"
                    },
                    {
                        "name": "Artem Harutyunyan"
                    },
                    {
                        "name": "Giovanni Campagna"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Campagna"
                },
                "author": "Giovanni Campagna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12681v1",
                "updated": "2025-04-17T06:16:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    6,
                    16,
                    32,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T06:16:32Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    6,
                    16,
                    32,
                    3,
                    107,
                    0
                ],
                "title": "GRAIL: Gradient-Based Adaptive Unlearning for Privacy and Copyright in\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GRAIL: Gradient-Based Adaptive Unlearning for Privacy and Copyright in\n  LLMs"
                },
                "summary": "Large Language Models (LLMs) trained on extensive datasets often learn\nsensitive information, which raises significant social and legal concerns under\nprinciples such as the \"Right to be forgotten.\" Retraining entire models from\nscratch to remove undesired information is both costly and impractical.\nFurthermore, existing single-domain unlearning methods fail to address\nmulti-domain scenarios, where knowledge is interwoven across domains such as\nprivacy and copyright, creating overlapping representations that lead to\nexcessive knowledge removal or degraded performance. To tackle these issues, we\npropose GRAIL (GRadient-based AdaptIve unLearning), a novel multi-domain\nunlearning framework. GRAIL leverages gradient information from multiple\ndomains to precisely distinguish the unlearning scope from the retention scope,\nand applies an adaptive parameter-wise localization strategy to selectively\nremove targeted knowledge while preserving critical parameters for each domain.\nExperimental results on unlearning benchmarks show that GRAIL achieves\nunlearning success on par with the existing approaches, while also\ndemonstrating up to 17% stronger knowledge retention success compared to the\nprevious state-of-art method. Our findings establish a new paradigm for\neffectively managing and regulating sensitive information in large-scale\npre-trained language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) trained on extensive datasets often learn\nsensitive information, which raises significant social and legal concerns under\nprinciples such as the \"Right to be forgotten.\" Retraining entire models from\nscratch to remove undesired information is both costly and impractical.\nFurthermore, existing single-domain unlearning methods fail to address\nmulti-domain scenarios, where knowledge is interwoven across domains such as\nprivacy and copyright, creating overlapping representations that lead to\nexcessive knowledge removal or degraded performance. To tackle these issues, we\npropose GRAIL (GRadient-based AdaptIve unLearning), a novel multi-domain\nunlearning framework. GRAIL leverages gradient information from multiple\ndomains to precisely distinguish the unlearning scope from the retention scope,\nand applies an adaptive parameter-wise localization strategy to selectively\nremove targeted knowledge while preserving critical parameters for each domain.\nExperimental results on unlearning benchmarks show that GRAIL achieves\nunlearning success on par with the existing approaches, while also\ndemonstrating up to 17% stronger knowledge retention success compared to the\nprevious state-of-art method. Our findings establish a new paradigm for\neffectively managing and regulating sensitive information in large-scale\npre-trained language models."
                },
                "authors": [
                    {
                        "name": "Kun-Woo Kim"
                    },
                    {
                        "name": "Ji-Hoon Park"
                    },
                    {
                        "name": "Ju-Min Han"
                    },
                    {
                        "name": "Seong-Whan Lee"
                    }
                ],
                "author_detail": {
                    "name": "Seong-Whan Lee"
                },
                "author": "Seong-Whan Lee",
                "arxiv_comment": "Accepted by IJCNN 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12652v1",
                "updated": "2025-04-17T05:23:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    5,
                    23,
                    7,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T05:23:07Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    5,
                    23,
                    7,
                    3,
                    107,
                    0
                ],
                "title": "AdaptoVision: A Multi-Resolution Image Recognition Model for Robust and\n  Scalable Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaptoVision: A Multi-Resolution Image Recognition Model for Robust and\n  Scalable Classification"
                },
                "summary": "This paper introduces AdaptoVision, a novel convolutional neural network\n(CNN) architecture designed to efficiently balance computational complexity and\nclassification accuracy. By leveraging enhanced residual units, depth-wise\nseparable convolutions, and hierarchical skip connections, AdaptoVision\nsignificantly reduces parameter count and computational requirements while\npreserving competitive performance across various benchmark and medical image\ndatasets. Extensive experimentation demonstrates that AdaptoVision achieves\nstate-of-the-art on BreakHis dataset and comparable accuracy levels, notably\n95.3\\% on CIFAR-10 and 85.77\\% on CIFAR-100, without relying on any pretrained\nweights. The model's streamlined architecture and strategic simplifications\npromote effective feature extraction and robust generalization, making it\nparticularly suitable for deployment in real-time and resource-constrained\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces AdaptoVision, a novel convolutional neural network\n(CNN) architecture designed to efficiently balance computational complexity and\nclassification accuracy. By leveraging enhanced residual units, depth-wise\nseparable convolutions, and hierarchical skip connections, AdaptoVision\nsignificantly reduces parameter count and computational requirements while\npreserving competitive performance across various benchmark and medical image\ndatasets. Extensive experimentation demonstrates that AdaptoVision achieves\nstate-of-the-art on BreakHis dataset and comparable accuracy levels, notably\n95.3\\% on CIFAR-10 and 85.77\\% on CIFAR-100, without relying on any pretrained\nweights. The model's streamlined architecture and strategic simplifications\npromote effective feature extraction and robust generalization, making it\nparticularly suitable for deployment in real-time and resource-constrained\nenvironments."
                },
                "authors": [
                    {
                        "name": "Md. Sanaullah Chowdhury Lameya Sabrin"
                    }
                ],
                "author_detail": {
                    "name": "Md. Sanaullah Chowdhury Lameya Sabrin"
                },
                "author": "Md. Sanaullah Chowdhury Lameya Sabrin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10925v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10925v2",
                "updated": "2025-04-17T04:59:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    4,
                    59,
                    47,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-15T07:12:00Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    7,
                    12,
                    0,
                    1,
                    105,
                    0
                ],
                "title": "Transfer Learning for Temporal Link Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transfer Learning for Temporal Link Prediction"
                },
                "summary": "Link prediction on graphs has applications spanning from recommender systems\nto drug discovery. Temporal link prediction (TLP) refers to predicting future\nlinks in a temporally evolving graph and adds additional complexity related to\nthe dynamic nature of graphs. State-of-the-art TLP models incorporate memory\nmodules alongside graph neural networks to learn both the temporal mechanisms\nof incoming nodes and the evolving graph topology. However, memory modules only\nstore information about nodes seen at train time, and hence such models cannot\nbe directly transferred to entirely new graphs at test time and deployment. In\nthis work, we study a new transfer learning task for temporal link prediction,\nand develop transfer-effective methods for memory-laden models. Specifically,\nmotivated by work showing the informativeness of structural signals for the TLP\ntask, we augment a structural mapping module to the existing TLP model\narchitectures, which learns a mapping from graph structural (topological)\nfeatures to memory embeddings. Our work paves the way for a memory-free\nfoundation model for TLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Link prediction on graphs has applications spanning from recommender systems\nto drug discovery. Temporal link prediction (TLP) refers to predicting future\nlinks in a temporally evolving graph and adds additional complexity related to\nthe dynamic nature of graphs. State-of-the-art TLP models incorporate memory\nmodules alongside graph neural networks to learn both the temporal mechanisms\nof incoming nodes and the evolving graph topology. However, memory modules only\nstore information about nodes seen at train time, and hence such models cannot\nbe directly transferred to entirely new graphs at test time and deployment. In\nthis work, we study a new transfer learning task for temporal link prediction,\nand develop transfer-effective methods for memory-laden models. Specifically,\nmotivated by work showing the informativeness of structural signals for the TLP\ntask, we augment a structural mapping module to the existing TLP model\narchitectures, which learns a mapping from graph structural (topological)\nfeatures to memory embeddings. Our work paves the way for a memory-free\nfoundation model for TLP."
                },
                "authors": [
                    {
                        "name": "Ayan Chatterjee"
                    },
                    {
                        "name": "Barbara Ikica"
                    },
                    {
                        "name": "Babak Ravandi"
                    },
                    {
                        "name": "John Palowitch"
                    }
                ],
                "author_detail": {
                    "name": "John Palowitch"
                },
                "author": "John Palowitch",
                "arxiv_comment": "14 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10925v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10925v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12637v1",
                "updated": "2025-04-17T04:46:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    4,
                    46,
                    57,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T04:46:57Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    4,
                    46,
                    57,
                    3,
                    107,
                    0
                ],
                "title": "Scaling Instruction-Tuned LLMs to Million-Token Contexts via\n  Hierarchical Synthetic Data Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Instruction-Tuned LLMs to Million-Token Contexts via\n  Hierarchical Synthetic Data Generation"
                },
                "summary": "Large Language Models (LLMs) struggle with long-context reasoning, not only\ndue to the quadratic scaling of computational complexity with sequence length\nbut also because of the scarcity and expense of annotating long-context data.\nThere has been barely any open-source work that systematically ablates\nlong-context data, nor is there any openly available instruction tuning dataset\nwith contexts surpassing 100K tokens. To bridge this gap, we introduce a novel\npost-training synthetic data generation strategy designed to efficiently extend\nthe context window of LLMs while preserving their general task performance. Our\napproach scalably extends to arbitrarily long context lengths, unconstrained by\nthe length of available real-world data, which effectively addresses the\nscarcity of raw long-context data. Through a step-by-step rotary position\nembedding (RoPE) scaling training strategy, we demonstrate that our model, with\na context length of up to 1M tokens, performs well on the RULER benchmark and\nInfiniteBench and maintains robust performance on general language tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) struggle with long-context reasoning, not only\ndue to the quadratic scaling of computational complexity with sequence length\nbut also because of the scarcity and expense of annotating long-context data.\nThere has been barely any open-source work that systematically ablates\nlong-context data, nor is there any openly available instruction tuning dataset\nwith contexts surpassing 100K tokens. To bridge this gap, we introduce a novel\npost-training synthetic data generation strategy designed to efficiently extend\nthe context window of LLMs while preserving their general task performance. Our\napproach scalably extends to arbitrarily long context lengths, unconstrained by\nthe length of available real-world data, which effectively addresses the\nscarcity of raw long-context data. Through a step-by-step rotary position\nembedding (RoPE) scaling training strategy, we demonstrate that our model, with\na context length of up to 1M tokens, performs well on the RULER benchmark and\nInfiniteBench and maintains robust performance on general language tasks."
                },
                "authors": [
                    {
                        "name": "Linda He"
                    },
                    {
                        "name": "Jue Wang"
                    },
                    {
                        "name": "Maurice Weber"
                    },
                    {
                        "name": "Shang Zhu"
                    },
                    {
                        "name": "Ben Athiwaratkun"
                    },
                    {
                        "name": "Ce Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ce Zhang"
                },
                "author": "Ce Zhang",
                "arxiv_comment": "26 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03160v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03160v4",
                "updated": "2025-04-17T04:46:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    4,
                    46,
                    8,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-04T04:41:28Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    4,
                    41,
                    28,
                    4,
                    94,
                    0
                ],
                "title": "DeepResearcher: Scaling Deep Research via Reinforcement Learning in\n  Real-world Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepResearcher: Scaling Deep Research via Reinforcement Learning in\n  Real-world Environments"
                },
                "summary": "Large Language Models (LLMs) equipped with web search capabilities have\ndemonstrated impressive potential for deep research tasks. However, current\napproaches predominantly rely on either manually engineered prompts (prompt\nengineering-based) with brittle performance or reinforcement learning within\ncontrolled Retrieval-Augmented Generation (RAG) environments (RAG-based) that\nfail to capture the complexities of real-world interaction. In this paper, we\nintroduce DeepResearcher, the first comprehensive framework for end-to-end\ntraining of LLM-based deep research agents through scaling reinforcement\nlearning (RL) in real-world environments with authentic web search\ninteractions. Unlike RAG-based approaches that assume all necessary information\nexists within a fixed corpus, our method trains agents to navigate the noisy,\nunstructured, and dynamic nature of the open web. We implement a specialized\nmulti-agent architecture where browsing agents extract relevant information\nfrom various webpage structures and overcoming significant technical\nchallenges. Extensive experiments on open-domain research tasks demonstrate\nthat DeepResearcher achieves substantial improvements of up to 28.9 points over\nprompt engineering-based baselines and up to 7.2 points over RAG-based RL\nagents. Our qualitative analysis reveals emergent cognitive behaviors from\nend-to-end RL training, including the ability to formulate plans,\ncross-validate information from multiple sources, engage in self-reflection to\nredirect research, and maintain honesty when unable to find definitive answers.\nOur results highlight that end-to-end training in real-world web environments\nis not merely an implementation detail but a fundamental requirement for\ndeveloping robust research capabilities aligned with real-world applications.\nWe release DeepResearcher at https://github.com/GAIR-NLP/DeepResearcher.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) equipped with web search capabilities have\ndemonstrated impressive potential for deep research tasks. However, current\napproaches predominantly rely on either manually engineered prompts (prompt\nengineering-based) with brittle performance or reinforcement learning within\ncontrolled Retrieval-Augmented Generation (RAG) environments (RAG-based) that\nfail to capture the complexities of real-world interaction. In this paper, we\nintroduce DeepResearcher, the first comprehensive framework for end-to-end\ntraining of LLM-based deep research agents through scaling reinforcement\nlearning (RL) in real-world environments with authentic web search\ninteractions. Unlike RAG-based approaches that assume all necessary information\nexists within a fixed corpus, our method trains agents to navigate the noisy,\nunstructured, and dynamic nature of the open web. We implement a specialized\nmulti-agent architecture where browsing agents extract relevant information\nfrom various webpage structures and overcoming significant technical\nchallenges. Extensive experiments on open-domain research tasks demonstrate\nthat DeepResearcher achieves substantial improvements of up to 28.9 points over\nprompt engineering-based baselines and up to 7.2 points over RAG-based RL\nagents. Our qualitative analysis reveals emergent cognitive behaviors from\nend-to-end RL training, including the ability to formulate plans,\ncross-validate information from multiple sources, engage in self-reflection to\nredirect research, and maintain honesty when unable to find definitive answers.\nOur results highlight that end-to-end training in real-world web environments\nis not merely an implementation detail but a fundamental requirement for\ndeveloping robust research capabilities aligned with real-world applications.\nWe release DeepResearcher at https://github.com/GAIR-NLP/DeepResearcher."
                },
                "authors": [
                    {
                        "name": "Yuxiang Zheng"
                    },
                    {
                        "name": "Dayuan Fu"
                    },
                    {
                        "name": "Xiangkun Hu"
                    },
                    {
                        "name": "Xiaojie Cai"
                    },
                    {
                        "name": "Lyumanshan Ye"
                    },
                    {
                        "name": "Pengrui Lu"
                    },
                    {
                        "name": "Pengfei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Pengfei Liu"
                },
                "author": "Pengfei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03160v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03160v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12633v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12633v1",
                "updated": "2025-04-17T04:20:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    4,
                    20,
                    5,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T04:20:05Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    4,
                    20,
                    5,
                    3,
                    107,
                    0
                ],
                "title": "Towards Characterizing Subjectivity of Individuals through Modeling\n  Value Conflicts and Trade-offs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Characterizing Subjectivity of Individuals through Modeling\n  Value Conflicts and Trade-offs"
                },
                "summary": "Large Language Models (LLMs) not only have solved complex reasoning problems\nbut also exhibit remarkable performance in tasks that require subjective\ndecision making. Existing studies suggest that LLM generations can be\nsubjectively grounded to some extent, yet exploring whether LLMs can account\nfor individual-level subjectivity has not been sufficiently studied. In this\npaper, we characterize subjectivity of individuals on social media and infer\ntheir moral judgments using LLMs. We propose a framework, SOLAR (Subjective\nGround with Value Abstraction), that observes value conflicts and trade-offs in\nthe user-generated texts to better represent subjective ground of individuals.\nEmpirical results show that our framework improves overall inference results as\nwell as performance on controversial situations. Additionally, we qualitatively\nshow that SOLAR provides explanations about individuals' value preferences,\nwhich can further account for their judgments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) not only have solved complex reasoning problems\nbut also exhibit remarkable performance in tasks that require subjective\ndecision making. Existing studies suggest that LLM generations can be\nsubjectively grounded to some extent, yet exploring whether LLMs can account\nfor individual-level subjectivity has not been sufficiently studied. In this\npaper, we characterize subjectivity of individuals on social media and infer\ntheir moral judgments using LLMs. We propose a framework, SOLAR (Subjective\nGround with Value Abstraction), that observes value conflicts and trade-offs in\nthe user-generated texts to better represent subjective ground of individuals.\nEmpirical results show that our framework improves overall inference results as\nwell as performance on controversial situations. Additionally, we qualitatively\nshow that SOLAR provides explanations about individuals' value preferences,\nwhich can further account for their judgments."
                },
                "authors": [
                    {
                        "name": "Younghun Lee"
                    },
                    {
                        "name": "Dan Goldwasser"
                    }
                ],
                "author_detail": {
                    "name": "Dan Goldwasser"
                },
                "author": "Dan Goldwasser",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12633v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12633v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12876v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12876v3",
                "updated": "2025-04-17T03:51:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    3,
                    51,
                    6,
                    3,
                    107,
                    0
                ],
                "published": "2024-10-15T05:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    1,
                    19,
                    1,
                    289,
                    0
                ],
                "title": "In-context KV-Cache Eviction for LLMs via Attention-Gate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context KV-Cache Eviction for LLMs via Attention-Gate"
                },
                "summary": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system. This paper enables a novel dynamic\nKV-Cache eviction policy by injecting a lightweight module called\nAttention-Gate to the model. It accepts the global context as input and yields\neviction flags for each token. The self-attention modules in the model proceed\naccording to the flags and cache only a subset of the KV states for next token\nprediction. The Attention-Gates can yield various flags for different heads and\nlayers and be easily tuned on top of a pre-trained LLM via continual\npre-training or supervised fine-tuning. The computational and memory overhead\nintroduced by Attention-Gates can be minimal. We empirically evaluate the\nproposed approach across multiple scenarios, showing that effective eviction of\nredundant tokens can not only improve efficiency but also enhance performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system. This paper enables a novel dynamic\nKV-Cache eviction policy by injecting a lightweight module called\nAttention-Gate to the model. It accepts the global context as input and yields\neviction flags for each token. The self-attention modules in the model proceed\naccording to the flags and cache only a subset of the KV states for next token\nprediction. The Attention-Gates can yield various flags for different heads and\nlayers and be easily tuned on top of a pre-trained LLM via continual\npre-training or supervised fine-tuning. The computational and memory overhead\nintroduced by Attention-Gates can be minimal. We empirically evaluate the\nproposed approach across multiple scenarios, showing that effective eviction of\nredundant tokens can not only improve efficiency but also enhance performance."
                },
                "authors": [
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Bokai Lin"
                    },
                    {
                        "name": "Tianqi Hou"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhijie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Deng"
                },
                "author": "Zhijie Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12876v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12876v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09593v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09593v2",
                "updated": "2025-04-17T03:50:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    3,
                    50,
                    19,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-13T14:18:35Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    18,
                    35,
                    6,
                    103,
                    0
                ],
                "title": "ControlNET: A Firewall for RAG-based LLM System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ControlNET: A Firewall for RAG-based LLM System"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has significantly enhanced the factual\naccuracy and domain adaptability of Large Language Models (LLMs). This\nadvancement has enabled their widespread deployment across sensitive domains\nsuch as healthcare, finance, and enterprise applications. RAG mitigates\nhallucinations by integrating external knowledge, yet introduces privacy risk\nand security risk, notably data breaching risk and data poisoning risk. While\nrecent studies have explored prompt injection and poisoning attacks, there\nremains a significant gap in comprehensive research on controlling inbound and\noutbound query flows to mitigate these threats. In this paper, we propose an AI\nfirewall, ControlNET, designed to safeguard RAG-based LLM systems from these\nvulnerabilities. ControlNET controls query flows by leveraging activation shift\nphenomena to detect adversarial queries and mitigate their impact through\nsemantic divergence. We conduct comprehensive experiments on four different\nbenchmark datasets including Msmarco, HotpotQA, FinQA, and MedicalSys using\nstate-of-the-art open source LLMs (Llama3, Vicuna, and Mistral). Our results\ndemonstrate that ControlNET achieves over 0.909 AUROC in detecting and\nmitigating security threats while preserving system harmlessness. Overall,\nControlNET offers an effective, robust, harmless defense mechanism, marking a\nsignificant advancement toward the secure deployment of RAG-based LLM systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has significantly enhanced the factual\naccuracy and domain adaptability of Large Language Models (LLMs). This\nadvancement has enabled their widespread deployment across sensitive domains\nsuch as healthcare, finance, and enterprise applications. RAG mitigates\nhallucinations by integrating external knowledge, yet introduces privacy risk\nand security risk, notably data breaching risk and data poisoning risk. While\nrecent studies have explored prompt injection and poisoning attacks, there\nremains a significant gap in comprehensive research on controlling inbound and\noutbound query flows to mitigate these threats. In this paper, we propose an AI\nfirewall, ControlNET, designed to safeguard RAG-based LLM systems from these\nvulnerabilities. ControlNET controls query flows by leveraging activation shift\nphenomena to detect adversarial queries and mitigate their impact through\nsemantic divergence. We conduct comprehensive experiments on four different\nbenchmark datasets including Msmarco, HotpotQA, FinQA, and MedicalSys using\nstate-of-the-art open source LLMs (Llama3, Vicuna, and Mistral). Our results\ndemonstrate that ControlNET achieves over 0.909 AUROC in detecting and\nmitigating security threats while preserving system harmlessness. Overall,\nControlNET offers an effective, robust, harmless defense mechanism, marking a\nsignificant advancement toward the secure deployment of RAG-based LLM systems."
                },
                "authors": [
                    {
                        "name": "Hongwei Yao"
                    },
                    {
                        "name": "Haoran Shi"
                    },
                    {
                        "name": "Yidou Chen"
                    },
                    {
                        "name": "Yixin Jiang"
                    },
                    {
                        "name": "Cong Wang"
                    },
                    {
                        "name": "Zhan Qin"
                    }
                ],
                "author_detail": {
                    "name": "Zhan Qin"
                },
                "author": "Zhan Qin",
                "arxiv_comment": "Project Page: https://ai.zjuicsr.cn/firewall",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09593v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09593v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15395v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15395v2",
                "updated": "2025-04-17T03:38:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    3,
                    38,
                    27,
                    3,
                    107,
                    0
                ],
                "published": "2025-02-21T11:46:04Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    11,
                    46,
                    4,
                    4,
                    52,
                    0
                ],
                "title": "Beyond Tools: Understanding How Heavy Users Integrate LLMs into Everyday\n  Tasks and Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Tools: Understanding How Heavy Users Integrate LLMs into Everyday\n  Tasks and Decision-Making"
                },
                "summary": "Large language models (LLMs) are increasingly used for both everyday and\nspecialized tasks. While HCI research focuses on domain-specific applications,\nlittle is known about how heavy users integrate LLMs into everyday\ndecision-making. Through qualitative interviews with heavy LLM users (n=7) who\nemploy these systems for both intuitive and analytical thinking tasks, our\nfindings show that participants use LLMs for social validation,\nself-regulation, and interpersonal guidance, seeking to build self-confidence\nand optimize cognitive resources. These users viewed LLMs either as rational,\nconsistent entities or average human decision-makers. Our findings suggest that\nheavy LLM users develop nuanced interaction patterns beyond simple delegation,\nhighlighting the need to reconsider how we study LLM integration in\ndecision-making processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used for both everyday and\nspecialized tasks. While HCI research focuses on domain-specific applications,\nlittle is known about how heavy users integrate LLMs into everyday\ndecision-making. Through qualitative interviews with heavy LLM users (n=7) who\nemploy these systems for both intuitive and analytical thinking tasks, our\nfindings show that participants use LLMs for social validation,\nself-regulation, and interpersonal guidance, seeking to build self-confidence\nand optimize cognitive resources. These users viewed LLMs either as rational,\nconsistent entities or average human decision-makers. Our findings suggest that\nheavy LLM users develop nuanced interaction patterns beyond simple delegation,\nhighlighting the need to reconsider how we study LLM integration in\ndecision-making processes."
                },
                "authors": [
                    {
                        "name": "Eunhye Kim"
                    },
                    {
                        "name": "Kiroong Choe"
                    },
                    {
                        "name": "Minju Yoo"
                    },
                    {
                        "name": "Sadat Shams Chowdhury"
                    },
                    {
                        "name": "Jinwook Seo"
                    }
                ],
                "author_detail": {
                    "name": "Jinwook Seo"
                },
                "author": "Jinwook Seo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15395v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15395v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10982v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10982v3",
                "updated": "2025-04-17T03:27:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    3,
                    27,
                    55,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-15T08:46:39Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    8,
                    46,
                    39,
                    1,
                    105,
                    0
                ],
                "title": "Exploring the Role of Knowledge Graph-Based RAG in Japanese Medical\n  Question Answering with Small-Scale LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Role of Knowledge Graph-Based RAG in Japanese Medical\n  Question Answering with Small-Scale LLMs"
                },
                "summary": "Large language models (LLMs) perform well in medical QA, but their\neffectiveness in Japanese contexts is limited due to privacy constraints that\nprevent the use of commercial models like GPT-4 in clinical settings. As a\nresult, recent efforts focus on instruction-tuning open-source LLMs, though the\npotential of combining them with retrieval-augmented generation (RAG) remains\nunderexplored. To bridge this gap, we are the first to explore a knowledge\ngraph-based (KG) RAG framework for Japanese medical QA small-scale open-source\nLLMs. Experimental results show that KG-based RAG has only a limited impact on\nJapanese medical QA using small-scale open-source LLMs. Further case studies\nreveal that the effectiveness of the RAG is sensitive to the quality and\nrelevance of the external retrieved content. These findings offer valuable\ninsights into the challenges and potential of applying RAG in Japanese medical\nQA, while also serving as a reference for other low-resource languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) perform well in medical QA, but their\neffectiveness in Japanese contexts is limited due to privacy constraints that\nprevent the use of commercial models like GPT-4 in clinical settings. As a\nresult, recent efforts focus on instruction-tuning open-source LLMs, though the\npotential of combining them with retrieval-augmented generation (RAG) remains\nunderexplored. To bridge this gap, we are the first to explore a knowledge\ngraph-based (KG) RAG framework for Japanese medical QA small-scale open-source\nLLMs. Experimental results show that KG-based RAG has only a limited impact on\nJapanese medical QA using small-scale open-source LLMs. Further case studies\nreveal that the effectiveness of the RAG is sensitive to the quality and\nrelevance of the external retrieved content. These findings offer valuable\ninsights into the challenges and potential of applying RAG in Japanese medical\nQA, while also serving as a reference for other low-resource languages."
                },
                "authors": [
                    {
                        "name": "Yingjian Chen"
                    },
                    {
                        "name": "Feiyang Li"
                    },
                    {
                        "name": "Xingyu Song"
                    },
                    {
                        "name": "Tianxiao Li"
                    },
                    {
                        "name": "Issey Sukeda"
                    },
                    {
                        "name": "Irene Li"
                    }
                ],
                "author_detail": {
                    "name": "Irene Li"
                },
                "author": "Irene Li",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10982v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10982v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02269v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02269v2",
                "updated": "2025-04-17T03:14:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    3,
                    14,
                    31,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-03T04:30:10Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    4,
                    30,
                    10,
                    3,
                    93,
                    0
                ],
                "title": "Engineering Artificial Intelligence: Framework, Challenges, and Future\n  Direction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Engineering Artificial Intelligence: Framework, Challenges, and Future\n  Direction"
                },
                "summary": "Over the past ten years, the application of artificial intelligence (AI) and\nmachine learning (ML) in engineering domains has gained significant popularity,\nshowcasing their potential in data-driven contexts. However, the complexity and\ndiversity of engineering problems often require the development of\ndomain-specific AI approaches, which are frequently hindered by a lack of\nsystematic methodologies, scalability, and robustness during the development\nprocess. To address this gap, this paper introduces the \"ABCDE\" as the key\nelements of Engineering AI and proposes a unified, systematic engineering AI\necosystem framework, including eight essential layers, along with attributes,\ngoals, and applications, to guide the development and deployment of AI\nsolutions for specific engineering needs. Additionally, key challenges are\nexamined, and eight future research directions are highlighted. By providing a\ncomprehensive perspective, this paper aims to advance the strategic\nimplementation of AI, fostering the development of next-generation engineering\nAI solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over the past ten years, the application of artificial intelligence (AI) and\nmachine learning (ML) in engineering domains has gained significant popularity,\nshowcasing their potential in data-driven contexts. However, the complexity and\ndiversity of engineering problems often require the development of\ndomain-specific AI approaches, which are frequently hindered by a lack of\nsystematic methodologies, scalability, and robustness during the development\nprocess. To address this gap, this paper introduces the \"ABCDE\" as the key\nelements of Engineering AI and proposes a unified, systematic engineering AI\necosystem framework, including eight essential layers, along with attributes,\ngoals, and applications, to guide the development and deployment of AI\nsolutions for specific engineering needs. Additionally, key challenges are\nexamined, and eight future research directions are highlighted. By providing a\ncomprehensive perspective, this paper aims to advance the strategic\nimplementation of AI, fostering the development of next-generation engineering\nAI solutions."
                },
                "authors": [
                    {
                        "name": "Jay Lee"
                    },
                    {
                        "name": "Hanqi Su"
                    },
                    {
                        "name": "Dai-Yan Ji"
                    },
                    {
                        "name": "Takanobu Minami"
                    }
                ],
                "author_detail": {
                    "name": "Takanobu Minami"
                },
                "author": "Takanobu Minami",
                "arxiv_comment": "The paper submitted to the Journal Machine Learning: Engineering has\n  been accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02269v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02269v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12608v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12608v1",
                "updated": "2025-04-17T03:13:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    3,
                    13,
                    39,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T03:13:39Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    3,
                    13,
                    39,
                    3,
                    107,
                    0
                ],
                "title": "Code Copycat Conundrum: Demystifying Repetition in LLM-based Code\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code Copycat Conundrum: Demystifying Repetition in LLM-based Code\n  Generation"
                },
                "summary": "Despite recent advances in Large Language Models (LLMs) for code generation,\nthe quality of LLM-generated code still faces significant challenges. One\nsignificant issue is code repetition, which refers to the model's tendency to\ngenerate structurally redundant code, resulting in inefficiencies and reduced\nreadability. To address this, we conduct the first empirical study to\ninvestigate the prevalence and nature of repetition across 19 state-of-the-art\ncode LLMs using three widely-used benchmarks. Our study includes both\nquantitative and qualitative analyses, revealing that repetition is pervasive\nand manifests at various granularities and extents, including character,\nstatement, and block levels. We further summarize a taxonomy of 20 repetition\npatterns. Building on our findings, we propose DeRep, a rule-based technique\ndesigned to detect and mitigate repetition in generated code. We evaluate DeRep\nusing both open-source benchmarks and in an industrial setting. Our results\ndemonstrate that DeRep significantly outperforms baselines in reducing\nrepetition (with an average improvements of 91.3%, 93.5%, and 79.9% in rep-3,\nrep-line, and sim-line metrics) and enhancing code quality (with a Pass@1\nincrease of 208.3% over greedy search). Furthermore, integrating DeRep improves\nthe performance of existing repetition mitigation methods, with Pass@1\nimprovements ranging from 53.7% to 215.7%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent advances in Large Language Models (LLMs) for code generation,\nthe quality of LLM-generated code still faces significant challenges. One\nsignificant issue is code repetition, which refers to the model's tendency to\ngenerate structurally redundant code, resulting in inefficiencies and reduced\nreadability. To address this, we conduct the first empirical study to\ninvestigate the prevalence and nature of repetition across 19 state-of-the-art\ncode LLMs using three widely-used benchmarks. Our study includes both\nquantitative and qualitative analyses, revealing that repetition is pervasive\nand manifests at various granularities and extents, including character,\nstatement, and block levels. We further summarize a taxonomy of 20 repetition\npatterns. Building on our findings, we propose DeRep, a rule-based technique\ndesigned to detect and mitigate repetition in generated code. We evaluate DeRep\nusing both open-source benchmarks and in an industrial setting. Our results\ndemonstrate that DeRep significantly outperforms baselines in reducing\nrepetition (with an average improvements of 91.3%, 93.5%, and 79.9% in rep-3,\nrep-line, and sim-line metrics) and enhancing code quality (with a Pass@1\nincrease of 208.3% over greedy search). Furthermore, integrating DeRep improves\nthe performance of existing repetition mitigation methods, with Pass@1\nimprovements ranging from 53.7% to 215.7%."
                },
                "authors": [
                    {
                        "name": "Mingwei Liu"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Ying Wang"
                    },
                    {
                        "name": "Xueying Du"
                    },
                    {
                        "name": "Zuoyu Ou"
                    },
                    {
                        "name": "Qiuyuan Chen"
                    },
                    {
                        "name": "Bingxu An"
                    },
                    {
                        "name": "Zhao Wei"
                    },
                    {
                        "name": "Yong Xu"
                    },
                    {
                        "name": "Fangming Zou"
                    },
                    {
                        "name": "Xin Peng"
                    },
                    {
                        "name": "Yiling Lou"
                    }
                ],
                "author_detail": {
                    "name": "Yiling Lou"
                },
                "author": "Yiling Lou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12608v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12608v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07246v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07246v5",
                "updated": "2025-04-17T03:01:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    3,
                    1,
                    41,
                    3,
                    107,
                    0
                ],
                "published": "2024-08-14T01:16:40Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    1,
                    16,
                    40,
                    2,
                    227,
                    0
                ],
                "title": "ChemVLM: Exploring the Power of Multimodal Large Language Models in\n  Chemistry Area",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChemVLM: Exploring the Power of Multimodal Large Language Models in\n  Chemistry Area"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success and have been\napplied across various scientific fields, including chemistry. However, many\nchemical tasks require the processing of visual information, which cannot be\nsuccessfully handled by existing chemical LLMs. This brings a growing need for\nmodels capable of integrating multimodal information in the chemical domain. In\nthis paper, we introduce \\textbf{ChemVLM}, an open-source chemical multimodal\nlarge language model specifically designed for chemical applications. ChemVLM\nis trained on a carefully curated bilingual multimodal dataset that enhances\nits ability to understand both textual and visual chemical information,\nincluding molecular structures, reactions, and chemistry examination questions.\nWe develop three datasets for comprehensive evaluation, tailored to Chemical\nOptical Character Recognition (OCR), Multimodal Chemical Reasoning (MMCR), and\nMultimodal Molecule Understanding tasks. We benchmark ChemVLM against a range\nof open-source and proprietary multimodal large language models on various\ntasks. Experimental results demonstrate that ChemVLM achieves competitive\nperformance across all evaluated tasks. Our model can be found at\nhttps://huggingface.co/AI4Chem/ChemVLM-26B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success and have been\napplied across various scientific fields, including chemistry. However, many\nchemical tasks require the processing of visual information, which cannot be\nsuccessfully handled by existing chemical LLMs. This brings a growing need for\nmodels capable of integrating multimodal information in the chemical domain. In\nthis paper, we introduce \\textbf{ChemVLM}, an open-source chemical multimodal\nlarge language model specifically designed for chemical applications. ChemVLM\nis trained on a carefully curated bilingual multimodal dataset that enhances\nits ability to understand both textual and visual chemical information,\nincluding molecular structures, reactions, and chemistry examination questions.\nWe develop three datasets for comprehensive evaluation, tailored to Chemical\nOptical Character Recognition (OCR), Multimodal Chemical Reasoning (MMCR), and\nMultimodal Molecule Understanding tasks. We benchmark ChemVLM against a range\nof open-source and proprietary multimodal large language models on various\ntasks. Experimental results demonstrate that ChemVLM achieves competitive\nperformance across all evaluated tasks. Our model can be found at\nhttps://huggingface.co/AI4Chem/ChemVLM-26B."
                },
                "authors": [
                    {
                        "name": "Junxian Li"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Xunzhi Wang"
                    },
                    {
                        "name": "Zeying Hao"
                    },
                    {
                        "name": "Jingdi Lei"
                    },
                    {
                        "name": "Qian Tan"
                    },
                    {
                        "name": "Cai Zhou"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Yaotian Yang"
                    },
                    {
                        "name": "Xinrui Xiong"
                    },
                    {
                        "name": "Weiyun Wang"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Wenhai Wang"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Shufei Zhang"
                    },
                    {
                        "name": "Mao Su"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Yuqiang Li"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Dongzhan Zhou"
                },
                "author": "Dongzhan Zhou",
                "arxiv_comment": "11 pages, updated version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07246v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07246v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01797v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01797v2",
                "updated": "2025-04-17T02:54:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    2,
                    54,
                    57,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-02T15:05:32Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    15,
                    5,
                    32,
                    2,
                    92,
                    0
                ],
                "title": "Rethinking industrial artificial intelligence: a unified foundation\n  framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking industrial artificial intelligence: a unified foundation\n  framework"
                },
                "summary": "Recent advancements in industrial artificial intelligence (AI) are reshaping\nthe industry by driving smarter manufacturing, predictive maintenance, and\nintelligent decision-making. However, existing approaches often focus primarily\non algorithms and models while overlooking the importance of systematically\nintegrating domain knowledge, data, and models to develop more comprehensive\nand effective AI solutions. Therefore, the effective development and deployment\nof industrial AI require a more comprehensive and systematic approach. To\naddress this gap, this paper reviews previous research, rethinks the role of\nindustrial AI, and proposes a unified industrial AI foundation framework\ncomprising three core modules: the knowledge module, data module, and model\nmodule. These modules help to extend and enhance the industrial AI methodology\nplatform, supporting various industrial applications. In addition, a case study\non rotating machinery diagnosis is presented to demonstrate the effectiveness\nof the proposed framework, and several future directions are highlighted for\nthe development of the industrial AI foundation framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in industrial artificial intelligence (AI) are reshaping\nthe industry by driving smarter manufacturing, predictive maintenance, and\nintelligent decision-making. However, existing approaches often focus primarily\non algorithms and models while overlooking the importance of systematically\nintegrating domain knowledge, data, and models to develop more comprehensive\nand effective AI solutions. Therefore, the effective development and deployment\nof industrial AI require a more comprehensive and systematic approach. To\naddress this gap, this paper reviews previous research, rethinks the role of\nindustrial AI, and proposes a unified industrial AI foundation framework\ncomprising three core modules: the knowledge module, data module, and model\nmodule. These modules help to extend and enhance the industrial AI methodology\nplatform, supporting various industrial applications. In addition, a case study\non rotating machinery diagnosis is presented to demonstrate the effectiveness\nof the proposed framework, and several future directions are highlighted for\nthe development of the industrial AI foundation framework."
                },
                "authors": [
                    {
                        "name": "Jay Lee"
                    },
                    {
                        "name": "Hanqi Su"
                    }
                ],
                "author_detail": {
                    "name": "Hanqi Su"
                },
                "author": "Hanqi Su",
                "arxiv_doi": "10.36922/IJAMD025080006",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.36922/IJAMD025080006",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.01797v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01797v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "The paper submitted to IJAMD, the International Journal of AI for\n  Materials and Design, has been accepted",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07791v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07791v8",
                "updated": "2025-04-17T02:43:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    2,
                    43,
                    35,
                    3,
                    107,
                    0
                ],
                "published": "2024-06-12T01:12:28Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    1,
                    12,
                    28,
                    2,
                    164,
                    0
                ],
                "title": "Judging the Judges: A Systematic Study of Position Bias in\n  LLM-as-a-Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Judging the Judges: A Systematic Study of Position Bias in\n  LLM-as-a-Judge"
                },
                "summary": "LLM-as-a-Judge has emerged as a promising alternative to human evaluators\nacross various tasks, yet inherent biases - particularly position bias, the\ntendency to favor solutions based on their position within the prompt -\ncompromise its reliability. This exploratory study evaluates position bias in\nLLM judges across pairwise and list-wise comparison settings, introducing three\nmetrics: repetition stability, position consistency, and preference fairness.\nOur experiments, involving 15 LLM judges across MTBench and DevBench with 22\ntasks and approximately 40 solution-generating models, result in over 150,000\nevaluation instances. We identify Judge-Level, Candidate-Level, and Task-Level\nfactors contributing to bias. The findings confirm that position bias is not\ndue to random chance and varies significantly across judges and tasks. While\nposition bias is weakly influenced by the length of prompt components, it is\nstrongly affected by the quality gap between solutions. Our agreement and\ndisagreement analysis among judges further provides insights into the\ndistribution of judging difficulty across the dataset, and highlights the\npotential for dataset modifications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-as-a-Judge has emerged as a promising alternative to human evaluators\nacross various tasks, yet inherent biases - particularly position bias, the\ntendency to favor solutions based on their position within the prompt -\ncompromise its reliability. This exploratory study evaluates position bias in\nLLM judges across pairwise and list-wise comparison settings, introducing three\nmetrics: repetition stability, position consistency, and preference fairness.\nOur experiments, involving 15 LLM judges across MTBench and DevBench with 22\ntasks and approximately 40 solution-generating models, result in over 150,000\nevaluation instances. We identify Judge-Level, Candidate-Level, and Task-Level\nfactors contributing to bias. The findings confirm that position bias is not\ndue to random chance and varies significantly across judges and tasks. While\nposition bias is weakly influenced by the length of prompt components, it is\nstrongly affected by the quality gap between solutions. Our agreement and\ndisagreement analysis among judges further provides insights into the\ndistribution of judging difficulty across the dataset, and highlights the\npotential for dataset modifications."
                },
                "authors": [
                    {
                        "name": "Lin Shi"
                    },
                    {
                        "name": "Chiyu Ma"
                    },
                    {
                        "name": "Wenhua Liang"
                    },
                    {
                        "name": "Xingjian Diao"
                    },
                    {
                        "name": "Weicheng Ma"
                    },
                    {
                        "name": "Soroush Vosoughi"
                    }
                ],
                "author_detail": {
                    "name": "Soroush Vosoughi"
                },
                "author": "Soroush Vosoughi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07791v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07791v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11711v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11711v2",
                "updated": "2025-04-17T02:28:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    2,
                    28,
                    35,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-16T02:17:06Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    2,
                    17,
                    6,
                    2,
                    106,
                    0
                ],
                "title": "The Hitchhiker's Guide to Program Analysis, Part II: Deep Thoughts by\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hitchhiker's Guide to Program Analysis, Part II: Deep Thoughts by\n  LLMs"
                },
                "summary": "Static analysis is a cornerstone for software vulnerability detection, yet it\noften struggles with the classic precision-scalability trade-off. In practice,\nsuch tools often produce high false positive rates, particularly in large\ncodebases like the Linux kernel. This imprecision can arise from simplified\nvulnerability modeling and over-approximation of path and data constraints.\nWhile large language models (LLMs) show promise in code understanding, their\nnaive application to program analysis yields unreliable results due to inherent\nreasoning limitations. We introduce BugLens, a post-refinement framework that\nsignificantly improves static analysis precision. BugLens guides an LLM to\nfollow traditional analysis steps by assessing buggy code patterns for security\nimpact and validating the constraints associated with static warnings.\nEvaluated on real-world Linux kernel bugs, BugLens raises precision from 0.10\n(raw) and 0.50 (semi-automated refinement) to 0.72, substantially reducing\nfalse positives and revealing four previously unreported vulnerabilities. Our\nresults suggest that a structured LLM-based workflow can meaningfully enhance\nthe effectiveness of static analysis tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static analysis is a cornerstone for software vulnerability detection, yet it\noften struggles with the classic precision-scalability trade-off. In practice,\nsuch tools often produce high false positive rates, particularly in large\ncodebases like the Linux kernel. This imprecision can arise from simplified\nvulnerability modeling and over-approximation of path and data constraints.\nWhile large language models (LLMs) show promise in code understanding, their\nnaive application to program analysis yields unreliable results due to inherent\nreasoning limitations. We introduce BugLens, a post-refinement framework that\nsignificantly improves static analysis precision. BugLens guides an LLM to\nfollow traditional analysis steps by assessing buggy code patterns for security\nimpact and validating the constraints associated with static warnings.\nEvaluated on real-world Linux kernel bugs, BugLens raises precision from 0.10\n(raw) and 0.50 (semi-automated refinement) to 0.72, substantially reducing\nfalse positives and revealing four previously unreported vulnerabilities. Our\nresults suggest that a structured LLM-based workflow can meaningfully enhance\nthe effectiveness of static analysis tools."
                },
                "authors": [
                    {
                        "name": "Haonan Li"
                    },
                    {
                        "name": "Hang Zhang"
                    },
                    {
                        "name": "Kexin Pei"
                    },
                    {
                        "name": "Zhiyun Qian"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyun Qian"
                },
                "author": "Zhiyun Qian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11711v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11711v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10673v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10673v2",
                "updated": "2025-04-17T02:14:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    2,
                    14,
                    21,
                    3,
                    107,
                    0
                ],
                "published": "2025-03-10T16:54:27Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    16,
                    54,
                    27,
                    0,
                    69,
                    0
                ],
                "title": "ZeroSumEval: An Extensible Framework For Scaling LLM Evaluation with\n  Inter-Model Competition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZeroSumEval: An Extensible Framework For Scaling LLM Evaluation with\n  Inter-Model Competition"
                },
                "summary": "We introduce ZeroSumEval, a dynamic, competition-based, and evolving\nevaluation framework for Large Language Models (LLMs) that leverages\ncompetitive games. ZeroSumEval encompasses a diverse suite of games, including\nsecurity challenges (Capture the Flag), classic board games (chess), and\nknowledge tests (MathQuiz). These games are designed to evaluate a range of\ncapabilities such as strategic reasoning, planning, knowledge application,\nsafety, and adaptability. Building upon recent studies that highlight the\neffectiveness of game-based evaluations for LLMs, ZeroSumEval enhances these\napproaches by providing a standardized and extensible framework for easily\nimplementing games and leverages DSPy to provide a better abstraction for LLM\nplayer strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ZeroSumEval, a dynamic, competition-based, and evolving\nevaluation framework for Large Language Models (LLMs) that leverages\ncompetitive games. ZeroSumEval encompasses a diverse suite of games, including\nsecurity challenges (Capture the Flag), classic board games (chess), and\nknowledge tests (MathQuiz). These games are designed to evaluate a range of\ncapabilities such as strategic reasoning, planning, knowledge application,\nsafety, and adaptability. Building upon recent studies that highlight the\neffectiveness of game-based evaluations for LLMs, ZeroSumEval enhances these\napproaches by providing a standardized and extensible framework for easily\nimplementing games and leverages DSPy to provide a better abstraction for LLM\nplayer strategies."
                },
                "authors": [
                    {
                        "name": "Hisham A. Alyahya"
                    },
                    {
                        "name": "Haidar Khan"
                    },
                    {
                        "name": "Yazeed Alnumay"
                    },
                    {
                        "name": "M Saiful Bari"
                    },
                    {
                        "name": "Bülent Yener"
                    }
                ],
                "author_detail": {
                    "name": "Bülent Yener"
                },
                "author": "Bülent Yener",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10673v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10673v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12589v1",
                "updated": "2025-04-17T02:08:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    2,
                    8,
                    51,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T02:08:51Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    2,
                    8,
                    51,
                    3,
                    107,
                    0
                ],
                "title": "Efficient MAP Estimation of LLM Judgment Performance with Prior Transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient MAP Estimation of LLM Judgment Performance with Prior Transfer"
                },
                "summary": "LLM ensembles are widely used for LLM judges. However, how to estimate their\naccuracy, especially in an efficient way, is unknown. In this paper, we present\na principled maximum a posteriori (MAP) framework for an economical and precise\nestimation of the performance of LLM ensemble judgment. We first propose a\nmixture of Beta-Binomial distributions to model the judgment distribution,\nrevising from the vanilla Binomial distribution. Next, we introduce a conformal\nprediction-driven approach that enables adaptive stopping during iterative\nsampling to balance accuracy with efficiency. Furthermore, we design a prior\ntransfer mechanism that utilizes learned distributions on open-source datasets\nto improve estimation on a target dataset when only scarce annotations are\navailable. Finally, we present BetaConform, a framework that integrates our\ndistribution assumption, adaptive stopping, and the prior transfer mechanism to\ndeliver a theoretically guaranteed distribution estimation of LLM ensemble\njudgment with minimum labeled samples. BetaConform is also validated\nempirically. For instance, with only 10 samples from the TruthfulQA dataset,\nfor a Llama ensembled judge, BetaConform gauges its performance with error\nmargin as small as 3.37%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM ensembles are widely used for LLM judges. However, how to estimate their\naccuracy, especially in an efficient way, is unknown. In this paper, we present\na principled maximum a posteriori (MAP) framework for an economical and precise\nestimation of the performance of LLM ensemble judgment. We first propose a\nmixture of Beta-Binomial distributions to model the judgment distribution,\nrevising from the vanilla Binomial distribution. Next, we introduce a conformal\nprediction-driven approach that enables adaptive stopping during iterative\nsampling to balance accuracy with efficiency. Furthermore, we design a prior\ntransfer mechanism that utilizes learned distributions on open-source datasets\nto improve estimation on a target dataset when only scarce annotations are\navailable. Finally, we present BetaConform, a framework that integrates our\ndistribution assumption, adaptive stopping, and the prior transfer mechanism to\ndeliver a theoretically guaranteed distribution estimation of LLM ensemble\njudgment with minimum labeled samples. BetaConform is also validated\nempirically. For instance, with only 10 samples from the TruthfulQA dataset,\nfor a Llama ensembled judge, BetaConform gauges its performance with error\nmargin as small as 3.37%."
                },
                "authors": [
                    {
                        "name": "Huaizhi Qu"
                    },
                    {
                        "name": "Inyoung Choi"
                    },
                    {
                        "name": "Zhen Tan"
                    },
                    {
                        "name": "Song Wang"
                    },
                    {
                        "name": "Sukwon Yun"
                    },
                    {
                        "name": "Qi Long"
                    },
                    {
                        "name": "Faizan Siddiqui"
                    },
                    {
                        "name": "Kwonjoon Lee"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.06691v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.06691v2",
                "updated": "2025-04-17T02:02:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    2,
                    2,
                    57,
                    3,
                    107,
                    0
                ],
                "published": "2024-05-07T09:36:23Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    9,
                    36,
                    23,
                    1,
                    128,
                    0
                ],
                "title": "Fleet of Agents: Coordinated Problem Solving with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fleet of Agents: Coordinated Problem Solving with Large Language Models"
                },
                "summary": "While numerous frameworks have been developed to enhance the reasoning\nabilities of large language models (LLMs), there is a scarcity of methods that\neffectively balance the trade-off between cost and quality. In this paper, we\nintroduce Fleet of Agents (FoA), a novel and intuitive yet principled framework\nutilizing LLMs as agents to navigate through dynamic tree searches, employing a\ngenetic-type particle filtering approach. FoA spawns a multitude of agents,\neach exploring the search space autonomously, followed by a selection phase\nwhere resampling based on a heuristic value function optimizes the balance\nbetween exploration and exploitation. This mechanism enables dynamic branching,\nadapting the exploration strategy based on discovered solutions. We conduct\nextensive experiments on three benchmark tasks, ``Game of 24'',\n``Mini-Crosswords'', and ``WebShop'', utilizing four different LLMs,\n``GPT-3.5'', ``GPT-4'', ``LLaMA3.2-11B'', and ``LLaMA3.2-90B''. On average\nacross all tasks and LLMs, FoA obtains a quality improvement of ~5% while\nrequiring only ~40% of the cost of previous SOTA methods. Notably, our analyses\nreveal that (1) FoA achieves the best cost-quality trade-off among all\nbenchmarked methods and (2) FoA + LLaMA3.2-11B surpasses the Llama3.2-90B\nmodel. FoA is publicly available at https://github.com/au-clan/FoA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While numerous frameworks have been developed to enhance the reasoning\nabilities of large language models (LLMs), there is a scarcity of methods that\neffectively balance the trade-off between cost and quality. In this paper, we\nintroduce Fleet of Agents (FoA), a novel and intuitive yet principled framework\nutilizing LLMs as agents to navigate through dynamic tree searches, employing a\ngenetic-type particle filtering approach. FoA spawns a multitude of agents,\neach exploring the search space autonomously, followed by a selection phase\nwhere resampling based on a heuristic value function optimizes the balance\nbetween exploration and exploitation. This mechanism enables dynamic branching,\nadapting the exploration strategy based on discovered solutions. We conduct\nextensive experiments on three benchmark tasks, ``Game of 24'',\n``Mini-Crosswords'', and ``WebShop'', utilizing four different LLMs,\n``GPT-3.5'', ``GPT-4'', ``LLaMA3.2-11B'', and ``LLaMA3.2-90B''. On average\nacross all tasks and LLMs, FoA obtains a quality improvement of ~5% while\nrequiring only ~40% of the cost of previous SOTA methods. Notably, our analyses\nreveal that (1) FoA achieves the best cost-quality trade-off among all\nbenchmarked methods and (2) FoA + LLaMA3.2-11B surpasses the Llama3.2-90B\nmodel. FoA is publicly available at https://github.com/au-clan/FoA."
                },
                "authors": [
                    {
                        "name": "Nearchos Potamitis"
                    },
                    {
                        "name": "Lars Klein"
                    },
                    {
                        "name": "Roland Aydin"
                    },
                    {
                        "name": "Robert West"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    },
                    {
                        "name": "Akhil Arora"
                    }
                ],
                "author_detail": {
                    "name": "Akhil Arora"
                },
                "author": "Akhil Arora",
                "arxiv_comment": "28 pages, 68 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.06691v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.06691v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11717v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11717v2",
                "updated": "2025-04-17T02:02:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    2,
                    2,
                    32,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-16T02:42:08Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    2,
                    42,
                    8,
                    2,
                    106,
                    0
                ],
                "title": "Safety with Agency: Human-Centered Safety Filter with Application to\n  AI-Assisted Motorsports",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety with Agency: Human-Centered Safety Filter with Application to\n  AI-Assisted Motorsports"
                },
                "summary": "We propose a human-centered safety filter (HCSF) for shared autonomy that\nsignificantly enhances system safety without compromising human agency. Our\nHCSF is built on a neural safety value function, which we first learn scalably\nthrough black-box interactions and then use at deployment to enforce a novel\nstate-action control barrier function (Q-CBF) safety constraint. Since this\nQ-CBF safety filter does not require any knowledge of the system dynamics for\nboth synthesis and runtime safety monitoring and intervention, our method\napplies readily to complex, black-box shared autonomy systems. Notably, our\nHCSF's CBF-based interventions modify the human's actions minimally and\nsmoothly, avoiding the abrupt, last-moment corrections delivered by many\nconventional safety filters. We validate our approach in a comprehensive\nin-person user study using Assetto Corsa-a high-fidelity car racing simulator\nwith black-box dynamics-to assess robustness in \"driving on the edge\"\nscenarios. We compare both trajectory data and drivers' perceptions of our HCSF\nassistance against unassisted driving and a conventional safety filter.\nExperimental results show that 1) compared to having no assistance, our HCSF\nimproves both safety and user satisfaction without compromising human agency or\ncomfort, and 2) relative to a conventional safety filter, our proposed HCSF\nboosts human agency, comfort, and satisfaction while maintaining robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a human-centered safety filter (HCSF) for shared autonomy that\nsignificantly enhances system safety without compromising human agency. Our\nHCSF is built on a neural safety value function, which we first learn scalably\nthrough black-box interactions and then use at deployment to enforce a novel\nstate-action control barrier function (Q-CBF) safety constraint. Since this\nQ-CBF safety filter does not require any knowledge of the system dynamics for\nboth synthesis and runtime safety monitoring and intervention, our method\napplies readily to complex, black-box shared autonomy systems. Notably, our\nHCSF's CBF-based interventions modify the human's actions minimally and\nsmoothly, avoiding the abrupt, last-moment corrections delivered by many\nconventional safety filters. We validate our approach in a comprehensive\nin-person user study using Assetto Corsa-a high-fidelity car racing simulator\nwith black-box dynamics-to assess robustness in \"driving on the edge\"\nscenarios. We compare both trajectory data and drivers' perceptions of our HCSF\nassistance against unassisted driving and a conventional safety filter.\nExperimental results show that 1) compared to having no assistance, our HCSF\nimproves both safety and user satisfaction without compromising human agency or\ncomfort, and 2) relative to a conventional safety filter, our proposed HCSF\nboosts human agency, comfort, and satisfaction while maintaining robustness."
                },
                "authors": [
                    {
                        "name": "Donggeon David Oh"
                    },
                    {
                        "name": "Justin Lidard"
                    },
                    {
                        "name": "Haimin Hu"
                    },
                    {
                        "name": "Himani Sinhmar"
                    },
                    {
                        "name": "Elle Lazarski"
                    },
                    {
                        "name": "Deepak Gopinath"
                    },
                    {
                        "name": "Emily S. Sumner"
                    },
                    {
                        "name": "Jonathan A. DeCastro"
                    },
                    {
                        "name": "Guy Rosman"
                    },
                    {
                        "name": "Naomi Ehrich Leonard"
                    },
                    {
                        "name": "Jaime Fernández Fisac"
                    }
                ],
                "author_detail": {
                    "name": "Jaime Fernández Fisac"
                },
                "author": "Jaime Fernández Fisac",
                "arxiv_comment": "Accepted to Robotics: Science and Systems (R:SS) 2025, 22 pages, 16\n  figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11717v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11717v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07717v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07717v2",
                "updated": "2025-04-17T02:01:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    2,
                    1,
                    42,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-10T13:09:50Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    13,
                    9,
                    50,
                    3,
                    100,
                    0
                ],
                "title": "PR-Attack: Coordinated Prompt-RAG Attacks on Retrieval-Augmented\n  Generation in Large Language Models via Bilevel Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PR-Attack: Coordinated Prompt-RAG Attacks on Retrieval-Augmented\n  Generation in Large Language Models via Bilevel Optimization"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\na wide range of applications, e.g., medical question-answering, mathematical\nsciences, and code generation. However, they also exhibit inherent limitations,\nsuch as outdated knowledge and susceptibility to hallucinations.\nRetrieval-Augmented Generation (RAG) has emerged as a promising paradigm to\naddress these issues, but it also introduces new vulnerabilities. Recent\nefforts have focused on the security of RAG-based LLMs, yet existing attack\nmethods face three critical challenges: (1) their effectiveness declines\nsharply when only a limited number of poisoned texts can be injected into the\nknowledge database, (2) they lack sufficient stealth, as the attacks are often\ndetectable by anomaly detection systems, which compromises their effectiveness,\nand (3) they rely on heuristic approaches to generate poisoned texts, lacking\nformal optimization frameworks and theoretic guarantees, which limits their\neffectiveness and applicability. To address these issues, we propose\ncoordinated Prompt-RAG attack (PR-attack), a novel optimization-driven attack\nthat introduces a small number of poisoned texts into the knowledge database\nwhile embedding a backdoor trigger within the prompt. When activated, the\ntrigger causes the LLM to generate pre-designed responses to targeted queries,\nwhile maintaining normal behavior in other contexts. This ensures both high\neffectiveness and stealth. We formulate the attack generation process as a\nbilevel optimization problem leveraging a principled optimization framework to\ndevelop optimal poisoned texts and triggers. Extensive experiments across\ndiverse LLMs and datasets demonstrate the effectiveness of PR-Attack, achieving\na high attack success rate even with a limited number of poisoned texts and\nsignificantly improved stealth compared to existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable performance across\na wide range of applications, e.g., medical question-answering, mathematical\nsciences, and code generation. However, they also exhibit inherent limitations,\nsuch as outdated knowledge and susceptibility to hallucinations.\nRetrieval-Augmented Generation (RAG) has emerged as a promising paradigm to\naddress these issues, but it also introduces new vulnerabilities. Recent\nefforts have focused on the security of RAG-based LLMs, yet existing attack\nmethods face three critical challenges: (1) their effectiveness declines\nsharply when only a limited number of poisoned texts can be injected into the\nknowledge database, (2) they lack sufficient stealth, as the attacks are often\ndetectable by anomaly detection systems, which compromises their effectiveness,\nand (3) they rely on heuristic approaches to generate poisoned texts, lacking\nformal optimization frameworks and theoretic guarantees, which limits their\neffectiveness and applicability. To address these issues, we propose\ncoordinated Prompt-RAG attack (PR-attack), a novel optimization-driven attack\nthat introduces a small number of poisoned texts into the knowledge database\nwhile embedding a backdoor trigger within the prompt. When activated, the\ntrigger causes the LLM to generate pre-designed responses to targeted queries,\nwhile maintaining normal behavior in other contexts. This ensures both high\neffectiveness and stealth. We formulate the attack generation process as a\nbilevel optimization problem leveraging a principled optimization framework to\ndevelop optimal poisoned texts and triggers. Extensive experiments across\ndiverse LLMs and datasets demonstrate the effectiveness of PR-Attack, achieving\na high attack success rate even with a limited number of poisoned texts and\nsignificantly improved stealth compared to existing methods."
                },
                "authors": [
                    {
                        "name": "Yang Jiao"
                    },
                    {
                        "name": "Xiaodong Wang"
                    },
                    {
                        "name": "Kai Yang"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yang"
                },
                "author": "Kai Yang",
                "arxiv_comment": "Accepted at SIGIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07717v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07717v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12585v1",
                "updated": "2025-04-17T02:00:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    2,
                    0,
                    53,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-17T02:00:53Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    2,
                    0,
                    53,
                    3,
                    107,
                    0
                ],
                "title": "Identifying and Mitigating the Influence of the Prior Distribution in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying and Mitigating the Influence of the Prior Distribution in\n  Large Language Models"
                },
                "summary": "Large language models (LLMs) sometimes fail to respond appropriately to\ndeterministic tasks -- such as counting or forming acronyms -- because the\nimplicit prior distribution they have learned over sequences of tokens\ninfluences their responses. In this work, we show that, in at least some cases,\nLLMs actually compute the information needed to perform these tasks correctly,\nand we identify some interventions that can allow them to access this\ninformation to improve their performance. First, we show that simply prompting\nthe language model to not rely on its prior knowledge leads to dramatic\nimprovements in prior-dominated tasks. We then use mechanistic interpretability\ntechniques to localize the prior within the LLM and manipulate the extent to\nwhich that prior influences its responses. Specifically, we show that it is\npossible to identify layers of the underlying neural network that correlate\nwith the prior probability of a response and that lightweight finetuning of\nthese layers with basic prompts on prior-dominated tasks achieves high\nperformance on held-out answers. These results suggest that the information\nrequired to produce a correct response is contained within the representations\nof the problems formed by the models. Furthermore, we show that this finetuning\nis significantly more effective for prior-dominated tasks, and that the error\nafter finetuning is no longer correlated with the prior. Our results suggest\nthat it may be possible to define effective methods for manipulating the extent\nto which LLMs rely upon their priors in solving problems, potentially\nincreasing their performance in settings where LLMs hallucinate for reasons\nrelated to the prior probability of token sequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) sometimes fail to respond appropriately to\ndeterministic tasks -- such as counting or forming acronyms -- because the\nimplicit prior distribution they have learned over sequences of tokens\ninfluences their responses. In this work, we show that, in at least some cases,\nLLMs actually compute the information needed to perform these tasks correctly,\nand we identify some interventions that can allow them to access this\ninformation to improve their performance. First, we show that simply prompting\nthe language model to not rely on its prior knowledge leads to dramatic\nimprovements in prior-dominated tasks. We then use mechanistic interpretability\ntechniques to localize the prior within the LLM and manipulate the extent to\nwhich that prior influences its responses. Specifically, we show that it is\npossible to identify layers of the underlying neural network that correlate\nwith the prior probability of a response and that lightweight finetuning of\nthese layers with basic prompts on prior-dominated tasks achieves high\nperformance on held-out answers. These results suggest that the information\nrequired to produce a correct response is contained within the representations\nof the problems formed by the models. Furthermore, we show that this finetuning\nis significantly more effective for prior-dominated tasks, and that the error\nafter finetuning is no longer correlated with the prior. Our results suggest\nthat it may be possible to define effective methods for manipulating the extent\nto which LLMs rely upon their priors in solving problems, potentially\nincreasing their performance in settings where LLMs hallucinate for reasons\nrelated to the prior probability of token sequences."
                },
                "authors": [
                    {
                        "name": "Liyi Zhang"
                    },
                    {
                        "name": "Veniamin Veselovsky"
                    },
                    {
                        "name": "R. Thomas McCoy"
                    },
                    {
                        "name": "Thomas L. Griffiths"
                    }
                ],
                "author_detail": {
                    "name": "Thomas L. Griffiths"
                },
                "author": "Thomas L. Griffiths",
                "arxiv_comment": "16 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]