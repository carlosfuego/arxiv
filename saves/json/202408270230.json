[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2408.11049v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11049v3",
                "updated": "2024-08-23T17:54:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    54,
                    34,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-20T17:57:31Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    31,
                    1,
                    233,
                    0
                ],
                "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/."
                },
                "authors": [
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Vashisth Tiwari"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Jinyuan Shi"
                    },
                    {
                        "name": "Ian En-Hsu Yen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11049v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11049v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13165v1",
                "updated": "2024-08-23T15:39:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    15,
                    39,
                    20,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T15:39:20Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    15,
                    39,
                    20,
                    4,
                    236,
                    0
                ],
                "title": "Cyclic Wrap-Around Multi-Access Coded Caching with Private Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cyclic Wrap-Around Multi-Access Coded Caching with Private Caches"
                },
                "summary": "We consider a variant of the coded caching problem where users connect to two\ntypes of caches, called private caches and access caches. The problem setting\nconsists of a server having a library of files and a set of access caches.\nEvery user, equipped with a private cache, connects to $L$ neighboring access\ncaches in a cyclic wrap-around fashion. The server populates the private and\naccess caches with file contents in either coded or uncoded format. For this\nsetting, we derive a lower bound on the optimal worst-case transmission rate\nusing cut-set arguments. This lower bound applies to both coded and uncoded\nplacements. We then provide an achievable scheme with uncoded placement and\nshow that our scheme specializes to the well-known Maddah-Ali-Niesen scheme for\nthe dedicated cache network in the absence of access caches. Finally, we show\nthat the proposed scheme achieves optimality in large memory regimes and\nprovide numerical plots comparing the rate of the proposed scheme with the\nderived lower bound, demonstrating the optimality of our scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a variant of the coded caching problem where users connect to two\ntypes of caches, called private caches and access caches. The problem setting\nconsists of a server having a library of files and a set of access caches.\nEvery user, equipped with a private cache, connects to $L$ neighboring access\ncaches in a cyclic wrap-around fashion. The server populates the private and\naccess caches with file contents in either coded or uncoded format. For this\nsetting, we derive a lower bound on the optimal worst-case transmission rate\nusing cut-set arguments. This lower bound applies to both coded and uncoded\nplacements. We then provide an achievable scheme with uncoded placement and\nshow that our scheme specializes to the well-known Maddah-Ali-Niesen scheme for\nthe dedicated cache network in the absence of access caches. Finally, we show\nthat the proposed scheme achieves optimality in large memory regimes and\nprovide numerical plots comparing the rate of the proposed scheme with the\nderived lower bound, demonstrating the optimality of our scheme."
                },
                "authors": [
                    {
                        "name": "Dhruv Pratap Singh"
                    },
                    {
                        "name": "Anjana A. Mahesh"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "15 pages, 5 figures and one table. Some overlap of introductory and\n  background materials with our earlier submission arXiv:2407.00677v1 dated 30\n  June 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.05332v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.05332v5",
                "updated": "2024-08-23T13:25:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    13,
                    25,
                    7,
                    4,
                    236,
                    0
                ],
                "published": "2023-05-09T10:41:36Z",
                "published_parsed": [
                    2023,
                    5,
                    9,
                    10,
                    41,
                    36,
                    1,
                    129,
                    0
                ],
                "title": "Fundamental Limits of Multi-Message Private Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fundamental Limits of Multi-Message Private Computation"
                },
                "summary": "In a typical formulation of the private information retrieval (PIR) problem,\na single user wishes to retrieve one out of $ K$ files from $N$ servers without\nrevealing the demanded file index to any server. This paper formulates an\nextended model of PIR, referred to as multi-message private computation\n(MM-PC), where instead of retrieving a single file, the user wishes to retrieve\n$P>1$ linear combinations of files while preserving the privacy of the demand\ninformation. The MM-PC problem is a generalization of the private computation\n(PC) problem (where the user requests one linear combination of the files), and\nthe multi-message private information retrieval (MM-PIR) problem (where the\nuser requests $P>1$ files). A baseline achievable scheme repeats the optimal PC\nscheme by Sun and Jafar $P$ times, or treats each possible demanded linear\ncombination as an independent file and then uses the near optimal MM-PIR scheme\nby Banawan and Ulukus. In this paper, we propose a new MM-PC scheme that\nsignificantly improves upon the baseline schemes. In doing so, we design the\nqueries inspired by the structure in the cache-aided scalar linear function\nretrieval scheme by Wan {\\it et al.}, which leverages the dependency between\nlinear functions to reduce the amount of communications. To ensure the\ndecodability of our scheme, we propose a new method to benefit from the\nexisting dependency, referred to as the sign assignment step. In the end, we\nuse Maximum Distance Separable matrices to code the queries, which allows the\nreduction of download from the servers, while preserving privacy. By the\nproposed schemes, we characterize the capacity within a multiplicative factor\nof $2$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a typical formulation of the private information retrieval (PIR) problem,\na single user wishes to retrieve one out of $ K$ files from $N$ servers without\nrevealing the demanded file index to any server. This paper formulates an\nextended model of PIR, referred to as multi-message private computation\n(MM-PC), where instead of retrieving a single file, the user wishes to retrieve\n$P>1$ linear combinations of files while preserving the privacy of the demand\ninformation. The MM-PC problem is a generalization of the private computation\n(PC) problem (where the user requests one linear combination of the files), and\nthe multi-message private information retrieval (MM-PIR) problem (where the\nuser requests $P>1$ files). A baseline achievable scheme repeats the optimal PC\nscheme by Sun and Jafar $P$ times, or treats each possible demanded linear\ncombination as an independent file and then uses the near optimal MM-PIR scheme\nby Banawan and Ulukus. In this paper, we propose a new MM-PC scheme that\nsignificantly improves upon the baseline schemes. In doing so, we design the\nqueries inspired by the structure in the cache-aided scalar linear function\nretrieval scheme by Wan {\\it et al.}, which leverages the dependency between\nlinear functions to reduce the amount of communications. To ensure the\ndecodability of our scheme, we propose a new method to benefit from the\nexisting dependency, referred to as the sign assignment step. In the end, we\nuse Maximum Distance Separable matrices to code the queries, which allows the\nreduction of download from the servers, while preserving privacy. By the\nproposed schemes, we characterize the capacity within a multiplicative factor\nof $2$."
                },
                "authors": [
                    {
                        "name": "Ali Gholami"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Tayyebeh Jahani-Nezhad"
                    },
                    {
                        "name": "Hua Sun"
                    },
                    {
                        "name": "Mingyue Ji"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "arxiv_comment": "A version of this paper is submitted to IEEE Transactions on\n  Communications. A short version was accepted and presented at ISIT 2024 in\n  Athens",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.05332v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.05332v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12947v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12947v1",
                "updated": "2024-08-23T09:54:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    54,
                    22,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T09:54:22Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    54,
                    22,
                    4,
                    236,
                    0
                ],
                "title": "Which Part of the Heap is Useful? Improving Heap Liveness Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Which Part of the Heap is Useful? Improving Heap Liveness Analysis"
                },
                "summary": "With the growing sizes of data structures allocated in heap, understanding\nthe actual use of heap memory is critically important for minimizing cache\nmisses and reclaiming unused memory. A static analysis aimed at this is\ndifficult because the heap locations are unnamed. Using allocation sites to\nname them creates very few distinctions making it difficult to identify\nallocated heap locations that are not used. Heap liveness analysis using access\ngraphs solves this problem by (a) using a storeless model of heap memory by\nnaming the locations with access paths, and (b) representing the unbounded sets\nof access paths (which are regular languages) as finite automata.\n  We improve the scalability and efficiency of heap liveness analysis, and\nreduce the amount of computed heap liveness information by using deterministic\nautomata and by minimizing the inclusion of aliased access paths in the\nlanguage. Practically, our field-, flow-, context-sensitive liveness analysis\non SPEC CPU2006 benchmarks scales to 36 kLoC (existing analysis scales to 10.5\nkLoC) and improves efficiency even up to 99%. For some of the benchmarks, our\ntechnique shows multifold reduction in the computed liveness information,\nranging from 2 to 100 times (in terms of the number of live access paths),\nwithout compromising on soundness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing sizes of data structures allocated in heap, understanding\nthe actual use of heap memory is critically important for minimizing cache\nmisses and reclaiming unused memory. A static analysis aimed at this is\ndifficult because the heap locations are unnamed. Using allocation sites to\nname them creates very few distinctions making it difficult to identify\nallocated heap locations that are not used. Heap liveness analysis using access\ngraphs solves this problem by (a) using a storeless model of heap memory by\nnaming the locations with access paths, and (b) representing the unbounded sets\nof access paths (which are regular languages) as finite automata.\n  We improve the scalability and efficiency of heap liveness analysis, and\nreduce the amount of computed heap liveness information by using deterministic\nautomata and by minimizing the inclusion of aliased access paths in the\nlanguage. Practically, our field-, flow-, context-sensitive liveness analysis\non SPEC CPU2006 benchmarks scales to 36 kLoC (existing analysis scales to 10.5\nkLoC) and improves efficiency even up to 99%. For some of the benchmarks, our\ntechnique shows multifold reduction in the computed liveness information,\nranging from 2 to 100 times (in terms of the number of live access paths),\nwithout compromising on soundness."
                },
                "authors": [
                    {
                        "name": "Vini Kanvar"
                    },
                    {
                        "name": "Uday P. Khedker"
                    }
                ],
                "author_detail": {
                    "name": "Uday P. Khedker"
                },
                "author": "Uday P. Khedker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12947v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12947v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12592v1",
                "updated": "2024-08-22T17:56:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T17:56:29Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "title": "Exposing Shadow Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposing Shadow Branches"
                },
                "summary": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation."
                },
                "authors": [
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "Daniel A. Jim√©nez"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "arxiv_comment": "13 pages, 16 figures, Submitted to ASPLOS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.14533v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.14533v2",
                "updated": "2024-08-22T17:47:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    47,
                    49,
                    3,
                    235,
                    0
                ],
                "published": "2023-09-25T21:17:17Z",
                "published_parsed": [
                    2023,
                    9,
                    25,
                    21,
                    17,
                    17,
                    0,
                    268,
                    0
                ],
                "title": "Stable CoO$_2$ Nanoscrolls With Outstanding Electrical Properties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stable CoO$_2$ Nanoscrolls With Outstanding Electrical Properties"
                },
                "summary": "Layered CoO$_2$ is of great interest for its promising properties but is\nmeta-stable in its bulk form. CoO$_2$ was synthesized by converting the\nquasi-one-dimensional crystal structure of bulk Ca$_3$Co$_2$O$_6$ via a\nhydrothermal treatment. The resulting nanostructures were predominantly\nnanoscrolls with very thin walls, which exhibit long-term stability. A detailed\nstructural investigation reveals that the CoO$_2$ is found to crystallize in\nmonoclinic form, similar to the related CaCoO$_2$-CoO$_2$ misfit structure.\nIndividual nanoscrolls are characterized electrically and show a p-type\nsemiconducting nature with a high current-carrying capacity of 4$\\cdot$10$^5$ A\ncm$^{-2}$ and an extremely high breakdown voltage of up to 270 kV/cm. The\nresults demonstrate the possibility to stabilize meta-stable materials in\nlow-dimensional forms and a promising application of the nanoscrolls as\ninterconnect in high-voltage electronic circuitry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layered CoO$_2$ is of great interest for its promising properties but is\nmeta-stable in its bulk form. CoO$_2$ was synthesized by converting the\nquasi-one-dimensional crystal structure of bulk Ca$_3$Co$_2$O$_6$ via a\nhydrothermal treatment. The resulting nanostructures were predominantly\nnanoscrolls with very thin walls, which exhibit long-term stability. A detailed\nstructural investigation reveals that the CoO$_2$ is found to crystallize in\nmonoclinic form, similar to the related CaCoO$_2$-CoO$_2$ misfit structure.\nIndividual nanoscrolls are characterized electrically and show a p-type\nsemiconducting nature with a high current-carrying capacity of 4$\\cdot$10$^5$ A\ncm$^{-2}$ and an extremely high breakdown voltage of up to 270 kV/cm. The\nresults demonstrate the possibility to stabilize meta-stable materials in\nlow-dimensional forms and a promising application of the nanoscrolls as\ninterconnect in high-voltage electronic circuitry."
                },
                "authors": [
                    {
                        "name": "Simon Hettler"
                    },
                    {
                        "name": "Kankona Singha Roy"
                    },
                    {
                        "name": "Raul Arenal"
                    },
                    {
                        "name": "Leela S. Panchakarla"
                    }
                ],
                "author_detail": {
                    "name": "Leela S. Panchakarla"
                },
                "author": "Leela S. Panchakarla",
                "arxiv_doi": "10.1002/admi.202400317",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1002/admi.202400317",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2309.14533v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.14533v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Adv. Mater. Interfaces 2024, 2400317",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11506v1",
                "updated": "2024-08-21T10:26:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    26,
                    26,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T10:26:26Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    26,
                    26,
                    2,
                    234,
                    0
                ],
                "title": "Rheological behavior of molybdenum disulfide (MoS2) inks under electric\n  fields: influence of concentration and voltage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rheological behavior of molybdenum disulfide (MoS2) inks under electric\n  fields: influence of concentration and voltage"
                },
                "summary": "This work provides a complete rheological characterization of molybdenum\ndisulfide (MoS2) inks in the presence of electric fields. Several\nconcentrations of MoS2 are studied and dispersed in a viscoelastic fluid. The\nlubrication effects are present in the ink when the MoS2 concentration is\nhigher than 0.10% w/w. The dielectric properties show the impossibility of a\npositive electrorheological effect for all MoS2-inks studied. The formation of\nvortices and electromigration of MoS2 particles occur under the influence of an\nexternal electric field. These two phenomena affect the rheological behavior of\nMoS2-inks under shear flow condition. Relatively to the extensional rheology\nexperiments, the particle migration and the vortex formation promote anisotropy\non the rheological properties of the inks which affects the relaxation time,\nthe formation of beads-on-a-string and the uniaxial elongational flow condition\nis no longer valid. When the electric field strength is 1.5 kV/mm, the\nformation of Taylor's cone is observed and independent of MoS2 concentration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work provides a complete rheological characterization of molybdenum\ndisulfide (MoS2) inks in the presence of electric fields. Several\nconcentrations of MoS2 are studied and dispersed in a viscoelastic fluid. The\nlubrication effects are present in the ink when the MoS2 concentration is\nhigher than 0.10% w/w. The dielectric properties show the impossibility of a\npositive electrorheological effect for all MoS2-inks studied. The formation of\nvortices and electromigration of MoS2 particles occur under the influence of an\nexternal electric field. These two phenomena affect the rheological behavior of\nMoS2-inks under shear flow condition. Relatively to the extensional rheology\nexperiments, the particle migration and the vortex formation promote anisotropy\non the rheological properties of the inks which affects the relaxation time,\nthe formation of beads-on-a-string and the uniaxial elongational flow condition\nis no longer valid. When the electric field strength is 1.5 kV/mm, the\nformation of Taylor's cone is observed and independent of MoS2 concentration."
                },
                "authors": [
                    {
                        "name": "Pedro C Rijo"
                    },
                    {
                        "name": "Francisco J. Galindo-Rosales"
                    }
                ],
                "author_detail": {
                    "name": "Francisco J. Galindo-Rosales"
                },
                "author": "Francisco J. Galindo-Rosales",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.10685v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.10685v2",
                "updated": "2024-08-21T06:10:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    6,
                    10,
                    2,
                    2,
                    234,
                    0
                ],
                "published": "2024-01-19T13:32:55Z",
                "published_parsed": [
                    2024,
                    1,
                    19,
                    13,
                    32,
                    55,
                    4,
                    19,
                    0
                ],
                "title": "Towards End-to-End GPS Localization with Neural Pseudorange Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards End-to-End GPS Localization with Neural Pseudorange Correction"
                },
                "summary": "The pseudorange error is one of the root causes of localization inaccuracy in\nGPS. Previous data-driven methods regress and eliminate pseudorange errors\nusing handcrafted intermediate labels. Unlike them, we propose an end-to-end\nGPS localization framework, E2E-PrNet, to train a neural network for\npseudorange correction (PrNet) directly using the final task loss calculated\nwith the ground truth of GPS receiver states. The gradients of the loss with\nrespect to learnable parameters are backpropagated through a Differentiable\nNonlinear Least Squares (DNLS) optimizer to PrNet. The feasibility of fusing\nthe data-driven neural network and the model-based DNLS module is verified with\nGPS data collected by Android phones, showing that E2E-PrNet outperforms the\nbaseline weighted least squares method and the state-of-the-art end-to-end\ndata-driven approach. Finally, we discuss the explainability of E2E-PrNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pseudorange error is one of the root causes of localization inaccuracy in\nGPS. Previous data-driven methods regress and eliminate pseudorange errors\nusing handcrafted intermediate labels. Unlike them, we propose an end-to-end\nGPS localization framework, E2E-PrNet, to train a neural network for\npseudorange correction (PrNet) directly using the final task loss calculated\nwith the ground truth of GPS receiver states. The gradients of the loss with\nrespect to learnable parameters are backpropagated through a Differentiable\nNonlinear Least Squares (DNLS) optimizer to PrNet. The feasibility of fusing\nthe data-driven neural network and the model-based DNLS module is verified with\nGPS data collected by Android phones, showing that E2E-PrNet outperforms the\nbaseline weighted least squares method and the state-of-the-art end-to-end\ndata-driven approach. Finally, we discuss the explainability of E2E-PrNet."
                },
                "authors": [
                    {
                        "name": "Xu Weng"
                    },
                    {
                        "name": "KV Ling"
                    },
                    {
                        "name": "Haochen Liu"
                    },
                    {
                        "name": "Kun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Kun Cao"
                },
                "author": "Kun Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.10685v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.10685v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11325v1",
                "updated": "2024-08-21T04:16:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    16,
                    49,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T04:16:49Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    16,
                    49,
                    2,
                    234,
                    0
                ],
                "title": "Telepathic Datacenters: Fast RPCs using Shared CXL Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Telepathic Datacenters: Fast RPCs using Shared CXL Memory"
                },
                "summary": "Datacenter applications often rely on remote procedure calls (RPCs) for fast,\nefficient, and secure communication. However, RPCs are slow, inefficient, and\nhard to use as they require expensive serialization and compression to\ncommunicate over a packetized serial network link. Compute Express Link 3.0\n(CXL) offers an alternative solution, allowing applications to share data using\na cache-coherent, shared-memory interface across clusters of machines.\n  RPCool is a new framework that exploits CXL's shared memory capabilities.\nRPCool avoids serialization by passing pointers to data structures in shared\nmemory. While avoiding serialization is useful, directly sharing pointer-rich\ndata eliminates the isolation that copying data over traditional networks\nprovides, leaving the receiver vulnerable to invalid pointers and concurrent\nupdates to shared data by the sender. RPCool restores this safety with careful\nand efficient management of memory permissions. Another significant challenge\nwith CXL shared memory capabilities is that they are unlikely to scale to an\nentire datacenter. RPCool addresses this by falling back to RDMA-based\ncommunication.\n  Overall, RPCool reduces the round-trip latency by 1.93$\\times$ and\n7.2$\\times$ compared to state-of-the-art RDMA and CXL-based RPC mechanisms,\nrespectively. Moreover, RPCool performs either comparably or better than other\nRPC mechanisms across a range of workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Datacenter applications often rely on remote procedure calls (RPCs) for fast,\nefficient, and secure communication. However, RPCs are slow, inefficient, and\nhard to use as they require expensive serialization and compression to\ncommunicate over a packetized serial network link. Compute Express Link 3.0\n(CXL) offers an alternative solution, allowing applications to share data using\na cache-coherent, shared-memory interface across clusters of machines.\n  RPCool is a new framework that exploits CXL's shared memory capabilities.\nRPCool avoids serialization by passing pointers to data structures in shared\nmemory. While avoiding serialization is useful, directly sharing pointer-rich\ndata eliminates the isolation that copying data over traditional networks\nprovides, leaving the receiver vulnerable to invalid pointers and concurrent\nupdates to shared data by the sender. RPCool restores this safety with careful\nand efficient management of memory permissions. Another significant challenge\nwith CXL shared memory capabilities is that they are unlikely to scale to an\nentire datacenter. RPCool addresses this by falling back to RDMA-based\ncommunication.\n  Overall, RPCool reduces the round-trip latency by 1.93$\\times$ and\n7.2$\\times$ compared to state-of-the-art RDMA and CXL-based RPC mechanisms,\nrespectively. Moreover, RPCool performs either comparably or better than other\nRPC mechanisms across a range of workloads."
                },
                "authors": [
                    {
                        "name": "Suyash Mahar"
                    },
                    {
                        "name": "Ehsan Hajyjasini"
                    },
                    {
                        "name": "Seungjin Lee"
                    },
                    {
                        "name": "Zifeng Zhang"
                    },
                    {
                        "name": "Mingyao Shen"
                    },
                    {
                        "name": "Steven Swanson"
                    }
                ],
                "author_detail": {
                    "name": "Steven Swanson"
                },
                "author": "Steven Swanson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03637v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03637v3",
                "updated": "2024-08-21T02:32:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    2,
                    32,
                    43,
                    2,
                    234,
                    0
                ],
                "published": "2024-07-04T05:13:58Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    5,
                    13,
                    58,
                    3,
                    186,
                    0
                ],
                "title": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering"
                },
                "summary": "Matrix quantization compresses matrix elements into a more compact form to\nreduce storage requirements, with dequantization enabling reconstruction for\nuse. We define the Quantization Error Minimization (QEM) problem as minimizing\nthe difference between the original and quantized matrices while ensuring the\nquantized matrix remains within fixed memory constraints. This technique is\ncrucial in applications like Large Language Model (LLM) weight compression and\nKV cache compression, where large matrix sizes demand efficient storage\nsolutions.\n  As modern LLMs like GPT-4 and BERT continue to grow, effective matrix\ncompression is increasingly important. These models contain billions of\nparameters in matrix form, making efficient weight quantization essential for\nboth storage and computational efficiency. Similarly, KV caches, storing\nintermediate inference results, are matrix-based and benefit significantly from\noptimized compression techniques.\n  To address the QEM problem in the context of LLM weight and KV cache\ncompression, we propose Quantum Entanglement Trees (QET). QET leverages the\nlocal structure of matrix elements by iteratively swapping elements to create a\nlocally ordered matrix, which is then grouped and quantized column by column.\nTo enhance QET, we introduce two optimizations: residual quantization to\nfurther reduce Mean Squared Error (MSE) and masking with batch processing to\naccelerate the algorithm.\n  Our experiments demonstrate that QET can reduce MSE to 12.3% of its original\nvalue at the same compression ratio, outperforming leading baseline methods.\nOur contributions include framing the QEM problem specifically for LLM and KV\ncache compression, developing the QET algorithm, and implementing optimizations\nthat improve accuracy and processing speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrix quantization compresses matrix elements into a more compact form to\nreduce storage requirements, with dequantization enabling reconstruction for\nuse. We define the Quantization Error Minimization (QEM) problem as minimizing\nthe difference between the original and quantized matrices while ensuring the\nquantized matrix remains within fixed memory constraints. This technique is\ncrucial in applications like Large Language Model (LLM) weight compression and\nKV cache compression, where large matrix sizes demand efficient storage\nsolutions.\n  As modern LLMs like GPT-4 and BERT continue to grow, effective matrix\ncompression is increasingly important. These models contain billions of\nparameters in matrix form, making efficient weight quantization essential for\nboth storage and computational efficiency. Similarly, KV caches, storing\nintermediate inference results, are matrix-based and benefit significantly from\noptimized compression techniques.\n  To address the QEM problem in the context of LLM weight and KV cache\ncompression, we propose Quantum Entanglement Trees (QET). QET leverages the\nlocal structure of matrix elements by iteratively swapping elements to create a\nlocally ordered matrix, which is then grouped and quantized column by column.\nTo enhance QET, we introduce two optimizations: residual quantization to\nfurther reduce Mean Squared Error (MSE) and masking with batch processing to\naccelerate the algorithm.\n  Our experiments demonstrate that QET can reduce MSE to 12.3% of its original\nvalue at the same compression ratio, outperforming leading baseline methods.\nOur contributions include framing the QEM problem specifically for LLM and KV\ncache compression, developing the QET algorithm, and implementing optimizations\nthat improve accuracy and processing speed."
                },
                "authors": [
                    {
                        "name": "Yanshu Wang"
                    },
                    {
                        "name": "Wang Li"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03637v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03637v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10970v1",
                "updated": "2024-08-20T16:02:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    2,
                    54,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T16:02:54Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    2,
                    54,
                    1,
                    233,
                    0
                ],
                "title": "Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical\n  Planning and Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical\n  Planning and Control"
                },
                "summary": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work has demonstrated that a class of hybrid state-space\nmodel known as recurrent switching linear dynamical systems (rSLDS) discover\nmeaningful behavioural units via the piecewise linear decomposition of complex\ncontinuous dynamics (Linderman et al., 2016). Furthermore, they model how the\nunderlying continuous states drive these discrete mode switches. We propose\nthat the rich representations formed by an rSLDS can provide useful\nabstractions for planning and control. We present a novel hierarchical\nmodel-based algorithm inspired by Active Inference in which a discrete MDP sits\nabove a low-level linear-quadratic controller. The recurrent transition\ndynamics learned by the rSLDS allow us to (1) specify temporally-abstracted\nsub-goals in a method reminiscent of the options framework, (2) lift the\nexploration into discrete space allowing us to exploit information-theoretic\nexploration bonuses and (3) `cache' the approximate solutions to low-level\nproblems in the discrete planner. We successfully apply our model to the sparse\nContinuous Mountain Car task, demonstrating fast system identification via\nenhanced exploration and non-trivial planning through the delineation of\nabstract sub-goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work has demonstrated that a class of hybrid state-space\nmodel known as recurrent switching linear dynamical systems (rSLDS) discover\nmeaningful behavioural units via the piecewise linear decomposition of complex\ncontinuous dynamics (Linderman et al., 2016). Furthermore, they model how the\nunderlying continuous states drive these discrete mode switches. We propose\nthat the rich representations formed by an rSLDS can provide useful\nabstractions for planning and control. We present a novel hierarchical\nmodel-based algorithm inspired by Active Inference in which a discrete MDP sits\nabove a low-level linear-quadratic controller. The recurrent transition\ndynamics learned by the rSLDS allow us to (1) specify temporally-abstracted\nsub-goals in a method reminiscent of the options framework, (2) lift the\nexploration into discrete space allowing us to exploit information-theoretic\nexploration bonuses and (3) `cache' the approximate solutions to low-level\nproblems in the discrete planner. We successfully apply our model to the sparse\nContinuous Mountain Car task, demonstrating fast system identification via\nenhanced exploration and non-trivial planning through the delineation of\nabstract sub-goals."
                },
                "authors": [
                    {
                        "name": "Poppy Collis"
                    },
                    {
                        "name": "Ryan Singh"
                    },
                    {
                        "name": "Paul F Kinghorn"
                    },
                    {
                        "name": "Christopher L Buckley"
                    }
                ],
                "author_detail": {
                    "name": "Christopher L Buckley"
                },
                "author": "Christopher L Buckley",
                "arxiv_comment": "4 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10746v1",
                "updated": "2024-08-20T11:30:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    30,
                    12,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T11:30:12Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    30,
                    12,
                    1,
                    233,
                    0
                ],
                "title": "Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI\n  Framework for Personal LLMs Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI\n  Framework for Personal LLMs Fine-Tuning"
                },
                "summary": "Large language models (LLMs) have unlocked a plethora of powerful\napplications at the network edge, such as intelligent personal assistants. Data\nprivacy and security concerns have prompted a shift towards edge-based\nfine-tuning of personal LLMs, away from cloud reliance. However, this raises\nissues of computational intensity and resource scarcity, hindering training\nefficiency and feasibility. While current studies investigate\nparameter-efficient fine-tuning (PEFT) techniques to mitigate resource\nconstraints, our analysis indicates that these techniques are not sufficiently\nresource-efficient for edge devices. To tackle these challenges, we propose\nPluto and Charon (PAC), a time and memory efficient collaborative edge AI\nframework for personal LLMs fine-tuning. PAC breaks the resource wall of\npersonal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1)\nAlgorithmically, PAC implements a personal LLMs fine-tuning technique that is\nefficient in terms of parameters, time, and memory. It utilizes Parallel\nAdapters to circumvent the need for a full backward pass through the LLM\nbackbone. Additionally, an activation cache mechanism further streamlining the\nprocess by negating the necessity for repeated forward passes across multiple\nepochs. (2) Systematically, PAC leverages edge devices in close proximity,\npooling them as a collective resource for in-situ personal LLMs fine-tuning,\nutilizing a hybrid data and pipeline parallelism to orchestrate distributed\ntraining. The use of the activation cache eliminates the need for forward pass\nthrough the LLM backbone,enabling exclusive fine-tuning of the Parallel\nAdapters using data parallelism. Extensive evaluation based on prototype\nimplementation demonstrates that PAC remarkably outperforms state-of-the-art\napproaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction\nin memory footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have unlocked a plethora of powerful\napplications at the network edge, such as intelligent personal assistants. Data\nprivacy and security concerns have prompted a shift towards edge-based\nfine-tuning of personal LLMs, away from cloud reliance. However, this raises\nissues of computational intensity and resource scarcity, hindering training\nefficiency and feasibility. While current studies investigate\nparameter-efficient fine-tuning (PEFT) techniques to mitigate resource\nconstraints, our analysis indicates that these techniques are not sufficiently\nresource-efficient for edge devices. To tackle these challenges, we propose\nPluto and Charon (PAC), a time and memory efficient collaborative edge AI\nframework for personal LLMs fine-tuning. PAC breaks the resource wall of\npersonal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1)\nAlgorithmically, PAC implements a personal LLMs fine-tuning technique that is\nefficient in terms of parameters, time, and memory. It utilizes Parallel\nAdapters to circumvent the need for a full backward pass through the LLM\nbackbone. Additionally, an activation cache mechanism further streamlining the\nprocess by negating the necessity for repeated forward passes across multiple\nepochs. (2) Systematically, PAC leverages edge devices in close proximity,\npooling them as a collective resource for in-situ personal LLMs fine-tuning,\nutilizing a hybrid data and pipeline parallelism to orchestrate distributed\ntraining. The use of the activation cache eliminates the need for forward pass\nthrough the LLM backbone,enabling exclusive fine-tuning of the Parallel\nAdapters using data parallelism. Extensive evaluation based on prototype\nimplementation demonstrates that PAC remarkably outperforms state-of-the-art\napproaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction\nin memory footprint."
                },
                "authors": [
                    {
                        "name": "Bei Ouyang"
                    },
                    {
                        "name": "Shengyuan Ye"
                    },
                    {
                        "name": "Liekang Zeng"
                    },
                    {
                        "name": "Tianyi Qian"
                    },
                    {
                        "name": "Jingyi Li"
                    },
                    {
                        "name": "Xu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xu Chen"
                },
                "author": "Xu Chen",
                "arxiv_comment": "Accepted by The 53rd International Conference on Parallel Processing\n  (ICPP'24)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09697v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09697v2",
                "updated": "2024-08-20T04:46:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    4,
                    46,
                    18,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-19T04:43:56Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    4,
                    43,
                    56,
                    0,
                    232,
                    0
                ],
                "title": "Heta: Distributed Training of Heterogeneous Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heta: Distributed Training of Heterogeneous Graph Neural Networks"
                },
                "summary": "Heterogeneous Graph Neural Networks (HGNNs) leverage diverse semantic\nrelationships in Heterogeneous Graphs (HetGs) and have demonstrated remarkable\nlearning performance in various applications. However, current distributed GNN\ntraining systems often overlook unique characteristics of HetGs, such as\nvarying feature dimensions and the prevalence of missing features among nodes,\nleading to suboptimal performance or even incompatibility with distributed HGNN\ntraining. We introduce Heta, a framework designed to address the communication\nbottleneck in distributed HGNN training. Heta leverages the inherent structure\nof HGNNs - independent relation-specific aggregations for each relation,\nfollowed by a cross-relation aggregation - and advocates for a novel\nRelation-Aggregation-First computation paradigm. It performs relation-specific\naggregations within graph partitions and then exchanges partial aggregations.\nThis design, coupled with a new graph partitioning method that divides a HetG\nbased on its graph schema and HGNN computation dependency, substantially\nreduces communication overhead. Heta further incorporates an innovative GPU\nfeature caching strategy that accounts for the different cache miss-penalties\nassociated with diverse node types. Comprehensive evaluations of various HGNN\nmodels and large heterogeneous graph datasets demonstrate that Heta outperforms\nstate-of-the-art systems like DGL and GraphLearn by up to 5.8x and 2.3x in\nend-to-end epoch time, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Graph Neural Networks (HGNNs) leverage diverse semantic\nrelationships in Heterogeneous Graphs (HetGs) and have demonstrated remarkable\nlearning performance in various applications. However, current distributed GNN\ntraining systems often overlook unique characteristics of HetGs, such as\nvarying feature dimensions and the prevalence of missing features among nodes,\nleading to suboptimal performance or even incompatibility with distributed HGNN\ntraining. We introduce Heta, a framework designed to address the communication\nbottleneck in distributed HGNN training. Heta leverages the inherent structure\nof HGNNs - independent relation-specific aggregations for each relation,\nfollowed by a cross-relation aggregation - and advocates for a novel\nRelation-Aggregation-First computation paradigm. It performs relation-specific\naggregations within graph partitions and then exchanges partial aggregations.\nThis design, coupled with a new graph partitioning method that divides a HetG\nbased on its graph schema and HGNN computation dependency, substantially\nreduces communication overhead. Heta further incorporates an innovative GPU\nfeature caching strategy that accounts for the different cache miss-penalties\nassociated with diverse node types. Comprehensive evaluations of various HGNN\nmodels and large heterogeneous graph datasets demonstrate that Heta outperforms\nstate-of-the-art systems like DGL and GraphLearn by up to 5.8x and 2.3x in\nend-to-end epoch time, respectively."
                },
                "authors": [
                    {
                        "name": "Yuchen Zhong"
                    },
                    {
                        "name": "Junwei Su"
                    },
                    {
                        "name": "Chuan Wu"
                    },
                    {
                        "name": "Minjie Wang"
                    }
                ],
                "author_detail": {
                    "name": "Minjie Wang"
                },
                "author": "Minjie Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09697v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09697v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10104v1",
                "updated": "2024-08-19T15:47:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    47,
                    17,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T15:47:17Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    47,
                    17,
                    0,
                    232,
                    0
                ],
                "title": "Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory"
                },
                "summary": "The strong electric field between the sample and the extractor is the core of\ncathode lenses and a pivotal determinant of high resolution. Nevertheless,\nfields in the range of 3-8 kV/mm can be a source of complications. Local field\nenhancement at sharp edges or microscopic protrusions of cleaved samples may\nresult in field emission or flashovers. Moreover, slow background electrons are\ndrawn into the microscope column, where they contribute to space charge\neffects. A novel front lens configuration, optimized through ray-tracing\nsimulations, significantly reduces the field at the sample and allows even for\nzero field or retarding field, which serves to suppress space charge effects.\nOne or several annular electrodes, situated in a concentric position relative\nto the extractor, serve to form an additional lens within the gap between the\nsample and the extractor. The refractory power of this lens, and consequently\nthe field at the sample surface, can be modified by adjusting the potentials of\nthe annular electrodes. The imaging properties and aberrations of this gap lens\nhave been investigated with regard to momentum imaging and XPEEM. The study\nencompasses the energy range from the few-eV level for laser-ARPES to 6 keV,\nfor hard X-ray ARPES. The additional converging lens situated in close\nproximity to the sample exhibits a reduced field curvature of the k-image in\nthe backfocal plane. This allows for the acquisition of larger fields of view\nin both momentum and real-space imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The strong electric field between the sample and the extractor is the core of\ncathode lenses and a pivotal determinant of high resolution. Nevertheless,\nfields in the range of 3-8 kV/mm can be a source of complications. Local field\nenhancement at sharp edges or microscopic protrusions of cleaved samples may\nresult in field emission or flashovers. Moreover, slow background electrons are\ndrawn into the microscope column, where they contribute to space charge\neffects. A novel front lens configuration, optimized through ray-tracing\nsimulations, significantly reduces the field at the sample and allows even for\nzero field or retarding field, which serves to suppress space charge effects.\nOne or several annular electrodes, situated in a concentric position relative\nto the extractor, serve to form an additional lens within the gap between the\nsample and the extractor. The refractory power of this lens, and consequently\nthe field at the sample surface, can be modified by adjusting the potentials of\nthe annular electrodes. The imaging properties and aberrations of this gap lens\nhave been investigated with regard to momentum imaging and XPEEM. The study\nencompasses the energy range from the few-eV level for laser-ARPES to 6 keV,\nfor hard X-ray ARPES. The additional converging lens situated in close\nproximity to the sample exhibits a reduced field curvature of the k-image in\nthe backfocal plane. This allows for the acquisition of larger fields of view\nin both momentum and real-space imaging."
                },
                "authors": [
                    {
                        "name": "Olena Tkach"
                    },
                    {
                        "name": "Gerd Schoenhense"
                    }
                ],
                "author_detail": {
                    "name": "Gerd Schoenhense"
                },
                "author": "Gerd Schoenhense",
                "arxiv_comment": "17 pages, 4 figures, 44 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09848v1",
                "updated": "2024-08-19T09:50:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    50,
                    35,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T09:50:35Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    50,
                    35,
                    0,
                    232,
                    0
                ],
                "title": "Abstract Environment Trimming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abstract Environment Trimming"
                },
                "summary": "Variable sharing is a fundamental property in the static analysis of logic\nprograms, since it is instrumental for ensuring correctness and increasing\nprecision while inferring many useful program properties. Such properties\ninclude modes, determinacy, non-failure, cost, etc. This has motivated\nsignificant work on developing abstract domains to improve the precision and\nperformance of sharing analyses. Much of this work has centered around the\nfamily of set-sharing domains, because of the high precision they offer.\nHowever, this comes at a price: their scalability to a wide set of realistic\nprograms remains challenging and this hinders their wider adoption. In this\nwork, rather than defining new sharing abstract domains, we focus instead on\ndeveloping techniques which can be incorporated in the analyzers to address\naspects that are known to affect the efficiency of these domains, such as the\nnumber of variables, without affecting precision. These techniques are inspired\nin others used in the context of compiler optimizations, such as expression\nreassociation and variable trimming. We present several such techniques and\nprovide an extensive experimental evaluation of over 1100 program modules taken\nfrom both production code and classical benchmarks. This includes the\nSpectector cache analyzer, the s(CASP) system, the libraries of the Ciao\nsystem, the LPdoc documenter, the PLAI analyzer itself, etc. The experimental\nresults are quite encouraging: we have obtained significant speed-ups, and,\nmore importantly, the number of modules that require a timeout was cut in half.\nAs a result, many more programs can be analyzed precisely in reasonable times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variable sharing is a fundamental property in the static analysis of logic\nprograms, since it is instrumental for ensuring correctness and increasing\nprecision while inferring many useful program properties. Such properties\ninclude modes, determinacy, non-failure, cost, etc. This has motivated\nsignificant work on developing abstract domains to improve the precision and\nperformance of sharing analyses. Much of this work has centered around the\nfamily of set-sharing domains, because of the high precision they offer.\nHowever, this comes at a price: their scalability to a wide set of realistic\nprograms remains challenging and this hinders their wider adoption. In this\nwork, rather than defining new sharing abstract domains, we focus instead on\ndeveloping techniques which can be incorporated in the analyzers to address\naspects that are known to affect the efficiency of these domains, such as the\nnumber of variables, without affecting precision. These techniques are inspired\nin others used in the context of compiler optimizations, such as expression\nreassociation and variable trimming. We present several such techniques and\nprovide an extensive experimental evaluation of over 1100 program modules taken\nfrom both production code and classical benchmarks. This includes the\nSpectector cache analyzer, the s(CASP) system, the libraries of the Ciao\nsystem, the LPdoc documenter, the PLAI analyzer itself, etc. The experimental\nresults are quite encouraging: we have obtained significant speed-ups, and,\nmore importantly, the number of modules that require a timeout was cut in half.\nAs a result, many more programs can be analyzed precisely in reasonable times."
                },
                "authors": [
                    {
                        "name": "Daniel Jurjo-Rivas"
                    },
                    {
                        "name": "Jose F. Morales"
                    },
                    {
                        "name": "Pedro L√≥pez-Garc√≠a"
                    },
                    {
                        "name": "Manuel V. Hermenegildo"
                    }
                ],
                "author_detail": {
                    "name": "Manuel V. Hermenegildo"
                },
                "author": "Manuel V. Hermenegildo",
                "arxiv_comment": "61 pages, 10 figures, 7 tables, submitted to ICLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10284v1",
                "updated": "2024-08-19T03:27:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    27,
                    15,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T03:27:15Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    27,
                    15,
                    0,
                    232,
                    0
                ],
                "title": "AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for\n  Efficient MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for\n  Efficient MoE Inference"
                },
                "summary": "Mixture-of-Experts (MoE) models are designed to enhance the efficiency of\nlarge language models (LLMs) without proportionally increasing the\ncomputational demands. However, their deployment on edge devices still faces\nsignificant challenges due to high on-demand loading overheads from managing\nsparsely activated experts. This paper introduces AdapMoE, an algorithm-system\nco-design framework for efficient MoE inference. AdapMoE features adaptive\nexpert gating and management to reduce the on-demand loading overheads. We\nobserve the heterogeneity of experts loading across layers and tokens, based on\nwhich we propose a sensitivity-based strategy to adjust the number of activated\nexperts dynamically. Meanwhile, we also integrate advanced prefetching and\ncache management techniques to further reduce the loading latency. Through\ncomprehensive evaluations on various platforms, we demonstrate AdapMoE\nconsistently outperforms existing techniques, reducing the average number of\nactivated experts by 25% and achieving a 1.35x speedup without accuracy\ndegradation. Code is available at: https://github.com/PKU-SEC-Lab/AdapMoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models are designed to enhance the efficiency of\nlarge language models (LLMs) without proportionally increasing the\ncomputational demands. However, their deployment on edge devices still faces\nsignificant challenges due to high on-demand loading overheads from managing\nsparsely activated experts. This paper introduces AdapMoE, an algorithm-system\nco-design framework for efficient MoE inference. AdapMoE features adaptive\nexpert gating and management to reduce the on-demand loading overheads. We\nobserve the heterogeneity of experts loading across layers and tokens, based on\nwhich we propose a sensitivity-based strategy to adjust the number of activated\nexperts dynamically. Meanwhile, we also integrate advanced prefetching and\ncache management techniques to further reduce the loading latency. Through\ncomprehensive evaluations on various platforms, we demonstrate AdapMoE\nconsistently outperforms existing techniques, reducing the average number of\nactivated experts by 25% and achieving a 1.35x speedup without accuracy\ndegradation. Code is available at: https://github.com/PKU-SEC-Lab/AdapMoE."
                },
                "authors": [
                    {
                        "name": "Shuzhang Zhong"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Ru Huang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_doi": "10.1145/3676536.3676741",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676536.3676741",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.10284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07092v2",
                "updated": "2024-08-18T17:27:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    17,
                    27,
                    17,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-11T18:40:36Z",
                "published_parsed": [
                    2024,
                    8,
                    11,
                    18,
                    40,
                    36,
                    6,
                    224,
                    0
                ],
                "title": "Post-Training Sparse Attention with Double Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-Training Sparse Attention with Double Sparsity"
                },
                "summary": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\nhttps://github.com/andy-yang-1/DoubleSparse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\nhttps://github.com/andy-yang-1/DoubleSparse."
                },
                "authors": [
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Lianmin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Lianmin Zheng"
                },
                "author": "Lianmin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09483v1",
                "updated": "2024-08-18T13:54:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    13,
                    54,
                    46,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-18T13:54:46Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    13,
                    54,
                    46,
                    6,
                    231,
                    0
                ],
                "title": "CMD: A Cache-assisted GPU Memory Deduplication Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMD: A Cache-assisted GPU Memory Deduplication Architecture"
                },
                "summary": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we\ndivided these accesses into three types: (1) Write, (2) Data-Read, and (3)\nRead-Only. Besides, We find that many writes are duplicate, and the duplication\ncan be inter-dup and intra-dup. While inter-dup means different memory blocks\nare identical, and intra-dup means all the 4B elements in a line are the same.\nIn this work, we propose a cache-assisted GPU memory deduplication architecture\nnamed CMD to reduce the off-chip accesses via utilizing the data duplication in\nGPU applications. CMD includes three key design contributions which aim to\nreduce the three kinds of accesses: (1) A novel GPU memory deduplication\narchitecture that removes the inter-dup and inter-dup lines. As for the\ninter-dup detection, we reduce the extra read requests caused by the\ntraditional read-verify hash process. Besides, we design several techniques to\nmanage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce\nthe reads to duplicate data. When an L2 cache miss wants to read the duplicate\nblock, if the reference block has been fetched to L2 and it is clean, we can\ncopy it to the L2 missed block without accessing off-chip DRAM. As for the\nreads to intra-dup data, CMD uses the on-chip metadata cache to get the data.\n(3) When a cache line is evicted, the clean sectors in the line are invalidated\nwhile the dirty sectors are written back. However, most read-only victims are\nre-referenced from DRAM more than twice. Therefore, we add a full-associate\nFIFO to accommodate the read-only (it is also clean) victims to reduce the\nre-reference counts. Experiments show that CMD can decrease the off-chip\naccesses by 31.01%, reduce the energy by 32.78% and improve performance by\n37.79%. Besides, CMD can improve the performance of memory-intensive workloads\nby 50.18%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we\ndivided these accesses into three types: (1) Write, (2) Data-Read, and (3)\nRead-Only. Besides, We find that many writes are duplicate, and the duplication\ncan be inter-dup and intra-dup. While inter-dup means different memory blocks\nare identical, and intra-dup means all the 4B elements in a line are the same.\nIn this work, we propose a cache-assisted GPU memory deduplication architecture\nnamed CMD to reduce the off-chip accesses via utilizing the data duplication in\nGPU applications. CMD includes three key design contributions which aim to\nreduce the three kinds of accesses: (1) A novel GPU memory deduplication\narchitecture that removes the inter-dup and inter-dup lines. As for the\ninter-dup detection, we reduce the extra read requests caused by the\ntraditional read-verify hash process. Besides, we design several techniques to\nmanage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce\nthe reads to duplicate data. When an L2 cache miss wants to read the duplicate\nblock, if the reference block has been fetched to L2 and it is clean, we can\ncopy it to the L2 missed block without accessing off-chip DRAM. As for the\nreads to intra-dup data, CMD uses the on-chip metadata cache to get the data.\n(3) When a cache line is evicted, the clean sectors in the line are invalidated\nwhile the dirty sectors are written back. However, most read-only victims are\nre-referenced from DRAM more than twice. Therefore, we add a full-associate\nFIFO to accommodate the read-only (it is also clean) victims to reduce the\nre-reference counts. Experiments show that CMD can decrease the off-chip\naccesses by 31.01%, reduce the energy by 32.78% and improve performance by\n37.79%. Besides, CMD can improve the performance of memory-intensive workloads\nby 50.18%."
                },
                "authors": [
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Dan Feng"
                    },
                    {
                        "name": "Wei Tong"
                    },
                    {
                        "name": "Xueliang Wei"
                    },
                    {
                        "name": "Bing Wu"
                    }
                ],
                "author_detail": {
                    "name": "Bing Wu"
                },
                "author": "Bing Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08795v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08795v2",
                "updated": "2024-08-26T04:32:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    4,
                    32,
                    56,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-16T15:11:12Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    15,
                    11,
                    12,
                    4,
                    229,
                    0
                ],
                "title": "RollingCache: Using Runtime Behavior to Defend Against Cache Side\n  Channel Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RollingCache: Using Runtime Behavior to Defend Against Cache Side\n  Channel Attacks"
                },
                "summary": "Shared caches are vulnerable to side channel attacks through contention in\ncache sets. Besides being a simple source of information leak, these side\nchannels form useful gadgets for more sophisticated attacks that compromise the\nsecurity of shared systems.\n  The fundamental design aspect that contention attacks exploit is the\ndeterministic nature of the set of addresses contending for a cache set. In\nthis paper, we present RollingCache, a cache design that defends against\ncontention attacks by dynamically changing the set of addresses contending for\ncache sets. Unlike prior defenses, RollingCache does not rely on address\nencryption/decryption, data relocation, or cache partitioning. We use one level\nof indirection to implement dynamic mapping controlled by the whole-cache\nruntime behavior. Our solution does not depend on having defined security\ndomains, and can defend against an attacker running on the same or another\ncore.\n  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our\nsecurity evaluation shows that our dynamic mapping removes the deterministic\nability to identify the source of contention. The performance evaluation shows\nan impact of 1.67\\% over a mix of workloads, with a corresponding",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared caches are vulnerable to side channel attacks through contention in\ncache sets. Besides being a simple source of information leak, these side\nchannels form useful gadgets for more sophisticated attacks that compromise the\nsecurity of shared systems.\n  The fundamental design aspect that contention attacks exploit is the\ndeterministic nature of the set of addresses contending for a cache set. In\nthis paper, we present RollingCache, a cache design that defends against\ncontention attacks by dynamically changing the set of addresses contending for\ncache sets. Unlike prior defenses, RollingCache does not rely on address\nencryption/decryption, data relocation, or cache partitioning. We use one level\nof indirection to implement dynamic mapping controlled by the whole-cache\nruntime behavior. Our solution does not depend on having defined security\ndomains, and can defend against an attacker running on the same or another\ncore.\n  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our\nsecurity evaluation shows that our dynamic mapping removes the deterministic\nability to identify the source of contention. The performance evaluation shows\nan impact of 1.67\\% over a mix of workloads, with a corresponding"
                },
                "authors": [
                    {
                        "name": "Divya Ojha"
                    },
                    {
                        "name": "Sandhya Dwarkadas"
                    }
                ],
                "author_detail": {
                    "name": "Sandhya Dwarkadas"
                },
                "author": "Sandhya Dwarkadas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08795v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08795v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11550v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11550v3",
                "updated": "2024-08-16T08:46:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    8,
                    46,
                    33,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-16T09:53:32Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    53,
                    32,
                    1,
                    198,
                    0
                ],
                "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference"
                },
                "summary": "Large Language Models have excelled in various fields but encounter\nchallenges in memory and time efficiency due to the expanding Key-Value (KV)\ncache required for long-sequence inference. Recent efforts try to reduce KV\ncache size to a given memory budget by evicting vast non-critical cache\nelements during runtime, while preserving generation quality. Our revisiting of\ncurrent eviction methods reveals that they fundamentally minimize an upper\nbound of the $L_1$ eviction loss between the pre- and post-eviction outputs of\nmulti-head self-attention mechanisms. Moreover, our analysis indicates that the\ncommon practices of uniformly assigning budgets across attention heads harm\ntheir post-eviction generation quality. In light of these findings, we propose\na simple yet effective adaptive budget allocation algorithm. This algorithm not\nonly optimizes the theoretical loss upper bound but also reduces the $L_1$\neviction loss in practice by aligning with the varied characteristics across\ndifferent heads. By integrating this algorithm into two state-of-the-art\nmethods, we demonstrate the effectiveness of using adaptive budget allocation\nto optimize KV cache eviction. Extensive evaluations on 16 datasets and the\nNeedle-in-a-Haystack test confirm significant performance improvements across\nvarious tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have excelled in various fields but encounter\nchallenges in memory and time efficiency due to the expanding Key-Value (KV)\ncache required for long-sequence inference. Recent efforts try to reduce KV\ncache size to a given memory budget by evicting vast non-critical cache\nelements during runtime, while preserving generation quality. Our revisiting of\ncurrent eviction methods reveals that they fundamentally minimize an upper\nbound of the $L_1$ eviction loss between the pre- and post-eviction outputs of\nmulti-head self-attention mechanisms. Moreover, our analysis indicates that the\ncommon practices of uniformly assigning budgets across attention heads harm\ntheir post-eviction generation quality. In light of these findings, we propose\na simple yet effective adaptive budget allocation algorithm. This algorithm not\nonly optimizes the theoretical loss upper bound but also reduces the $L_1$\neviction loss in practice by aligning with the varied characteristics across\ndifferent heads. By integrating this algorithm into two state-of-the-art\nmethods, we demonstrate the effectiveness of using adaptive budget allocation\nto optimize KV cache eviction. Extensive evaluations on 16 datasets and the\nNeedle-in-a-Haystack test confirm significant performance improvements across\nvarious tasks."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11550v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11550v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v1",
                "updated": "2024-08-16T06:11:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have gained increased popularity due to their\nremarkable success across various tasks, which has led to the active\ndevelopment of a large set of diverse LLMs. However, individual LLMs have\nlimitations when applied to complex tasks because of such factors as training\nbiases, model sizes, and the datasets used. A promising approach is to\nefficiently harness the diverse capabilities of LLMs to overcome these\nindividual limitations. Towards this goal, we introduce a novel LLM selection\nalgorithm called SelectLLM. This algorithm directs input queries to the most\nsuitable subset of LLMs from a large pool, ensuring they collectively provide\nthe correct response efficiently. SelectLLM uses a multi-label classifier,\nutilizing the classifier's predictions and confidence scores to design optimal\npolicies for selecting an optimal, query-aware, and lightweight subset of LLMs.\nOur findings show that the proposed model outperforms individual LLMs and\nachieves competitive performance compared to similarly sized, computationally\nexpensive top-performing LLM subsets. Specifically, with a similarly sized\ntop-performing LLM subset, we achieve a significant reduction in latency on two\nstandard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower\nlatency for MMLU. Additionally, we conduct comprehensive analyses and ablation\nstudies, which validate the robustness of the proposed model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have gained increased popularity due to their\nremarkable success across various tasks, which has led to the active\ndevelopment of a large set of diverse LLMs. However, individual LLMs have\nlimitations when applied to complex tasks because of such factors as training\nbiases, model sizes, and the datasets used. A promising approach is to\nefficiently harness the diverse capabilities of LLMs to overcome these\nindividual limitations. Towards this goal, we introduce a novel LLM selection\nalgorithm called SelectLLM. This algorithm directs input queries to the most\nsuitable subset of LLMs from a large pool, ensuring they collectively provide\nthe correct response efficiently. SelectLLM uses a multi-label classifier,\nutilizing the classifier's predictions and confidence scores to design optimal\npolicies for selecting an optimal, query-aware, and lightweight subset of LLMs.\nOur findings show that the proposed model outperforms individual LLMs and\nachieves competitive performance compared to similarly sized, computationally\nexpensive top-performing LLM subsets. Specifically, with a similarly sized\ntop-performing LLM subset, we achieve a significant reduction in latency on two\nstandard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower\nlatency for MMLU. Additionally, we conduct comprehensive analyses and ablation\nstudies, which validate the robustness of the proposed model."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19291v2",
                "updated": "2024-08-16T04:12:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    4,
                    12,
                    25,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-27T16:20:21Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    16,
                    20,
                    21,
                    5,
                    209,
                    0
                ],
                "title": "Symmetric Locality: Definition and Initial Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Locality: Definition and Initial Results"
                },
                "summary": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs."
                },
                "authors": [
                    {
                        "name": "Giordan Escalona"
                    },
                    {
                        "name": "Dylan McKellips"
                    },
                    {
                        "name": "Chen Ding"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ding"
                },
                "author": "Chen Ding",
                "arxiv_comment": "6 pages, 2nd ver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v3",
                "updated": "2024-08-15T05:24:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    5,
                    24,
                    19,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07853v1",
                "updated": "2024-08-14T23:42:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    42,
                    46,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T23:42:46Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    42,
                    46,
                    2,
                    227,
                    0
                ],
                "title": "A Case for Enabling Delegation of 5G Core Decisions to the RAN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Case for Enabling Delegation of 5G Core Decisions to the RAN"
                },
                "summary": "Under conventional 5G system design, the authentication and continuous\nmonitoring of user equipment (UE) demands a reliable backhaul connection\nbetween the radio access network (RAN) and the core network functions (AMF,\nAUSF, UDM, etc.). This is not a given, especially in disaster response and\nmilitary operations. We propose that, in these scenarios, decisions made by\ncore functions can be effectively delegated to the RAN by leveraging the RAN's\ncomputing resources and the micro-service programmability of the O-RAN system\narchitecture. This paper presents several concrete designs of core-RAN decision\ndelegation, including caching of core decisions and replicating some of the\ncore decision logic. Each design has revealed interesting performance and\nsecurity trade-offs that warrant further investigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Under conventional 5G system design, the authentication and continuous\nmonitoring of user equipment (UE) demands a reliable backhaul connection\nbetween the radio access network (RAN) and the core network functions (AMF,\nAUSF, UDM, etc.). This is not a given, especially in disaster response and\nmilitary operations. We propose that, in these scenarios, decisions made by\ncore functions can be effectively delegated to the RAN by leveraging the RAN's\ncomputing resources and the micro-service programmability of the O-RAN system\narchitecture. This paper presents several concrete designs of core-RAN decision\ndelegation, including caching of core decisions and replicating some of the\ncore decision logic. Each design has revealed interesting performance and\nsecurity trade-offs that warrant further investigation."
                },
                "authors": [
                    {
                        "name": "Lucas Vancina"
                    },
                    {
                        "name": "Geoffrey Xie"
                    }
                ],
                "author_detail": {
                    "name": "Geoffrey Xie"
                },
                "author": "Geoffrey Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15440v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15440v2",
                "updated": "2024-08-14T09:18:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    9,
                    18,
                    2,
                    2,
                    227,
                    0
                ],
                "published": "2024-07-22T07:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    7,
                    42,
                    57,
                    0,
                    204,
                    0
                ],
                "title": "The Bicameral Cache: a split cache for vector architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache: a split cache for vector architectures"
                },
                "summary": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value."
                },
                "authors": [
                    {
                        "name": "Susana Rebolledo"
                    },
                    {
                        "name": "Borja Perez"
                    },
                    {
                        "name": "Jose Luis Bosque"
                    },
                    {
                        "name": "Peter Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Peter Hsu"
                },
                "author": "Peter Hsu",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15440v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15440v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07304v1",
                "updated": "2024-08-14T05:42:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    42,
                    35,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T05:42:35Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    42,
                    35,
                    2,
                    227,
                    0
                ],
                "title": "At Least Factor-of-Two Optimization for RWLE-Based Homomorphic\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At Least Factor-of-Two Optimization for RWLE-Based Homomorphic\n  Encryption"
                },
                "summary": "Many modern applications that deal with sensitive data, such as healthcare\nand government services, outsource computation to cloud platforms. In such\nuntrusted environments, privacy is of vital importance. One solution to this\nproblem is homomorphic encryption (HE), a family of cryptographic schemes that\nsupport certain algebraic operations on encrypted data without the need for\ndecryption. However, despite major advancements, encryption in modern HE\nschemes still comes with a non-trivial computational overhead that can hamper\ndata-intensive workloads. To resolve this, recent research has shown that\nleveraging caching techniques, such as Rache, can significantly enhance the\nperformance of HE schemes while maintaining security. Rache unfortunately\ndisplays a key limitation in the time complexity of its caching procedure,\nwhich scales with the size of the plaintext space. Smuche is another caching\nscheme that simultaneously improves the scalability of the caching procedure\nand turns the encryption process into a constant-time operation, utilizing only\na single scalar multiplication. Even still, more can be done. In this paper, we\npresent an encryption method we call ``Zinc\" which entirely forgoes the\nmultiple caching process, replacing it with a single scalar addition, and then\ninjecting randomness that takes constant time with respect to the plaintext\nspace. This injection of randomness is similar to Smuche, and a great\nimprovement from Rache, allowing Zinc to achieve efficiency without\ncompromising security. We implement the scheme using Microsoft SEAL and compare\nits performance to vanilla CKKS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many modern applications that deal with sensitive data, such as healthcare\nand government services, outsource computation to cloud platforms. In such\nuntrusted environments, privacy is of vital importance. One solution to this\nproblem is homomorphic encryption (HE), a family of cryptographic schemes that\nsupport certain algebraic operations on encrypted data without the need for\ndecryption. However, despite major advancements, encryption in modern HE\nschemes still comes with a non-trivial computational overhead that can hamper\ndata-intensive workloads. To resolve this, recent research has shown that\nleveraging caching techniques, such as Rache, can significantly enhance the\nperformance of HE schemes while maintaining security. Rache unfortunately\ndisplays a key limitation in the time complexity of its caching procedure,\nwhich scales with the size of the plaintext space. Smuche is another caching\nscheme that simultaneously improves the scalability of the caching procedure\nand turns the encryption process into a constant-time operation, utilizing only\na single scalar multiplication. Even still, more can be done. In this paper, we\npresent an encryption method we call ``Zinc\" which entirely forgoes the\nmultiple caching process, replacing it with a single scalar addition, and then\ninjecting randomness that takes constant time with respect to the plaintext\nspace. This injection of randomness is similar to Smuche, and a great\nimprovement from Rache, allowing Zinc to achieve efficiency without\ncompromising security. We implement the scheme using Microsoft SEAL and compare\nits performance to vanilla CKKS."
                },
                "authors": [
                    {
                        "name": "Jonathan Ly"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Ly"
                },
                "author": "Jonathan Ly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15743v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15743v2",
                "updated": "2024-08-13T13:56:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    56,
                    14,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-22T15:42:59Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    15,
                    42,
                    59,
                    0,
                    204,
                    0
                ],
                "title": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization"
                },
                "summary": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper."
                },
                "authors": [
                    {
                        "name": "Mohammad NaseriTehrani"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti T√∂lli"
                    }
                ],
                "author_detail": {
                    "name": "Antti T√∂lli"
                },
                "author": "Antti T√∂lli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15743v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15743v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04043v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04043v3",
                "updated": "2024-08-13T13:31:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    31,
                    34,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-07T18:51:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    18,
                    51,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "Ownership in low-level intermediate representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ownership in low-level intermediate representation"
                },
                "summary": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving."
                },
                "authors": [
                    {
                        "name": "Siddharth Priya"
                    },
                    {
                        "name": "Arie Gurfinkel"
                    }
                ],
                "author_detail": {
                    "name": "Arie Gurfinkel"
                },
                "author": "Arie Gurfinkel",
                "arxiv_comment": "FMCAD 2024 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04043v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04043v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06876v2",
                "updated": "2024-08-26T11:29:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    11,
                    29,
                    7,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-13T13:14:54Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    14,
                    54,
                    1,
                    226,
                    0
                ],
                "title": "Decision-Focused Learning to Predict Action Costs for Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision-Focused Learning to Predict Action Costs for Planning"
                },
                "summary": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements."
                },
                "authors": [
                    {
                        "name": "Jayanta Mandi"
                    },
                    {
                        "name": "Marco Foschini"
                    },
                    {
                        "name": "Daniel Holler"
                    },
                    {
                        "name": "Sylvie Thiebaux"
                    },
                    {
                        "name": "Jorg Hoffmann"
                    },
                    {
                        "name": "Tias Guns"
                    }
                ],
                "author_detail": {
                    "name": "Tias Guns"
                },
                "author": "Tias Guns",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18003v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18003v3",
                "updated": "2024-08-13T09:55:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    55,
                    43,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-25T12:56:22Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    12,
                    56,
                    22,
                    3,
                    207,
                    0
                ],
                "title": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption"
                },
                "summary": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Hongyi Zhang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "to be published in CoLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18003v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18003v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00167v2",
                "updated": "2024-08-13T09:08:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    8,
                    55,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-31T21:33:56Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    21,
                    33,
                    56,
                    2,
                    213,
                    0
                ],
                "title": "Finch: Prompt-guided Key-Value Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finch: Prompt-guided Key-Value Cache Compression"
                },
                "summary": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning."
                },
                "authors": [
                    {
                        "name": "Giulio Corallo"
                    },
                    {
                        "name": "Paolo Papotti"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Papotti"
                },
                "author": "Paolo Papotti",
                "arxiv_comment": "Accepted for publication at TACL - pre-MIT Press publication version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05996v1",
                "updated": "2024-08-12T08:46:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T08:46:30Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "title": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles"
                },
                "summary": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio."
                },
                "authors": [
                    {
                        "name": "Yantong Wang"
                    },
                    {
                        "name": "Ke Liu"
                    },
                    {
                        "name": "Hui Ji"
                    },
                    {
                        "name": "Jiande Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jiande Sun"
                },
                "author": "Jiande Sun",
                "arxiv_comment": "14 pages,10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19895v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19895v2",
                "updated": "2024-08-12T07:47:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    7,
                    47,
                    28,
                    0,
                    225,
                    0
                ],
                "published": "2024-07-29T11:17:26Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    11,
                    17,
                    26,
                    0,
                    211,
                    0
                ],
                "title": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor"
                },
                "summary": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area."
                },
                "authors": [
                    {
                        "name": "Riccardo Tedeschi"
                    },
                    {
                        "name": "Luca Valente"
                    },
                    {
                        "name": "Gianmarco Ottavi"
                    },
                    {
                        "name": "Enrico Zelioli"
                    },
                    {
                        "name": "Nils Wistoff"
                    },
                    {
                        "name": "Massimiliano Giacometti"
                    },
                    {
                        "name": "Abdul Basit Sajjad"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Davide Rossi"
                    }
                ],
                "author_detail": {
                    "name": "Davide Rossi"
                },
                "author": "Davide Rossi",
                "arxiv_comment": "4 pages, 4 figures, DSD2024 and SEAA2024 Works in Progress Session\n  AUG 2024; Updated the acknowledgments",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19895v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19895v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05912v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05912v1",
                "updated": "2024-08-12T03:53:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T03:53:51Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "title": "Correct Wrong Path",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Correct Wrong Path"
                },
                "summary": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP."
                },
                "authors": [
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Sankara Prasad Ramesh"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Svilen Kanev"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "Daniel A. Jim√©nez"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "arxiv_comment": "5 pages, 7 Figures, Submited to Computer Architecture Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05912v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05912v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12747v2",
                "updated": "2024-08-11T16:35:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    16,
                    35,
                    10,
                    6,
                    224,
                    0
                ],
                "published": "2024-05-21T12:59:59Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    12,
                    59,
                    59,
                    1,
                    142,
                    0
                ],
                "title": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay"
                },
                "summary": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "Added Section IV - (performance analysis of proposed HPDA\n  construction). The term 'coding delay' is formally defined (page no. 5). 14\n  pages, 10 figures and 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.19410v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.19410v2",
                "updated": "2024-08-11T08:07:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    8,
                    7,
                    28,
                    6,
                    224,
                    0
                ],
                "published": "2024-02-29T18:07:58Z",
                "published_parsed": [
                    2024,
                    2,
                    29,
                    18,
                    7,
                    58,
                    3,
                    60,
                    0
                ],
                "title": "Genie: Smart ROS-based Caching for Connected Autonomous Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Genie: Smart ROS-based Caching for Connected Autonomous Robots"
                },
                "summary": "Despite the promising future of autonomous robots, several key issues\ncurrently remain that can lead to compromised performance and safety. One such\nissue is latency, where we find that even the latest embedded platforms from\nNVIDIA fail to execute intelligence tasks (e.g., object detection) of\nautonomous vehicles in a real-time fashion. One remedy to this problem is the\npromising paradigm of edge computing. Through collaboration with our industry\npartner, we identify key prohibitive limitations of the current edge mindset:\n(1) servers are not distributed enough and thus, are not close enough to\nvehicles, (2) current proposed edge solutions do not provide substantially\nbetter performance and extra information specific to autonomous vehicles to\nwarrant their cost to the user, and (3) the state-of-the-art solutions are not\ncompatible with popular frameworks used in autonomous systems, particularly the\nRobot Operating System (ROS).\n  To remedy these issues, we provide Genie, an encapsulation technique that can\nenable transparent caching in ROS in a non-intrusive way (i.e., without\nmodifying the source code), can build the cache in a distributed manner (in\ncontrast to traditional central caching methods), and can construct a\ncollective three-dimensional object map to provide substantially better latency\n(even on low-power edge servers) and higher quality data to all vehicles in a\ncertain locality. We fully implement our design on state-of-the-art\nindustry-adopted embedded and edge platforms, using the prominent autonomous\ndriving software Autoware, and find that Genie can enhance the latency of\nAutoware Vision Detector by 82% on average, enable object reusability 31% of\nthe time on average and as much as 67% for the incoming requests, and boost the\nconfidence in its object map considerably over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the promising future of autonomous robots, several key issues\ncurrently remain that can lead to compromised performance and safety. One such\nissue is latency, where we find that even the latest embedded platforms from\nNVIDIA fail to execute intelligence tasks (e.g., object detection) of\nautonomous vehicles in a real-time fashion. One remedy to this problem is the\npromising paradigm of edge computing. Through collaboration with our industry\npartner, we identify key prohibitive limitations of the current edge mindset:\n(1) servers are not distributed enough and thus, are not close enough to\nvehicles, (2) current proposed edge solutions do not provide substantially\nbetter performance and extra information specific to autonomous vehicles to\nwarrant their cost to the user, and (3) the state-of-the-art solutions are not\ncompatible with popular frameworks used in autonomous systems, particularly the\nRobot Operating System (ROS).\n  To remedy these issues, we provide Genie, an encapsulation technique that can\nenable transparent caching in ROS in a non-intrusive way (i.e., without\nmodifying the source code), can build the cache in a distributed manner (in\ncontrast to traditional central caching methods), and can construct a\ncollective three-dimensional object map to provide substantially better latency\n(even on low-power edge servers) and higher quality data to all vehicles in a\ncertain locality. We fully implement our design on state-of-the-art\nindustry-adopted embedded and edge platforms, using the prominent autonomous\ndriving software Autoware, and find that Genie can enhance the latency of\nAutoware Vision Detector by 82% on average, enable object reusability 31% of\nthe time on average and as much as 67% for the incoming requests, and boost the\nconfidence in its object map considerably over time."
                },
                "authors": [
                    {
                        "name": "Zexin Li"
                    },
                    {
                        "name": "Soroush Bateni"
                    },
                    {
                        "name": "Cong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Cong Liu"
                },
                "author": "Cong Liu",
                "arxiv_comment": "Submitted to ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.19410v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.19410v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05646v1",
                "updated": "2024-08-10T22:47:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T22:47:12Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "title": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression"
                },
                "summary": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Gobinda Saha"
                    },
                    {
                        "name": "Sakshi Choudhary"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "12 page, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05614v1",
                "updated": "2024-08-10T19:17:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    19,
                    17,
                    46,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T19:17:46Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    19,
                    17,
                    46,
                    5,
                    223,
                    0
                ],
                "title": "ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using\n  Gaussian Mixture Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using\n  Gaussian Mixture Model"
                },
                "summary": "Compute Express Link (CXL) emerges as a solution for wide gap between\ncomputational speed and data communication rates among host and multiple\ndevices. It fosters a unified and coherent memory space between host and CXL\nstorage devices such as such as Solid-state drive (SSD) for memory expansion,\nwith a corresponding DRAM implemented as the device cache. However, this\nintroduces challenges such as substantial cache miss penalties, sub-optimal\ncaching due to data access granularity mismatch between the DRAM \"cache\" and\nSSD \"memory\", and inefficient hardware cache management. To address these\nissues, we propose a novel solution, named ICGMM, which optimizes caching and\neviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based\napproach. We prototype our solution on an FPGA board, which demonstrates a\nnoteworthy improvement compared to the classic Least Recently Used (LRU) cache\nstrategy. We observe a decrease in the cache miss rate ranging from 0.32% to\n6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD\naccess latency. Furthermore, when compared to the state-of-the-art Long\nShort-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA\nshowcases an impressive latency reduction of over 10,000 times. Remarkably,\nthis is achieved while demanding much fewer hardware resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Express Link (CXL) emerges as a solution for wide gap between\ncomputational speed and data communication rates among host and multiple\ndevices. It fosters a unified and coherent memory space between host and CXL\nstorage devices such as such as Solid-state drive (SSD) for memory expansion,\nwith a corresponding DRAM implemented as the device cache. However, this\nintroduces challenges such as substantial cache miss penalties, sub-optimal\ncaching due to data access granularity mismatch between the DRAM \"cache\" and\nSSD \"memory\", and inefficient hardware cache management. To address these\nissues, we propose a novel solution, named ICGMM, which optimizes caching and\neviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based\napproach. We prototype our solution on an FPGA board, which demonstrates a\nnoteworthy improvement compared to the classic Least Recently Used (LRU) cache\nstrategy. We observe a decrease in the cache miss rate ranging from 0.32% to\n6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD\naccess latency. Furthermore, when compared to the state-of-the-art Long\nShort-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA\nshowcases an impressive latency reduction of over 10,000 times. Remarkably,\nthis is achieved while demanding much fewer hardware resources."
                },
                "authors": [
                    {
                        "name": "Hanqiu Chen"
                    },
                    {
                        "name": "Yitu Wang"
                    },
                    {
                        "name": "Luis Vitorio Cargnini"
                    },
                    {
                        "name": "Mohammadreza Soltaniyeh"
                    },
                    {
                        "name": "Dongyang Li"
                    },
                    {
                        "name": "Gongjin Sun"
                    },
                    {
                        "name": "Pradeep Subedi"
                    },
                    {
                        "name": "Andrew Chang"
                    },
                    {
                        "name": "Yiran Chen"
                    },
                    {
                        "name": "Cong Hao"
                    }
                ],
                "author_detail": {
                    "name": "Cong Hao"
                },
                "author": "Cong Hao",
                "arxiv_comment": "This paper is accepted by DAC2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05171v1",
                "updated": "2024-08-09T16:48:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T16:48:01Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "title": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch"
                },
                "summary": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin."
                },
                "authors": [
                    {
                        "name": "R. A. Ryan"
                    },
                    {
                        "name": "P. E. Tsai"
                    },
                    {
                        "name": "A. R. Johansen"
                    },
                    {
                        "name": "A. Youmans"
                    },
                    {
                        "name": "D. P. Higginson"
                    },
                    {
                        "name": "J. M. Mitrani"
                    },
                    {
                        "name": "C. S. Adams"
                    },
                    {
                        "name": "D. A. Sutherland"
                    },
                    {
                        "name": "B. Levitt"
                    },
                    {
                        "name": "U. Shumlak"
                    }
                ],
                "author_detail": {
                    "name": "U. Shumlak"
                },
                "author": "U. Shumlak",
                "arxiv_comment": "16 pages, 11 figures, submitted to Journal of Nuclear Fusion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03675v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03675v2",
                "updated": "2024-08-08T01:20:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    8,
                    1,
                    20,
                    13,
                    3,
                    221,
                    0
                ],
                "published": "2024-08-07T10:31:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    10,
                    31,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time"
                },
                "summary": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL."
                },
                "authors": [
                    {
                        "name": "Yilong Chen"
                    },
                    {
                        "name": "Guoxia Wang"
                    },
                    {
                        "name": "Junyuan Shang"
                    },
                    {
                        "name": "Shiyao Cui"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Tingwen Liu"
                    },
                    {
                        "name": "Shuohuan Wang"
                    },
                    {
                        "name": "Yu Sun"
                    },
                    {
                        "name": "Dianhai Yu"
                    },
                    {
                        "name": "Hua Wu"
                    }
                ],
                "author_detail": {
                    "name": "Hua Wu"
                },
                "author": "Hua Wu",
                "arxiv_comment": "Accepted by ACL 2024 (main conference, long paper)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03675v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03675v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2210.10978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2210.10978v2",
                "updated": "2024-08-07T23:48:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    23,
                    48,
                    59,
                    2,
                    220,
                    0
                ],
                "published": "2022-10-20T02:58:36Z",
                "published_parsed": [
                    2022,
                    10,
                    20,
                    2,
                    58,
                    36,
                    3,
                    293,
                    0
                ],
                "title": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends"
                },
                "summary": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem."
                },
                "authors": [
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Youyang Qu"
                    },
                    {
                        "name": "Yong Xiang"
                    },
                    {
                        "name": "Md Palash Uddin"
                    },
                    {
                        "name": "Dezhong Peng"
                    },
                    {
                        "name": "Longxiang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Longxiang Gao"
                },
                "author": "Longxiang Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2210.10978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2210.10978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04107v1",
                "updated": "2024-08-07T22:10:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "published": "2024-08-07T22:10:26Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "title": "Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference"
                },
                "summary": "In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19547v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19547v2",
                "updated": "2024-08-07T20:43:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    20,
                    43,
                    10,
                    2,
                    220,
                    0
                ],
                "published": "2024-07-28T17:46:15Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    17,
                    46,
                    15,
                    6,
                    210,
                    0
                ],
                "title": "Temporal Feature Matters: A Framework for Diffusion Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Feature Matters: A Framework for Diffusion Model Quantization"
                },
                "summary": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration..",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration.."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2311.16503",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19547v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19547v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03652v1",
                "updated": "2024-08-07T09:34:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    34,
                    55,
                    2,
                    220,
                    0
                ],
                "published": "2024-08-07T09:34:55Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    34,
                    55,
                    2,
                    220,
                    0
                ],
                "title": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search"
                },
                "summary": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task."
                },
                "authors": [
                    {
                        "name": "Ahmed Abdou"
                    },
                    {
                        "name": "Tasneem Mohsen"
                    }
                ],
                "author_detail": {
                    "name": "Tasneem Mohsen"
                },
                "author": "Tasneem Mohsen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03308v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03308v1",
                "updated": "2024-08-06T17:16:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T17:16:19Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "title": "Potential and Limitation of High-Frequency Cores and Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Potential and Limitation of High-Frequency Cores and Caches"
                },
                "summary": "This paper explores the potential of cryogenic computing and superconducting\nelectronics as promising alternatives to traditional semiconductor devices. As\nsemiconductor devices face challenges such as increased leakage currents and\nreduced performance at higher temperatures, these novel technologies offer high\nperformance and low power computation. Cryogenic computing operates at\nultra-low temperatures near 77 K, leading to lower leakage currents and\nimproved electron mobility. On the other hand, superconducting electronics,\noperating near 0 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconducting electronics and cryogenic computing in gem5. We\nevaluate the performance of these components using workloads representative of\nreal-world applications like NPB, SPEC CPU2006, and GAPBS. Our results show the\npotential speedups achievable by these components and the limitations posed by\ncache bandwidth. This work provides valuable insights into the performance\nimplications and design trade-offs associated with cryogenic and\nsuperconducting technologies, laying the foundation for future research in this\nfield using gem5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the potential of cryogenic computing and superconducting\nelectronics as promising alternatives to traditional semiconductor devices. As\nsemiconductor devices face challenges such as increased leakage currents and\nreduced performance at higher temperatures, these novel technologies offer high\nperformance and low power computation. Cryogenic computing operates at\nultra-low temperatures near 77 K, leading to lower leakage currents and\nimproved electron mobility. On the other hand, superconducting electronics,\noperating near 0 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconducting electronics and cryogenic computing in gem5. We\nevaluate the performance of these components using workloads representative of\nreal-world applications like NPB, SPEC CPU2006, and GAPBS. Our results show the\npotential speedups achievable by these components and the limitations posed by\ncache bandwidth. This work provides valuable insights into the performance\nimplications and design trade-offs associated with cryogenic and\nsuperconducting technologies, laying the foundation for future research in this\nfield using gem5."
                },
                "authors": [
                    {
                        "name": "Kunal Pai"
                    },
                    {
                        "name": "Anusheel Nand"
                    },
                    {
                        "name": "Jason Lowe-Power"
                    }
                ],
                "author_detail": {
                    "name": "Jason Lowe-Power"
                },
                "author": "Jason Lowe-Power",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03308v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03308v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02999v1",
                "updated": "2024-08-06T07:12:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    12,
                    9,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T07:12:09Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    12,
                    9,
                    1,
                    219,
                    0
                ],
                "title": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning"
                },
                "summary": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop."
                },
                "authors": [
                    {
                        "name": "Lekai Chen"
                    },
                    {
                        "name": "Ashutosh Trivedi"
                    },
                    {
                        "name": "Alvaro Velasquez"
                    }
                ],
                "author_detail": {
                    "name": "Alvaro Velasquez"
                },
                "author": "Alvaro Velasquez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.FL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02911v1",
                "updated": "2024-08-06T02:51:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    51,
                    22,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T02:51:22Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    51,
                    22,
                    1,
                    219,
                    0
                ],
                "title": "NVPC: A Transparent NVM Page Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVPC: A Transparent NVM Page Cache"
                },
                "summary": "Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases."
                },
                "authors": [
                    {
                        "name": "Guoyu Wang"
                    },
                    {
                        "name": "Xilong Che"
                    },
                    {
                        "name": "Haoyang Wei"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Puyi He"
                    },
                    {
                        "name": "Juncheng Hu"
                    }
                ],
                "author_detail": {
                    "name": "Juncheng Hu"
                },
                "author": "Juncheng Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02409v1",
                "updated": "2024-08-05T12:09:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    9,
                    50,
                    0,
                    218,
                    0
                ],
                "published": "2024-08-05T12:09:50Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    9,
                    50,
                    0,
                    218,
                    0
                ],
                "title": "Electron-beam-induced modification of gold microparticles in an SEM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced modification of gold microparticles in an SEM"
                },
                "summary": "Electron-beam-induced conversion of materials in a transmission electron\nmicroscope uses the high power density of a localized electron beam of\nacceleration voltages above 100 kV as an energy source to transform matter at\nthe sub-micron scale. Here, the e-beam-induced transformation of precursor\nmicroparticles employing a low-energy e-beam with an acceleration voltage of 30\nkV in a scanning electron microscope is developed to increase the versatility\nand efficiency of the technique. Under these conditions, the technique can be\nclassified between e-beam lithography, where the e-beam is used to mill holes\nin or grow some different material onto a substrate, and e-beam welding, where\nmatter can be welded together when overcoming the melting phase. Modifying gold\nmicroparticles on an amorphous SiOx substrate reveals the dominant role of\ninelastic electron-matter interaction and subsequent localized heating for the\nobserved melting and vaporization of the precursor microparticles under the\nelectron beam. Monte-Carlo scattering simulations and thermodynamic modeling\nfurther support the findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced conversion of materials in a transmission electron\nmicroscope uses the high power density of a localized electron beam of\nacceleration voltages above 100 kV as an energy source to transform matter at\nthe sub-micron scale. Here, the e-beam-induced transformation of precursor\nmicroparticles employing a low-energy e-beam with an acceleration voltage of 30\nkV in a scanning electron microscope is developed to increase the versatility\nand efficiency of the technique. Under these conditions, the technique can be\nclassified between e-beam lithography, where the e-beam is used to mill holes\nin or grow some different material onto a substrate, and e-beam welding, where\nmatter can be welded together when overcoming the melting phase. Modifying gold\nmicroparticles on an amorphous SiOx substrate reveals the dominant role of\ninelastic electron-matter interaction and subsequent localized heating for the\nobserved melting and vaporization of the precursor microparticles under the\nelectron beam. Monte-Carlo scattering simulations and thermodynamic modeling\nfurther support the findings."
                },
                "authors": [
                    {
                        "name": "Kristina Weinel"
                    },
                    {
                        "name": "Marc Benjamin Hahn"
                    },
                    {
                        "name": "Axel Lubk"
                    },
                    {
                        "name": "Wen Feng"
                    },
                    {
                        "name": "Ignacio Gonzalez Martinez"
                    },
                    {
                        "name": "Bernd B√ºchner"
                    },
                    {
                        "name": "Leonardo Agudo J√°come"
                    }
                ],
                "author_detail": {
                    "name": "Leonardo Agudo J√°come"
                },
                "author": "Leonardo Agudo J√°come",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05235v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05235v1",
                "updated": "2024-08-05T09:07:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "published": "2024-08-05T09:07:06Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "title": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference\n  Serving"
                },
                "summary": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry\nGPUs places ever-increasing energy demands, raising environmental and monetary\nconcerns. Inference dominates LLM workloads, presenting a critical challenge\nfor providers: minimizing energy costs under Service-Level Objectives (SLOs)\nthat ensure optimal user experience. In this paper, we present\n\\textit{throttLL'eM}, a framework that reduces energy consumption while meeting\nSLOs through the use of instance and GPU frequency scaling.\n\\textit{throttLL'eM} features mechanisms that project future KV cache usage and\nbatch size. Leveraging a Machine-Learning (ML) model that receives these\nprojections as inputs, \\textit{throttLL'eM} manages performance at the\niteration level to satisfy SLOs with reduced frequencies and instance sizes. We\nshow that the proposed ML model achieves $R^2$ scores greater than 0.97 and\nmiss-predicts performance by less than 1 iteration per second on average.\nExperimental results on LLM inference traces show that \\textit{throttLL'eM}\nachieves up to 43.8\\% lower energy consumption and an energy efficiency\nimprovement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's\nTriton server.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry\nGPUs places ever-increasing energy demands, raising environmental and monetary\nconcerns. Inference dominates LLM workloads, presenting a critical challenge\nfor providers: minimizing energy costs under Service-Level Objectives (SLOs)\nthat ensure optimal user experience. In this paper, we present\n\\textit{throttLL'eM}, a framework that reduces energy consumption while meeting\nSLOs through the use of instance and GPU frequency scaling.\n\\textit{throttLL'eM} features mechanisms that project future KV cache usage and\nbatch size. Leveraging a Machine-Learning (ML) model that receives these\nprojections as inputs, \\textit{throttLL'eM} manages performance at the\niteration level to satisfy SLOs with reduced frequencies and instance sizes. We\nshow that the proposed ML model achieves $R^2$ scores greater than 0.97 and\nmiss-predicts performance by less than 1 iteration per second on average.\nExperimental results on LLM inference traces show that \\textit{throttLL'eM}\nachieves up to 43.8\\% lower energy consumption and an energy efficiency\nimprovement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's\nTriton server."
                },
                "authors": [
                    {
                        "name": "Andreas Kosmas Kakolyris"
                    },
                    {
                        "name": "Dimosthenis Masouros"
                    },
                    {
                        "name": "Petros Vavaroutsos"
                    },
                    {
                        "name": "Sotirios Xydis"
                    },
                    {
                        "name": "Dimitrios Soudris"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Soudris"
                },
                "author": "Dimitrios Soudris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05235v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05235v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11912v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11912v3",
                "updated": "2024-08-04T00:58:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    58,
                    4,
                    6,
                    217,
                    0
                ],
                "published": "2024-04-18T05:25:54Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    5,
                    25,
                    54,
                    3,
                    109,
                    0
                ],
                "title": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding"
                },
                "summary": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11912v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11912v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01890v1",
                "updated": "2024-08-04T00:38:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "published": "2024-08-04T00:38:34Z",
                "published_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "title": "Cross-layer Attention Sharing for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-layer Attention Sharing for Large Language Models"
                },
                "summary": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B."
                },
                "authors": [
                    {
                        "name": "Yongyu Mu"
                    },
                    {
                        "name": "Yuzhang Wu"
                    },
                    {
                        "name": "Yuchun Fan"
                    },
                    {
                        "name": "Chenglong Wang"
                    },
                    {
                        "name": "Hengyu Li"
                    },
                    {
                        "name": "Qiaozhi He"
                    },
                    {
                        "name": "Murun Yang"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "Working in process",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01519v1",
                "updated": "2024-08-02T18:25:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    25,
                    57,
                    4,
                    215,
                    0
                ],
                "published": "2024-08-02T18:25:57Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    25,
                    57,
                    4,
                    215,
                    0
                ],
                "title": "Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling"
                },
                "summary": "Many spectral CT applications require accurate material decomposition.\nExisting material decomposition algorithms are often susceptible to significant\nnoise magnification or, in the case of one-step model-based approaches,\nhampered by slow convergence rates and large computational requirements. In\nthis work, we proposed a novel framework - spectral diffusion posterior\nsampling (spectral DPS) - for one-step reconstruction and multi-material\ndecomposition, which combines sophisticated prior information captured by\none-time unsupervised learning and an arbitrary analytic physical system model.\nSpectral DPS is built upon a general DPS framework for nonlinear inverse\nproblems. Several strategies developed in previous work, including jumpstart\nsampling, Jacobian approximation, and multi-step likelihood updates are applied\nfacilitate stable and accurate decompositions. The effectiveness of spectral\nDPS was evaluated on a simulated dual-layer and a kV-switching spectral system\nas well as on a physical cone-beam CT (CBCT) test bench. In simulation studies,\nspectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53%\nto 57.30% over MBMD, depending on the the region of interest. In physical\nphantom study, spectral DPS achieved a <1% error in estimating the mean density\nin a homogeneous region. Compared with baseline DPS, spectral DPS effectively\navoided generating false structures in the homogeneous phantom and reduced the\nvariability around edges. Both simulation and physical phantom studies\ndemonstrated the superior performance of spectral DPS for stable and accurate\nmaterial decomposition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many spectral CT applications require accurate material decomposition.\nExisting material decomposition algorithms are often susceptible to significant\nnoise magnification or, in the case of one-step model-based approaches,\nhampered by slow convergence rates and large computational requirements. In\nthis work, we proposed a novel framework - spectral diffusion posterior\nsampling (spectral DPS) - for one-step reconstruction and multi-material\ndecomposition, which combines sophisticated prior information captured by\none-time unsupervised learning and an arbitrary analytic physical system model.\nSpectral DPS is built upon a general DPS framework for nonlinear inverse\nproblems. Several strategies developed in previous work, including jumpstart\nsampling, Jacobian approximation, and multi-step likelihood updates are applied\nfacilitate stable and accurate decompositions. The effectiveness of spectral\nDPS was evaluated on a simulated dual-layer and a kV-switching spectral system\nas well as on a physical cone-beam CT (CBCT) test bench. In simulation studies,\nspectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53%\nto 57.30% over MBMD, depending on the the region of interest. In physical\nphantom study, spectral DPS achieved a <1% error in estimating the mean density\nin a homogeneous region. Compared with baseline DPS, spectral DPS effectively\navoided generating false structures in the homogeneous phantom and reduced the\nvariability around edges. Both simulation and physical phantom studies\ndemonstrated the superior performance of spectral DPS for stable and accurate\nmaterial decomposition."
                },
                "authors": [
                    {
                        "name": "Xiao Jiang"
                    },
                    {
                        "name": "Grace J. Gang"
                    },
                    {
                        "name": "J. Webster Stayman"
                    }
                ],
                "author_detail": {
                    "name": "J. Webster Stayman"
                },
                "author": "J. Webster Stayman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00327v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00327v2",
                "updated": "2024-08-02T07:37:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    2,
                    7,
                    37,
                    51,
                    4,
                    215,
                    0
                ],
                "published": "2024-08-01T07:00:18Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    7,
                    0,
                    18,
                    3,
                    214,
                    0
                ],
                "title": "Search-in-Memory (SiM): Reliable, Versatile, and Efficient Data Matching\n  in SSD's NAND Flash Memory Chip for Data Indexing Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search-in-Memory (SiM): Reliable, Versatile, and Efficient Data Matching\n  in SSD's NAND Flash Memory Chip for Data Indexing Acceleration"
                },
                "summary": "To index the increasing volume of data, modern data indexes are typically\nstored on SSDs and cached in DRAM. However, searching such an index has\nresulted in significant I/O traffic due to limited access locality and\ninefficient cache utilization. At the heart of index searching is the operation\nof filtering through vast data spans to isolate a small, relevant subset, which\ninvolves basic equality tests rather than the complex arithmetic provided by\nmodern CPUs. This paper introduces the Search-in-Memory (SiM) chip, which\ndemonstrates the feasibility of performing data filtering directly within a\nNAND flash memory chip, transmitting only relevant search results rather than\ncomplete pages. Instead of adding complex circuits, we propose repurposing\nexisting circuitry for efficient and accurate bitwise parallel matching. We\ndemonstrate how different data structures can use our flexible SIMD command\ninterface to offload index searches. This strategy not only frees up the CPU\nfor more computationally demanding tasks, but it also optimizes DRAM usage for\nwrite buffering, significantly lowering energy consumption associated with I/O\ntransmission between the CPU and DRAM. Extensive testing across a wide range of\nworkloads reveals up to a 9X speedup in write-heavy workloads and up to 45%\nenergy savings due to reduced read and write I/O. Furthermore, we achieve\nsignificant reductions in median and tail read latencies of up to 89% and 85%\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To index the increasing volume of data, modern data indexes are typically\nstored on SSDs and cached in DRAM. However, searching such an index has\nresulted in significant I/O traffic due to limited access locality and\ninefficient cache utilization. At the heart of index searching is the operation\nof filtering through vast data spans to isolate a small, relevant subset, which\ninvolves basic equality tests rather than the complex arithmetic provided by\nmodern CPUs. This paper introduces the Search-in-Memory (SiM) chip, which\ndemonstrates the feasibility of performing data filtering directly within a\nNAND flash memory chip, transmitting only relevant search results rather than\ncomplete pages. Instead of adding complex circuits, we propose repurposing\nexisting circuitry for efficient and accurate bitwise parallel matching. We\ndemonstrate how different data structures can use our flexible SIMD command\ninterface to offload index searches. This strategy not only frees up the CPU\nfor more computationally demanding tasks, but it also optimizes DRAM usage for\nwrite buffering, significantly lowering energy consumption associated with I/O\ntransmission between the CPU and DRAM. Extensive testing across a wide range of\nworkloads reveals up to a 9X speedup in write-heavy workloads and up to 45%\nenergy savings due to reduced read and write I/O. Furthermore, we achieve\nsignificant reductions in median and tail read latencies of up to 89% and 85%\nrespectively."
                },
                "authors": [
                    {
                        "name": "Yun-Chih Chen"
                    },
                    {
                        "name": "Yuan-Hao Chang"
                    },
                    {
                        "name": "Tei-Wei Kuo"
                    }
                ],
                "author_detail": {
                    "name": "Tei-Wei Kuo"
                },
                "author": "Tei-Wei Kuo",
                "arxiv_comment": "This paper has been accepted for presentation at the The\n  International Conference on Hardware/Software Codesign and System Synthesis\n  (CODES+ISSS) in September, 2024. An extended abstract of this paper was\n  presented in Design, Automation & Test in Europe Conference & Exhibition\n  (DATE), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00327v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00327v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00957v1",
                "updated": "2024-08-01T23:52:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    23,
                    52,
                    43,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T23:52:43Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    23,
                    52,
                    43,
                    3,
                    214,
                    0
                ],
                "title": "Caching Aided Multi-Tenant Serverless Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching Aided Multi-Tenant Serverless Computing"
                },
                "summary": "One key to enabling high-performance serverless computing is to mitigate\ncold-starts. Current solutions utilize a warm pool to keep function alive: a\nwarm-start can be analogous to a CPU cache-hit. However, modern cache has\nmultiple hierarchies and the last-level cache is shared among cores, whereas\nthe warm pool is limited to a single tenant for security concerns. Also, the\nwarm pool keep-alive policy can be further optimized using cache replacement\nalgorithms. In this paper, we borrow practical optimizations from caching, and\ndesign FaasCamp, a caching-aided multi-tenant serverless computing framework.\nFaasCamp extends the single-tier warm pool into multi-tiers, with a reclaim\npool introduced enabling secure function instance sharing among tenants. Also,\nFaasCamp leverages machine learning to approximate the optimal cache\nreplacement policy to improve the warm rate. We have implemented a prototype\nand conducted extensive experiments under multiple scenarios. The results show\nthat FaasCamp can outperform existing platforms with minimal overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One key to enabling high-performance serverless computing is to mitigate\ncold-starts. Current solutions utilize a warm pool to keep function alive: a\nwarm-start can be analogous to a CPU cache-hit. However, modern cache has\nmultiple hierarchies and the last-level cache is shared among cores, whereas\nthe warm pool is limited to a single tenant for security concerns. Also, the\nwarm pool keep-alive policy can be further optimized using cache replacement\nalgorithms. In this paper, we borrow practical optimizations from caching, and\ndesign FaasCamp, a caching-aided multi-tenant serverless computing framework.\nFaasCamp extends the single-tier warm pool into multi-tiers, with a reclaim\npool introduced enabling secure function instance sharing among tenants. Also,\nFaasCamp leverages machine learning to approximate the optimal cache\nreplacement policy to improve the warm rate. We have implemented a prototype\nand conducted extensive experiments under multiple scenarios. The results show\nthat FaasCamp can outperform existing platforms with minimal overhead."
                },
                "authors": [
                    {
                        "name": "Chu Qiao"
                    },
                    {
                        "name": "Cong Wang"
                    },
                    {
                        "name": "Zhenkai Zhang"
                    },
                    {
                        "name": "Yuede Ji"
                    },
                    {
                        "name": "Xing Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xing Gao"
                },
                "author": "Xing Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00859v2",
                "updated": "2024-08-01T21:21:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    21,
                    21,
                    28,
                    3,
                    214,
                    0
                ],
                "published": "2024-04-01T02:01:28Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    2,
                    1,
                    28,
                    0,
                    92,
                    0
                ],
                "title": "Do language models plan ahead for future tokens?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do language models plan ahead for future tokens?"
                },
                "summary": "Do transformers \"think ahead\" during inference at a given position? It is\nknown transformers prepare information in the hidden states of the forward pass\nat time step $t$ that is then used in future forward passes $t+\\tau$. We posit\ntwo explanations for this phenomenon: pre-caching, in which off-diagonal\ngradient terms present during training result in the model computing features\nat $t$ irrelevant to the present inference task but useful for the future, and\nbreadcrumbs, in which features most relevant to time step $t$ are already the\nsame as those that would most benefit inference at time $t+\\tau$. We test these\nhypotheses by training language models without propagating gradients to past\ntimesteps, a scheme we formalize as myopic training. In a constructed synthetic\ndata setting, we find clear evidence for pre-caching. In the autoregressive\nlanguage modeling setting, our experiments are more suggestive of the\nbreadcrumbs hypothesis, though pre-caching increases with model scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do transformers \"think ahead\" during inference at a given position? It is\nknown transformers prepare information in the hidden states of the forward pass\nat time step $t$ that is then used in future forward passes $t+\\tau$. We posit\ntwo explanations for this phenomenon: pre-caching, in which off-diagonal\ngradient terms present during training result in the model computing features\nat $t$ irrelevant to the present inference task but useful for the future, and\nbreadcrumbs, in which features most relevant to time step $t$ are already the\nsame as those that would most benefit inference at time $t+\\tau$. We test these\nhypotheses by training language models without propagating gradients to past\ntimesteps, a scheme we formalize as myopic training. In a constructed synthetic\ndata setting, we find clear evidence for pre-caching. In the autoregressive\nlanguage modeling setting, our experiments are more suggestive of the\nbreadcrumbs hypothesis, though pre-caching increases with model scale."
                },
                "authors": [
                    {
                        "name": "Wilson Wu"
                    },
                    {
                        "name": "John X. Morris"
                    },
                    {
                        "name": "Lionel Levine"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Levine"
                },
                "author": "Lionel Levine",
                "arxiv_comment": "24 pages, 11 figures. Camera-ready for COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00539v1",
                "updated": "2024-08-01T13:22:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    22,
                    1,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T13:22:01Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    22,
                    1,
                    3,
                    214,
                    0
                ],
                "title": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs"
                },
                "summary": "Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance."
                },
                "authors": [
                    {
                        "name": "Mingcong Lu"
                    },
                    {
                        "name": "Jiangcai Zhu"
                    },
                    {
                        "name": "Wang Hao"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Shusheng Zhang"
                    },
                    {
                        "name": "Kailai Shao"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Nan Li"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Xin Lu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Lu"
                },
                "author": "Xin Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14361v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14361v2",
                "updated": "2024-08-01T13:21:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    21,
                    24,
                    3,
                    214,
                    0
                ],
                "published": "2024-01-25T18:07:50Z",
                "published_parsed": [
                    2024,
                    1,
                    25,
                    18,
                    7,
                    50,
                    3,
                    25,
                    0
                ],
                "title": "MoE-Infinity: Offloading-Efficient MoE Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-Infinity: Offloading-Efficient MoE Model Serving"
                },
                "summary": "This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity"
                },
                "authors": [
                    {
                        "name": "Leyang Xue"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Zhan Lu"
                    },
                    {
                        "name": "Luo Mai"
                    },
                    {
                        "name": "Mahesh Marina"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Marina"
                },
                "author": "Mahesh Marina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.14361v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14361v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15220v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15220v4",
                "updated": "2024-08-01T07:51:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    7,
                    51,
                    25,
                    3,
                    214,
                    0
                ],
                "published": "2024-02-23T09:29:19Z",
                "published_parsed": [
                    2024,
                    2,
                    23,
                    9,
                    29,
                    19,
                    4,
                    54,
                    0
                ],
                "title": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition"
                },
                "summary": "Self-attention is an essential component of large language models (LLM) but a\nsignificant source of inference latency for long sequences. In multi-tenant LLM\nserving scenarios, the compute and memory operation cost of self-attention can\nbe optimized by using the probability that multiple LLM requests have shared\nsystem prompts in prefixes. In this paper, we introduce ChunkAttention, a\nprefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the state-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-attention is an essential component of large language models (LLM) but a\nsignificant source of inference latency for long sequences. In multi-tenant LLM\nserving scenarios, the compute and memory operation cost of self-attention can\nbe optimized by using the probability that multiple LLM requests have shared\nsystem prompts in prefixes. In this paper, we introduce ChunkAttention, a\nprefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the state-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096."
                },
                "authors": [
                    {
                        "name": "Lu Ye"
                    },
                    {
                        "name": "Ze Tao"
                    },
                    {
                        "name": "Yong Huang"
                    },
                    {
                        "name": "Yang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yang Li"
                },
                "author": "Yang Li",
                "arxiv_comment": "ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.15220v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15220v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00232v1",
                "updated": "2024-08-01T01:57:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    1,
                    57,
                    9,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T01:57:09Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    1,
                    57,
                    9,
                    3,
                    214,
                    0
                ],
                "title": "CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph\n  Neural Network Training with Communication Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph\n  Neural Network Training with Communication Reduction"
                },
                "summary": "Graph neural network training is mainly categorized into mini-batch and\nfull-batch training methods. The mini-batch training method samples subgraphs\nfrom the original graph in each iteration. This sampling operation introduces\nextra computation overhead and reduces the training accuracy. Meanwhile, the\nfull-batch training method calculates the features and corresponding gradients\nof all vertices in each iteration, and therefore has higher convergence\naccuracy. However, in the distributed cluster, frequent remote accesses of\nvertex features and gradients lead to huge communication overhead, thus\nrestricting the overall training efficiency.\n  In this paper, we introduce the cached-based distributed full-batch graph\nneural network training framework (CDFGNN). We propose the adaptive cache\nmechanism to reduce the remote vertex access by caching the historical features\nand gradients of neighbor vertices. Besides, we further optimize the\ncommunication overhead by quantifying the messages and designing the graph\npartition algorithm for the hierarchical communication architecture.\nExperiments show that the adaptive cache mechanism reduces remote vertex\naccesses by 63.14% on average. Combined with communication quantization and\nhierarchical GP algorithm, CDFGNN outperforms the state-of-the-art distributed\nfull-batch training frameworks by 30.39% in our experiments. Our results\nindicate that CDFGNN has great potential in accelerating distributed full-batch\nGNN training tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural network training is mainly categorized into mini-batch and\nfull-batch training methods. The mini-batch training method samples subgraphs\nfrom the original graph in each iteration. This sampling operation introduces\nextra computation overhead and reduces the training accuracy. Meanwhile, the\nfull-batch training method calculates the features and corresponding gradients\nof all vertices in each iteration, and therefore has higher convergence\naccuracy. However, in the distributed cluster, frequent remote accesses of\nvertex features and gradients lead to huge communication overhead, thus\nrestricting the overall training efficiency.\n  In this paper, we introduce the cached-based distributed full-batch graph\nneural network training framework (CDFGNN). We propose the adaptive cache\nmechanism to reduce the remote vertex access by caching the historical features\nand gradients of neighbor vertices. Besides, we further optimize the\ncommunication overhead by quantifying the messages and designing the graph\npartition algorithm for the hierarchical communication architecture.\nExperiments show that the adaptive cache mechanism reduces remote vertex\naccesses by 63.14% on average. Combined with communication quantization and\nhierarchical GP algorithm, CDFGNN outperforms the state-of-the-art distributed\nfull-batch training frameworks by 30.39% in our experiments. Our results\nindicate that CDFGNN has great potential in accelerating distributed full-batch\nGNN training tasks."
                },
                "authors": [
                    {
                        "name": "Shuai Zhang"
                    },
                    {
                        "name": "Zite Jiang"
                    },
                    {
                        "name": "Haihang You"
                    }
                ],
                "author_detail": {
                    "name": "Haihang You"
                },
                "author": "Haihang You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21324v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21324v2",
                "updated": "2024-08-01T00:41:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    0,
                    41,
                    52,
                    3,
                    214,
                    0
                ],
                "published": "2024-07-31T04:16:20Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    4,
                    16,
                    20,
                    2,
                    213,
                    0
                ],
                "title": "Towards Variable-Length In-Network Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Variable-Length In-Network Caching"
                },
                "summary": "We present StarCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, StarCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement a StarCache prototype on an Intel Tofino\nswitch. Our experimental results show that StarCache can balance highly skewed\nworkloads with various key and value sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present StarCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, StarCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement a StarCache prototype on an Intel Tofino\nswitch. Our experimental results show that StarCache can balance highly skewed\nworkloads with various key and value sizes."
                },
                "authors": [
                    {
                        "name": "Gyuyeong Kim"
                    }
                ],
                "author_detail": {
                    "name": "Gyuyeong Kim"
                },
                "author": "Gyuyeong Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21324v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21324v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20485v2",
                "updated": "2024-07-31T02:02:40Z",
                "updated_parsed": [
                    2024,
                    7,
                    31,
                    2,
                    2,
                    40,
                    2,
                    213,
                    0
                ],
                "published": "2024-07-30T01:13:42Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    1,
                    13,
                    42,
                    1,
                    212,
                    0
                ],
                "title": "A2SF: Accumulative Attention Scoring with Forgetting Factor for Token\n  Pruning in Transformer Decoder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A2SF: Accumulative Attention Scoring with Forgetting Factor for Token\n  Pruning in Transformer Decoder"
                },
                "summary": "Recently, large language models (LLM) based on transformers are facing memory\nbottleneck issues due to KV cache, especially in long sequence handling.\nPrevious researches proposed KV cache compression techniques that identify\ninsignificant tokens based on Accumulative Attention Scores and removes their\nitems from KV cache, noting that only few tokens play an important role in\nattention operations. However, we have observed that the existing Accumulative\nAttention Score is not suitable for the transformer decoder structure. In the\ndecoder model, the number of times the Attention Score accumulates varies\ndepending on the order of token appearance due to the effect of masking,\ncausing an uneven comparison between tokens. To solve this, we propose\nAccumulative Attention Score with Forgetting Factor (A2SF) technique, which\nintroduces a Forgetting Factor in the Attention Score accumulation process.\nA2SF applies a penalty to the past Attention Score generated from old tokens by\nrepeatedly multiplying the Forgetting Factor to the Attention Score over time.\nTherefore, older tokens receive a larger penalty, providing fairness among\ndifferent ages of tokens. Through the fair comparison among tokens, we can more\neffectively select important tokens. We have verified the accuracy improvement\nthrough A2SF in the OPT and LLaMA models and A2SF improves the accuracy of\nLLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLM) based on transformers are facing memory\nbottleneck issues due to KV cache, especially in long sequence handling.\nPrevious researches proposed KV cache compression techniques that identify\ninsignificant tokens based on Accumulative Attention Scores and removes their\nitems from KV cache, noting that only few tokens play an important role in\nattention operations. However, we have observed that the existing Accumulative\nAttention Score is not suitable for the transformer decoder structure. In the\ndecoder model, the number of times the Attention Score accumulates varies\ndepending on the order of token appearance due to the effect of masking,\ncausing an uneven comparison between tokens. To solve this, we propose\nAccumulative Attention Score with Forgetting Factor (A2SF) technique, which\nintroduces a Forgetting Factor in the Attention Score accumulation process.\nA2SF applies a penalty to the past Attention Score generated from old tokens by\nrepeatedly multiplying the Forgetting Factor to the Attention Score over time.\nTherefore, older tokens receive a larger penalty, providing fairness among\ndifferent ages of tokens. Through the fair comparison among tokens, we can more\neffectively select important tokens. We have verified the accuracy improvement\nthrough A2SF in the OPT and LLaMA models and A2SF improves the accuracy of\nLLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot."
                },
                "authors": [
                    {
                        "name": "Hyun-rae Jo"
                    },
                    {
                        "name": "Dongkun Shin"
                    }
                ],
                "author_detail": {
                    "name": "Dongkun Shin"
                },
                "author": "Dongkun Shin",
                "arxiv_comment": "11 pages(9 pages + reference 2 pages), 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21201v1",
                "updated": "2024-07-30T21:27:00Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    21,
                    27,
                    0,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T21:27:00Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    21,
                    27,
                    0,
                    1,
                    212,
                    0
                ],
                "title": "Electric field control of magnetocaloric effect in cylindrical MnAs/PZT\n  magnetoelectric composite",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electric field control of magnetocaloric effect in cylindrical MnAs/PZT\n  magnetoelectric composite"
                },
                "summary": "The possibility of electric field control of magnetocaloric effect through\nquasi-isostatic compression as a result of the converse piezoelectric effect\nwas demonstrated on cylindrical type magnetoelectric composite MnAs/PZT. It was\nshown that an electric voltage of 100 V corresponding to an electric field of E\n~0.3 kV/mm applied to the walls of the piezoelectric component PZT of the\nMnAs/PZT composite contributes to an increase in the maximum adiabatic\ntemperature change by 0.2 K in the temperature range of the magnetostructural\nphase transition of MnAs ~317 K at magnetic field change of 1.8 T. Calculations\nusing the finite element method have shown that an electric field voltage of\n100 V is capable of creating a quasi-isostatic mechanical stress in the region\ninside a cylindrical PZT tube of ~3 MPa. Moreover, in the region of weak\npressures up to 10 MPa, the contribution to the MCE from piezo compression\nlinearly depends on the electrical voltage that can be used for control the MCE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The possibility of electric field control of magnetocaloric effect through\nquasi-isostatic compression as a result of the converse piezoelectric effect\nwas demonstrated on cylindrical type magnetoelectric composite MnAs/PZT. It was\nshown that an electric voltage of 100 V corresponding to an electric field of E\n~0.3 kV/mm applied to the walls of the piezoelectric component PZT of the\nMnAs/PZT composite contributes to an increase in the maximum adiabatic\ntemperature change by 0.2 K in the temperature range of the magnetostructural\nphase transition of MnAs ~317 K at magnetic field change of 1.8 T. Calculations\nusing the finite element method have shown that an electric field voltage of\n100 V is capable of creating a quasi-isostatic mechanical stress in the region\ninside a cylindrical PZT tube of ~3 MPa. Moreover, in the region of weak\npressures up to 10 MPa, the contribution to the MCE from piezo compression\nlinearly depends on the electrical voltage that can be used for control the MCE"
                },
                "authors": [
                    {
                        "name": "Abdulkarim A. Amirov"
                    },
                    {
                        "name": "Maksim A. Koliushenkov"
                    },
                    {
                        "name": "Abdula A. Mukhuchev"
                    },
                    {
                        "name": "Dibir M. Yusupov"
                    },
                    {
                        "name": "Valeriya V. Govorina"
                    },
                    {
                        "name": "Dmitriy S. Neznakhin"
                    },
                    {
                        "name": "Gennady A. Govor"
                    },
                    {
                        "name": "Akhmed M. Aliev"
                    }
                ],
                "author_detail": {
                    "name": "Akhmed M. Aliev"
                },
                "author": "Akhmed M. Aliev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21118v1",
                "updated": "2024-07-30T18:19:38Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T18:19:38Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "title": "Palu: Compressing KV-Cache with Low-Rank Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Palu: Compressing KV-Cache with Low-Rank Projection"
                },
                "summary": "KV-Cache compression methods generally sample a KV-Cache of effectual tokens\nor quantize it into lower bits. However, these methods cannot exploit the\nredundancy of the hidden dimension of KV tensors. This paper investigates a\nunique hidden dimension approach called Palu, a novel KV-Cache compression\nframework that utilizes low-rank projection. Palu decomposes the linear layers\ninto low-rank matrices, caches the smaller intermediate states, and\nreconstructs the full keys and values on the fly. To improve accuracy,\ncompression rate, and efficiency, Palu further encompasses (1) a medium-grained\nlow-rank decomposition scheme, (2) an efficient rank search algorithm, (3) a\nlow-rank-aware quantization algorithm, and (4) matrix fusion with optimized GPU\nkernels. Our extensive experiments with popular LLMs show that Palu can\ncompress KV-Cache by more than 91.25% while maintaining a significantly better\naccuracy (up to 1.19 lower perplexity) than state-of-the-art KV-Cache\nquantization methods at a similar or even higher memory usage. When compressing\nKV-Cache for 50%, Palu delivers up to 1.61x end-to-end speedup for the\nattention module. Our code is publicly available at\nhttps://github.com/shadowpa0327/Palu.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Cache compression methods generally sample a KV-Cache of effectual tokens\nor quantize it into lower bits. However, these methods cannot exploit the\nredundancy of the hidden dimension of KV tensors. This paper investigates a\nunique hidden dimension approach called Palu, a novel KV-Cache compression\nframework that utilizes low-rank projection. Palu decomposes the linear layers\ninto low-rank matrices, caches the smaller intermediate states, and\nreconstructs the full keys and values on the fly. To improve accuracy,\ncompression rate, and efficiency, Palu further encompasses (1) a medium-grained\nlow-rank decomposition scheme, (2) an efficient rank search algorithm, (3) a\nlow-rank-aware quantization algorithm, and (4) matrix fusion with optimized GPU\nkernels. Our extensive experiments with popular LLMs show that Palu can\ncompress KV-Cache by more than 91.25% while maintaining a significantly better\naccuracy (up to 1.19 lower perplexity) than state-of-the-art KV-Cache\nquantization methods at a similar or even higher memory usage. When compressing\nKV-Cache for 50%, Palu delivers up to 1.61x end-to-end speedup for the\nattention module. Our code is publicly available at\nhttps://github.com/shadowpa0327/Palu."
                },
                "authors": [
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Wei-Cheng Lin"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Chong-Yan Chen"
                    },
                    {
                        "name": "Yu-Fang Hu"
                    },
                    {
                        "name": "Pei-Shuo Wang"
                    },
                    {
                        "name": "Ning-Chi Huang"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Chiang Wu"
                },
                "author": "Kai-Chiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21018v1",
                "updated": "2024-07-30T17:59:08Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T17:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "title": "ThinK: Thinner Key Cache by Query-Driven Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinK: Thinner Key Cache by Query-Driven Pruning"
                },
                "summary": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications by leveraging increased model sizes and sequence lengths. However,\nthe associated rise in computational and memory costs poses significant\nchallenges, particularly in managing long sequences due to the quadratic\ncomplexity of the transformer attention mechanism. This paper focuses on the\nlong-context scenario, addressing the inefficiencies in KV cache memory\nconsumption during inference. Unlike existing approaches that optimize the\nmemory based on the sequence lengths, we uncover that the channel dimension of\nthe KV cache exhibits significant redundancy, characterized by unbalanced\nmagnitude distribution and low-rank structure in attention weights. Based on\nthese observations, we propose ThinK, a novel query-dependent KV cache pruning\nmethod designed to minimize attention weight loss while selectively pruning the\nleast significant channels. Our approach not only maintains or enhances model\naccuracy but also achieves a reduction in memory costs by over 20% compared\nwith vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and\nMistral models across various long-sequence datasets confirm the efficacy of\nThinK, setting a new precedent for efficient LLM deployment without\ncompromising performance. We also outline the potential of extending our method\nto value cache pruning, demonstrating ThinK's versatility and broad\napplicability in reducing both memory and computational overheads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications by leveraging increased model sizes and sequence lengths. However,\nthe associated rise in computational and memory costs poses significant\nchallenges, particularly in managing long sequences due to the quadratic\ncomplexity of the transformer attention mechanism. This paper focuses on the\nlong-context scenario, addressing the inefficiencies in KV cache memory\nconsumption during inference. Unlike existing approaches that optimize the\nmemory based on the sequence lengths, we uncover that the channel dimension of\nthe KV cache exhibits significant redundancy, characterized by unbalanced\nmagnitude distribution and low-rank structure in attention weights. Based on\nthese observations, we propose ThinK, a novel query-dependent KV cache pruning\nmethod designed to minimize attention weight loss while selectively pruning the\nleast significant channels. Our approach not only maintains or enhances model\naccuracy but also achieves a reduction in memory costs by over 20% compared\nwith vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and\nMistral models across various long-sequence datasets confirm the efficacy of\nThinK, setting a new precedent for efficient LLM deployment without\ncompromising performance. We also outline the potential of extending our method\nto value cache pruning, demonstrating ThinK's versatility and broad\napplicability in reducing both memory and computational overheads."
                },
                "authors": [
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Zhanming Jie"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Amrita Saha"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Doyen Sahoo"
                    }
                ],
                "author_detail": {
                    "name": "Doyen Sahoo"
                },
                "author": "Doyen Sahoo",
                "arxiv_comment": "20 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.06944v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.06944v2",
                "updated": "2024-07-30T13:06:36Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    13,
                    6,
                    36,
                    1,
                    212,
                    0
                ],
                "published": "2023-04-14T06:21:57Z",
                "published_parsed": [
                    2023,
                    4,
                    14,
                    6,
                    21,
                    57,
                    4,
                    104,
                    0
                ],
                "title": "SpChar: Characterizing the Sparse Puzzle via Decision Trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpChar: Characterizing the Sparse Puzzle via Decision Trees"
                },
                "summary": "Sparse matrix computation is crucial in various modern applications,\nincluding large-scale graph analytics, deep learning, and recommender systems.\nThe performance of sparse kernels varies greatly depending on the structure of\nthe input matrix, making it difficult to gain a comprehensive understanding of\nsparse computation and its relationship to inputs, algorithms, and target\nmachine architecture. Despite extensive research on certain sparse kernels,\nsuch as Sparse Matrix-Vector Multiplication (SpMV), the overall family of\nsparse algorithms has yet to be investigated as a whole. This paper introduces\nSpChar, a workload characterization methodology for general sparse computation.\nSpChar employs tree-based models to identify the most relevant hardware and\ninput characteristics, starting from hardware and input-related metrics\ngathered from Performance Monitoring Counters (PMCs) and matrices. Our analysis\nenables the creation of a characterization loop that facilitates the\noptimization of sparse computation by mapping the impact of architectural\nfeatures to inputs and algorithmic choices. We apply SpChar to more than 600\nmatrices from the SuiteSparse Matrix collection and three state-of-the-art Arm\nCPUs to determine the critical hardware and software characteristics that\naffect sparse computation. In our analysis, we determine that the biggest\nlimiting factors for high-performance sparse computation are (1) the latency of\nthe memory system, (2) the pipeline flush overhead resulting from branch\nmisprediction, and (3) the poor reuse of cached elements. Additionally, we\npropose software and hardware optimizations that designers can implement to\ncreate a platform suitable for sparse computation. We then investigate these\noptimizations using the gem5 simulator to achieve a significant speedup of up\nto 2.63x compared to a CPU where the optimizations are not applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse matrix computation is crucial in various modern applications,\nincluding large-scale graph analytics, deep learning, and recommender systems.\nThe performance of sparse kernels varies greatly depending on the structure of\nthe input matrix, making it difficult to gain a comprehensive understanding of\nsparse computation and its relationship to inputs, algorithms, and target\nmachine architecture. Despite extensive research on certain sparse kernels,\nsuch as Sparse Matrix-Vector Multiplication (SpMV), the overall family of\nsparse algorithms has yet to be investigated as a whole. This paper introduces\nSpChar, a workload characterization methodology for general sparse computation.\nSpChar employs tree-based models to identify the most relevant hardware and\ninput characteristics, starting from hardware and input-related metrics\ngathered from Performance Monitoring Counters (PMCs) and matrices. Our analysis\nenables the creation of a characterization loop that facilitates the\noptimization of sparse computation by mapping the impact of architectural\nfeatures to inputs and algorithmic choices. We apply SpChar to more than 600\nmatrices from the SuiteSparse Matrix collection and three state-of-the-art Arm\nCPUs to determine the critical hardware and software characteristics that\naffect sparse computation. In our analysis, we determine that the biggest\nlimiting factors for high-performance sparse computation are (1) the latency of\nthe memory system, (2) the pipeline flush overhead resulting from branch\nmisprediction, and (3) the poor reuse of cached elements. Additionally, we\npropose software and hardware optimizations that designers can implement to\ncreate a platform suitable for sparse computation. We then investigate these\noptimizations using the gem5 simulator to achieve a significant speedup of up\nto 2.63x compared to a CPU where the optimizations are not applied."
                },
                "authors": [
                    {
                        "name": "Francesco Sgherzi"
                    },
                    {
                        "name": "Marco Siracusa"
                    },
                    {
                        "name": "Ivan Fernandez"
                    },
                    {
                        "name": "Adri√† Armejach"
                    },
                    {
                        "name": "Miquel Moret√≥"
                    }
                ],
                "author_detail": {
                    "name": "Miquel Moret√≥"
                },
                "author": "Miquel Moret√≥",
                "arxiv_comment": "27 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.06944v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.06944v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.8.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20773v1",
                "updated": "2024-07-30T12:16:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    12,
                    16,
                    39,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T12:16:39Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    12,
                    16,
                    39,
                    1,
                    212,
                    0
                ],
                "title": "UpDown: Programmable fine-grained Events for Scalable Performance on\n  Irregular Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UpDown: Programmable fine-grained Events for Scalable Performance on\n  Irregular Applications"
                },
                "summary": "Applications with irregular data structures, data-dependent control flows and\nfine-grained data transfers (e.g., real-world graph computations) perform\npoorly on cache-based systems. We propose the UpDown accelerator that supports\nfine-grained execution with novel architecture mechanisms - lightweight\nthreading, event-driven scheduling, efficient ultra-short threads, and\nsplit-transaction DRAM access with software-controlled synchronization. These\nhardware primitives support software programmable events, enabling high\nperformance on diverse data structures and algorithms. UpDown also supports\nscalable performance; hardware replication enables programs to scale up\nperformance. Evaluation results show UpDown's flexibility and scalability\nenable it to outperform CPUs on graph mining and analytics computations by up\nto 116-195x geomean speedup and more than 4x speedup over prior accelerators.\nWe show that UpDown generates high memory parallelism (~4.6x over CPU) required\nfor memory intensive graph computations. We present measurements that attribute\nthe performance of UpDown (23x architectural advantage) to its individual\narchitectural mechanisms. Finally, we also analyze the area and power cost of\nUpDown's mechanisms for software programmability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applications with irregular data structures, data-dependent control flows and\nfine-grained data transfers (e.g., real-world graph computations) perform\npoorly on cache-based systems. We propose the UpDown accelerator that supports\nfine-grained execution with novel architecture mechanisms - lightweight\nthreading, event-driven scheduling, efficient ultra-short threads, and\nsplit-transaction DRAM access with software-controlled synchronization. These\nhardware primitives support software programmable events, enabling high\nperformance on diverse data structures and algorithms. UpDown also supports\nscalable performance; hardware replication enables programs to scale up\nperformance. Evaluation results show UpDown's flexibility and scalability\nenable it to outperform CPUs on graph mining and analytics computations by up\nto 116-195x geomean speedup and more than 4x speedup over prior accelerators.\nWe show that UpDown generates high memory parallelism (~4.6x over CPU) required\nfor memory intensive graph computations. We present measurements that attribute\nthe performance of UpDown (23x architectural advantage) to its individual\narchitectural mechanisms. Finally, we also analyze the area and power cost of\nUpDown's mechanisms for software programmability."
                },
                "authors": [
                    {
                        "name": "Andronicus Rajasukumar"
                    },
                    {
                        "name": "Jiya Su"
                    },
                    {
                        "name": "Yuqing"
                    },
                    {
                        "name": "Wang"
                    },
                    {
                        "name": "Tianshuo Su"
                    },
                    {
                        "name": "Marziyeh Nourian"
                    },
                    {
                        "name": "Jose M Monsalve Diaz"
                    },
                    {
                        "name": "Tianchi Zhang"
                    },
                    {
                        "name": "Jianru Ding"
                    },
                    {
                        "name": "Wenyi Wang"
                    },
                    {
                        "name": "Ziyi Zhang"
                    },
                    {
                        "name": "Moubarak Jeje"
                    },
                    {
                        "name": "Henry Hoffmann"
                    },
                    {
                        "name": "Yanjing Li"
                    },
                    {
                        "name": "Andrew A. Chien"
                    }
                ],
                "author_detail": {
                    "name": "Andrew A. Chien"
                },
                "arxiv_affiliation": "Ivy",
                "author": "Andrew A. Chien",
                "arxiv_comment": "14 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.14928v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.14928v3",
                "updated": "2024-07-30T08:39:52Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    8,
                    39,
                    52,
                    1,
                    212,
                    0
                ],
                "published": "2023-09-26T13:35:31Z",
                "published_parsed": [
                    2023,
                    9,
                    26,
                    13,
                    35,
                    31,
                    1,
                    269,
                    0
                ],
                "title": "Noise-Tolerant Few-Shot Unsupervised Adapter for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Noise-Tolerant Few-Shot Unsupervised Adapter for Vision-Language Models"
                },
                "summary": "Recent advances in large-scale vision-language models have achieved\nimpressive performance in various zero-shot image classification tasks. While\nprior studies have demonstrated significant improvements by introducing\nfew-shot labelled target samples, they still require labelling of target\nsamples, which greatly degrades their scalability and generalizability while\nhandling various visual recognition tasks. We design NtUA, a Noise-tolerant\nUnsupervised Adapter that allows the learning of effective target models with\nfew unlabelled target samples. NtUA works as a key-value cache that formulates\nvisual features and predicted pseudo-labels of the few unlabelled target\nsamples as key-value pairs. It consists of two complementary designs. The first\nis adaptive cache formation that combats pseudo-label noises by weighting the\nkey-value pairs according to their prediction confidence. The second is\nknowledge-guided cache refinement, which refines pair values (i.e.,\npseudo-labels) and cache weights by leveraging knowledge distillation from\nlarge-scale vision language models. Extensive experiments show that NtUA\nachieves superior performance consistently across multiple widely adopted\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large-scale vision-language models have achieved\nimpressive performance in various zero-shot image classification tasks. While\nprior studies have demonstrated significant improvements by introducing\nfew-shot labelled target samples, they still require labelling of target\nsamples, which greatly degrades their scalability and generalizability while\nhandling various visual recognition tasks. We design NtUA, a Noise-tolerant\nUnsupervised Adapter that allows the learning of effective target models with\nfew unlabelled target samples. NtUA works as a key-value cache that formulates\nvisual features and predicted pseudo-labels of the few unlabelled target\nsamples as key-value pairs. It consists of two complementary designs. The first\nis adaptive cache formation that combats pseudo-label noises by weighting the\nkey-value pairs according to their prediction confidence. The second is\nknowledge-guided cache refinement, which refines pair values (i.e.,\npseudo-labels) and cache weights by leveraging knowledge distillation from\nlarge-scale vision language models. Extensive experiments show that NtUA\nachieves superior performance consistently across multiple widely adopted\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Eman Ali"
                    },
                    {
                        "name": "Muhammad Haris Khan"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Haris Khan"
                },
                "author": "Muhammad Haris Khan",
                "arxiv_comment": "Accepted at BMVC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.14928v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.14928v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03088v2",
                "updated": "2024-07-30T08:19:53Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    8,
                    19,
                    53,
                    1,
                    212,
                    0
                ],
                "published": "2024-04-03T22:03:28Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    22,
                    3,
                    28,
                    2,
                    94,
                    0
                ],
                "title": "Robust Federated Learning for Wireless Networks: A Demonstration with\n  Channel Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Federated Learning for Wireless Networks: A Demonstration with\n  Channel Estimation"
                },
                "summary": "Federated learning (FL) offers a privacy-preserving collaborative approach\nfor training models in wireless networks, with channel estimation emerging as a\npromising application. Despite extensive studies on FL-empowered channel\nestimation, the security concerns associated with FL require meticulous\nattention. In a scenario where small base stations (SBSs) serve as local models\ntrained on cached data, and a macro base station (MBS) functions as the global\nmodel setting, an attacker can exploit the vulnerability of FL, launching\nattacks with various adversarial attacks or deployment tactics. In this paper,\nwe analyze such vulnerabilities, corresponding solutions were brought forth,\nand validated through simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) offers a privacy-preserving collaborative approach\nfor training models in wireless networks, with channel estimation emerging as a\npromising application. Despite extensive studies on FL-empowered channel\nestimation, the security concerns associated with FL require meticulous\nattention. In a scenario where small base stations (SBSs) serve as local models\ntrained on cached data, and a macro base station (MBS) functions as the global\nmodel setting, an attacker can exploit the vulnerability of FL, launching\nattacks with various adversarial attacks or deployment tactics. In this paper,\nwe analyze such vulnerabilities, corresponding solutions were brought forth,\nand validated through simulation."
                },
                "authors": [
                    {
                        "name": "Zexin Fang"
                    },
                    {
                        "name": "Bin Han"
                    },
                    {
                        "name": "Hans D. Schotten"
                    }
                ],
                "author_detail": {
                    "name": "Hans D. Schotten"
                },
                "author": "Hans D. Schotten",
                "arxiv_comment": "Submitted to IEEE GLOBECOM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16219v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16219v3",
                "updated": "2024-07-30T04:01:25Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    4,
                    1,
                    25,
                    1,
                    212,
                    0
                ],
                "published": "2024-04-24T21:35:12Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    21,
                    35,
                    12,
                    2,
                    115,
                    0
                ],
                "title": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)"
                },
                "summary": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU."
                },
                "authors": [
                    {
                        "name": "Ziyue Qiu"
                    },
                    {
                        "name": "Juncheng Yang"
                    },
                    {
                        "name": "Mor Harchol-Balter"
                    }
                ],
                "author_detail": {
                    "name": "Mor Harchol-Balter"
                },
                "author": "Mor Harchol-Balter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16219v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16219v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19637v1",
                "updated": "2024-07-29T01:43:26Z",
                "updated_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    43,
                    26,
                    0,
                    211,
                    0
                ],
                "published": "2024-07-29T01:43:26Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    43,
                    26,
                    0,
                    211,
                    0
                ],
                "title": "STT-RAM-based Hierarchical In-Memory Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STT-RAM-based Hierarchical In-Memory Computing"
                },
                "summary": "In-memory computing promises to overcome the von Neumann bottleneck in\ncomputer systems by performing computations directly within the memory.\nPrevious research has suggested using Spin-Transfer Torque RAM (STT-RAM) for\nin-memory computing due to its non-volatility, low leakage power, high density,\nendurance, and commercial viability. This paper explores hierarchical in-memory\ncomputing, where different levels of the memory hierarchy are augmented with\nprocessing elements to optimize workload execution. The paper investigates\nprocessing in memory (PiM) using non-volatile STT-RAM and processing in cache\n(PiC) using volatile STT-RAM with relaxed retention, which helps mitigate\nSTT-RAM's write latency and energy overheads. We analyze tradeoffs and\noverheads associated with data movement for PiC versus write overheads for PiM\nusing STT-RAMs for various workloads. We examine workload characteristics, such\nas computational intensity and CPU-dependent workloads with limited\ninstruction-level parallelism, and their impact on PiC/PiM tradeoffs. Using\nthese workloads, we evaluate computing in STT-RAM versus SRAM at different\ncache hierarchy levels and explore the potential of heterogeneous STT-RAM cache\narchitectures with various retention times for PiC and CPU-based computing. Our\nexperiments reveal significant advantages of STT-RAM-based PiC over PiM for\nspecific workloads. Finally, we describe open research problems in hierarchical\nin-memory computing architectures to further enhance this paradigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-memory computing promises to overcome the von Neumann bottleneck in\ncomputer systems by performing computations directly within the memory.\nPrevious research has suggested using Spin-Transfer Torque RAM (STT-RAM) for\nin-memory computing due to its non-volatility, low leakage power, high density,\nendurance, and commercial viability. This paper explores hierarchical in-memory\ncomputing, where different levels of the memory hierarchy are augmented with\nprocessing elements to optimize workload execution. The paper investigates\nprocessing in memory (PiM) using non-volatile STT-RAM and processing in cache\n(PiC) using volatile STT-RAM with relaxed retention, which helps mitigate\nSTT-RAM's write latency and energy overheads. We analyze tradeoffs and\noverheads associated with data movement for PiC versus write overheads for PiM\nusing STT-RAMs for various workloads. We examine workload characteristics, such\nas computational intensity and CPU-dependent workloads with limited\ninstruction-level parallelism, and their impact on PiC/PiM tradeoffs. Using\nthese workloads, we evaluate computing in STT-RAM versus SRAM at different\ncache hierarchy levels and explore the potential of heterogeneous STT-RAM cache\narchitectures with various retention times for PiC and CPU-based computing. Our\nexperiments reveal significant advantages of STT-RAM-based PiC over PiM for\nspecific workloads. Finally, we describe open research problems in hierarchical\nin-memory computing architectures to further enhance this paradigm."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Kevin Antony Gomez"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1109/TPDS.2024.3430853",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TPDS.2024.3430853",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in: IEEE Transactions on Parallel and Distributed Systems (\n  Volume: 35, Issue: 9, September 2024)",
                "arxiv_journal_ref": "IEEE Transactions on Parallel and Distributed Systems, vol. 35,\n  no. 9, pp. 1615-1629, Sept. 2024",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19627v1",
                "updated": "2024-07-29T01:17:54Z",
                "updated_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    17,
                    54,
                    0,
                    211,
                    0
                ],
                "published": "2024-07-29T01:17:54Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    17,
                    54,
                    0,
                    211,
                    0
                ],
                "title": "CHIME: Energy-Efficient STT-RAM-based Concurrent Hierarchical In-Memory\n  Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHIME: Energy-Efficient STT-RAM-based Concurrent Hierarchical In-Memory\n  Processing"
                },
                "summary": "Processing-in-cache (PiC) and Processing-in-memory (PiM) architectures,\nespecially those utilizing bit-line computing, offer promising solutions to\nmitigate data movement bottlenecks within the memory hierarchy. While previous\nstudies have explored the integration of compute units within individual memory\nlevels, the complexity and potential overheads associated with these designs\nhave often limited their capabilities. This paper introduces a novel PiC/PiM\narchitecture, Concurrent Hierarchical In-Memory Processing (CHIME), which\nstrategically incorporates heterogeneous compute units across multiple levels\nof the memory hierarchy. This design targets the efficient execution of\ndiverse, domain-specific workloads by placing computations closest to the data\nwhere it optimizes performance, energy consumption, data movement costs, and\narea. CHIME employs STT-RAM due to its various advantages in PiC/PiM computing,\nsuch as high density, low leakage, and better resiliency to data corruption\nfrom activating multiple word lines. We demonstrate that CHIME enhances\nconcurrency and improves compute unit utilization at each level of the memory\nhierarchy. We present strategies for exploring the design space, grouping, and\nplacing the compute units across the memory hierarchy. Experiments reveal that,\ncompared to the state-of-the-art bit-line computing approaches, CHIME achieves\nsignificant speedup and energy savings of 57.95% and 78.23% for various\ndomain-specific workloads, while reducing the overheads associated with\nsingle-level compute designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing-in-cache (PiC) and Processing-in-memory (PiM) architectures,\nespecially those utilizing bit-line computing, offer promising solutions to\nmitigate data movement bottlenecks within the memory hierarchy. While previous\nstudies have explored the integration of compute units within individual memory\nlevels, the complexity and potential overheads associated with these designs\nhave often limited their capabilities. This paper introduces a novel PiC/PiM\narchitecture, Concurrent Hierarchical In-Memory Processing (CHIME), which\nstrategically incorporates heterogeneous compute units across multiple levels\nof the memory hierarchy. This design targets the efficient execution of\ndiverse, domain-specific workloads by placing computations closest to the data\nwhere it optimizes performance, energy consumption, data movement costs, and\narea. CHIME employs STT-RAM due to its various advantages in PiC/PiM computing,\nsuch as high density, low leakage, and better resiliency to data corruption\nfrom activating multiple word lines. We demonstrate that CHIME enhances\nconcurrency and improves compute unit utilization at each level of the memory\nhierarchy. We present strategies for exploring the design space, grouping, and\nplacing the compute units across the memory hierarchy. Experiments reveal that,\ncompared to the state-of-the-art bit-line computing approaches, CHIME achieves\nsignificant speedup and energy savings of 57.95% and 78.23% for various\ndomain-specific workloads, while reducing the overheads associated with\nsingle-level compute designs."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    },
                    {
                        "name": "Kevin Gomez"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Gomez"
                },
                "author": "Kevin Gomez",
                "arxiv_comment": "Accepted in 35th IEEE International Conference on\n  Application-specific Systems, Architectures and Processors (ASAP 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19612v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19612v1",
                "updated": "2024-07-28T23:43:59Z",
                "updated_parsed": [
                    2024,
                    7,
                    28,
                    23,
                    43,
                    59,
                    6,
                    210,
                    0
                ],
                "published": "2024-07-28T23:43:59Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    23,
                    43,
                    59,
                    6,
                    210,
                    0
                ],
                "title": "ARC: DVFS-Aware Asymmetric-Retention STT-RAM Caches for Energy-Efficient\n  Multicore Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARC: DVFS-Aware Asymmetric-Retention STT-RAM Caches for Energy-Efficient\n  Multicore Processors"
                },
                "summary": "Relaxed retention (or volatile) spin-transfer torque RAM (STT-RAM) has been\nwidely studied as a way to reduce STT-RAM's write energy and latency overheads.\nGiven a relaxed retention time STT-RAM level one (L1) cache, we analyze the\nimpacts of dynamic voltage and frequency scaling (DVFS) -- a common\noptimization in modern processors -- on STT-RAM L1 cache design. Our analysis\nreveals that, apart from the fact that different applications may require\ndifferent retention times, the clock frequency, which is typically ignored in\nmost STT-RAM studies, may also significantly impact applications' retention\ntime needs. Based on our findings, we propose an asymmetric-retention core\n(ARC) design for multicore architectures. ARC features retention time\nheterogeneity to specialize STT-RAM retention times to applications' needs. We\nalso propose a runtime prediction model to determine the best core on which to\nrun an application, based on the applications' characteristics, their retention\ntime requirements, and available DVFS settings. Results reveal that the\nproposed approach can reduce the average cache energy by 20.19% and overall\nprocessor energy by 7.66%, compared to a homogeneous STT-RAM cache design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relaxed retention (or volatile) spin-transfer torque RAM (STT-RAM) has been\nwidely studied as a way to reduce STT-RAM's write energy and latency overheads.\nGiven a relaxed retention time STT-RAM level one (L1) cache, we analyze the\nimpacts of dynamic voltage and frequency scaling (DVFS) -- a common\noptimization in modern processors -- on STT-RAM L1 cache design. Our analysis\nreveals that, apart from the fact that different applications may require\ndifferent retention times, the clock frequency, which is typically ignored in\nmost STT-RAM studies, may also significantly impact applications' retention\ntime needs. Based on our findings, we propose an asymmetric-retention core\n(ARC) design for multicore architectures. ARC features retention time\nheterogeneity to specialize STT-RAM retention times to applications' needs. We\nalso propose a runtime prediction model to determine the best core on which to\nrun an application, based on the applications' characteristics, their retention\ntime requirements, and available DVFS settings. Results reveal that the\nproposed approach can reduce the average cache energy by 20.19% and overall\nprocessor energy by 7.66%, compared to a homogeneous STT-RAM cache design."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1145/3357526.3357553",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3357526.3357553",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19612v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proceedings of the international symposium on memory systems, pp.\n  439-450. 2019",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19604v1",
                "updated": "2024-07-28T22:34:20Z",
                "updated_parsed": [
                    2024,
                    7,
                    28,
                    22,
                    34,
                    20,
                    6,
                    210,
                    0
                ],
                "published": "2024-07-28T22:34:20Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    22,
                    34,
                    20,
                    6,
                    210,
                    0
                ],
                "title": "SCART: Predicting STT-RAM Cache Retention Times Using Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCART: Predicting STT-RAM Cache Retention Times Using Machine Learning"
                },
                "summary": "Prior studies have shown that the retention time of the non-volatile\nspin-transfer torque RAM (STT-RAM) can be relaxed in order to reduce STT-RAM's\nwrite energy and latency. However, since different applications may require\ndifferent retention times, STT-RAM retention times must be critically explored\nto satisfy various applications' needs. This process can be challenging due to\nexploration overhead, and exacerbated by the fact that STT-RAM caches are\nemerging and are not readily available for design time exploration. This paper\nexplores using known and easily obtainable statistics (e.g., SRAM statistics)\nto predict the appropriate STT-RAM retention times, in order to minimize\nexploration overhead. We propose an STT-RAM Cache Retention Time (SCART) model,\nwhich utilizes machine learning to enable design time or runtime prediction of\nright-provisioned STT-RAM retention times for latency or energy optimization.\nExperimental results show that, on average, SCART can reduce the latency and\nenergy by 20.34% and 29.12%, respectively, compared to a homogeneous retention\ntime while reducing the exploration overheads by 52.58% compared to prior work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior studies have shown that the retention time of the non-volatile\nspin-transfer torque RAM (STT-RAM) can be relaxed in order to reduce STT-RAM's\nwrite energy and latency. However, since different applications may require\ndifferent retention times, STT-RAM retention times must be critically explored\nto satisfy various applications' needs. This process can be challenging due to\nexploration overhead, and exacerbated by the fact that STT-RAM caches are\nemerging and are not readily available for design time exploration. This paper\nexplores using known and easily obtainable statistics (e.g., SRAM statistics)\nto predict the appropriate STT-RAM retention times, in order to minimize\nexploration overhead. We propose an STT-RAM Cache Retention Time (SCART) model,\nwhich utilizes machine learning to enable design time or runtime prediction of\nright-provisioned STT-RAM retention times for latency or energy optimization.\nExperimental results show that, on average, SCART can reduce the latency and\nenergy by 20.34% and 29.12%, respectively, compared to a homogeneous retention\ntime while reducing the exploration overheads by 52.58% compared to prior work."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Kyle Kuan"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1109/IGSC48788.2019.8957182",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/IGSC48788.2019.8957182",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in: 2019 Tenth International Green and Sustainable\n  Computing Conference (IGSC)",
                "arxiv_journal_ref": "2019 Tenth International Green and Sustainable Computing\n  Conference (IGSC), Alexandria, VA, USA, 2019, pp. 1-7,",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19318v1",
                "updated": "2024-07-27T18:26:32Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    18,
                    26,
                    32,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-27T18:26:32Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    18,
                    26,
                    32,
                    5,
                    209,
                    0
                ],
                "title": "Application State Management (ASM) in the Modern Web and Mobile\n  Applications: A Comprehensive Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application State Management (ASM) in the Modern Web and Mobile\n  Applications: A Comprehensive Review"
                },
                "summary": "The rapid evolution of web and mobile applications has necessitated robust\nmechanisms for managing application state to ensure consistency, performance,\nand user-friendliness. This comprehensive review examines the most effective\nApplication State Management (ASM) techniques, categorized into Local State\nManagement, State Management Libraries, and Server-Side State Management. By\nanalyzing popular front end frameworks the study delves into local state\nmanagement mechanisms. It also evaluates the state of front end management\nlibraries, highlighting their implementations, benefits, and limitations.\nServer-side state management techniques, particularly caching, are discussed\nfor their roles in enhancing data retrieval efficiency. This paper offers\nactionable insights for developers to build scalable, responsive applications,\naiming to bridge the gap between theoretical knowledge and practical\napplication. This study's critical analysis and recommendations aim to guide\nfuture research and development in ASM, contributing to the advancement of\nmodern application architecture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of web and mobile applications has necessitated robust\nmechanisms for managing application state to ensure consistency, performance,\nand user-friendliness. This comprehensive review examines the most effective\nApplication State Management (ASM) techniques, categorized into Local State\nManagement, State Management Libraries, and Server-Side State Management. By\nanalyzing popular front end frameworks the study delves into local state\nmanagement mechanisms. It also evaluates the state of front end management\nlibraries, highlighting their implementations, benefits, and limitations.\nServer-side state management techniques, particularly caching, are discussed\nfor their roles in enhancing data retrieval efficiency. This paper offers\nactionable insights for developers to build scalable, responsive applications,\naiming to bridge the gap between theoretical knowledge and practical\napplication. This study's critical analysis and recommendations aim to guide\nfuture research and development in ASM, contributing to the advancement of\nmodern application architecture."
                },
                "authors": [
                    {
                        "name": "Anujkumarsinh Donvir"
                    },
                    {
                        "name": "Apeksha Jain"
                    },
                    {
                        "name": "Pradeep Kumar Saraswathi"
                    }
                ],
                "author_detail": {
                    "name": "Pradeep Kumar Saraswathi"
                },
                "author": "Pradeep Kumar Saraswathi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13996v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13996v2",
                "updated": "2024-07-27T08:52:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    52,
                    39,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-19T03:01:32Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    3,
                    1,
                    32,
                    4,
                    201,
                    0
                ],
                "title": "Missile: Fine-Grained, Hardware-Level GPU Resource Isolation for\n  Multi-Tenant DNN Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Missile: Fine-Grained, Hardware-Level GPU Resource Isolation for\n  Multi-Tenant DNN Inference"
                },
                "summary": "Colocating high-priority, latency-sensitive (LS) and low-priority,\nbest-effort (BE) DNN inference services reduces the total cost of ownership\n(TCO) of GPU clusters. Limited by bottlenecks such as VRAM channel conflicts\nand PCIe bus contentions, existing GPU sharing solutions are unable to avoid\nresource conflicts among concurrently executing tasks, failing to achieve both\nlow latency for LS tasks and high throughput for BE tasks. To bridge this gap,\nthis paper presents Missile, a general GPU sharing solution for multi-tenant\nDNN inference on NVIDIA GPUs. Missile approximates fine-grained GPU hardware\nresource isolation between multiple LS and BE DNN tasks at software level.\nThrough comprehensive reverse engineering, Missile first reveals a general VRAM\nchannel hash mapping architecture of NVIDIA GPUs and eliminates VRAM channel\nconflicts using software-level cache coloring. It also isolates the PCIe bus\nand fairly allocates PCIe bandwidth using completely fair scheduler. We\nevaluate 12 mainstream DNNs with synthetic and real-world workloads on four\nGPUs. The results show that compared to the state-of-the-art GPU sharing\nsolutions, Missile reduces tail latency for LS services by up to ~50%, achieves\nup to 6.1x BE job throughput, and allocates PCIe bus bandwidth to tenants\non-demand for optimal performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Colocating high-priority, latency-sensitive (LS) and low-priority,\nbest-effort (BE) DNN inference services reduces the total cost of ownership\n(TCO) of GPU clusters. Limited by bottlenecks such as VRAM channel conflicts\nand PCIe bus contentions, existing GPU sharing solutions are unable to avoid\nresource conflicts among concurrently executing tasks, failing to achieve both\nlow latency for LS tasks and high throughput for BE tasks. To bridge this gap,\nthis paper presents Missile, a general GPU sharing solution for multi-tenant\nDNN inference on NVIDIA GPUs. Missile approximates fine-grained GPU hardware\nresource isolation between multiple LS and BE DNN tasks at software level.\nThrough comprehensive reverse engineering, Missile first reveals a general VRAM\nchannel hash mapping architecture of NVIDIA GPUs and eliminates VRAM channel\nconflicts using software-level cache coloring. It also isolates the PCIe bus\nand fairly allocates PCIe bandwidth using completely fair scheduler. We\nevaluate 12 mainstream DNNs with synthetic and real-world workloads on four\nGPUs. The results show that compared to the state-of-the-art GPU sharing\nsolutions, Missile reduces tail latency for LS services by up to ~50%, achieves\nup to 6.1x BE job throughput, and allocates PCIe bus bandwidth to tenants\non-demand for optimal performance."
                },
                "authors": [
                    {
                        "name": "Yongkang Zhang"
                    },
                    {
                        "name": "Haoxuan Yu"
                    },
                    {
                        "name": "Chenxia Han"
                    },
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Huaicheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Huaicheng Li"
                },
                "author": "Huaicheng Li",
                "arxiv_comment": "18 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13996v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13996v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.9; I.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19205v1",
                "updated": "2024-07-27T08:21:14Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    21,
                    14,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-27T08:21:14Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    21,
                    14,
                    5,
                    209,
                    0
                ],
                "title": "Faster Image2Video Generation: A Closer Look at CLIP Image Embedding's\n  Impact on Spatio-Temporal Cross-Attentions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster Image2Video Generation: A Closer Look at CLIP Image Embedding's\n  Impact on Spatio-Temporal Cross-Attentions"
                },
                "summary": "This paper investigates the role of CLIP image embeddings within the Stable\nVideo Diffusion (SVD) framework, focusing on their impact on video generation\nquality and computational efficiency. Our findings indicate that CLIP\nembeddings, while crucial for aesthetic quality, do not significantly\ncontribute towards the subject and background consistency of video outputs.\nMoreover, the computationally expensive cross-attention mechanism can be\neffectively replaced by a simpler linear layer. This layer is computed only\nonce at the first diffusion inference step, and its output is then cached and\nreused throughout the inference process, thereby enhancing efficiency while\nmaintaining high-quality outputs. Building on these insights, we introduce the\nVCUT, a training-free approach optimized for efficiency within the SVD\narchitecture. VCUT eliminates temporal cross-attention and replaces spatial\ncross-attention with a one-time computed linear layer, significantly reducing\ncomputational load. The implementation of VCUT leads to a reduction of up to\n322T Multiple-Accumulate Operations (MACs) per video and a decrease in model\nparameters by up to 50M, achieving a 20% reduction in latency compared to the\nbaseline. Our approach demonstrates that conditioning during the Semantic\nBinding stage is sufficient, eliminating the need for continuous computation\nacross all inference steps and setting a new standard for efficient video\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the role of CLIP image embeddings within the Stable\nVideo Diffusion (SVD) framework, focusing on their impact on video generation\nquality and computational efficiency. Our findings indicate that CLIP\nembeddings, while crucial for aesthetic quality, do not significantly\ncontribute towards the subject and background consistency of video outputs.\nMoreover, the computationally expensive cross-attention mechanism can be\neffectively replaced by a simpler linear layer. This layer is computed only\nonce at the first diffusion inference step, and its output is then cached and\nreused throughout the inference process, thereby enhancing efficiency while\nmaintaining high-quality outputs. Building on these insights, we introduce the\nVCUT, a training-free approach optimized for efficiency within the SVD\narchitecture. VCUT eliminates temporal cross-attention and replaces spatial\ncross-attention with a one-time computed linear layer, significantly reducing\ncomputational load. The implementation of VCUT leads to a reduction of up to\n322T Multiple-Accumulate Operations (MACs) per video and a decrease in model\nparameters by up to 50M, achieving a 20% reduction in latency compared to the\nbaseline. Our approach demonstrates that conditioning during the Semantic\nBinding stage is sufficient, eliminating the need for continuous computation\nacross all inference steps and setting a new standard for efficient video\ngeneration."
                },
                "authors": [
                    {
                        "name": "Ashkan Taghipour"
                    },
                    {
                        "name": "Morteza Ghahremani"
                    },
                    {
                        "name": "Mohammed Bennamoun"
                    },
                    {
                        "name": "Aref Miri Rekavandi"
                    },
                    {
                        "name": "Zinuo Li"
                    },
                    {
                        "name": "Hamid Laga"
                    },
                    {
                        "name": "Farid Boussaid"
                    }
                ],
                "author_detail": {
                    "name": "Farid Boussaid"
                },
                "author": "Farid Boussaid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19090v1",
                "updated": "2024-07-26T21:11:58Z",
                "updated_parsed": [
                    2024,
                    7,
                    26,
                    21,
                    11,
                    58,
                    4,
                    208,
                    0
                ],
                "published": "2024-07-26T21:11:58Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    21,
                    11,
                    58,
                    4,
                    208,
                    0
                ],
                "title": "MetaHive: A Cache-Optimized Metadata Management for Heterogeneous\n  Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaHive: A Cache-Optimized Metadata Management for Heterogeneous\n  Key-Value Stores"
                },
                "summary": "Cloud key-value (KV) stores provide businesses with a cost-effective and\nadaptive alternative to traditional on-premise data management solutions. KV\nstores frequently consist of heterogeneous clusters, characterized by varying\nhardware specifications of the deployment nodes, with each node potentially\nrunning a distinct version of the KV store software. This heterogeneity is\naccompanied by the diverse metadata that they need to manage. In this study, we\nintroduce MetaHive, a cache-optimized approach to managing metadata in\nheterogeneous KV store clusters. MetaHive disaggregates the original data from\nits associated metadata to promote independence between them, while maintaining\ntheir interconnection during usage. This makes the metadata opaque from the\ndownstream processes and the other KV stores in the cluster. MetaHive also\nensures that the KV and metadata entries are stored in the vicinity of each\nother in memory and storage. This allows MetaHive to optimally utilize the\ncaching mechanism without extra storage read overhead for metadata retrieval.\nWe deploy MetaHive to ensure data integrity in RocksDB and demonstrate its\nrapid data validation with minimal effect on performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud key-value (KV) stores provide businesses with a cost-effective and\nadaptive alternative to traditional on-premise data management solutions. KV\nstores frequently consist of heterogeneous clusters, characterized by varying\nhardware specifications of the deployment nodes, with each node potentially\nrunning a distinct version of the KV store software. This heterogeneity is\naccompanied by the diverse metadata that they need to manage. In this study, we\nintroduce MetaHive, a cache-optimized approach to managing metadata in\nheterogeneous KV store clusters. MetaHive disaggregates the original data from\nits associated metadata to promote independence between them, while maintaining\ntheir interconnection during usage. This makes the metadata opaque from the\ndownstream processes and the other KV stores in the cluster. MetaHive also\nensures that the KV and metadata entries are stored in the vicinity of each\nother in memory and storage. This allows MetaHive to optimally utilize the\ncaching mechanism without extra storage read overhead for metadata retrieval.\nWe deploy MetaHive to ensure data integrity in RocksDB and demonstrate its\nrapid data validation with minimal effect on performance."
                },
                "authors": [
                    {
                        "name": "Alireza Heidari"
                    },
                    {
                        "name": "Amirhossein Ahmadi"
                    },
                    {
                        "name": "Zefeng Zhi"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "arxiv_comment": "Cloud Databases",
                "arxiv_journal_ref": "VLDB 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18121v1",
                "updated": "2024-07-25T15:29:05Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    15,
                    29,
                    5,
                    3,
                    207,
                    0
                ],
                "published": "2024-07-25T15:29:05Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    15,
                    29,
                    5,
                    3,
                    207,
                    0
                ],
                "title": "Efficient Inference of Vision Instruction-Following Models with Elastic\n  Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Inference of Vision Instruction-Following Models with Elastic\n  Cache"
                },
                "summary": "In the field of instruction-following large vision-language models (LVLMs),\nthe efficient deployment of these models faces challenges, notably due to the\nhigh memory demands of their key-value (KV) caches. Conventional cache\nmanagement strategies for LLMs focus on cache eviction, which often fails to\naddress the specific needs of multimodal instruction-following models.\nRecognizing this gap, in this paper, we introduce Elastic Cache, a novel\napproach that benefits from applying distinct acceleration methods for\ninstruction encoding and output generation stages. We investigate the metrics\nof importance in different stages and propose an importance-driven cache\nmerging strategy to prune redundancy caches. Instead of discarding less\nimportant caches, our strategy identifies important key/value vectors as anchor\npoints. Surrounding less important caches are then merged with these anchors,\nenhancing the preservation of contextual information in the KV caches while\nyielding an arbitrary acceleration ratio. For instruction encoding, we utilize\nthe frequency to evaluate the importance of caches. Regarding output\ngeneration, we prioritize tokens based on their distance with an offset, by\nwhich both the initial and most recent tokens are retained. Results on a range\nof LVLMs demonstrate that Elastic Cache not only boosts efficiency but also\nnotably outperforms existing pruning methods in language generation across\nvarious tasks. Code is available at https://github.com/liuzuyan/ElasticCache",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of instruction-following large vision-language models (LVLMs),\nthe efficient deployment of these models faces challenges, notably due to the\nhigh memory demands of their key-value (KV) caches. Conventional cache\nmanagement strategies for LLMs focus on cache eviction, which often fails to\naddress the specific needs of multimodal instruction-following models.\nRecognizing this gap, in this paper, we introduce Elastic Cache, a novel\napproach that benefits from applying distinct acceleration methods for\ninstruction encoding and output generation stages. We investigate the metrics\nof importance in different stages and propose an importance-driven cache\nmerging strategy to prune redundancy caches. Instead of discarding less\nimportant caches, our strategy identifies important key/value vectors as anchor\npoints. Surrounding less important caches are then merged with these anchors,\nenhancing the preservation of contextual information in the KV caches while\nyielding an arbitrary acceleration ratio. For instruction encoding, we utilize\nthe frequency to evaluate the importance of caches. Regarding output\ngeneration, we prioritize tokens based on their distance with an offset, by\nwhich both the initial and most recent tokens are retained. Results on a range\nof LVLMs demonstrate that Elastic Cache not only boosts efficiency but also\nnotably outperforms existing pruning methods in language generation across\nvarious tasks. Code is available at https://github.com/liuzuyan/ElasticCache"
                },
                "authors": [
                    {
                        "name": "Zuyan Liu"
                    },
                    {
                        "name": "Benlin Liu"
                    },
                    {
                        "name": "Jiahui Wang"
                    },
                    {
                        "name": "Yuhao Dong"
                    },
                    {
                        "name": "Guangyi Chen"
                    },
                    {
                        "name": "Yongming Rao"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "Accepted to ECCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.02750v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.02750v2",
                "updated": "2024-07-25T09:16:05Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    9,
                    16,
                    5,
                    3,
                    207,
                    0
                ],
                "published": "2024-02-05T06:06:47Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    6,
                    6,
                    47,
                    0,
                    36,
                    0
                ],
                "title": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache"
                },
                "summary": "Efficiently serving large language models (LLMs) requires batching of many\nrequests to reduce the cost per request. Yet, with larger batch sizes and\nlonger context lengths, the key-value (KV) cache, which stores attention keys\nand values to avoid re-computations, significantly increases memory demands and\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\nof the KV cache causes the computational core to be idle, which limits the\ninference speed. A straightforward and effective solution to reduce KV cache\nsize is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\nalmost the same quality while using $\\mathbf{2.6\\times}$ less peak memory\n(including model weight). This reduction in memory usage enables up to\n$\\mathbf{4\\times}$ larger batch size, bringing $\\mathbf{2.35\\times \\sim\n3.47\\times}$ throughput on real LLM inference workload. The source code is\navailable at https://github.com/jy-yuan/KIVI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently serving large language models (LLMs) requires batching of many\nrequests to reduce the cost per request. Yet, with larger batch sizes and\nlonger context lengths, the key-value (KV) cache, which stores attention keys\nand values to avoid re-computations, significantly increases memory demands and\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\nof the KV cache causes the computational core to be idle, which limits the\ninference speed. A straightforward and effective solution to reduce KV cache\nsize is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\nalmost the same quality while using $\\mathbf{2.6\\times}$ less peak memory\n(including model weight). This reduction in memory usage enables up to\n$\\mathbf{4\\times}$ larger batch size, bringing $\\mathbf{2.35\\times \\sim\n3.47\\times}$ throughput on real LLM inference workload. The source code is\navailable at https://github.com/jy-yuan/KIVI."
                },
                "authors": [
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Jiayi Yuan"
                    },
                    {
                        "name": "Hongye Jin"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Vladimir Braverman"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Xia Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xia Hu"
                },
                "author": "Xia Hu",
                "arxiv_doi": "10.13140/RG.2.2.28167.37282",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.13140/RG.2.2.28167.37282",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.02750v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.02750v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ICML2024",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20272v1",
                "updated": "2024-07-25T07:50:17Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    7,
                    50,
                    17,
                    3,
                    207,
                    0
                ],
                "published": "2024-07-25T07:50:17Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    7,
                    50,
                    17,
                    3,
                    207,
                    0
                ],
                "title": "An Efficient Inference Framework for Early-exit Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Inference Framework for Early-exit Large Language Models"
                },
                "summary": "Building efficient inference framework has gained increasing interests for\nresearch community. Early-exit models, a variant of LLMs, improves the\ninference efficiency of LLMs by skipping rest layers and directly generate\noutput tokens when they are confident enough. However, there is no work of LLM\ninference framework that takes early-exit models into consideration. This is\nnon-trivial as prior art on LLM inference cannot be directly applied to\nearly-exit models. In this work, we solves two key challenges in building\nefficient inference framework for early-exit models: (1) batch inference at\niteration-level granularity; and (2) KV cache management. For the former, we\npropose to process the batch until all sequences surpass the early-exit\nconfidence threshold. For the latter, we propose to fill the KV cache of rest\nlayers before the iteration terminates. Our evaluation shows that, compared\nwith the original vLLM operating at full layers, our solution achieves up to\n1.25x speed up.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building efficient inference framework has gained increasing interests for\nresearch community. Early-exit models, a variant of LLMs, improves the\ninference efficiency of LLMs by skipping rest layers and directly generate\noutput tokens when they are confident enough. However, there is no work of LLM\ninference framework that takes early-exit models into consideration. This is\nnon-trivial as prior art on LLM inference cannot be directly applied to\nearly-exit models. In this work, we solves two key challenges in building\nefficient inference framework for early-exit models: (1) batch inference at\niteration-level granularity; and (2) KV cache management. For the former, we\npropose to process the batch until all sequences surpass the early-exit\nconfidence threshold. For the latter, we propose to fill the KV cache of rest\nlayers before the iteration terminates. Our evaluation shows that, compared\nwith the original vLLM operating at full layers, our solution achieves up to\n1.25x speed up."
                },
                "authors": [
                    {
                        "name": "Ruijie Miao"
                    },
                    {
                        "name": "Yihan Yan"
                    },
                    {
                        "name": "Xinshuo Yao"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17678v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17678v1",
                "updated": "2024-07-25T00:27:07Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    0,
                    27,
                    7,
                    3,
                    207,
                    0
                ],
                "published": "2024-07-25T00:27:07Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    0,
                    27,
                    7,
                    3,
                    207,
                    0
                ],
                "title": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads"
                },
                "summary": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM."
                },
                "authors": [
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Barun Patra"
                    },
                    {
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "name": "Xia Song"
                    }
                ],
                "author_detail": {
                    "name": "Xia Song"
                },
                "author": "Xia Song",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17678v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17678v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2301.08711v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2301.08711v3",
                "updated": "2024-07-24T13:36:03Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    13,
                    36,
                    3,
                    2,
                    206,
                    0
                ],
                "published": "2023-01-20T18:13:38Z",
                "published_parsed": [
                    2023,
                    1,
                    20,
                    18,
                    13,
                    38,
                    4,
                    20,
                    0
                ],
                "title": "Robust, Secure and Private Cache-aided Scalar Linear Function Retrieval\n  from Distributed System with Blind and Adversarial Servers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust, Secure and Private Cache-aided Scalar Linear Function Retrieval\n  from Distributed System with Blind and Adversarial Servers"
                },
                "summary": "In this work, a distributed server system composed of multiple servers that\nholds some coded files and multiple users that are interested in retrieving the\nlinear functions of the files is investigated, where the servers are robust,\nblind and adversarial in the sense that any $J$ servers can together recover\nall files, while any $I$ colluding servers cannot obtain any information about\nthe files, and at most $A$ servers maliciously provides erroneous information.\nIn addition, the file library must be secure from a wiretapper who obtains all\nthe signals, and the demands of any subset of users must kept private from the\nother users and servers, even if they collude. A coding scheme is proposed by\nincorporating the ideas of Shamir's secret sharing and key superposition into\nthe framework of Placement Delivery Array (PDA), originally proposed to\ncharacterize the single-server coded caching system without any security or\nprivacy constraints. It is shown that PDAs associated to Maddah-Ali and\nNiesen's coded caching scheme results in an achievable\nmemory-storage-communication region, such that the storage size and\ncommunication load were optimal to within a multiplicative gap, except for the\nsmall memory regime when the number of files was smaller than the number of\nusers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, a distributed server system composed of multiple servers that\nholds some coded files and multiple users that are interested in retrieving the\nlinear functions of the files is investigated, where the servers are robust,\nblind and adversarial in the sense that any $J$ servers can together recover\nall files, while any $I$ colluding servers cannot obtain any information about\nthe files, and at most $A$ servers maliciously provides erroneous information.\nIn addition, the file library must be secure from a wiretapper who obtains all\nthe signals, and the demands of any subset of users must kept private from the\nother users and servers, even if they collude. A coding scheme is proposed by\nincorporating the ideas of Shamir's secret sharing and key superposition into\nthe framework of Placement Delivery Array (PDA), originally proposed to\ncharacterize the single-server coded caching system without any security or\nprivacy constraints. It is shown that PDAs associated to Maddah-Ali and\nNiesen's coded caching scheme results in an achievable\nmemory-storage-communication region, such that the storage size and\ncommunication load were optimal to within a multiplicative gap, except for the\nsmall memory regime when the number of files was smaller than the number of\nusers."
                },
                "authors": [
                    {
                        "name": "Qifa Yan"
                    },
                    {
                        "name": "Xiaohu Tang"
                    },
                    {
                        "name": "Zhengchun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zhengchun Zhou"
                },
                "author": "Zhengchun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2301.08711v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2301.08711v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15771v2",
                "updated": "2024-07-24T12:56:41Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    12,
                    56,
                    41,
                    2,
                    206,
                    0
                ],
                "published": "2024-03-13T17:47:39Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    17,
                    47,
                    39,
                    2,
                    73,
                    0
                ],
                "title": "Adaptive Splitting of Reusable Temporal Monitors for Rare Traffic\n  Violations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Splitting of Reusable Temporal Monitors for Rare Traffic\n  Violations"
                },
                "summary": "Autonomous Vehicles (AVs) are often tested in simulation to estimate the\nprobability they will violate safety specifications. Two common issues arise\nwhen using existing techniques to produce this estimation: If violations occur\nrarely, simple Monte-Carlo sampling techniques can fail to produce efficient\nestimates; if simulation horizons are too long, importance sampling techniques\n(which learn proposal distributions from past simulations) can fail to\nconverge. This paper addresses both issues by interleaving rare-event sampling\ntechniques with online specification monitoring algorithms. We use adaptive\nmulti-level splitting to decompose simulations into partial trajectories, then\ncalculate the distance of those partial trajectories to failure by leveraging\nrobustness metrics from Signal Temporal Logic (STL). By caching those partial\nrobustness metric values, we can efficiently re-use computations across\nmultiple sampling stages. Our experiments on an interstate lane-change scenario\nshow our method is viable for testing simulated AV-pipelines, efficiently\nestimating failure probabilities for STL specifications based on real traffic\nrules. We produce better estimates than Monte-Carlo and importance sampling in\nfewer simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Vehicles (AVs) are often tested in simulation to estimate the\nprobability they will violate safety specifications. Two common issues arise\nwhen using existing techniques to produce this estimation: If violations occur\nrarely, simple Monte-Carlo sampling techniques can fail to produce efficient\nestimates; if simulation horizons are too long, importance sampling techniques\n(which learn proposal distributions from past simulations) can fail to\nconverge. This paper addresses both issues by interleaving rare-event sampling\ntechniques with online specification monitoring algorithms. We use adaptive\nmulti-level splitting to decompose simulations into partial trajectories, then\ncalculate the distance of those partial trajectories to failure by leveraging\nrobustness metrics from Signal Temporal Logic (STL). By caching those partial\nrobustness metric values, we can efficiently re-use computations across\nmultiple sampling stages. Our experiments on an interstate lane-change scenario\nshow our method is viable for testing simulated AV-pipelines, efficiently\nestimating failure probabilities for STL specifications based on real traffic\nrules. We produce better estimates than Monte-Carlo and importance sampling in\nfewer simulations."
                },
                "authors": [
                    {
                        "name": "Craig Innes"
                    },
                    {
                        "name": "Subramanian Ramamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Subramanian Ramamoorthy"
                },
                "author": "Subramanian Ramamoorthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.15569v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.15569v2",
                "updated": "2024-07-24T08:56:11Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    8,
                    56,
                    11,
                    2,
                    206,
                    0
                ],
                "published": "2024-01-28T05:12:09Z",
                "published_parsed": [
                    2024,
                    1,
                    28,
                    5,
                    12,
                    9,
                    6,
                    28,
                    0
                ],
                "title": "Efficient Tuning and Inference for Large Language Models on Textual\n  Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Tuning and Inference for Large Language Models on Textual\n  Graphs"
                },
                "summary": "Rich textual and topological information of textual graphs need to be modeled\nin real-world applications such as webpages, e-commerce, and academic articles.\nPractitioners have been long following the path of adopting a shallow text\nencoder and a subsequent graph neural network (GNN) to solve this problem. In\nlight of recent advancements in large language models (LLMs), it is apparent\nthat integrating LLMs for enhanced textual encoding can substantially improve\nthe performance of textual graphs. Nevertheless, the efficiency of these\nmethods poses a significant challenge. In this paper, we propose ENGINE, a\nparameter- and memory-efficient fine-tuning method for textual graphs with an\nLLM encoder. The key insight is to combine the LLMs and GNNs through a tunable\nside structure, which significantly reduces the training complexity without\nimpairing the joint model's capacity. Extensive experiments on textual graphs\ndemonstrate our method's effectiveness by achieving the best model performance,\nmeanwhile having the lowest training cost compared to previous methods.\nMoreover, we introduce two variants with caching and dynamic early exit to\nfurther enhance training and inference speed. Specifically, caching accelerates\nENGINE's training by 12x, and dynamic early exit achieves up to 5x faster\ninference with a negligible performance drop (at maximum 1.17% relevant drop\nacross 7 datasets). Our codes are available at:\nhttps://github.com/ZhuYun97/ENGINE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rich textual and topological information of textual graphs need to be modeled\nin real-world applications such as webpages, e-commerce, and academic articles.\nPractitioners have been long following the path of adopting a shallow text\nencoder and a subsequent graph neural network (GNN) to solve this problem. In\nlight of recent advancements in large language models (LLMs), it is apparent\nthat integrating LLMs for enhanced textual encoding can substantially improve\nthe performance of textual graphs. Nevertheless, the efficiency of these\nmethods poses a significant challenge. In this paper, we propose ENGINE, a\nparameter- and memory-efficient fine-tuning method for textual graphs with an\nLLM encoder. The key insight is to combine the LLMs and GNNs through a tunable\nside structure, which significantly reduces the training complexity without\nimpairing the joint model's capacity. Extensive experiments on textual graphs\ndemonstrate our method's effectiveness by achieving the best model performance,\nmeanwhile having the lowest training cost compared to previous methods.\nMoreover, we introduce two variants with caching and dynamic early exit to\nfurther enhance training and inference speed. Specifically, caching accelerates\nENGINE's training by 12x, and dynamic early exit achieves up to 5x faster\ninference with a negligible performance drop (at maximum 1.17% relevant drop\nacross 7 datasets). Our codes are available at:\nhttps://github.com/ZhuYun97/ENGINE"
                },
                "authors": [
                    {
                        "name": "Yun Zhu"
                    },
                    {
                        "name": "Yaoke Wang"
                    },
                    {
                        "name": "Haizhou Shi"
                    },
                    {
                        "name": "Siliang Tang"
                    }
                ],
                "author_detail": {
                    "name": "Siliang Tang"
                },
                "author": "Siliang Tang",
                "arxiv_comment": "Accepted by IJCAI2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.15569v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.15569v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.09636v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.09636v2",
                "updated": "2024-07-23T17:55:30Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    55,
                    30,
                    1,
                    205,
                    0
                ],
                "published": "2024-03-14T17:59:26Z",
                "published_parsed": [
                    2024,
                    3,
                    14,
                    17,
                    59,
                    26,
                    3,
                    74,
                    0
                ],
                "title": "Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference"
                },
                "summary": "Transformers have emerged as the backbone of large language models (LLMs).\nHowever, generation remains inefficient due to the need to store in memory a\ncache of key-value representations for past tokens, whose size scales linearly\nwith the input sequence length and batch size. As a solution, we propose\nDynamic Memory Compression (DMC), a method for online key-value cache\ncompression at inference time. Most importantly, the model learns to apply\ndifferent compression ratios in different heads and layers. We retrofit\npre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,\nachieving up to 7x throughput increase during auto-regressive inference on an\nNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible\npercentage of the original data without adding any extra parameters. DMC\npreserves the original downstream performance with up to 4x cache compression,\noutperforming up-trained grouped-query attention (GQA) and key-value eviction\npolicies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded\ngains. Hence, DMC can serve as a drop-in replacement for KV caching in existing\nLLMs to fit longer contexts and larger batches within any given memory budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have emerged as the backbone of large language models (LLMs).\nHowever, generation remains inefficient due to the need to store in memory a\ncache of key-value representations for past tokens, whose size scales linearly\nwith the input sequence length and batch size. As a solution, we propose\nDynamic Memory Compression (DMC), a method for online key-value cache\ncompression at inference time. Most importantly, the model learns to apply\ndifferent compression ratios in different heads and layers. We retrofit\npre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,\nachieving up to 7x throughput increase during auto-regressive inference on an\nNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible\npercentage of the original data without adding any extra parameters. DMC\npreserves the original downstream performance with up to 4x cache compression,\noutperforming up-trained grouped-query attention (GQA) and key-value eviction\npolicies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded\ngains. Hence, DMC can serve as a drop-in replacement for KV caching in existing\nLLMs to fit longer contexts and larger batches within any given memory budget."
                },
                "authors": [
                    {
                        "name": "Piotr Nawrot"
                    },
                    {
                        "name": "Adrian ≈Åa≈Ñcucki"
                    },
                    {
                        "name": "Marcin Chochowski"
                    },
                    {
                        "name": "David Tarjan"
                    },
                    {
                        "name": "Edoardo M. Ponti"
                    }
                ],
                "author_detail": {
                    "name": "Edoardo M. Ponti"
                },
                "author": "Edoardo M. Ponti",
                "arxiv_journal_ref": "Proceedings of the 41st International Conference on Machine\n  Learning (2024) 37396-37412",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.09636v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.09636v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16672v1",
                "updated": "2024-07-23T17:42:57Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    42,
                    57,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T17:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    42,
                    57,
                    1,
                    205,
                    0
                ],
                "title": "6G at $\\frac{1}{6}g$: The Future of Cislunar Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6G at $\\frac{1}{6}g$: The Future of Cislunar Communications"
                },
                "summary": "What will the future of cislunar communications be? The ever-expanding\nhorizons of the space exploration missions, and the need for establishing\nsustainable space communication and navigation infrastructure necessitate to\nthink this question thoroughly. In this article, we examine how some of the\nconcepts of 6G technologies developed for terrestrial networks can be relevant\nin the context of cislunar networks. We discuss how 6G concepts, such as\nreconfigurable intelligent surfaces, quantum-resistant physical layer security,\nprivate information read/write/cache networks, semantic and goal-oriented\ncommunications, information freshness based quality of communication metrics,\nmulti-relay and cooperative networks, hold the potential to shape the future of\ncislunar communications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What will the future of cislunar communications be? The ever-expanding\nhorizons of the space exploration missions, and the need for establishing\nsustainable space communication and navigation infrastructure necessitate to\nthink this question thoroughly. In this article, we examine how some of the\nconcepts of 6G technologies developed for terrestrial networks can be relevant\nin the context of cislunar networks. We discuss how 6G concepts, such as\nreconfigurable intelligent surfaces, quantum-resistant physical layer security,\nprivate information read/write/cache networks, semantic and goal-oriented\ncommunications, information freshness based quality of communication metrics,\nmulti-relay and cooperative networks, hold the potential to shape the future of\ncislunar communications."
                },
                "authors": [
                    {
                        "name": "Sahan Liyanaarachchi"
                    },
                    {
                        "name": "Stavros Mitrolaris"
                    },
                    {
                        "name": "Purbesh Mitra"
                    },
                    {
                        "name": "Sennur Ulukus"
                    }
                ],
                "author_detail": {
                    "name": "Sennur Ulukus"
                },
                "author": "Sennur Ulukus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16303v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16303v1",
                "updated": "2024-07-23T08:58:06Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    58,
                    6,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T08:58:06Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    58,
                    6,
                    1,
                    205,
                    0
                ],
                "title": "Hidden Web Caches Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hidden Web Caches Discovery"
                },
                "summary": "Web caches play a crucial role in web performance and scalability. However,\ndetecting cached responses is challenging when web servers do not reliably\ncommunicate the cache status through standardized headers. This paper presents\na novel methodology for cache detection using timing analysis. Our approach\neliminates the dependency on cache status headers, making it applicable to any\nweb server. The methodology relies on sending paired requests using HTTP\nmultiplexing functionality and makes heavy use of cache-busting to control the\norigin of the responses. By measuring the time it takes to receive responses\nfrom paired requests, we can determine if a response is cached or not. In each\npair, one request is cache-busted to force retrieval from the origin server,\nwhile the other request is not and might be served from the cache, if present.\nA faster response time for the non-cache-busted request compared to the\ncache-busted one suggests the first one is coming from the cache. We\nimplemented this approach in a tool and achieved an estimated accuracy of 89.6%\ncompared to state-of-the-art methods based on cache status headers. Leveraging\nour cache detection approach, we conducted a large-scale experiment on the\nTranco Top 50k websites. We identified a significant presence of hidden caches\n(5.8%) that do not advertise themselves through headers. Additionally, we\nemployed our methodology to detect Web Cache Deception (WCD) vulnerabilities in\nthese hidden caches. We discovered that 1.020 of them are susceptible to WCD\nvulnerabilities, potentially leaking sensitive data. Our findings demonstrate\nthe effectiveness of our timing analysis methodology for cache discovery and\nhighlight the importance of a tool that does not rely on cache-communicated\ncache status headers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web caches play a crucial role in web performance and scalability. However,\ndetecting cached responses is challenging when web servers do not reliably\ncommunicate the cache status through standardized headers. This paper presents\na novel methodology for cache detection using timing analysis. Our approach\neliminates the dependency on cache status headers, making it applicable to any\nweb server. The methodology relies on sending paired requests using HTTP\nmultiplexing functionality and makes heavy use of cache-busting to control the\norigin of the responses. By measuring the time it takes to receive responses\nfrom paired requests, we can determine if a response is cached or not. In each\npair, one request is cache-busted to force retrieval from the origin server,\nwhile the other request is not and might be served from the cache, if present.\nA faster response time for the non-cache-busted request compared to the\ncache-busted one suggests the first one is coming from the cache. We\nimplemented this approach in a tool and achieved an estimated accuracy of 89.6%\ncompared to state-of-the-art methods based on cache status headers. Leveraging\nour cache detection approach, we conducted a large-scale experiment on the\nTranco Top 50k websites. We identified a significant presence of hidden caches\n(5.8%) that do not advertise themselves through headers. Additionally, we\nemployed our methodology to detect Web Cache Deception (WCD) vulnerabilities in\nthese hidden caches. We discovered that 1.020 of them are susceptible to WCD\nvulnerabilities, potentially leaking sensitive data. Our findings demonstrate\nthe effectiveness of our timing analysis methodology for cache discovery and\nhighlight the importance of a tool that does not rely on cache-communicated\ncache status headers."
                },
                "authors": [
                    {
                        "name": "Matteo Golinelli"
                    },
                    {
                        "name": "Bruno Crispo"
                    }
                ],
                "author_detail": {
                    "name": "Bruno Crispo"
                },
                "author": "Bruno Crispo",
                "arxiv_doi": "10.1145/3678890.3678931",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3678890.3678931",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.16303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16303v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "The definitive Version of Record was published in The 27th\n  International Symposium on Research in Attacks, Intrusions and Defenses (RAID\n  2024), September 30-October 02, 2024, Padua, Italy,\n  https://doi.org/10.1145/3678890.3678931",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16300v1",
                "updated": "2024-07-23T08:55:10Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    55,
                    10,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T08:55:10Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    55,
                    10,
                    1,
                    205,
                    0
                ],
                "title": "A Programming Model for Disaggregated Memory over CXL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Programming Model for Disaggregated Memory over CXL"
                },
                "summary": "CXL (Compute Express Link) is an emerging open industry-standard interconnect\nbetween processing and memory devices that is expected to revolutionize the way\nsystems are designed in the near future. It enables cache-coherent shared\nmemory pools in a disaggregated fashion at unprecedented scales, allowing\nalgorithms to interact with a variety of storage devices using simple loads and\nstores in a cacheline granularity. Alongside with unleashing unique\nopportunities for a wide range of applications, CXL introduces new challenges\nof data management and crash consistency. Alas, CXL lacks an adequate\nprogramming model, which makes reasoning about the correctness and expected\nbehaviors of algorithms and systems on top of it nearly impossible.\n  In this work, we present CXL0, the first programming model for concurrent\nprograms running on top of CXL. We propose a high-level abstraction for CXL\nmemory accesses and formally define operational semantics on top of that\nabstraction. We provide a set of general transformations that adapt concurrent\nalgorithms to the new disruptive technology. Using these transformations, every\nlinearizable algorithm can be easily transformed into its provably correct\nversion in the face of a full-system or sub-system crash. We believe that this\nwork will serve as the stepping stone for systems design and modelling on top\nof CXL, and support the development of future models as software and hardware\nevolve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXL (Compute Express Link) is an emerging open industry-standard interconnect\nbetween processing and memory devices that is expected to revolutionize the way\nsystems are designed in the near future. It enables cache-coherent shared\nmemory pools in a disaggregated fashion at unprecedented scales, allowing\nalgorithms to interact with a variety of storage devices using simple loads and\nstores in a cacheline granularity. Alongside with unleashing unique\nopportunities for a wide range of applications, CXL introduces new challenges\nof data management and crash consistency. Alas, CXL lacks an adequate\nprogramming model, which makes reasoning about the correctness and expected\nbehaviors of algorithms and systems on top of it nearly impossible.\n  In this work, we present CXL0, the first programming model for concurrent\nprograms running on top of CXL. We propose a high-level abstraction for CXL\nmemory accesses and formally define operational semantics on top of that\nabstraction. We provide a set of general transformations that adapt concurrent\nalgorithms to the new disruptive technology. Using these transformations, every\nlinearizable algorithm can be easily transformed into its provably correct\nversion in the face of a full-system or sub-system crash. We believe that this\nwork will serve as the stepping stone for systems design and modelling on top\nof CXL, and support the development of future models as software and hardware\nevolve."
                },
                "authors": [
                    {
                        "name": "Gal Assa"
                    },
                    {
                        "name": "Michal Friedman"
                    },
                    {
                        "name": "Ori Lahav"
                    }
                ],
                "author_detail": {
                    "name": "Ori Lahav"
                },
                "author": "Ori Lahav",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16286v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16286v1",
                "updated": "2024-07-23T08:40:27Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    40,
                    27,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T08:40:27Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    40,
                    27,
                    1,
                    205,
                    0
                ],
                "title": "A deeper look at depth pruning of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A deeper look at depth pruning of LLMs"
                },
                "summary": "Large Language Models (LLMs) are not only resource-intensive to train but\neven more costly to deploy in production. Therefore, recent work has attempted\nto prune blocks of LLMs based on cheap proxies for estimating block importance,\neffectively removing 10% of blocks in well-trained LLaMa-2 and Mistral 7b\nmodels without any significant degradation of downstream metrics. In this\npaper, we explore different block importance metrics by considering adaptive\nmetrics such as Shapley value in addition to static ones explored in prior\nwork. We show that adaptive metrics exhibit a trade-off in performance between\ntasks i.e., improvement on one task may degrade performance on the other due to\ndifferences in the computed block influences. Furthermore, we extend this\nanalysis from a complete block to individual self-attention and feed-forward\nlayers, highlighting the propensity of the self-attention layers to be more\namendable to pruning, even allowing removal of upto 33% of the self-attention\nlayers without incurring any performance degradation on MMLU for Mistral 7b\n(significant reduction in costly maintenance of KV-cache). Finally, we look at\nsimple performance recovery techniques to emulate the pruned layers by training\nlightweight additive bias or low-rank linear adapters. Performance recovery\nusing emulated updates avoids performance degradation for the initial blocks\n(up to 5% absolute improvement on MMLU), which is either competitive or\nsuperior to the learning-based technique.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are not only resource-intensive to train but\neven more costly to deploy in production. Therefore, recent work has attempted\nto prune blocks of LLMs based on cheap proxies for estimating block importance,\neffectively removing 10% of blocks in well-trained LLaMa-2 and Mistral 7b\nmodels without any significant degradation of downstream metrics. In this\npaper, we explore different block importance metrics by considering adaptive\nmetrics such as Shapley value in addition to static ones explored in prior\nwork. We show that adaptive metrics exhibit a trade-off in performance between\ntasks i.e., improvement on one task may degrade performance on the other due to\ndifferences in the computed block influences. Furthermore, we extend this\nanalysis from a complete block to individual self-attention and feed-forward\nlayers, highlighting the propensity of the self-attention layers to be more\namendable to pruning, even allowing removal of upto 33% of the self-attention\nlayers without incurring any performance degradation on MMLU for Mistral 7b\n(significant reduction in costly maintenance of KV-cache). Finally, we look at\nsimple performance recovery techniques to emulate the pruned layers by training\nlightweight additive bias or low-rank linear adapters. Performance recovery\nusing emulated updates avoids performance degradation for the initial blocks\n(up to 5% absolute improvement on MMLU), which is either competitive or\nsuperior to the learning-based technique."
                },
                "authors": [
                    {
                        "name": "Shoaib Ahmed Siddiqui"
                    },
                    {
                        "name": "Xin Dong"
                    },
                    {
                        "name": "Greg Heinrich"
                    },
                    {
                        "name": "Thomas Breuel"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "David Krueger"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    }
                ],
                "author_detail": {
                    "name": "Pavlo Molchanov"
                },
                "author": "Pavlo Molchanov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16286v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16286v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15309v1",
                "updated": "2024-07-22T14:37:58Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    14,
                    37,
                    58,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T14:37:58Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    14,
                    37,
                    58,
                    0,
                    204,
                    0
                ],
                "title": "vTensor: Flexible Virtual Tensor Management for Efficient LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vTensor: Flexible Virtual Tensor Management for Efficient LLM Serving"
                },
                "summary": "Large Language Models (LLMs) are widely used across various domains,\nprocessing millions of daily requests. This surge in demand poses significant\nchallenges in optimizing throughput and latency while keeping costs manageable.\nThe Key-Value (KV) cache, a standard method for retaining previous\ncomputations, makes LLM inference highly bounded by memory. While batching\nstrategies can enhance performance, they frequently lead to significant memory\nfragmentation. Even though cutting-edge systems like vLLM mitigate KV cache\nfragmentation using paged Attention mechanisms, they still suffer from\ninefficient memory and computational operations due to the tightly coupled page\nmanagement and computation kernels.\n  This study introduces the vTensor, an innovative tensor structure for LLM\ninference based on GPU virtual memory management (VMM). vTensor addresses\nexisting limitations by decoupling computation from memory defragmentation and\noffering dynamic extensibility. Our framework employs a CPU-GPU heterogeneous\napproach, ensuring efficient, fragmentation-free memory management while\naccommodating various computation kernels across different LLM architectures.\nExperimental results indicate that vTensor achieves an average speedup of 1.86x\nacross different models, with up to 2.42x in multi-turn chat scenarios.\nAdditionally, vTensor provides average speedups of 2.12x and 3.15x in kernel\nevaluation, reaching up to 3.92x and 3.27x compared to SGLang Triton\nprefix-prefilling kernels and vLLM paged Attention kernel, respectively.\nFurthermore, it frees approximately 71.25% (57GB) of memory on the NVIDIA A100\nGPU compared to vLLM, enabling more memory-intensive workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used across various domains,\nprocessing millions of daily requests. This surge in demand poses significant\nchallenges in optimizing throughput and latency while keeping costs manageable.\nThe Key-Value (KV) cache, a standard method for retaining previous\ncomputations, makes LLM inference highly bounded by memory. While batching\nstrategies can enhance performance, they frequently lead to significant memory\nfragmentation. Even though cutting-edge systems like vLLM mitigate KV cache\nfragmentation using paged Attention mechanisms, they still suffer from\ninefficient memory and computational operations due to the tightly coupled page\nmanagement and computation kernels.\n  This study introduces the vTensor, an innovative tensor structure for LLM\ninference based on GPU virtual memory management (VMM). vTensor addresses\nexisting limitations by decoupling computation from memory defragmentation and\noffering dynamic extensibility. Our framework employs a CPU-GPU heterogeneous\napproach, ensuring efficient, fragmentation-free memory management while\naccommodating various computation kernels across different LLM architectures.\nExperimental results indicate that vTensor achieves an average speedup of 1.86x\nacross different models, with up to 2.42x in multi-turn chat scenarios.\nAdditionally, vTensor provides average speedups of 2.12x and 3.15x in kernel\nevaluation, reaching up to 3.92x and 3.27x compared to SGLang Triton\nprefix-prefilling kernels and vLLM paged Attention kernel, respectively.\nFurthermore, it frees approximately 71.25% (57GB) of memory on the NVIDIA A100\nGPU compared to vLLM, enabling more memory-intensive workloads."
                },
                "authors": [
                    {
                        "name": "Jiale Xu"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Weiming Hu"
                    },
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Feiyang Wu"
                    },
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Shixuan Sun"
                    },
                    {
                        "name": "Changxu Shao"
                    },
                    {
                        "name": "Yuhong Guo"
                    },
                    {
                        "name": "Junping Zhao"
                    },
                    {
                        "name": "Ke Zhang"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jingwen Leng"
                    }
                ],
                "author_detail": {
                    "name": "Jingwen Leng"
                },
                "author": "Jingwen Leng",
                "arxiv_comment": "16 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15581v1",
                "updated": "2024-07-22T12:17:01Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    12,
                    17,
                    1,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T12:17:01Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    12,
                    17,
                    1,
                    0,
                    204,
                    0
                ],
                "title": "vLSM: Low tail latency and I/O amplification in LSM-based KV stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vLSM: Low tail latency and I/O amplification in LSM-based KV stores"
                },
                "summary": "LSM-based key-value (KV) stores are an important component in modern data\ninfrastructures. However, they suffer from high tail latency, in the order of\nseveral seconds, making them less attractive for user-facing applications. In\nthis paper, we introduce the notion of compaction chains and we analyse how\nthey affect tail latency. Then, we show that modern designs reduce tail\nlatency, by trading I/O amplification or require large amounts of memory. Based\non our analysis, we present vLSM, a new KV store design that improves tail\nlatency significantly without compromising on memory or I/O amplification. vLSM\nreduces (a) compaction chain width by using small SSTs and eliminating the\ntiering compaction required in L0 by modern systems and (b) compaction chain\nlength by using a larger than typical growth factor between L1 and L2 and\nintroducing overlap-aware SSTs in L1. We implement vLSM in RocksDB and evaluate\nit using db_bench and YCSB. Our evaluation highlights the underlying trade-off\namong memory requirements, I/O amplification, and tail latency, as well as the\nadvantage of vLSM over current approaches. vLSM improves P99 tail latency by up\nto 4.8x for writes and by up to 12.5x for reads, reduces cumulative write\nstalls by up to 60% while also slightly improves I/O amplification at the same\nmemory budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSM-based key-value (KV) stores are an important component in modern data\ninfrastructures. However, they suffer from high tail latency, in the order of\nseveral seconds, making them less attractive for user-facing applications. In\nthis paper, we introduce the notion of compaction chains and we analyse how\nthey affect tail latency. Then, we show that modern designs reduce tail\nlatency, by trading I/O amplification or require large amounts of memory. Based\non our analysis, we present vLSM, a new KV store design that improves tail\nlatency significantly without compromising on memory or I/O amplification. vLSM\nreduces (a) compaction chain width by using small SSTs and eliminating the\ntiering compaction required in L0 by modern systems and (b) compaction chain\nlength by using a larger than typical growth factor between L1 and L2 and\nintroducing overlap-aware SSTs in L1. We implement vLSM in RocksDB and evaluate\nit using db_bench and YCSB. Our evaluation highlights the underlying trade-off\namong memory requirements, I/O amplification, and tail latency, as well as the\nadvantage of vLSM over current approaches. vLSM improves P99 tail latency by up\nto 4.8x for writes and by up to 12.5x for reads, reduces cumulative write\nstalls by up to 60% while also slightly improves I/O amplification at the same\nmemory budget."
                },
                "authors": [
                    {
                        "name": "Giorgos Xanthakis"
                    },
                    {
                        "name": "Antonios Katsarakis"
                    },
                    {
                        "name": "Giorgos Saloustros"
                    },
                    {
                        "name": "Angelos Bilas"
                    }
                ],
                "author_detail": {
                    "name": "Angelos Bilas"
                },
                "author": "Angelos Bilas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.11055v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.11055v5",
                "updated": "2024-07-22T10:02:57Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    10,
                    2,
                    57,
                    0,
                    204,
                    0
                ],
                "published": "2022-12-21T14:59:23Z",
                "published_parsed": [
                    2022,
                    12,
                    21,
                    14,
                    59,
                    23,
                    2,
                    355,
                    0
                ],
                "title": "Coalgebraic Satisfiability Checking for Arithmetic $Œº$-Calculi",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coalgebraic Satisfiability Checking for Arithmetic $Œº$-Calculi"
                },
                "summary": "The coalgebraic $\\mu$-calculus provides a generic semantic framework for\nfixpoint logics over systems whose branching type goes beyond the standard\nrelational setup, e.g. probabilistic, weighted, or game-based. Previous work on\nthe coalgebraic $\\mu$-calculus includes an exponential-time upper bound on\nsatisfiability checking, which however relies on the availability of tableau\nrules for the next-step modalities that are sufficiently well-behaved in a\nformally defined sense; in particular, rule matches need to be representable by\npolynomial-sized codes, and the sequent duals of the rules need to absorb cut.\nWhile such rule sets have been identified for some important cases, they are\nnot known to exist in all cases of interest, in particular ones involving\neither integer weights as in the graded $\\mu$-calculus, or real-valued weights\nin combination with non-linear arithmetic. In the present work, we prove the\nsame upper complexity bound under more general assumptions, specifically\nregarding the complexity of the (much simpler) satisfiability problem for the\nunderlying one-step logic, roughly described as the nesting-free next-step\nfragment of the logic. The bound is realized by a generic global caching\nalgorithm that supports on-the-fly satisfiability checking. Notably, our\napproach directly accommodates unguarded formulae, and thus avoids use of the\nguardedness transformation. Example applications include new exponential-time\nupper bounds for satisfiability checking in an extension of the graded\n$\\mu$-calculus with polynomial inequalities (including positive Presburger\narithmetic), as well as an extension of the (two-valued) probabilistic\n$\\mu$-calculus with polynomial inequalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The coalgebraic $\\mu$-calculus provides a generic semantic framework for\nfixpoint logics over systems whose branching type goes beyond the standard\nrelational setup, e.g. probabilistic, weighted, or game-based. Previous work on\nthe coalgebraic $\\mu$-calculus includes an exponential-time upper bound on\nsatisfiability checking, which however relies on the availability of tableau\nrules for the next-step modalities that are sufficiently well-behaved in a\nformally defined sense; in particular, rule matches need to be representable by\npolynomial-sized codes, and the sequent duals of the rules need to absorb cut.\nWhile such rule sets have been identified for some important cases, they are\nnot known to exist in all cases of interest, in particular ones involving\neither integer weights as in the graded $\\mu$-calculus, or real-valued weights\nin combination with non-linear arithmetic. In the present work, we prove the\nsame upper complexity bound under more general assumptions, specifically\nregarding the complexity of the (much simpler) satisfiability problem for the\nunderlying one-step logic, roughly described as the nesting-free next-step\nfragment of the logic. The bound is realized by a generic global caching\nalgorithm that supports on-the-fly satisfiability checking. Notably, our\napproach directly accommodates unguarded formulae, and thus avoids use of the\nguardedness transformation. Example applications include new exponential-time\nupper bounds for satisfiability checking in an extension of the graded\n$\\mu$-calculus with polynomial inequalities (including positive Presburger\narithmetic), as well as an extension of the (two-valued) probabilistic\n$\\mu$-calculus with polynomial inequalities."
                },
                "authors": [
                    {
                        "name": "Daniel Hausmann"
                    },
                    {
                        "name": "Lutz Schr√∂der"
                    }
                ],
                "author_detail": {
                    "name": "Lutz Schr√∂der"
                },
                "author": "Lutz Schr√∂der",
                "arxiv_doi": "10.46298/lmcs-20(3:9)2024",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.46298/lmcs-20(3:9)2024",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2212.11055v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.11055v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Logical Methods in Computer Science, Volume 20, Issue 3 (July 23,\n  2024) lmcs:10532",
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "03B70, 03B44",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15360v1",
                "updated": "2024-07-22T04:07:26Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    4,
                    7,
                    26,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T04:07:26Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    4,
                    7,
                    26,
                    0,
                    204,
                    0
                ],
                "title": "Dissecting Multiplication in Transformers: Insights into LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting Multiplication in Transformers: Insights into LLMs"
                },
                "summary": "Transformer-based large language models have achieved remarkable performance\nacross various natural language processing tasks. However, they often struggle\nwith seemingly easy tasks like arithmetic despite their vast capabilities. This\nstark disparity raise human's concerns about their safe and ethical use, hinder\ntheir widespread adoption.In this paper, we focus on a typical arithmetic task,\ninteger multiplication, to explore and explain the imperfection of transformers\nin this domain. We provide comprehensive analysis of a vanilla transformer\ntrained to perform n-digit integer multiplication. Our observations indicate\nthat the model decomposes multiplication task into multiple parallel subtasks,\nsequentially optimizing each subtask for each digit to complete the final\nmultiplication. Based on observation and analysis, we infer the reasons of\ntransformers deficiencies in multiplication tasks lies in their difficulty in\ncalculating successive carryovers and caching intermediate results, and\nconfirmed this inference through experiments. Guided by these findings, we\npropose improvements to enhance transformers performance on multiplication\ntasks. These enhancements are validated through rigorous testing and\nmathematical modeling, not only enhance transformer's interpretability, but\nalso improve its performance, e.g., we achieve over 99.9% accuracy on 5-digit\ninteger multiplication with a tiny transformer, outperform LLMs GPT-4. Our\nmethod contributes to the broader fields of model understanding and\ninterpretability, paving the way for analyzing more complex tasks and\nTransformer models. This work underscores the importance of explainable AI,\nhelping to build trust in large language models and promoting their adoption in\ncritical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models have achieved remarkable performance\nacross various natural language processing tasks. However, they often struggle\nwith seemingly easy tasks like arithmetic despite their vast capabilities. This\nstark disparity raise human's concerns about their safe and ethical use, hinder\ntheir widespread adoption.In this paper, we focus on a typical arithmetic task,\ninteger multiplication, to explore and explain the imperfection of transformers\nin this domain. We provide comprehensive analysis of a vanilla transformer\ntrained to perform n-digit integer multiplication. Our observations indicate\nthat the model decomposes multiplication task into multiple parallel subtasks,\nsequentially optimizing each subtask for each digit to complete the final\nmultiplication. Based on observation and analysis, we infer the reasons of\ntransformers deficiencies in multiplication tasks lies in their difficulty in\ncalculating successive carryovers and caching intermediate results, and\nconfirmed this inference through experiments. Guided by these findings, we\npropose improvements to enhance transformers performance on multiplication\ntasks. These enhancements are validated through rigorous testing and\nmathematical modeling, not only enhance transformer's interpretability, but\nalso improve its performance, e.g., we achieve over 99.9% accuracy on 5-digit\ninteger multiplication with a tiny transformer, outperform LLMs GPT-4. Our\nmethod contributes to the broader fields of model understanding and\ninterpretability, paving the way for analyzing more complex tasks and\nTransformer models. This work underscores the importance of explainable AI,\nhelping to build trust in large language models and promoting their adoption in\ncritical applications."
                },
                "authors": [
                    {
                        "name": "Luyu Qiu"
                    },
                    {
                        "name": "Jianing Li"
                    },
                    {
                        "name": "Chi Su"
                    },
                    {
                        "name": "Chen Jason Zhang"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15891v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15891v1",
                "updated": "2024-07-22T01:12:23Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    1,
                    12,
                    23,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T01:12:23Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    1,
                    12,
                    23,
                    0,
                    204,
                    0
                ],
                "title": "RazorAttention: Efficient KV Cache Compression Through Retrieval Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RazorAttention: Efficient KV Cache Compression Through Retrieval Heads"
                },
                "summary": "The memory and computational demands of Key-Value (KV) cache present\nsignificant challenges for deploying long-context language models. Previous\napproaches attempt to mitigate this issue by selectively dropping tokens, which\nirreversibly erases critical information that might be needed for future\nqueries. In this paper, we propose a novel compression technique for KV cache\nthat preserves all token information. Our investigation reveals that: i) Most\nattention heads primarily focus on the local context; ii) Only a few heads,\ndenoted as retrieval heads, can essentially pay attention to all input tokens.\nThese key observations motivate us to use separate caching strategy for\nattention heads. Therefore, we propose RazorAttention, a training-free KV cache\ncompression algorithm, which maintains a full cache for these crucial retrieval\nheads and discards the remote tokens in non-retrieval heads. Furthermore, we\nintroduce a novel mechanism involving a \"compensation token\" to further recover\nthe information in the dropped tokens. Extensive evaluations across a diverse\nset of large language models (LLMs) demonstrate that RazorAttention achieves a\nreduction in KV cache size by over 70% without noticeable impacts on\nperformance. Additionally, RazorAttention is compatible with FlashAttention,\nrendering it an efficient and plug-and-play solution that enhances LLM\ninference efficiency without overhead or retraining of the original model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The memory and computational demands of Key-Value (KV) cache present\nsignificant challenges for deploying long-context language models. Previous\napproaches attempt to mitigate this issue by selectively dropping tokens, which\nirreversibly erases critical information that might be needed for future\nqueries. In this paper, we propose a novel compression technique for KV cache\nthat preserves all token information. Our investigation reveals that: i) Most\nattention heads primarily focus on the local context; ii) Only a few heads,\ndenoted as retrieval heads, can essentially pay attention to all input tokens.\nThese key observations motivate us to use separate caching strategy for\nattention heads. Therefore, we propose RazorAttention, a training-free KV cache\ncompression algorithm, which maintains a full cache for these crucial retrieval\nheads and discards the remote tokens in non-retrieval heads. Furthermore, we\nintroduce a novel mechanism involving a \"compensation token\" to further recover\nthe information in the dropped tokens. Extensive evaluations across a diverse\nset of large language models (LLMs) demonstrate that RazorAttention achieves a\nreduction in KV cache size by over 70% without noticeable impacts on\nperformance. Additionally, RazorAttention is compatible with FlashAttention,\nrendering it an efficient and plug-and-play solution that enhances LLM\ninference efficiency without overhead or retraining of the original model."
                },
                "authors": [
                    {
                        "name": "Hanlin Tang"
                    },
                    {
                        "name": "Yang Lin"
                    },
                    {
                        "name": "Jing Lin"
                    },
                    {
                        "name": "Qingsen Han"
                    },
                    {
                        "name": "Shikuan Hong"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Gongyi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Gongyi Wang"
                },
                "author": "Gongyi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15891v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15891v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15264v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15264v1",
                "updated": "2024-07-21T20:41:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    20,
                    41,
                    39,
                    6,
                    203,
                    0
                ],
                "published": "2024-07-21T20:41:39Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    20,
                    41,
                    39,
                    6,
                    203,
                    0
                ],
                "title": "LSM-GNN: Large-scale Storage-based Multi-GPU GNN Training by Optimizing\n  Data Transfer Scheme",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSM-GNN: Large-scale Storage-based Multi-GPU GNN Training by Optimizing\n  Data Transfer Scheme"
                },
                "summary": "Graph Neural Networks (GNNs) are widely used today in recommendation systems,\nfraud detection, and node/link classification tasks. Real world GNNs continue\nto scale in size and require a large memory footprint for storing graphs and\nembeddings that often exceed the memory capacities of the target GPUs used for\ntraining. To address limited memory capacities, traditional GNN training\napproaches use graph partitioning and sharding techniques to scale up across\nmultiple GPUs within a node and/or scale out across multiple nodes. However,\nthis approach suffers from the high computational costs of graph partitioning\nalgorithms and inefficient communication across GPUs.\n  To address these overheads, we propose Large-scale Storage-based Multi-GPU\nGNN framework (LSM-GNN), a storagebased approach to train GNN models that\nutilizes a novel communication layer enabling GPU software caches to function\nas a system-wide shared cache with low overheads.LSM-GNN incorporates a hybrid\neviction policy that intelligently manages cache space by using both static and\ndynamic node information to significantly enhance cache performance.\nFurthermore, we introduce the Preemptive Victim-buffer Prefetcher (PVP), a\nmechanism for prefetching node feature data from a Victim Buffer located in CPU\npinned-memory to further reduce the pressure on the storage devices.\nExperimental results show that despite the lower compute capabilities and\nmemory capacities, LSM-GNN in a single node with two GPUs offers superior\nperformance over two-node-four-GPU Dist-DGL baseline and provides up to 3.75x\nspeed up on end-to-end epoch time while running large-scale GNN training",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) are widely used today in recommendation systems,\nfraud detection, and node/link classification tasks. Real world GNNs continue\nto scale in size and require a large memory footprint for storing graphs and\nembeddings that often exceed the memory capacities of the target GPUs used for\ntraining. To address limited memory capacities, traditional GNN training\napproaches use graph partitioning and sharding techniques to scale up across\nmultiple GPUs within a node and/or scale out across multiple nodes. However,\nthis approach suffers from the high computational costs of graph partitioning\nalgorithms and inefficient communication across GPUs.\n  To address these overheads, we propose Large-scale Storage-based Multi-GPU\nGNN framework (LSM-GNN), a storagebased approach to train GNN models that\nutilizes a novel communication layer enabling GPU software caches to function\nas a system-wide shared cache with low overheads.LSM-GNN incorporates a hybrid\neviction policy that intelligently manages cache space by using both static and\ndynamic node information to significantly enhance cache performance.\nFurthermore, we introduce the Preemptive Victim-buffer Prefetcher (PVP), a\nmechanism for prefetching node feature data from a Victim Buffer located in CPU\npinned-memory to further reduce the pressure on the storage devices.\nExperimental results show that despite the lower compute capabilities and\nmemory capacities, LSM-GNN in a single node with two GPUs offers superior\nperformance over two-node-four-GPU Dist-DGL baseline and provides up to 3.75x\nspeed up on end-to-end epoch time while running large-scale GNN training"
                },
                "authors": [
                    {
                        "name": "Jeongmin Brian Park"
                    },
                    {
                        "name": "Kun Wu"
                    },
                    {
                        "name": "Vikram Sharma Mailthody"
                    },
                    {
                        "name": "Zaid Quresh"
                    },
                    {
                        "name": "Scott Mahlke"
                    },
                    {
                        "name": "Wen-mei Hwu"
                    }
                ],
                "author_detail": {
                    "name": "Wen-mei Hwu"
                },
                "author": "Wen-mei Hwu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15264v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15176v1",
                "updated": "2024-07-21T14:23:37Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    14,
                    23,
                    37,
                    6,
                    203,
                    0
                ],
                "published": "2024-07-21T14:23:37Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    14,
                    23,
                    37,
                    6,
                    203,
                    0
                ],
                "title": "Farewell to Length Extrapolation, a Training-Free Infinite Context with\n  Finite Attention Scope",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Farewell to Length Extrapolation, a Training-Free Infinite Context with\n  Finite Attention Scope"
                },
                "summary": "The maximum supported context length is a critical bottleneck limiting the\npractical application of the Large Language Model (LLM). Although existing\nlength extrapolation methods can extend the context of LLMs to millions of\ntokens, these methods all have an explicit upper bound. In this work, we\npropose LongCache, a training-free approach that enables LLM to support an\ninfinite context with finite context scope, through full-context cache\nselection and training-free integration. This effectively frees LLMs from the\nlength extrapolation issue. We validate LongCache on the LongBench and L-Eval\nand demonstrate its performance is on par with traditional full-attention\nmechanisms. Furthermore, we have applied LongCache on mainstream LLMs,\nincluding LLaMA3 and Mistral-v0.3, enabling them to support context lengths of\nat least 400K in Needle-In-A-Haystack tests. We will improve the efficiency of\nLongCache by GPU-aware optimization soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The maximum supported context length is a critical bottleneck limiting the\npractical application of the Large Language Model (LLM). Although existing\nlength extrapolation methods can extend the context of LLMs to millions of\ntokens, these methods all have an explicit upper bound. In this work, we\npropose LongCache, a training-free approach that enables LLM to support an\ninfinite context with finite context scope, through full-context cache\nselection and training-free integration. This effectively frees LLMs from the\nlength extrapolation issue. We validate LongCache on the LongBench and L-Eval\nand demonstrate its performance is on par with traditional full-attention\nmechanisms. Furthermore, we have applied LongCache on mainstream LLMs,\nincluding LLaMA3 and Mistral-v0.3, enabling them to support context lengths of\nat least 400K in Needle-In-A-Haystack tests. We will improve the efficiency of\nLongCache by GPU-aware optimization soon."
                },
                "authors": [
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Yuerong Song"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Kai Lv"
                    },
                    {
                        "name": "Hang Yan"
                    },
                    {
                        "name": "Linlin Li"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.00250v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.00250v3",
                "updated": "2024-07-21T11:47:04Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    11,
                    47,
                    4,
                    6,
                    203,
                    0
                ],
                "published": "2022-12-01T03:35:14Z",
                "published_parsed": [
                    2022,
                    12,
                    1,
                    3,
                    35,
                    14,
                    3,
                    335,
                    0
                ],
                "title": "Split Learning without Local Weight Sharing to Enhance Client-side Data\n  Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Split Learning without Local Weight Sharing to Enhance Client-side Data\n  Privacy"
                },
                "summary": "Split learning (SL) aims to protect user data privacy by distributing deep\nmodels between client-server and keeping private data locally. In SL training\nwith multiple clients, the local model weights are shared among the clients for\nlocal model update. This paper first reveals data privacy leakage exacerbated\nfrom local weight sharing among the clients in SL through model inversion\nattacks. Then, to reduce the data privacy leakage issue, we propose and analyze\nprivacy-enhanced SL (P-SL) (or SL without local weight sharing). We further\npropose parallelized P-SL to expedite the training process by duplicating\nmultiple server-side model instances without compromising accuracy. Finally, we\nexplore P-SL with late participating clients and devise a server-side\ncache-based training method to address the forgetting phenomenon in SL when\nlate clients join. Experimental results demonstrate that P-SL helps reduce up\nto 50% of client-side data leakage, which essentially achieves a better\nprivacy-accuracy trade-off than the current trend by using differential privacy\nmechanisms. Moreover, P-SL and its cache-based version achieve comparable\naccuracy to baseline SL under various data distributions, while cost less\ncomputation and communication. Additionally, caching-based training in P-SL\nmitigates the negative effect of forgetting, stabilizes the learning, and\nenables practical and low-complexity training in a dynamic environment with\nlate-arriving clients.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Split learning (SL) aims to protect user data privacy by distributing deep\nmodels between client-server and keeping private data locally. In SL training\nwith multiple clients, the local model weights are shared among the clients for\nlocal model update. This paper first reveals data privacy leakage exacerbated\nfrom local weight sharing among the clients in SL through model inversion\nattacks. Then, to reduce the data privacy leakage issue, we propose and analyze\nprivacy-enhanced SL (P-SL) (or SL without local weight sharing). We further\npropose parallelized P-SL to expedite the training process by duplicating\nmultiple server-side model instances without compromising accuracy. Finally, we\nexplore P-SL with late participating clients and devise a server-side\ncache-based training method to address the forgetting phenomenon in SL when\nlate clients join. Experimental results demonstrate that P-SL helps reduce up\nto 50% of client-side data leakage, which essentially achieves a better\nprivacy-accuracy trade-off than the current trend by using differential privacy\nmechanisms. Moreover, P-SL and its cache-based version achieve comparable\naccuracy to baseline SL under various data distributions, while cost less\ncomputation and communication. Additionally, caching-based training in P-SL\nmitigates the negative effect of forgetting, stabilizes the learning, and\nenables practical and low-complexity training in a dynamic environment with\nlate-arriving clients."
                },
                "authors": [
                    {
                        "name": "Ngoc Duy Pham"
                    },
                    {
                        "name": "Tran Khoa Phan"
                    },
                    {
                        "name": "Alsharif Abuadbba"
                    },
                    {
                        "name": "Yansong Gao"
                    },
                    {
                        "name": "Doan Nguyen"
                    },
                    {
                        "name": "Naveen Chilamkurti"
                    }
                ],
                "author_detail": {
                    "name": "Naveen Chilamkurti"
                },
                "author": "Naveen Chilamkurti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2212.00250v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.00250v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08454v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08454v2",
                "updated": "2024-07-21T02:37:11Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    2,
                    37,
                    11,
                    6,
                    203,
                    0
                ],
                "published": "2024-07-11T12:50:42Z",
                "published_parsed": [
                    2024,
                    7,
                    11,
                    12,
                    50,
                    42,
                    3,
                    193,
                    0
                ],
                "title": "Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on\n  Long-Context Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on\n  Long-Context Tasks"
                },
                "summary": "How to efficiently serve Large Language Models (LLMs) has become a pressing\nissue because of their huge computational cost in their autoregressive\ngeneration process. To mitigate computational costs, LLMs often employ the KV\nCache technique to improve the generation speed. While improving the\ncomputational efficiency, the storage requirements of the KV cache are\nsubstantial, particularly in long-context scenarios, leading to significant\nmemory consumption. Existing KV cache eviction methods often degrade the\nperformance of LLMs in long-context scenarios due to the information loss\nintroduced by eviction. In this paper, we propose a novel KV cache merging\napproach, called KVMerger, to achieve adaptive KV cache compression for\nlong-context tasks without significant performance degradation under\nconstrained memory budgets. Our approach is inspired by the intriguing\nobservation that key states exhibit high similarity at the token level within a\nsingle sequence. To facilitate merging, we develop an effective yet\nstraightforward merging set identification algorithm to identify suitable KV\nstates for merging. Our merging set identification algorithm stimulates the\nsecond observation that KV cache sparsity, from similarity perspective, is\nindependent of the dataset and remains persistent at the model level.\nSubsequently, we propose a Gaussian kernel weighted merging algorithm to\nselectively merge all states within each merging set. We conduct extensive\nexperiments to demonstrate the effectiveness of KVMerger for long-context tasks\nunder constrained memory budgets, applying it to models including\nLlama2-7B-chat and Llama2-13B-chat. Using the LongBench and ZeroScroll\nbenchmarks, we compare our method with other KV cache compression techniques,\nincluding H2O and CaM, showing that our method achieves superior performance\nacross tasks with both 50% and 35% KV cache budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to efficiently serve Large Language Models (LLMs) has become a pressing\nissue because of their huge computational cost in their autoregressive\ngeneration process. To mitigate computational costs, LLMs often employ the KV\nCache technique to improve the generation speed. While improving the\ncomputational efficiency, the storage requirements of the KV cache are\nsubstantial, particularly in long-context scenarios, leading to significant\nmemory consumption. Existing KV cache eviction methods often degrade the\nperformance of LLMs in long-context scenarios due to the information loss\nintroduced by eviction. In this paper, we propose a novel KV cache merging\napproach, called KVMerger, to achieve adaptive KV cache compression for\nlong-context tasks without significant performance degradation under\nconstrained memory budgets. Our approach is inspired by the intriguing\nobservation that key states exhibit high similarity at the token level within a\nsingle sequence. To facilitate merging, we develop an effective yet\nstraightforward merging set identification algorithm to identify suitable KV\nstates for merging. Our merging set identification algorithm stimulates the\nsecond observation that KV cache sparsity, from similarity perspective, is\nindependent of the dataset and remains persistent at the model level.\nSubsequently, we propose a Gaussian kernel weighted merging algorithm to\nselectively merge all states within each merging set. We conduct extensive\nexperiments to demonstrate the effectiveness of KVMerger for long-context tasks\nunder constrained memory budgets, applying it to models including\nLlama2-7B-chat and Llama2-13B-chat. Using the LongBench and ZeroScroll\nbenchmarks, we compare our method with other KV cache compression techniques,\nincluding H2O and CaM, showing that our method achieves superior performance\nacross tasks with both 50% and 35% KV cache budgets."
                },
                "authors": [
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Boxiao Jin"
                    },
                    {
                        "name": "Zhongzhi Yu"
                    },
                    {
                        "name": "Minjia Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Minjia Zhang"
                },
                "author": "Minjia Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08454v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08454v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.10516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.10516v2",
                "updated": "2024-07-20T22:14:42Z",
                "updated_parsed": [
                    2024,
                    7,
                    20,
                    22,
                    14,
                    42,
                    5,
                    202,
                    0
                ],
                "published": "2023-03-28T03:55:47Z",
                "published_parsed": [
                    2023,
                    3,
                    28,
                    3,
                    55,
                    47,
                    1,
                    87,
                    0
                ],
                "title": "Distributed Neural Representation for Reactive in situ Visualization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Neural Representation for Reactive in situ Visualization"
                },
                "summary": "Implicit neural representations (INRs) have emerged as a powerful tool for\ncompressing large-scale volume data. This opens up new possibilities for in\nsitu visualization. However, the efficient application of INRs to distributed\ndata remains an underexplored area. In this work, we develop a distributed\nvolumetric neural representation and optimize it for in situ visualization. Our\ntechnique eliminates data exchanges between processes, achieving\nstate-of-the-art compression speed, quality and ratios. Our technique also\nenables the implementation of an efficient strategy for caching large-scale\nsimulation data in high temporal frequencies, further facilitating the use of\nreactive in situ visualization in a wider range of scientific problems. We\nintegrate this system with the Ascent infrastructure and evaluate its\nperformance and usability using real-world simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit neural representations (INRs) have emerged as a powerful tool for\ncompressing large-scale volume data. This opens up new possibilities for in\nsitu visualization. However, the efficient application of INRs to distributed\ndata remains an underexplored area. In this work, we develop a distributed\nvolumetric neural representation and optimize it for in situ visualization. Our\ntechnique eliminates data exchanges between processes, achieving\nstate-of-the-art compression speed, quality and ratios. Our technique also\nenables the implementation of an efficient strategy for caching large-scale\nsimulation data in high temporal frequencies, further facilitating the use of\nreactive in situ visualization in a wider range of scientific problems. We\nintegrate this system with the Ascent infrastructure and evaluate its\nperformance and usability using real-world simulations."
                },
                "authors": [
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "Joseph A. Insley"
                    },
                    {
                        "name": "Victor A. Mateevitsi"
                    },
                    {
                        "name": "Silvio Rizzi"
                    },
                    {
                        "name": "Michael E. Papka"
                    },
                    {
                        "name": "Kwan-Liu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Liu Ma"
                },
                "author": "Kwan-Liu Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.10516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.10516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14801v1",
                "updated": "2024-07-20T08:21:46Z",
                "updated_parsed": [
                    2024,
                    7,
                    20,
                    8,
                    21,
                    46,
                    5,
                    202,
                    0
                ],
                "published": "2024-07-20T08:21:46Z",
                "published_parsed": [
                    2024,
                    7,
                    20,
                    8,
                    21,
                    46,
                    5,
                    202,
                    0
                ],
                "title": "SquareSort: a cache-oblivious sorting algorithm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SquareSort: a cache-oblivious sorting algorithm"
                },
                "summary": "In this paper we consider sorting in the cache-oblivious model of Frigo,\nLeiserson, Prokop, and Ramachandran (1999). We introduce a new simple sorting\nalgorithm in that model which has asymptotically optimal IO complexity\n$O(\\frac{n}{B} \\log_{M/B} n)$, where $n$ is the instance size, $M$ size of the\ncache and $B$ size of a memory block. This is the same as the complexity of the\nbest known cache-oblivious sorting algorithm FunnelSort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we consider sorting in the cache-oblivious model of Frigo,\nLeiserson, Prokop, and Ramachandran (1999). We introduce a new simple sorting\nalgorithm in that model which has asymptotically optimal IO complexity\n$O(\\frac{n}{B} \\log_{M/B} n)$, where $n$ is the instance size, $M$ size of the\ncache and $B$ size of a memory block. This is the same as the complexity of the\nbest known cache-oblivious sorting algorithm FunnelSort."
                },
                "authors": [
                    {
                        "name": "Michal Kouck√Ω"
                    },
                    {
                        "name": "Josef Matƒõjka"
                    }
                ],
                "author_detail": {
                    "name": "Josef Matƒõjka"
                },
                "author": "Josef Matƒõjka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2408.13257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13257v1",
                "updated": "2024-08-23T17:59:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    59,
                    51,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T17:59:51Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    59,
                    51,
                    4,
                    236,
                    0
                ],
                "title": "MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution\n  Real-World Scenarios that are Difficult for Humans?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution\n  Real-World Scenarios that are Difficult for Humans?"
                },
                "summary": "Comprehensive evaluation of Multimodal Large Language Models (MLLMs) has\nrecently garnered widespread attention in the research community. However, we\nobserve that existing benchmarks present several common barriers that make it\ndifficult to measure the significant challenges that models face in the real\nworld, including: 1) small data scale leads to a large performance variance; 2)\nreliance on model-based annotations results in restricted data quality; 3)\ninsufficient task difficulty, especially caused by the limited image\nresolution. To tackle these issues, we introduce MME-RealWorld. Specifically,\nwe collect more than $300$K images from public datasets and the Internet,\nfiltering $13,366$ high-quality images for annotation. This involves the\nefforts of professional $25$ annotators and $7$ experts in MLLMs, contributing\nto $29,429$ question-answer pairs that cover $43$ subtasks across $5$\nreal-world scenarios, extremely challenging even for humans. As far as we know,\nMME-RealWorld is the largest manually annotated benchmark to date, featuring\nthe highest resolution and a targeted focus on real-world applications. We\nfurther conduct a thorough evaluation involving $28$ prominent MLLMs, such as\nGPT-4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet. Our results show that even the\nmost advanced models struggle with our benchmarks, where none of them reach\n$60\\%$ accuracy. The challenges of perceiving high-resolution images and\nunderstanding complex real-world scenarios remain urgent issues to be\naddressed. The data and evaluation code are released at\nhttps://mme-realworld.github.io/ .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comprehensive evaluation of Multimodal Large Language Models (MLLMs) has\nrecently garnered widespread attention in the research community. However, we\nobserve that existing benchmarks present several common barriers that make it\ndifficult to measure the significant challenges that models face in the real\nworld, including: 1) small data scale leads to a large performance variance; 2)\nreliance on model-based annotations results in restricted data quality; 3)\ninsufficient task difficulty, especially caused by the limited image\nresolution. To tackle these issues, we introduce MME-RealWorld. Specifically,\nwe collect more than $300$K images from public datasets and the Internet,\nfiltering $13,366$ high-quality images for annotation. This involves the\nefforts of professional $25$ annotators and $7$ experts in MLLMs, contributing\nto $29,429$ question-answer pairs that cover $43$ subtasks across $5$\nreal-world scenarios, extremely challenging even for humans. As far as we know,\nMME-RealWorld is the largest manually annotated benchmark to date, featuring\nthe highest resolution and a targeted focus on real-world applications. We\nfurther conduct a thorough evaluation involving $28$ prominent MLLMs, such as\nGPT-4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet. Our results show that even the\nmost advanced models struggle with our benchmarks, where none of them reach\n$60\\%$ accuracy. The challenges of perceiving high-resolution images and\nunderstanding complex real-world scenarios remain urgent issues to be\naddressed. The data and evaluation code are released at\nhttps://mme-realworld.github.io/ ."
                },
                "authors": [
                    {
                        "name": "Yi-Fan Zhang"
                    },
                    {
                        "name": "Huanyu Zhang"
                    },
                    {
                        "name": "Haochen Tian"
                    },
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Shuangqing Zhang"
                    },
                    {
                        "name": "Junfei Wu"
                    },
                    {
                        "name": "Feng Li"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Qingsong Wen"
                    },
                    {
                        "name": "Zhang Zhang"
                    },
                    {
                        "name": "Liang Wang"
                    },
                    {
                        "name": "Rong Jin"
                    },
                    {
                        "name": "Tieniu Tan"
                    }
                ],
                "author_detail": {
                    "name": "Tieniu Tan"
                },
                "author": "Tieniu Tan",
                "arxiv_comment": "Project Page:\n  $\\href{https://mme-realworld.github.io/}{\\text{https://mme-realworld.github.io/}}$",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08750v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08750v3",
                "updated": "2024-08-23T17:55:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    55,
                    12,
                    4,
                    236,
                    0
                ],
                "published": "2023-10-12T22:30:15Z",
                "published_parsed": [
                    2023,
                    10,
                    12,
                    22,
                    30,
                    15,
                    3,
                    285,
                    0
                ],
                "title": "Search-Adaptor: Embedding Customization for Information Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search-Adaptor: Embedding Customization for Information Retrieval"
                },
                "summary": "Embeddings extracted by pre-trained Large Language Models (LLMs) have\nsignificant potential to improve information retrieval and search. Beyond the\nzero-shot setup in which they are being conventionally used, being able to take\nadvantage of the information from the relevant query-corpus paired data can\nfurther boost the LLM capabilities. In this paper, we propose a novel method,\nSearch-Adaptor, for customizing LLMs for information retrieval in an efficient\nand robust way. Search-Adaptor modifies the embeddings generated by pre-trained\nLLMs, and can be integrated with any LLM, including those only available via\nprediction APIs. On multiple English, multilingual, and multimodal retrieval\ndatasets, we show consistent and significant performance benefits for\nSearch-Adaptor -- e.g., more than 5% improvements for Google Embedding APIs in\nnDCG@10 averaged over 14 BEIR datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embeddings extracted by pre-trained Large Language Models (LLMs) have\nsignificant potential to improve information retrieval and search. Beyond the\nzero-shot setup in which they are being conventionally used, being able to take\nadvantage of the information from the relevant query-corpus paired data can\nfurther boost the LLM capabilities. In this paper, we propose a novel method,\nSearch-Adaptor, for customizing LLMs for information retrieval in an efficient\nand robust way. Search-Adaptor modifies the embeddings generated by pre-trained\nLLMs, and can be integrated with any LLM, including those only available via\nprediction APIs. On multiple English, multilingual, and multimodal retrieval\ndatasets, we show consistent and significant performance benefits for\nSearch-Adaptor -- e.g., more than 5% improvements for Google Embedding APIs in\nnDCG@10 averaged over 14 BEIR datasets."
                },
                "authors": [
                    {
                        "name": "Jinsung Yoon"
                    },
                    {
                        "name": "Sercan O Arik"
                    },
                    {
                        "name": "Yanfei Chen"
                    },
                    {
                        "name": "Tomas Pfister"
                    }
                ],
                "author_detail": {
                    "name": "Tomas Pfister"
                },
                "author": "Tomas Pfister",
                "arxiv_comment": "Published in 2024 ACL Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.08750v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08750v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11049v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11049v3",
                "updated": "2024-08-23T17:54:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    54,
                    34,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-20T17:57:31Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    31,
                    1,
                    233,
                    0
                ],
                "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/."
                },
                "authors": [
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Vashisth Tiwari"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Jinyuan Shi"
                    },
                    {
                        "name": "Ian En-Hsu Yen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11049v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11049v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.02974v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.02974v2",
                "updated": "2024-08-23T17:49:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    49,
                    1,
                    4,
                    236,
                    0
                ],
                "published": "2023-08-06T00:11:04Z",
                "published_parsed": [
                    2023,
                    8,
                    6,
                    0,
                    11,
                    4,
                    6,
                    218,
                    0
                ],
                "title": "Combining observational and experimental data for causal inference\n  considering data privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining observational and experimental data for causal inference\n  considering data privacy"
                },
                "summary": "Combining observational and experimental data for causal inference can\nimprove treatment effect estimation. However, many observational data sets\ncannot be released due to data privacy considerations, so one researcher may\nnot have access to both experimental and observational data. Nonetheless, a\nsmall amount of risk of disclosing sensitive information might be tolerable to\norganizations that house confidential data. In these cases, organizations can\nemploy data privacy techniques, which decrease disclosure risk, potentially at\nthe expense of data utility. In this paper, we explore disclosure limiting\ntransformations of observational data, which can be combined with experimental\ndata to estimate the sample and population average treatment effects. We\nconsider leveraging observational data to improve generalizability of treatment\neffect estimates when a randomized experiment (RCT) is not representative of\nthe population of interest, and to increase precision of treatment effect\nestimates. Through simulation studies, we illustrate the trade-off between\nprivacy and utility when employing different disclosure limiting\ntransformations. We find that leveraging transformed observational data in\ntreatment effect estimation can still improve estimation over only using data\nfrom an RCT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining observational and experimental data for causal inference can\nimprove treatment effect estimation. However, many observational data sets\ncannot be released due to data privacy considerations, so one researcher may\nnot have access to both experimental and observational data. Nonetheless, a\nsmall amount of risk of disclosing sensitive information might be tolerable to\norganizations that house confidential data. In these cases, organizations can\nemploy data privacy techniques, which decrease disclosure risk, potentially at\nthe expense of data utility. In this paper, we explore disclosure limiting\ntransformations of observational data, which can be combined with experimental\ndata to estimate the sample and population average treatment effects. We\nconsider leveraging observational data to improve generalizability of treatment\neffect estimates when a randomized experiment (RCT) is not representative of\nthe population of interest, and to increase precision of treatment effect\nestimates. Through simulation studies, we illustrate the trade-off between\nprivacy and utility when employing different disclosure limiting\ntransformations. We find that leveraging transformed observational data in\ntreatment effect estimation can still improve estimation over only using data\nfrom an RCT."
                },
                "authors": [
                    {
                        "name": "Charlotte Z. Mann"
                    },
                    {
                        "name": "Adam C. Sales"
                    },
                    {
                        "name": "Johann A. Gagnon-Bartsch"
                    }
                ],
                "author_detail": {
                    "name": "Johann A. Gagnon-Bartsch"
                },
                "author": "Johann A. Gagnon-Bartsch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.02974v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.02974v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13250v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13250v1",
                "updated": "2024-08-23T17:46:42Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    46,
                    42,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T17:46:42Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    46,
                    42,
                    4,
                    236,
                    0
                ],
                "title": "Variational inference of effective range parameters for\n  ${}^3$He-${}^4$He scattering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational inference of effective range parameters for\n  ${}^3$He-${}^4$He scattering"
                },
                "summary": "We use two different methods, Monte Carlo sampling and variational inference\n(VI), to perform a Bayesian calibration of the effective-range parameters in\n${}^3$He-${}^4$He elastic scattering. The parameters are calibrated to data\nfrom a recent set of $^{3}$He-${}^4$He elastic scattering differential cross\nsection measurements. Analysis of these data for $E_{\\rm lab} \\leq 4.3$ MeV\nyields a unimodal posterior for which both methods obtain the same structure.\nHowever, the effective-range expansion amplitude does not account for the\n$7/2^-$ state of ${}^7$Be so, even after calibration, the description of data\nat the upper end of this energy range is poor. The data up to $E_{\\rm lab}=2.6$\nMeV can be well described, but calibration to this lower-energy subset of the\ndata yields a bimodal posterior. After adapting VI to treat such a multi-modal\nposterior we find good agreement between the VI results and those obtained with\nparallel-tempered Monte Carlo sampling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We use two different methods, Monte Carlo sampling and variational inference\n(VI), to perform a Bayesian calibration of the effective-range parameters in\n${}^3$He-${}^4$He elastic scattering. The parameters are calibrated to data\nfrom a recent set of $^{3}$He-${}^4$He elastic scattering differential cross\nsection measurements. Analysis of these data for $E_{\\rm lab} \\leq 4.3$ MeV\nyields a unimodal posterior for which both methods obtain the same structure.\nHowever, the effective-range expansion amplitude does not account for the\n$7/2^-$ state of ${}^7$Be so, even after calibration, the description of data\nat the upper end of this energy range is poor. The data up to $E_{\\rm lab}=2.6$\nMeV can be well described, but calibration to this lower-energy subset of the\ndata yields a bimodal posterior. After adapting VI to treat such a multi-modal\nposterior we find good agreement between the VI results and those obtained with\nparallel-tempered Monte Carlo sampling."
                },
                "authors": [
                    {
                        "name": "Andrius Burnelis"
                    },
                    {
                        "name": "Vojta Kejzlar"
                    },
                    {
                        "name": "Daniel R. Phillips"
                    }
                ],
                "author_detail": {
                    "name": "Daniel R. Phillips"
                },
                "author": "Daniel R. Phillips",
                "arxiv_comment": "13 Pages, 6 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13250v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13250v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "nucl-th",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13247v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13247v1",
                "updated": "2024-08-23T17:42:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    42,
                    6,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T17:42:06Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    42,
                    6,
                    4,
                    236,
                    0
                ],
                "title": "Data Exposure from LLM Apps: An In-depth Investigation of OpenAI's GPTs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Exposure from LLM Apps: An In-depth Investigation of OpenAI's GPTs"
                },
                "summary": "LLM app ecosystems are quickly maturing and supporting a wide range of use\ncases, which requires them to collect excessive user data. Given that the LLM\napps are developed by third-parties and that anecdotal evidence suggests LLM\nplatforms currently do not strictly enforce their policies, user data shared\nwith arbitrary third-parties poses a significant privacy risk. In this paper we\naim to bring transparency in data practices of LLM apps. As a case study, we\nstudy OpenAI's GPT app ecosystem. We develop an LLM-based framework to conduct\nthe static analysis of natural language-based source code of GPTs and their\nActions (external services) to characterize their data collection practices.\nOur findings indicate that Actions collect expansive data about users,\nincluding sensitive information prohibited by OpenAI, such as passwords. We\nfind that some Actions, including related to advertising and analytics, are\nembedded in multiple GPTs, which allow them to track user activities across\nGPTs. Additionally, co-occurrence of Actions exposes as much as 9.5x more data\nto them, than it is exposed to individual Actions. Lastly, we develop an\nLLM-based privacy policy analysis framework to automatically check the\nconsistency of data collection by Actions with disclosures in their privacy\npolicies. Our measurements indicate that the disclosures for most of the\ncollected data types are omitted in privacy policies, with only 5.8% of Actions\nclearly disclosing their data collection practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM app ecosystems are quickly maturing and supporting a wide range of use\ncases, which requires them to collect excessive user data. Given that the LLM\napps are developed by third-parties and that anecdotal evidence suggests LLM\nplatforms currently do not strictly enforce their policies, user data shared\nwith arbitrary third-parties poses a significant privacy risk. In this paper we\naim to bring transparency in data practices of LLM apps. As a case study, we\nstudy OpenAI's GPT app ecosystem. We develop an LLM-based framework to conduct\nthe static analysis of natural language-based source code of GPTs and their\nActions (external services) to characterize their data collection practices.\nOur findings indicate that Actions collect expansive data about users,\nincluding sensitive information prohibited by OpenAI, such as passwords. We\nfind that some Actions, including related to advertising and analytics, are\nembedded in multiple GPTs, which allow them to track user activities across\nGPTs. Additionally, co-occurrence of Actions exposes as much as 9.5x more data\nto them, than it is exposed to individual Actions. Lastly, we develop an\nLLM-based privacy policy analysis framework to automatically check the\nconsistency of data collection by Actions with disclosures in their privacy\npolicies. Our measurements indicate that the disclosures for most of the\ncollected data types are omitted in privacy policies, with only 5.8% of Actions\nclearly disclosing their data collection practices."
                },
                "authors": [
                    {
                        "name": "Evin Jaff"
                    },
                    {
                        "name": "Yuhao Wu"
                    },
                    {
                        "name": "Ning Zhang"
                    },
                    {
                        "name": "Umar Iqbal"
                    }
                ],
                "author_detail": {
                    "name": "Umar Iqbal"
                },
                "author": "Umar Iqbal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13247v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05148v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05148v2",
                "updated": "2024-08-23T17:40:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    40,
                    15,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-09T16:07:37Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    7,
                    37,
                    4,
                    222,
                    0
                ],
                "title": "Impacts of floating-point non-associativity on reproducibility for HPC\n  and deep learning applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impacts of floating-point non-associativity on reproducibility for HPC\n  and deep learning applications"
                },
                "summary": "Run-by-run variability in parallel programs caused by floating-point\nnon-associativity (FPNA) has been known to significantly affect reproducibility\nin iterative algorithms, due to accumulating errors. Non-reproducibility\nnegatively affects efficiency and effectiveness of correctness testing for\nstochastic programs. Recently, the sensitivity of deep learning (DL) training\nand inference pipelines to FPNA have been found to be extreme, and can prevent\ncertification for commercial applications, accurate assessment of robustness\nand sensitivity, and bug detection. New approaches in scientific computing\napplications have coupled DL models with high-performance computing (HPC)\nsimulations, leading to an aggravation of debugging and testing challenges.\nHere we perform an investigation of the statistical properties of FPNA within\nmodern parallel programming models, analyze performance and productivity\nimpacts of replacing atomic operations with deterministic alternatives on GPUs,\nand examine the recently-added deterministic options within the PyTorch\nframework within the context of GPU deployment, uncovering and quantifying the\nimpacts of input parameters triggering run-by-run variability and reporting on\nthe reliability and completeness of the documentation. Finally, we evaluate the\nstrategy of exploiting automatic determinism provided by deterministic\nhardware, using the Groq LPU$^{TM}$ accelerator for inference portions of the\nDL pipeline. We demonstrate the benefits that this strategy can provide within\nreproducibility and correctness efforts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Run-by-run variability in parallel programs caused by floating-point\nnon-associativity (FPNA) has been known to significantly affect reproducibility\nin iterative algorithms, due to accumulating errors. Non-reproducibility\nnegatively affects efficiency and effectiveness of correctness testing for\nstochastic programs. Recently, the sensitivity of deep learning (DL) training\nand inference pipelines to FPNA have been found to be extreme, and can prevent\ncertification for commercial applications, accurate assessment of robustness\nand sensitivity, and bug detection. New approaches in scientific computing\napplications have coupled DL models with high-performance computing (HPC)\nsimulations, leading to an aggravation of debugging and testing challenges.\nHere we perform an investigation of the statistical properties of FPNA within\nmodern parallel programming models, analyze performance and productivity\nimpacts of replacing atomic operations with deterministic alternatives on GPUs,\nand examine the recently-added deterministic options within the PyTorch\nframework within the context of GPU deployment, uncovering and quantifying the\nimpacts of input parameters triggering run-by-run variability and reporting on\nthe reliability and completeness of the documentation. Finally, we evaluate the\nstrategy of exploiting automatic determinism provided by deterministic\nhardware, using the Groq LPU$^{TM}$ accelerator for inference portions of the\nDL pipeline. We demonstrate the benefits that this strategy can provide within\nreproducibility and correctness efforts."
                },
                "authors": [
                    {
                        "name": "Sanjif Shanmugavelu"
                    },
                    {
                        "name": "Mathieu Taillefumier"
                    },
                    {
                        "name": "Christopher Culver"
                    },
                    {
                        "name": "Oscar Hernandez"
                    },
                    {
                        "name": "Mark Coletti"
                    },
                    {
                        "name": "Ada Sedova"
                    }
                ],
                "author_detail": {
                    "name": "Ada Sedova"
                },
                "author": "Ada Sedova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05148v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05148v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13233v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13233v1",
                "updated": "2024-08-23T17:16:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    16,
                    43,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T17:16:43Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    16,
                    43,
                    4,
                    236,
                    0
                ],
                "title": "Multi-Layer Transformers Gradient Can be Approximated in Almost Linear\n  Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Layer Transformers Gradient Can be Approximated in Almost Linear\n  Time"
                },
                "summary": "The quadratic computational complexity in the self-attention mechanism of\npopular transformer architectures poses significant challenges for training and\ninference, particularly in terms of efficiency and memory requirements. Towards\naddressing these challenges, this paper introduces a novel fast computation\nmethod for gradient calculation in multi-layer transformer models. Our approach\nenables the computation of gradients for the entire multi-layer transformer\nmodel in almost linear time $n^{1+o(1)}$, where $n$ is the input sequence\nlength. This breakthrough significantly reduces the computational bottleneck\nassociated with the traditional quadratic time complexity. Our theory holds for\nany loss function and maintains a bounded approximation error across the entire\nmodel. Furthermore, our analysis can hold when the multi-layer transformer\nmodel contains many practical sub-modules, such as residual connection, casual\nmask, and multi-head attention. By improving the efficiency of gradient\ncomputation in large language models, we hope that our work will facilitate the\nmore effective training and deployment of long-context language models based on\nour theoretical results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quadratic computational complexity in the self-attention mechanism of\npopular transformer architectures poses significant challenges for training and\ninference, particularly in terms of efficiency and memory requirements. Towards\naddressing these challenges, this paper introduces a novel fast computation\nmethod for gradient calculation in multi-layer transformer models. Our approach\nenables the computation of gradients for the entire multi-layer transformer\nmodel in almost linear time $n^{1+o(1)}$, where $n$ is the input sequence\nlength. This breakthrough significantly reduces the computational bottleneck\nassociated with the traditional quadratic time complexity. Our theory holds for\nany loss function and maintains a bounded approximation error across the entire\nmodel. Furthermore, our analysis can hold when the multi-layer transformer\nmodel contains many practical sub-modules, such as residual connection, casual\nmask, and multi-head attention. By improving the efficiency of gradient\ncomputation in large language models, we hope that our work will facilitate the\nmore effective training and deployment of long-context language models based on\nour theoretical results."
                },
                "authors": [
                    {
                        "name": "Yingyu Liang"
                    },
                    {
                        "name": "Zhizhou Sha"
                    },
                    {
                        "name": "Zhenmei Shi"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yufa Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yufa Zhou"
                },
                "author": "Yufa Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13233v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13233v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11903v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11903v2",
                "updated": "2024-08-23T17:15:39Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    15,
                    39,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-21T18:00:21Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    18,
                    0,
                    21,
                    2,
                    234,
                    0
                ],
                "title": "Ancient Wisdom, Modern Tools: Exploring Retrieval-Augmented LLMs for\n  Ancient Indian Philosophy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ancient Wisdom, Modern Tools: Exploring Retrieval-Augmented LLMs for\n  Ancient Indian Philosophy"
                },
                "summary": "LLMs have revolutionized the landscape of information retrieval and knowledge\ndissemination. However, their application in specialized areas is often\nhindered by factual inaccuracies and hallucinations, especially in long-tail\nknowledge distributions. We explore the potential of retrieval-augmented\ngeneration (RAG) models for long-form question answering (LFQA) in a\nspecialized knowledge domain. We present VedantaNY-10M, a dataset curated from\nextensive public discourses on the ancient Indian philosophy of Advaita\nVedanta. We develop and benchmark a RAG model against a standard, non-RAG LLM,\nfocusing on transcription, retrieval, and generation performance. Human\nevaluations by computational linguists and domain experts show that the RAG\nmodel significantly outperforms the standard model in producing factual and\ncomprehensive responses having fewer hallucinations. In addition, a\nkeyword-based hybrid retriever that emphasizes unique low-frequency terms\nfurther improves results. Our study provides insights into effectively\nintegrating modern large language models with ancient knowledge systems.\nProject page with dataset and code: https://sites.google.com/view/vedantany-10m",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have revolutionized the landscape of information retrieval and knowledge\ndissemination. However, their application in specialized areas is often\nhindered by factual inaccuracies and hallucinations, especially in long-tail\nknowledge distributions. We explore the potential of retrieval-augmented\ngeneration (RAG) models for long-form question answering (LFQA) in a\nspecialized knowledge domain. We present VedantaNY-10M, a dataset curated from\nextensive public discourses on the ancient Indian philosophy of Advaita\nVedanta. We develop and benchmark a RAG model against a standard, non-RAG LLM,\nfocusing on transcription, retrieval, and generation performance. Human\nevaluations by computational linguists and domain experts show that the RAG\nmodel significantly outperforms the standard model in producing factual and\ncomprehensive responses having fewer hallucinations. In addition, a\nkeyword-based hybrid retriever that emphasizes unique low-frequency terms\nfurther improves results. Our study provides insights into effectively\nintegrating modern large language models with ancient knowledge systems.\nProject page with dataset and code: https://sites.google.com/view/vedantany-10m"
                },
                "authors": [
                    {
                        "name": "Priyanka Mandikal"
                    }
                ],
                "author_detail": {
                    "name": "Priyanka Mandikal"
                },
                "author": "Priyanka Mandikal",
                "arxiv_comment": "Outstanding Paper at the Machine Learning for Ancient Languages\n  Workshop, 2024.ml4al-1.23, Association for Computational Linguistics (ACL)\n  2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11903v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11903v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13230v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13230v1",
                "updated": "2024-08-23T17:11:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    11,
                    4,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T17:11:04Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    11,
                    4,
                    4,
                    236,
                    0
                ],
                "title": "Amortized Bayesian Multilevel Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amortized Bayesian Multilevel Models"
                },
                "summary": "Multilevel models (MLMs) are a central building block of the Bayesian\nworkflow. They enable joint, interpretable modeling of data across hierarchical\nlevels and provide a fully probabilistic quantification of uncertainty. Despite\ntheir well-recognized advantages, MLMs pose significant computational\nchallenges, often rendering their estimation and evaluation intractable within\nreasonable time constraints. Recent advances in simulation-based inference\noffer promising solutions for addressing complex probabilistic models using\ndeep generative networks. However, the utility and reliability of deep learning\nmethods for estimating Bayesian MLMs remains largely unexplored, especially\nwhen compared with gold-standard samplers. To this end, we explore a family of\nneural network architectures that leverage the probabilistic factorization of\nmultilevel models to facilitate efficient neural network training and\nsubsequent near-instant posterior inference on unseen data sets. We test our\nmethod on several real-world case studies and provide comprehensive comparisons\nto Stan as a gold-standard method where possible. Finally, we provide an\nopen-source implementation of our methods to stimulate further research in the\nnascent field of amortized Bayesian inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilevel models (MLMs) are a central building block of the Bayesian\nworkflow. They enable joint, interpretable modeling of data across hierarchical\nlevels and provide a fully probabilistic quantification of uncertainty. Despite\ntheir well-recognized advantages, MLMs pose significant computational\nchallenges, often rendering their estimation and evaluation intractable within\nreasonable time constraints. Recent advances in simulation-based inference\noffer promising solutions for addressing complex probabilistic models using\ndeep generative networks. However, the utility and reliability of deep learning\nmethods for estimating Bayesian MLMs remains largely unexplored, especially\nwhen compared with gold-standard samplers. To this end, we explore a family of\nneural network architectures that leverage the probabilistic factorization of\nmultilevel models to facilitate efficient neural network training and\nsubsequent near-instant posterior inference on unseen data sets. We test our\nmethod on several real-world case studies and provide comprehensive comparisons\nto Stan as a gold-standard method where possible. Finally, we provide an\nopen-source implementation of our methods to stimulate further research in the\nnascent field of amortized Bayesian inference."
                },
                "authors": [
                    {
                        "name": "Daniel Habermann"
                    },
                    {
                        "name": "Marvin Schmitt"
                    },
                    {
                        "name": "Lars K√ºhmichel"
                    },
                    {
                        "name": "Andreas Bulling"
                    },
                    {
                        "name": "Stefan T. Radev"
                    },
                    {
                        "name": "Paul-Christian B√ºrkner"
                    }
                ],
                "author_detail": {
                    "name": "Paul-Christian B√ºrkner"
                },
                "author": "Paul-Christian B√ºrkner",
                "arxiv_comment": "24 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13230v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13230v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15264v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15264v2",
                "updated": "2024-08-23T17:04:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    4,
                    38,
                    4,
                    236,
                    0
                ],
                "published": "2024-06-21T15:57:24Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    15,
                    57,
                    24,
                    4,
                    173,
                    0
                ],
                "title": "Towards Fine-Grained Citation Evaluation in Generated Text: A\n  Comparative Analysis of Faithfulness Metrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Fine-Grained Citation Evaluation in Generated Text: A\n  Comparative Analysis of Faithfulness Metrics"
                },
                "summary": "Large language models (LLMs) often produce unsupported or unverifiable\ncontent, known as \"hallucinations.\" To mitigate this, retrieval-augmented LLMs\nincorporate citations, grounding the content in verifiable sources. Despite\nsuch developments, manually assessing how well a citation supports the\nassociated statement remains a major challenge. Previous studies use\nfaithfulness metrics to estimate citation support automatically but are limited\nto binary classification, overlooking fine-grained citation support in\npractical scenarios. To investigate the effectiveness of faithfulness metrics\nin fine-grained scenarios, we propose a comparative evaluation framework that\nassesses the metric effectiveness in distinguishing citations between\nthree-category support levels: full, partial, and no support. Our framework\nemploys correlation analysis, classification evaluation, and retrieval\nevaluation to measure the alignment between metric scores and human judgments\ncomprehensively. Our results show no single metric consistently excels across\nall evaluations, revealing the complexity of assessing fine-grained support.\nBased on the findings, we provide practical recommendations for developing more\neffective metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often produce unsupported or unverifiable\ncontent, known as \"hallucinations.\" To mitigate this, retrieval-augmented LLMs\nincorporate citations, grounding the content in verifiable sources. Despite\nsuch developments, manually assessing how well a citation supports the\nassociated statement remains a major challenge. Previous studies use\nfaithfulness metrics to estimate citation support automatically but are limited\nto binary classification, overlooking fine-grained citation support in\npractical scenarios. To investigate the effectiveness of faithfulness metrics\nin fine-grained scenarios, we propose a comparative evaluation framework that\nassesses the metric effectiveness in distinguishing citations between\nthree-category support levels: full, partial, and no support. Our framework\nemploys correlation analysis, classification evaluation, and retrieval\nevaluation to measure the alignment between metric scores and human judgments\ncomprehensively. Our results show no single metric consistently excels across\nall evaluations, revealing the complexity of assessing fine-grained support.\nBased on the findings, we provide practical recommendations for developing more\neffective metrics."
                },
                "authors": [
                    {
                        "name": "Weijia Zhang"
                    },
                    {
                        "name": "Mohammad Aliannejadi"
                    },
                    {
                        "name": "Yifei Yuan"
                    },
                    {
                        "name": "Jiahuan Pei"
                    },
                    {
                        "name": "Jia-Hong Huang"
                    },
                    {
                        "name": "Evangelos Kanoulas"
                    }
                ],
                "author_detail": {
                    "name": "Evangelos Kanoulas"
                },
                "author": "Evangelos Kanoulas",
                "arxiv_comment": "Accepted by the 17th International Natural Language Generation\n  Conference (INLG 2024) as an oral presentation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15264v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15264v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13220v1",
                "updated": "2024-08-23T16:54:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    16,
                    54,
                    41,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T16:54:41Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    16,
                    54,
                    41,
                    4,
                    236,
                    0
                ],
                "title": "A New Perspective to Fish Trajectory Imputation: A Methodology for\n  Spatiotemporal Modeling of Acoustically Tagged Fish Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Perspective to Fish Trajectory Imputation: A Methodology for\n  Spatiotemporal Modeling of Acoustically Tagged Fish Data"
                },
                "summary": "The focus of this paper is a key component of a methodology for\nunderstanding, interpolating, and predicting fish movement patterns based on\nspatiotemporal data recorded by spatially static acoustic receivers. For\nperiods of time, fish may be far from the receivers, resulting in the absence\nof observations. The lack of information on the fish's location for extended\ntime periods poses challenges to the understanding of fish movement patterns,\nand hence, the identification of proper statistical inference frameworks for\nmodeling the trajectories. As the initial step in our methodology, in this\npaper, we implement an imputation strategy that relies on both Markov chain and\nBrownian motion principles to enhance our dataset over time. This methodology\nwill be generalizable and applicable to all fish species with similar migration\npatterns or data with similar structures due to the use of static acoustic\nreceivers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The focus of this paper is a key component of a methodology for\nunderstanding, interpolating, and predicting fish movement patterns based on\nspatiotemporal data recorded by spatially static acoustic receivers. For\nperiods of time, fish may be far from the receivers, resulting in the absence\nof observations. The lack of information on the fish's location for extended\ntime periods poses challenges to the understanding of fish movement patterns,\nand hence, the identification of proper statistical inference frameworks for\nmodeling the trajectories. As the initial step in our methodology, in this\npaper, we implement an imputation strategy that relies on both Markov chain and\nBrownian motion principles to enhance our dataset over time. This methodology\nwill be generalizable and applicable to all fish species with similar migration\npatterns or data with similar structures due to the use of static acoustic\nreceivers."
                },
                "authors": [
                    {
                        "name": "Mahshid Ahmadian"
                    },
                    {
                        "name": "Edward L. Boone"
                    },
                    {
                        "name": "Grace S. Chiu"
                    }
                ],
                "author_detail": {
                    "name": "Grace S. Chiu"
                },
                "author": "Grace S. Chiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11972v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11972v2",
                "updated": "2024-08-23T16:52:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    16,
                    52,
                    49,
                    4,
                    236,
                    0
                ],
                "published": "2024-06-17T18:00:10Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    18,
                    0,
                    10,
                    0,
                    169,
                    0
                ],
                "title": "Mapping the Inner 0.1 pc of a Supermassive Black Hole Environment with\n  the Tidal Disruption Event and Extreme Coronal Line Emitter AT 2022upj",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping the Inner 0.1 pc of a Supermassive Black Hole Environment with\n  the Tidal Disruption Event and Extreme Coronal Line Emitter AT 2022upj"
                },
                "summary": "Extreme coronal line emitters (ECLEs) are objects showing transient\nhigh-ionization lines in the centers of galaxies. They have been attributed to\nechoes of high-energy flares of ionizing radiation, such as those produced by\ntidal disruption events (TDEs), but have only recently been observed within\nhundreds of days after an optical transient was detected. AT 2022upj is a\nnuclear UV-optical flare at z=0.054 with spectra showing [Fe X] {\\lambda}6375\nand [Fe XIV] {\\lambda}5303 during the optical peak, the earliest presence of\nextreme coronal lines during an ongoing transient. AT 2022upj is also the\nsecond ever ECLE (and first with a concurrent flare) to show broad He II\n{\\lambda}4686 emission, a key signature of optical/UV TDEs. We also detect\nX-ray emission during the optical transient phase, which may be related to the\nsource of ionizing photons for the extreme coronal lines. Finally, we analyze\nthe spectroscopic evolution of each emission line and find that [Fe X] and [Fe\nXIV] weaken within 400d of optical peak, while [Fe VII] {\\lambda}5720, [Fe VII]\n{\\lambda}6087, and [O III] {\\lambda}{\\lambda}4959,5007 emerge over the same\nperiod. The velocities of the iron lines indicate circumnuclear gas within\n0.1pc of the central supermassive black hole (SMBH), while a dust echo inferred\nfrom NEOWISE data indicates that circumnuclear dust lies at a minimum of 0.4pc\naway, providing evidence of stratified material around a SMBH. AT 2022upj is\nthe first confirmed ECLE-TDE with clear signatures of both classes. This\nevent's spectroscopic evolution on a $\\sim$year unveils the impact of highly\nenergetic flares such as TDEs on the complex environments around SMBHs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extreme coronal line emitters (ECLEs) are objects showing transient\nhigh-ionization lines in the centers of galaxies. They have been attributed to\nechoes of high-energy flares of ionizing radiation, such as those produced by\ntidal disruption events (TDEs), but have only recently been observed within\nhundreds of days after an optical transient was detected. AT 2022upj is a\nnuclear UV-optical flare at z=0.054 with spectra showing [Fe X] {\\lambda}6375\nand [Fe XIV] {\\lambda}5303 during the optical peak, the earliest presence of\nextreme coronal lines during an ongoing transient. AT 2022upj is also the\nsecond ever ECLE (and first with a concurrent flare) to show broad He II\n{\\lambda}4686 emission, a key signature of optical/UV TDEs. We also detect\nX-ray emission during the optical transient phase, which may be related to the\nsource of ionizing photons for the extreme coronal lines. Finally, we analyze\nthe spectroscopic evolution of each emission line and find that [Fe X] and [Fe\nXIV] weaken within 400d of optical peak, while [Fe VII] {\\lambda}5720, [Fe VII]\n{\\lambda}6087, and [O III] {\\lambda}{\\lambda}4959,5007 emerge over the same\nperiod. The velocities of the iron lines indicate circumnuclear gas within\n0.1pc of the central supermassive black hole (SMBH), while a dust echo inferred\nfrom NEOWISE data indicates that circumnuclear dust lies at a minimum of 0.4pc\naway, providing evidence of stratified material around a SMBH. AT 2022upj is\nthe first confirmed ECLE-TDE with clear signatures of both classes. This\nevent's spectroscopic evolution on a $\\sim$year unveils the impact of highly\nenergetic flares such as TDEs on the complex environments around SMBHs."
                },
                "authors": [
                    {
                        "name": "Megan Newsome"
                    },
                    {
                        "name": "Iair Arcavi"
                    },
                    {
                        "name": "D. Andrew Howell"
                    },
                    {
                        "name": "Curtis McCully"
                    },
                    {
                        "name": "Giacomo Terreran"
                    },
                    {
                        "name": "Griffin Hosseinzadeh"
                    },
                    {
                        "name": "K. Azalee Bostroem"
                    },
                    {
                        "name": "Yael Dgany"
                    },
                    {
                        "name": "Joseph Farah"
                    },
                    {
                        "name": "Sara Faris"
                    },
                    {
                        "name": "Estefania Padilla-Gonzalez"
                    },
                    {
                        "name": "Craig Pellegrino"
                    },
                    {
                        "name": "Moira Andrews"
                    }
                ],
                "author_detail": {
                    "name": "Moira Andrews"
                },
                "author": "Moira Andrews",
                "arxiv_comment": "19 pages, 17 figures. Under review by ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11972v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11972v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.07265v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.07265v2",
                "updated": "2024-08-23T16:42:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    16,
                    42,
                    41,
                    4,
                    236,
                    0
                ],
                "published": "2023-03-13T16:44:28Z",
                "published_parsed": [
                    2023,
                    3,
                    13,
                    16,
                    44,
                    28,
                    0,
                    72,
                    0
                ],
                "title": "Multimodal Reinforcement Learning for Robots Collaborating with Humans",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Reinforcement Learning for Robots Collaborating with Humans"
                },
                "summary": "Robot assistants for older adults and people with disabilities need to\ninteract with their users in collaborative tasks. The core component of these\nsystems is an interaction manager whose job is to observe and assess the task,\nand infer the state of the human and their intent to choose the best course of\naction for the robot. Due to the sparseness of the data in this domain, the\npolicy for such multi-modal systems is often crafted by hand; as the complexity\nof interactions grows this process is not scalable. In this paper, we propose a\nreinforcement learning (RL) approach to learn the robot policy. In contrast to\nthe dialog systems, our agent is trained with a simulator developed by using\nhuman data and can deal with multiple modalities such as language and physical\nactions. We conducted a human study to evaluate the performance of the system\nin the interaction with a user. Our designed system shows promising preliminary\nresults when it is used by a real user.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robot assistants for older adults and people with disabilities need to\ninteract with their users in collaborative tasks. The core component of these\nsystems is an interaction manager whose job is to observe and assess the task,\nand infer the state of the human and their intent to choose the best course of\naction for the robot. Due to the sparseness of the data in this domain, the\npolicy for such multi-modal systems is often crafted by hand; as the complexity\nof interactions grows this process is not scalable. In this paper, we propose a\nreinforcement learning (RL) approach to learn the robot policy. In contrast to\nthe dialog systems, our agent is trained with a simulator developed by using\nhuman data and can deal with multiple modalities such as language and physical\nactions. We conducted a human study to evaluate the performance of the system\nin the interaction with a user. Our designed system shows promising preliminary\nresults when it is used by a real user."
                },
                "authors": [
                    {
                        "name": "Afagh Mehri Shervedani"
                    },
                    {
                        "name": "Siyu Li"
                    },
                    {
                        "name": "Natawut Monaikul"
                    },
                    {
                        "name": "Bahareh Abbasi"
                    },
                    {
                        "name": "Barbara Di Eugenio"
                    },
                    {
                        "name": "Milos Zefran"
                    }
                ],
                "author_detail": {
                    "name": "Milos Zefran"
                },
                "author": "Milos Zefran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.07265v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.07265v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07018v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07018v2",
                "updated": "2024-08-23T16:41:47Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    16,
                    41,
                    47,
                    4,
                    236,
                    0
                ],
                "published": "2024-07-09T16:38:48Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    16,
                    38,
                    48,
                    1,
                    191,
                    0
                ],
                "title": "End-To-End Causal Effect Estimation from Unstructured Natural Language\n  Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-To-End Causal Effect Estimation from Unstructured Natural Language\n  Data"
                },
                "summary": "Knowing the effect of an intervention is critical for human decision-making,\nbut current approaches for causal effect estimation rely on manual data\ncollection and structuring, regardless of the causal assumptions. This\nincreases both the cost and time-to-completion for studies. We show how large,\ndiverse observational text data can be mined with large language models (LLMs)\nto produce inexpensive causal effect estimates under appropriate causal\nassumptions. We introduce NATURAL, a novel family of causal effect estimators\nbuilt with LLMs that operate over datasets of unstructured text. Our estimators\nuse LLM conditional distributions (over variables of interest, given the text\ndata) to assist in the computation of classical estimators of causal effect. We\novercome a number of technical challenges to realize this idea, such as\nautomating data curation and using LLMs to impute missing information. We\nprepare six (two synthetic and four real) observational datasets, paired with\ncorresponding ground truth in the form of randomized trials, which we used to\nsystematically evaluate each step of our pipeline. NATURAL estimators\ndemonstrate remarkable performance, yielding causal effect estimates that fall\nwithin 3 percentage points of their ground truth counterparts, including on\nreal-world Phase 3/4 clinical trials. Our results suggest that unstructured\ntext data is a rich source of causal effect information, and NATURAL is a first\nstep towards an automated pipeline to tap this resource.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowing the effect of an intervention is critical for human decision-making,\nbut current approaches for causal effect estimation rely on manual data\ncollection and structuring, regardless of the causal assumptions. This\nincreases both the cost and time-to-completion for studies. We show how large,\ndiverse observational text data can be mined with large language models (LLMs)\nto produce inexpensive causal effect estimates under appropriate causal\nassumptions. We introduce NATURAL, a novel family of causal effect estimators\nbuilt with LLMs that operate over datasets of unstructured text. Our estimators\nuse LLM conditional distributions (over variables of interest, given the text\ndata) to assist in the computation of classical estimators of causal effect. We\novercome a number of technical challenges to realize this idea, such as\nautomating data curation and using LLMs to impute missing information. We\nprepare six (two synthetic and four real) observational datasets, paired with\ncorresponding ground truth in the form of randomized trials, which we used to\nsystematically evaluate each step of our pipeline. NATURAL estimators\ndemonstrate remarkable performance, yielding causal effect estimates that fall\nwithin 3 percentage points of their ground truth counterparts, including on\nreal-world Phase 3/4 clinical trials. Our results suggest that unstructured\ntext data is a rich source of causal effect information, and NATURAL is a first\nstep towards an automated pipeline to tap this resource."
                },
                "authors": [
                    {
                        "name": "Nikita Dhawan"
                    },
                    {
                        "name": "Leonardo Cotta"
                    },
                    {
                        "name": "Karen Ullrich"
                    },
                    {
                        "name": "Rahul G. Krishnan"
                    },
                    {
                        "name": "Chris J. Maddison"
                    }
                ],
                "author_detail": {
                    "name": "Chris J. Maddison"
                },
                "author": "Chris J. Maddison",
                "arxiv_comment": "28 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07018v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07018v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13204v1",
                "updated": "2024-08-23T16:33:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    16,
                    33,
                    58,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T16:33:58Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    16,
                    33,
                    58,
                    4,
                    236,
                    0
                ],
                "title": "DOMAINEVAL: An Auto-Constructed Benchmark for Multi-Domain Code\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DOMAINEVAL: An Auto-Constructed Benchmark for Multi-Domain Code\n  Generation"
                },
                "summary": "Code benchmarks such as HumanEval are widely adopted to evaluate the\ncapabilities of Large Language Models (LLMs), providing insights into their\nstrengths and weaknesses. However, current benchmarks primarily exercise LLMs'\ncapability on common coding tasks (e.g., bubble sort, greatest common divisor),\nleaving domain-specific coding tasks (e.g., computation, system, cryptography)\nunexplored. To fill this gap, we propose a multi-domain code benchmark,\nDOMAINEVAL, designed to evaluate LLMs' coding capabilities thoroughly. Our\npipeline works in a fully automated manner, enabling a push-bottom construction\nfrom code repositories into formatted subjects under study. Interesting\nfindings are observed by evaluating 12 representative LLMs against DOMAINEVAL.\nWe notice that LLMs are generally good at computation tasks while falling short\non cryptography and system coding tasks. The performance gap can be as much as\n68.94% (80.94% - 12.0%) in some LLMs. We also observe that generating more\nsamples can increase the overall performance of LLMs, while the domain bias may\neven increase. The contributions of this study include a code generation\nbenchmark dataset DOMAINEVAL, encompassing six popular domains, a fully\nautomated pipeline for constructing code benchmarks, and an identification of\nthe limitations of LLMs in code generation tasks based on their performance on\nDOMAINEVAL, providing directions for future research improvements. The\nleaderboard is available at https://domaineval.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code benchmarks such as HumanEval are widely adopted to evaluate the\ncapabilities of Large Language Models (LLMs), providing insights into their\nstrengths and weaknesses. However, current benchmarks primarily exercise LLMs'\ncapability on common coding tasks (e.g., bubble sort, greatest common divisor),\nleaving domain-specific coding tasks (e.g., computation, system, cryptography)\nunexplored. To fill this gap, we propose a multi-domain code benchmark,\nDOMAINEVAL, designed to evaluate LLMs' coding capabilities thoroughly. Our\npipeline works in a fully automated manner, enabling a push-bottom construction\nfrom code repositories into formatted subjects under study. Interesting\nfindings are observed by evaluating 12 representative LLMs against DOMAINEVAL.\nWe notice that LLMs are generally good at computation tasks while falling short\non cryptography and system coding tasks. The performance gap can be as much as\n68.94% (80.94% - 12.0%) in some LLMs. We also observe that generating more\nsamples can increase the overall performance of LLMs, while the domain bias may\neven increase. The contributions of this study include a code generation\nbenchmark dataset DOMAINEVAL, encompassing six popular domains, a fully\nautomated pipeline for constructing code benchmarks, and an identification of\nthe limitations of LLMs in code generation tasks based on their performance on\nDOMAINEVAL, providing directions for future research improvements. The\nleaderboard is available at https://domaineval.github.io/."
                },
                "authors": [
                    {
                        "name": "Qiming Zhu"
                    },
                    {
                        "name": "Jialun Cao"
                    },
                    {
                        "name": "Yaojie Lu"
                    },
                    {
                        "name": "Hongyu Lin"
                    },
                    {
                        "name": "Xianpei Han"
                    },
                    {
                        "name": "Le Sun"
                    },
                    {
                        "name": "Shing-Chi Cheung"
                    }
                ],
                "author_detail": {
                    "name": "Shing-Chi Cheung"
                },
                "author": "Shing-Chi Cheung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13195v1",
                "updated": "2024-08-23T16:25:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    16,
                    25,
                    33,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T16:25:33Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    16,
                    25,
                    33,
                    4,
                    236,
                    0
                ],
                "title": "NAS-Cap: Deep-Learning Driven 3-D Capacitance Extraction with Neural\n  Architecture Search and Data Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NAS-Cap: Deep-Learning Driven 3-D Capacitance Extraction with Neural\n  Architecture Search and Data Augmentation"
                },
                "summary": "More accurate capacitance extraction is demanded for designing integrated\ncircuits under advanced process technology. The pattern matching approach and\nthe field solver for capacitance extraction have the drawbacks of inaccuracy\nand large computational cost, respectively. Recent work \\cite{yang2023cnn}\nproposes a grid-based data representation and a convolutional neural network\n(CNN) based capacitance models (called CNN-Cap), which opens the third way for\n3-D capacitance extraction to get accurate results with much less time cost\nthan field solver. In this work, the techniques of neural architecture search\n(NAS) and data augmentation are proposed to train better CNN models for 3-D\ncapacitance extraction. Experimental results on datasets from different designs\nshow that the obtained NAS-Cap models achieve remarkably higher accuracy than\nCNN-Cap, while consuming less runtime for inference and space for model\nstorage. Meanwhile, the transferability of the NAS is validated, as the once\nsearched architecture brought similar error reduction on coupling/total\ncapacitance for the test cases from different design and/or process technology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More accurate capacitance extraction is demanded for designing integrated\ncircuits under advanced process technology. The pattern matching approach and\nthe field solver for capacitance extraction have the drawbacks of inaccuracy\nand large computational cost, respectively. Recent work \\cite{yang2023cnn}\nproposes a grid-based data representation and a convolutional neural network\n(CNN) based capacitance models (called CNN-Cap), which opens the third way for\n3-D capacitance extraction to get accurate results with much less time cost\nthan field solver. In this work, the techniques of neural architecture search\n(NAS) and data augmentation are proposed to train better CNN models for 3-D\ncapacitance extraction. Experimental results on datasets from different designs\nshow that the obtained NAS-Cap models achieve remarkably higher accuracy than\nCNN-Cap, while consuming less runtime for inference and space for model\nstorage. Meanwhile, the transferability of the NAS is validated, as the once\nsearched architecture brought similar error reduction on coupling/total\ncapacitance for the test cases from different design and/or process technology."
                },
                "authors": [
                    {
                        "name": "Haoyuan Li"
                    },
                    {
                        "name": "Dingcheng Yang"
                    },
                    {
                        "name": "Chunyan Pei"
                    },
                    {
                        "name": "Wenjian Yu"
                    }
                ],
                "author_detail": {
                    "name": "Wenjian Yu"
                },
                "author": "Wenjian Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10128v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10128v2",
                "updated": "2024-08-23T16:15:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    16,
                    15,
                    30,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-19T16:15:09Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    16,
                    15,
                    9,
                    0,
                    232,
                    0
                ],
                "title": "Advancing Voice Cloning for Nepali: Leveraging Transfer Learning in a\n  Low-Resource Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Voice Cloning for Nepali: Leveraging Transfer Learning in a\n  Low-Resource Language"
                },
                "summary": "Voice cloning is a prominent feature in personalized speech interfaces. A\nneural vocal cloning system can mimic someone's voice using just a few audio\nsamples. Both speaker encoding and speaker adaptation are topics of research in\nthe field of voice cloning. Speaker adaptation relies on fine-tuning a\nmulti-speaker generative model, which involves training a separate model to\ninfer a new speaker embedding used for speaker encoding. Both methods can\nachieve excellent performance, even with a small number of cloning audios, in\nterms of the speech's naturalness and similarity to the original speaker.\nSpeaker encoding approaches are more appropriate for low-resource deployment\nsince they require significantly less memory and have a faster cloning time\nthan speaker adaption, which can offer slightly greater naturalness and\nsimilarity. The main goal is to create a vocal cloning system that produces\naudio output with a Nepali accent or that sounds like Nepali. For the further\nadvancement of TTS, the idea of transfer learning was effectively used to\naddress several issues that were encountered in the development of this system,\nincluding the poor audio quality and the lack of available data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Voice cloning is a prominent feature in personalized speech interfaces. A\nneural vocal cloning system can mimic someone's voice using just a few audio\nsamples. Both speaker encoding and speaker adaptation are topics of research in\nthe field of voice cloning. Speaker adaptation relies on fine-tuning a\nmulti-speaker generative model, which involves training a separate model to\ninfer a new speaker embedding used for speaker encoding. Both methods can\nachieve excellent performance, even with a small number of cloning audios, in\nterms of the speech's naturalness and similarity to the original speaker.\nSpeaker encoding approaches are more appropriate for low-resource deployment\nsince they require significantly less memory and have a faster cloning time\nthan speaker adaption, which can offer slightly greater naturalness and\nsimilarity. The main goal is to create a vocal cloning system that produces\naudio output with a Nepali accent or that sounds like Nepali. For the further\nadvancement of TTS, the idea of transfer learning was effectively used to\naddress several issues that were encountered in the development of this system,\nincluding the poor audio quality and the lack of available data."
                },
                "authors": [
                    {
                        "name": "Manjil Karki"
                    },
                    {
                        "name": "Pratik Shakya"
                    },
                    {
                        "name": "Sandesh Acharya"
                    },
                    {
                        "name": "Ravi Pandit"
                    },
                    {
                        "name": "Dinesh Gothe"
                    }
                ],
                "author_detail": {
                    "name": "Dinesh Gothe"
                },
                "author": "Dinesh Gothe",
                "arxiv_comment": "6 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10128v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10128v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "91F20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19094v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19094v2",
                "updated": "2024-08-23T16:06:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    16,
                    6,
                    27,
                    4,
                    236,
                    0
                ],
                "published": "2024-07-26T21:18:57Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    21,
                    18,
                    57,
                    4,
                    208,
                    0
                ],
                "title": "Solving Robotics Problems in Zero-Shot with Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving Robotics Problems in Zero-Shot with Vision-Language Models"
                },
                "summary": "We introduce Wonderful Team, a multi-agent visual LLM (VLLM) framework for\nsolving robotics problems in the zero-shot regime. By zero-shot we mean that,\nfor a novel environment, we feed a VLLM an image of the robot's environment and\na description of the task, and have the VLLM output the sequence of actions\nnecessary for the robot to complete the task. Prior work on VLLMs in robotics\nhas largely focused on settings where some part of the pipeline is fine-tuned,\nsuch as tuning an LLM on robot data or training a separate vision encoder for\nperception and action generation. Surprisingly, due to recent advances in the\ncapabilities of VLLMs, this type of fine-tuning may no longer be necessary for\nmany tasks. In this work, we show that with careful engineering, we can prompt\na single off-the-shelf VLLM to handle all aspects of a robotics task, from\nhigh-level planning to low-level location-extraction and action-execution.\nWonderful Team builds on recent advances in multi-agent LLMs to partition tasks\nacross an agent hierarchy, making it self-corrective and able to effectively\npartition and solve even long-horizon tasks. Extensive experiments on VIMABench\nand real-world robotic environments demonstrate the system's capability to\nhandle a variety of robotic tasks, including manipulation, visual\ngoal-reaching, and visual reasoning, all in a zero-shot manner. These results\nunderscore a key point: vision-language models have progressed rapidly in the\npast year, and should strongly be considered as a backbone for robotics\nproblems going forward.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Wonderful Team, a multi-agent visual LLM (VLLM) framework for\nsolving robotics problems in the zero-shot regime. By zero-shot we mean that,\nfor a novel environment, we feed a VLLM an image of the robot's environment and\na description of the task, and have the VLLM output the sequence of actions\nnecessary for the robot to complete the task. Prior work on VLLMs in robotics\nhas largely focused on settings where some part of the pipeline is fine-tuned,\nsuch as tuning an LLM on robot data or training a separate vision encoder for\nperception and action generation. Surprisingly, due to recent advances in the\ncapabilities of VLLMs, this type of fine-tuning may no longer be necessary for\nmany tasks. In this work, we show that with careful engineering, we can prompt\na single off-the-shelf VLLM to handle all aspects of a robotics task, from\nhigh-level planning to low-level location-extraction and action-execution.\nWonderful Team builds on recent advances in multi-agent LLMs to partition tasks\nacross an agent hierarchy, making it self-corrective and able to effectively\npartition and solve even long-horizon tasks. Extensive experiments on VIMABench\nand real-world robotic environments demonstrate the system's capability to\nhandle a variety of robotic tasks, including manipulation, visual\ngoal-reaching, and visual reasoning, all in a zero-shot manner. These results\nunderscore a key point: vision-language models have progressed rapidly in the\npast year, and should strongly be considered as a backbone for robotics\nproblems going forward."
                },
                "authors": [
                    {
                        "name": "Zidan Wang"
                    },
                    {
                        "name": "Rui Shen"
                    },
                    {
                        "name": "Bradly Stadie"
                    }
                ],
                "author_detail": {
                    "name": "Bradly Stadie"
                },
                "author": "Bradly Stadie",
                "arxiv_comment": "aka Wonderful Team",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19094v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19094v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13184v1",
                "updated": "2024-08-23T16:02:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    16,
                    2,
                    54,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T16:02:54Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    16,
                    2,
                    54,
                    4,
                    236,
                    0
                ],
                "title": "Can LLM be a Good Path Planner based on Prompt Engineering? Mitigating\n  the Hallucination for Path Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLM be a Good Path Planner based on Prompt Engineering? Mitigating\n  the Hallucination for Path Planning"
                },
                "summary": "Spatial reasoning in Large Language Models (LLMs) is the foundation for\nembodied intelligence. However, even in simple maze environments, LLMs still\nencounter challenges in long-term path-planning, primarily influenced by their\nspatial hallucination and context inconsistency hallucination by long-term\nreasoning. To address this challenge, this study proposes an innovative model,\nSpatial-to-Relational Transformation and Curriculum Q-Learning (S2RCQL). To\naddress the spatial hallucination of LLMs, we propose the Spatial-to-Relational\napproach, which transforms spatial prompts into entity relations and paths\nrepresenting entity relation chains. This approach fully taps the potential of\nLLMs in terms of sequential thinking. As a result, we design a path-planning\nalgorithm based on Q-learning to mitigate the context inconsistency\nhallucination, which enhances the reasoning ability of LLMs. Using the Q-value\nof state-action as auxiliary information for prompts, we correct the\nhallucinations of LLMs, thereby guiding LLMs to learn the optimal path.\nFinally, we propose a reverse curriculum learning technique based on LLMs to\nfurther mitigate the context inconsistency hallucination. LLMs can rapidly\naccumulate successful experiences by reducing task difficulty and leveraging\nthem to tackle more complex tasks. We performed comprehensive experiments based\non Baidu's self-developed LLM: ERNIE-Bot 4.0. The results showed that our\nS2RCQL achieved a 23%--40% improvement in both success and optimality rates\ncompared with advanced prompt engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial reasoning in Large Language Models (LLMs) is the foundation for\nembodied intelligence. However, even in simple maze environments, LLMs still\nencounter challenges in long-term path-planning, primarily influenced by their\nspatial hallucination and context inconsistency hallucination by long-term\nreasoning. To address this challenge, this study proposes an innovative model,\nSpatial-to-Relational Transformation and Curriculum Q-Learning (S2RCQL). To\naddress the spatial hallucination of LLMs, we propose the Spatial-to-Relational\napproach, which transforms spatial prompts into entity relations and paths\nrepresenting entity relation chains. This approach fully taps the potential of\nLLMs in terms of sequential thinking. As a result, we design a path-planning\nalgorithm based on Q-learning to mitigate the context inconsistency\nhallucination, which enhances the reasoning ability of LLMs. Using the Q-value\nof state-action as auxiliary information for prompts, we correct the\nhallucinations of LLMs, thereby guiding LLMs to learn the optimal path.\nFinally, we propose a reverse curriculum learning technique based on LLMs to\nfurther mitigate the context inconsistency hallucination. LLMs can rapidly\naccumulate successful experiences by reducing task difficulty and leveraging\nthem to tackle more complex tasks. We performed comprehensive experiments based\non Baidu's self-developed LLM: ERNIE-Bot 4.0. The results showed that our\nS2RCQL achieved a 23%--40% improvement in both success and optimality rates\ncompared with advanced prompt engineering."
                },
                "authors": [
                    {
                        "name": "Hourui Deng"
                    },
                    {
                        "name": "Hongjie Zhang"
                    },
                    {
                        "name": "Jie Ou"
                    },
                    {
                        "name": "Chaosheng Feng"
                    }
                ],
                "author_detail": {
                    "name": "Chaosheng Feng"
                },
                "author": "Chaosheng Feng",
                "arxiv_comment": "Submitted to ICASSP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11480v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11480v2",
                "updated": "2024-08-23T15:59:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    15,
                    59,
                    16,
                    4,
                    236,
                    0
                ],
                "published": "2024-04-17T15:33:45Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    15,
                    33,
                    45,
                    2,
                    108,
                    0
                ],
                "title": "Impact of lensing of gravitational waves on the observed distribution of\n  neutron star masses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impact of lensing of gravitational waves on the observed distribution of\n  neutron star masses"
                },
                "summary": "The distribution of masses of neutron stars, particularly the maximum mass\nvalue, is considered a probe of their formation, evolution and internal physics\n(i.e., equation of state). This mass distribution could in principle be\ninferred from the detection of gravitational waves from binary neutron star\nmergers. Using mock catalogues of $10^5$ dark sirens events, expected to be\ndetected by Einstein Telescope over an operational period of $\\sim1\\, \\rm year$\n, we show how the biased luminosity distance measurement induced by\ngravitational lensing affects the inferred redshift and mass of the merger.\nThis results in higher observed masses than expected. Up to $2\\%$ of the events\nare predicted to fall above the maximum allowed neutron star mass depending on\nthe intrinsic mass distribution and signal-to-noise ratio threshold adopted.\nThe underlying true mass distribution and maximum mass could still be\napproximately recovered in the case of bright standard sirens.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The distribution of masses of neutron stars, particularly the maximum mass\nvalue, is considered a probe of their formation, evolution and internal physics\n(i.e., equation of state). This mass distribution could in principle be\ninferred from the detection of gravitational waves from binary neutron star\nmergers. Using mock catalogues of $10^5$ dark sirens events, expected to be\ndetected by Einstein Telescope over an operational period of $\\sim1\\, \\rm year$\n, we show how the biased luminosity distance measurement induced by\ngravitational lensing affects the inferred redshift and mass of the merger.\nThis results in higher observed masses than expected. Up to $2\\%$ of the events\nare predicted to fall above the maximum allowed neutron star mass depending on\nthe intrinsic mass distribution and signal-to-noise ratio threshold adopted.\nThe underlying true mass distribution and maximum mass could still be\napproximately recovered in the case of bright standard sirens."
                },
                "authors": [
                    {
                        "name": "Sofia Canevarolo"
                    },
                    {
                        "name": "Loek van Vonderen"
                    },
                    {
                        "name": "Nora Elisa Chisari"
                    }
                ],
                "author_detail": {
                    "name": "Nora Elisa Chisari"
                },
                "author": "Nora Elisa Chisari",
                "arxiv_doi": "10.33232/001c.122856",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.33232/001c.122856",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.11480v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11480v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "9 pages, 5 figures, 1 table, accepted for publication in the OJA",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13162v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13162v1",
                "updated": "2024-08-23T15:35:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    15,
                    35,
                    37,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T15:35:37Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    15,
                    35,
                    37,
                    4,
                    236,
                    0
                ],
                "title": "A latent space model for multivariate count data time series analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A latent space model for multivariate count data time series analysis"
                },
                "summary": "Motivated by a dataset of burglaries in Chicago, USA, we introduce a novel\nframework to analyze time series of count data combining common multivariate\ntime series models with latent position network models. This novel methodology\nallows us to gain a new latent variable perspective on the crime dataset that\nwe consider, allowing us to disentangle and explain the complex patterns\nexhibited by the data, while providing a natural time series framework that can\nbe used to make future predictions. Our model is underpinned by two well known\nstatistical approaches: a log-linear vector autoregressive model, which is\nprominent in the literature on multivariate count time series, and a latent\nprojection model, which is a popular latent variable model for networks. The\nrole of the projection model is to characterize the interaction parameters of\nthe vector autoregressive model, thus uncovering the underlying network that is\nassociated with the pairwise relationships between the time series. Estimation\nand inferential procedures are performed using an optimization algorithm and a\nHamiltonian Monte Carlo procedure for efficient Bayesian inference. We also\ninclude a simulation study to illustrate the merits of our methodology in\nrecovering consistent parameter estimates, and in making accurate future\npredictions for the time series. As we demonstrate in our application to the\ncrime dataset, this new methodology can provide very meaningful model-based\ninterpretations of the data, and it can be generalized to other time series\ncontexts and applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by a dataset of burglaries in Chicago, USA, we introduce a novel\nframework to analyze time series of count data combining common multivariate\ntime series models with latent position network models. This novel methodology\nallows us to gain a new latent variable perspective on the crime dataset that\nwe consider, allowing us to disentangle and explain the complex patterns\nexhibited by the data, while providing a natural time series framework that can\nbe used to make future predictions. Our model is underpinned by two well known\nstatistical approaches: a log-linear vector autoregressive model, which is\nprominent in the literature on multivariate count time series, and a latent\nprojection model, which is a popular latent variable model for networks. The\nrole of the projection model is to characterize the interaction parameters of\nthe vector autoregressive model, thus uncovering the underlying network that is\nassociated with the pairwise relationships between the time series. Estimation\nand inferential procedures are performed using an optimization algorithm and a\nHamiltonian Monte Carlo procedure for efficient Bayesian inference. We also\ninclude a simulation study to illustrate the merits of our methodology in\nrecovering consistent parameter estimates, and in making accurate future\npredictions for the time series. As we demonstrate in our application to the\ncrime dataset, this new methodology can provide very meaningful model-based\ninterpretations of the data, and it can be generalized to other time series\ncontexts and applications."
                },
                "authors": [
                    {
                        "name": "Hardeep Kaur"
                    },
                    {
                        "name": "Riccardo Rastelli"
                    }
                ],
                "author_detail": {
                    "name": "Riccardo Rastelli"
                },
                "author": "Riccardo Rastelli",
                "arxiv_comment": "36 pages, 21 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13162v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13162v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13161v1",
                "updated": "2024-08-23T15:34:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    15,
                    34,
                    33,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T15:34:33Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    15,
                    34,
                    33,
                    4,
                    236,
                    0
                ],
                "title": "Say No to Freeloader: Protecting Intellectual Property of Your Deep\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Say No to Freeloader: Protecting Intellectual Property of Your Deep\n  Model"
                },
                "summary": "Model intellectual property (IP) protection has attracted growing attention\nas science and technology advancements stem from human intellectual labor and\ncomputational expenses. Ensuring IP safety for trainers and owners is of utmost\nimportance, particularly in domains where ownership verification and\napplicability authorization are required. A notable approach to safeguarding\nmodel IP involves proactively preventing the use of well-trained models of\nauthorized domains from unauthorized domains. In this paper, we introduce a\nnovel Compact Un-transferable Pyramid Isolation Domain (CUPI-Domain) which\nserves as a barrier against illegal transfers from authorized to unauthorized\ndomains. Drawing inspiration from human transitive inference and learning\nabilities, the CUPI-Domain is designed to obstruct cross-domain transfers by\nemphasizing the distinctive style features of the authorized domain. This\nemphasis leads to failure in recognizing irrelevant private style features on\nunauthorized domains. To this end, we propose novel CUPI-Domain generators,\nwhich select features from both authorized and CUPI-Domain as anchors. Then, we\nfuse the style features and semantic features of these anchors to generate\nlabeled and style-rich CUPI-Domain. Additionally, we design external\nDomain-Information Memory Banks (DIMB) for storing and updating labeled pyramid\nfeatures to obtain stable domain class features and domain class-wise style\nfeatures. Based on the proposed whole method, the novel style and\ndiscriminative loss functions are designed to effectively enhance the\ndistinction in style and discriminative features between authorized and\nunauthorized domains, respectively. Moreover, we provide two solutions for\nutilizing CUPI-Domain based on whether the unauthorized domain is known:\ntarget-specified CUPI-Domain and target-free CUPI-Domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model intellectual property (IP) protection has attracted growing attention\nas science and technology advancements stem from human intellectual labor and\ncomputational expenses. Ensuring IP safety for trainers and owners is of utmost\nimportance, particularly in domains where ownership verification and\napplicability authorization are required. A notable approach to safeguarding\nmodel IP involves proactively preventing the use of well-trained models of\nauthorized domains from unauthorized domains. In this paper, we introduce a\nnovel Compact Un-transferable Pyramid Isolation Domain (CUPI-Domain) which\nserves as a barrier against illegal transfers from authorized to unauthorized\ndomains. Drawing inspiration from human transitive inference and learning\nabilities, the CUPI-Domain is designed to obstruct cross-domain transfers by\nemphasizing the distinctive style features of the authorized domain. This\nemphasis leads to failure in recognizing irrelevant private style features on\nunauthorized domains. To this end, we propose novel CUPI-Domain generators,\nwhich select features from both authorized and CUPI-Domain as anchors. Then, we\nfuse the style features and semantic features of these anchors to generate\nlabeled and style-rich CUPI-Domain. Additionally, we design external\nDomain-Information Memory Banks (DIMB) for storing and updating labeled pyramid\nfeatures to obtain stable domain class features and domain class-wise style\nfeatures. Based on the proposed whole method, the novel style and\ndiscriminative loss functions are designed to effectively enhance the\ndistinction in style and discriminative features between authorized and\nunauthorized domains, respectively. Moreover, we provide two solutions for\nutilizing CUPI-Domain based on whether the unauthorized domain is known:\ntarget-specified CUPI-Domain and target-free CUPI-Domain."
                },
                "authors": [
                    {
                        "name": "Lianyu Wang"
                    },
                    {
                        "name": "Meng Wang"
                    },
                    {
                        "name": "Huazhu Fu"
                    },
                    {
                        "name": "Daoqiang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Daoqiang Zhang"
                },
                "author": "Daoqiang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04180v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04180v2",
                "updated": "2024-08-23T15:29:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    15,
                    29,
                    5,
                    4,
                    236,
                    0
                ],
                "published": "2023-12-07T10:06:34Z",
                "published_parsed": [
                    2023,
                    12,
                    7,
                    10,
                    6,
                    34,
                    3,
                    341,
                    0
                ],
                "title": "AI and Jobs: Has the Inflection Point Arrived? Evidence from an Online\n  Labor Platform",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI and Jobs: Has the Inflection Point Arrived? Evidence from an Online\n  Labor Platform"
                },
                "summary": "The emergence of Large Language Models (LLMs) has renewed the debate on the\nimportant issue of \"technology displacement\". While prior research has\ninvestigated the effect of information technology in general on human labor\nfrom a macro perspective, this paper complements the literature by examining\nthe impact of LLMs on freelancers from a micro perspective. Specifically, we\nleverage the release of ChatGPT to investigate how AI influences freelancers\nacross different online labor markets (OLMs). Employing the\nDifference-in-Differences method, we discovered two distinct scenarios\nfollowing ChatGPT's release: 1) the displacement effect of LLMs, featuring\nreduced work volume and earnings, as is exemplified by the translation &\nlocalization OLM; 2) the productivity effect of LLMs, featuring increased work\nvolume and earnings, as is exemplified by the web development OLM. To shed\nlight on the underlying mechanisms, we developed a Cournot-type competition\nmodel to highlight the existence of an inflection point for each occupation\nwhich separates the timeline of AI progress into a honeymoon phase and a\nsubstitution phase. Before AI performance crosses the inflection point, human\nlabor benefits each time AI improves, resulting in the honeymoon phase.\nHowever, after AI performance crosses the inflection point, additional AI\nenhancement hurts human labor. Further analyzing the progression from ChatGPT\n3.5 to 4.0, we found three effect scenarios (i.e., productivity to\nproductivity, displacement to displacement, and productivity to displacement),\nconsistent with the inflection point conjecture. Heterogeneous analyses reveal\nthat U.S. web developers tend to benefit more from the release of ChatGPT\ncompared to their counterparts in other regions, and somewhat surprisingly,\nexperienced translators seem more likely to exit the market than less\nexperienced translators after the release of ChatGPT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of Large Language Models (LLMs) has renewed the debate on the\nimportant issue of \"technology displacement\". While prior research has\ninvestigated the effect of information technology in general on human labor\nfrom a macro perspective, this paper complements the literature by examining\nthe impact of LLMs on freelancers from a micro perspective. Specifically, we\nleverage the release of ChatGPT to investigate how AI influences freelancers\nacross different online labor markets (OLMs). Employing the\nDifference-in-Differences method, we discovered two distinct scenarios\nfollowing ChatGPT's release: 1) the displacement effect of LLMs, featuring\nreduced work volume and earnings, as is exemplified by the translation &\nlocalization OLM; 2) the productivity effect of LLMs, featuring increased work\nvolume and earnings, as is exemplified by the web development OLM. To shed\nlight on the underlying mechanisms, we developed a Cournot-type competition\nmodel to highlight the existence of an inflection point for each occupation\nwhich separates the timeline of AI progress into a honeymoon phase and a\nsubstitution phase. Before AI performance crosses the inflection point, human\nlabor benefits each time AI improves, resulting in the honeymoon phase.\nHowever, after AI performance crosses the inflection point, additional AI\nenhancement hurts human labor. Further analyzing the progression from ChatGPT\n3.5 to 4.0, we found three effect scenarios (i.e., productivity to\nproductivity, displacement to displacement, and productivity to displacement),\nconsistent with the inflection point conjecture. Heterogeneous analyses reveal\nthat U.S. web developers tend to benefit more from the release of ChatGPT\ncompared to their counterparts in other regions, and somewhat surprisingly,\nexperienced translators seem more likely to exit the market than less\nexperienced translators after the release of ChatGPT."
                },
                "authors": [
                    {
                        "name": "Dandan Qiao"
                    },
                    {
                        "name": "Huaxia Rui"
                    },
                    {
                        "name": "Qian Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Qian Xiong"
                },
                "author": "Qian Xiong",
                "arxiv_comment": "41 pages, 3 figures, 13 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.04180v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04180v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13131v1",
                "updated": "2024-08-23T14:57:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    14,
                    57,
                    46,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T14:57:46Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    14,
                    57,
                    46,
                    4,
                    236,
                    0
                ],
                "title": "DeTPP: Leveraging Object Detection for Robust Long-Horizon Event\n  Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeTPP: Leveraging Object Detection for Robust Long-Horizon Event\n  Prediction"
                },
                "summary": "Forecasting future events over extended periods, known as long-horizon\nprediction, is a fundamental task in various domains, including retail,\nfinance, healthcare, and social networks. Traditional methods, such as Marked\nTemporal Point Processes (MTPP), typically use autoregressive models to predict\nmultiple future events. However, these models frequently encounter issues such\nas converging to constant or repetitive outputs, which significantly limits\ntheir effectiveness and applicability. To overcome these limitations, we\npropose DeTPP (Detection-based Temporal Point Processes), a novel approach\ninspired by object detection methods from computer vision. DeTPP utilizes a\nnovel matching-based loss function that selectively focuses on reliably\npredictable events, enhancing both training robustness and inference diversity.\nOur method sets a new state-of-the-art in long-horizon event prediction,\nsignificantly outperforming existing MTPP and next-K approaches. The\nimplementation of DeTPP is publicly available on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting future events over extended periods, known as long-horizon\nprediction, is a fundamental task in various domains, including retail,\nfinance, healthcare, and social networks. Traditional methods, such as Marked\nTemporal Point Processes (MTPP), typically use autoregressive models to predict\nmultiple future events. However, these models frequently encounter issues such\nas converging to constant or repetitive outputs, which significantly limits\ntheir effectiveness and applicability. To overcome these limitations, we\npropose DeTPP (Detection-based Temporal Point Processes), a novel approach\ninspired by object detection methods from computer vision. DeTPP utilizes a\nnovel matching-based loss function that selectively focuses on reliably\npredictable events, enhancing both training robustness and inference diversity.\nOur method sets a new state-of-the-art in long-horizon event prediction,\nsignificantly outperforming existing MTPP and next-K approaches. The\nimplementation of DeTPP is publicly available on GitHub."
                },
                "authors": [
                    {
                        "name": "Ivan Karpukhin"
                    },
                    {
                        "name": "Andrey Savchenko"
                    }
                ],
                "author_detail": {
                    "name": "Andrey Savchenko"
                },
                "author": "Andrey Savchenko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13129v1",
                "updated": "2024-08-23T14:55:39Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    14,
                    55,
                    39,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T14:55:39Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    14,
                    55,
                    39,
                    4,
                    236,
                    0
                ],
                "title": "Optical activation function using a metamaterial waveguide for an\n  all-optical neural network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optical activation function using a metamaterial waveguide for an\n  all-optical neural network"
                },
                "summary": "In this study, we experimentally demonstrated that the nonlinear optical\ncoefficient of the original Si can be enhanced by incorporating a metamaterial\nstructure into an existing silicon waveguide. The two-photon absorption\ncoefficient enhanced by the metamaterial structure was 424 cm/GW, which is\n1.2x10^3 times higher than that of Si. Using this metamaterial waveguide-based\nnonlinear optical activation function, we achieved a high inference accuracy of\n98.36% in the handwritten character recognition task, comparable to that\nobtained with the ReLU function as the activation function. Therefore, our\napproach can contribute to the realization of more power-efficient and compact\nall-optical neural networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we experimentally demonstrated that the nonlinear optical\ncoefficient of the original Si can be enhanced by incorporating a metamaterial\nstructure into an existing silicon waveguide. The two-photon absorption\ncoefficient enhanced by the metamaterial structure was 424 cm/GW, which is\n1.2x10^3 times higher than that of Si. Using this metamaterial waveguide-based\nnonlinear optical activation function, we achieved a high inference accuracy of\n98.36% in the handwritten character recognition task, comparable to that\nobtained with the ReLU function as the activation function. Therefore, our\napproach can contribute to the realization of more power-efficient and compact\nall-optical neural networks."
                },
                "authors": [
                    {
                        "name": "Yoshihiro Honda"
                    },
                    {
                        "name": "Yuya Shoji"
                    },
                    {
                        "name": "Tomohiro Amemiya"
                    }
                ],
                "author_detail": {
                    "name": "Tomohiro Amemiya"
                },
                "author": "Tomohiro Amemiya",
                "arxiv_comment": "4 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13123v1",
                "updated": "2024-08-23T14:50:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    14,
                    50,
                    49,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T14:50:49Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    14,
                    50,
                    49,
                    4,
                    236,
                    0
                ],
                "title": "Evidential Deep Partial Multi-View Classification With Discount Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evidential Deep Partial Multi-View Classification With Discount Fusion"
                },
                "summary": "Incomplete multi-view data classification poses significant challenges due to\nthe common issue of missing views in real-world scenarios. Despite\nadvancements, existing methods often fail to provide reliable predictions,\nlargely due to the uncertainty of missing views and the inconsistent quality of\nimputed data. To tackle these problems, we propose a novel framework called\nEvidential Deep Partial Multi-View Classification (EDP-MVC). Initially, we use\nK-means imputation to address missing views, creating a complete set of\nmulti-view data. However, the potential conflicts and uncertainties within this\nimputed data can affect the reliability of downstream inferences. To manage\nthis, we introduce a Conflict-Aware Evidential Fusion Network (CAEFN), which\ndynamically adjusts based on the reliability of the evidence, ensuring\ntrustworthy discount fusion and producing reliable inference outcomes.\nComprehensive experiments on various benchmark datasets reveal EDP-MVC not only\nmatches but often surpasses the performance of state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incomplete multi-view data classification poses significant challenges due to\nthe common issue of missing views in real-world scenarios. Despite\nadvancements, existing methods often fail to provide reliable predictions,\nlargely due to the uncertainty of missing views and the inconsistent quality of\nimputed data. To tackle these problems, we propose a novel framework called\nEvidential Deep Partial Multi-View Classification (EDP-MVC). Initially, we use\nK-means imputation to address missing views, creating a complete set of\nmulti-view data. However, the potential conflicts and uncertainties within this\nimputed data can affect the reliability of downstream inferences. To manage\nthis, we introduce a Conflict-Aware Evidential Fusion Network (CAEFN), which\ndynamically adjusts based on the reliability of the evidence, ensuring\ntrustworthy discount fusion and producing reliable inference outcomes.\nComprehensive experiments on various benchmark datasets reveal EDP-MVC not only\nmatches but often surpasses the performance of state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Haojian Huang"
                    },
                    {
                        "name": "Zhe Liu"
                    },
                    {
                        "name": "Sukumar Letchmunan"
                    },
                    {
                        "name": "Mingwei Lin"
                    },
                    {
                        "name": "Muhammet Deveci"
                    },
                    {
                        "name": "Witold Pedrycz"
                    },
                    {
                        "name": "Patrick Siarry"
                    }
                ],
                "author_detail": {
                    "name": "Patrick Siarry"
                },
                "author": "Patrick Siarry",
                "arxiv_comment": "Ongoing work. 13 pages, 3 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.11449v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.11449v2",
                "updated": "2024-08-23T14:50:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    14,
                    50,
                    29,
                    4,
                    236,
                    0
                ],
                "published": "2023-04-22T17:10:28Z",
                "published_parsed": [
                    2023,
                    4,
                    22,
                    17,
                    10,
                    28,
                    5,
                    112,
                    0
                ],
                "title": "Posterior Sampling in High Dimension via Diffusion Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Posterior Sampling in High Dimension via Diffusion Processes"
                },
                "summary": "Sampling from the posterior is a key technical problem in Bayesian\nstatistics. Rigorous guarantees are difficult to obtain for Markov Chain Monte\nCarlo algorithms of common use. In this paper, we study an alternative class of\nalgorithms based on diffusion processes and variational methods. The diffusion\nis constructed in such a way that, at its final time, it approximates the\ntarget posterior distribution. The drift of this diffusion is given by the\nposterior expectation of the unknown parameter vector ${\\boldsymbol \\theta}$\ngiven the data and the additional noisy observations.\n  In order to construct an efficient sampling algorithm, we use a simple Euler\ndiscretization of the diffusion process, and leverage message passing\nalgorithms and variational inference techniques to approximate the posterior\nexpectation oracle.\n  We apply this method to posterior sampling in two canonical problems in\nhigh-dimensional statistics: sparse regression and low-rank matrix estimation\nwithin the spiked model. In both cases we develop the first algorithms with\naccuracy guarantees in the regime of constant signal-to-noise ratios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sampling from the posterior is a key technical problem in Bayesian\nstatistics. Rigorous guarantees are difficult to obtain for Markov Chain Monte\nCarlo algorithms of common use. In this paper, we study an alternative class of\nalgorithms based on diffusion processes and variational methods. The diffusion\nis constructed in such a way that, at its final time, it approximates the\ntarget posterior distribution. The drift of this diffusion is given by the\nposterior expectation of the unknown parameter vector ${\\boldsymbol \\theta}$\ngiven the data and the additional noisy observations.\n  In order to construct an efficient sampling algorithm, we use a simple Euler\ndiscretization of the diffusion process, and leverage message passing\nalgorithms and variational inference techniques to approximate the posterior\nexpectation oracle.\n  We apply this method to posterior sampling in two canonical problems in\nhigh-dimensional statistics: sparse regression and low-rank matrix estimation\nwithin the spiked model. In both cases we develop the first algorithms with\naccuracy guarantees in the regime of constant signal-to-noise ratios."
                },
                "authors": [
                    {
                        "name": "Andrea Montanari"
                    },
                    {
                        "name": "Yuchen Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yuchen Wu"
                },
                "author": "Yuchen Wu",
                "arxiv_comment": "70 pages; 3 pdf figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.11449v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.11449v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.01230v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.01230v4",
                "updated": "2024-08-23T14:17:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    14,
                    17,
                    24,
                    4,
                    236,
                    0
                ],
                "published": "2023-08-02T15:38:52Z",
                "published_parsed": [
                    2023,
                    8,
                    2,
                    15,
                    38,
                    52,
                    2,
                    214,
                    0
                ],
                "title": "JADES. The diverse population of infant Black Holes at 4<z<11: merging,\n  tiny, poor, but mighty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JADES. The diverse population of infant Black Holes at 4<z<11: merging,\n  tiny, poor, but mighty"
                },
                "summary": "We present 12 new AGN at 4<z<7 in the JADES survey (in addition to the\npreviously identified AGN in GN-z11 at z=10.6) revealed through the detection\nof a Broad Line Region as seen in Halpha. The depth of JADES, together with the\nuse of three different spectral resolutions, enables us to probe a lower mass\nregime relative to previous studies. In a few cases we find evidence for two\nbroad components of Halpha which suggests that these could be candidate merging\nblack holes (BHs). The inferred BH masses range between 8 x 10^7 Msun down to 4\nx 10^5 Msun, interestingly probing the regime expected for Direct Collapse\nBlack Holes (DCBHs). The inferred AGN bolometric luminosities (~10^44-10^45\nerg/s) imply accretion rates that are < 0.5 times the Eddington rate in most\ncases. However, small BHs, with M_BH ~ 10^6 Msun, tend to accrete at Eddington\nor super-Eddington rates. These BH at z~4-11 are over-massive relative to their\nhost galaxies stellar masses when compared to the local M_BH-Mstar relation,\nand even approaching M_BH~Mstar, as expected for DCBHs and super-Eddington\nscenarios. However, we find that these early BHs tend to be more consistent\nwith the local relation between M_BH and velocity dispersion, as well as\nbetween M_BH and dynamical mass, suggesting that these are more fundamental and\nuniversal relations. On the BPT excitation-diagnostic diagram these AGN are\nlocated in the region that is that is locally occupied by star-forming\ngalaxies, implying that they would be missed by the standard classification\ntechniques if they did not display broad lines. Their location on the diagram\nis consistent with what expected for AGN hosted in metal poor galaxies (Z ~\n0.1-0.2 Zsun). The fraction of broad line AGN with L_AGN > 10^44 erg/s, among\ngalaxies in the redshift range 4<z<6, is about 10%, suggesting that the\ncontribution of AGN and their hosts to the reionization of the Universe is >\n10%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present 12 new AGN at 4<z<7 in the JADES survey (in addition to the\npreviously identified AGN in GN-z11 at z=10.6) revealed through the detection\nof a Broad Line Region as seen in Halpha. The depth of JADES, together with the\nuse of three different spectral resolutions, enables us to probe a lower mass\nregime relative to previous studies. In a few cases we find evidence for two\nbroad components of Halpha which suggests that these could be candidate merging\nblack holes (BHs). The inferred BH masses range between 8 x 10^7 Msun down to 4\nx 10^5 Msun, interestingly probing the regime expected for Direct Collapse\nBlack Holes (DCBHs). The inferred AGN bolometric luminosities (~10^44-10^45\nerg/s) imply accretion rates that are < 0.5 times the Eddington rate in most\ncases. However, small BHs, with M_BH ~ 10^6 Msun, tend to accrete at Eddington\nor super-Eddington rates. These BH at z~4-11 are over-massive relative to their\nhost galaxies stellar masses when compared to the local M_BH-Mstar relation,\nand even approaching M_BH~Mstar, as expected for DCBHs and super-Eddington\nscenarios. However, we find that these early BHs tend to be more consistent\nwith the local relation between M_BH and velocity dispersion, as well as\nbetween M_BH and dynamical mass, suggesting that these are more fundamental and\nuniversal relations. On the BPT excitation-diagnostic diagram these AGN are\nlocated in the region that is that is locally occupied by star-forming\ngalaxies, implying that they would be missed by the standard classification\ntechniques if they did not display broad lines. Their location on the diagram\nis consistent with what expected for AGN hosted in metal poor galaxies (Z ~\n0.1-0.2 Zsun). The fraction of broad line AGN with L_AGN > 10^44 erg/s, among\ngalaxies in the redshift range 4<z<6, is about 10%, suggesting that the\ncontribution of AGN and their hosts to the reionization of the Universe is >\n10%."
                },
                "authors": [
                    {
                        "name": "Roberto Maiolino"
                    },
                    {
                        "name": "Jan Scholtz"
                    },
                    {
                        "name": "Emma Curtis-Lake"
                    },
                    {
                        "name": "Stefano Carniani"
                    },
                    {
                        "name": "William Baker"
                    },
                    {
                        "name": "Anna de Graaff"
                    },
                    {
                        "name": "Sandro Tacchella"
                    },
                    {
                        "name": "Hannah √úbler"
                    },
                    {
                        "name": "Francesco D'Eugenio"
                    },
                    {
                        "name": "Joris Witstok"
                    },
                    {
                        "name": "Mirko Curti"
                    },
                    {
                        "name": "Santiago Arribas"
                    },
                    {
                        "name": "Andrew J. Bunker"
                    },
                    {
                        "name": "St√©phane Charlot"
                    },
                    {
                        "name": "Jacopo Chevallard"
                    },
                    {
                        "name": "Daniel J. Eisenstein"
                    },
                    {
                        "name": "Eiichi Egami"
                    },
                    {
                        "name": "Zhiyuan Ji"
                    },
                    {
                        "name": "Gareth C. Jones"
                    },
                    {
                        "name": "Jianwei Lyu"
                    },
                    {
                        "name": "Tim Rawle"
                    },
                    {
                        "name": "Brant Robertson"
                    },
                    {
                        "name": "Wiphu Rujopakarn"
                    },
                    {
                        "name": "Michele Perna"
                    },
                    {
                        "name": "Fengwu Sun"
                    },
                    {
                        "name": "Giacomo Venturi"
                    },
                    {
                        "name": "Christina C. Williams"
                    },
                    {
                        "name": "Chris Willott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Willott"
                },
                "author": "Chris Willott",
                "arxiv_comment": "Accepted for publication in A&A. 26 pages, 13 figures, 4 tables.\n  Replaced with the accepted version (minor changes)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.01230v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.01230v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13092v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13092v1",
                "updated": "2024-08-23T14:17:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    14,
                    17,
                    17,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T14:17:17Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    14,
                    17,
                    17,
                    4,
                    236,
                    0
                ],
                "title": "Diffusion-based Episodes Augmentation for Offline Multi-Agent\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based Episodes Augmentation for Offline Multi-Agent\n  Reinforcement Learning"
                },
                "summary": "Offline multi-agent reinforcement learning (MARL) is increasingly recognized\nas crucial for effectively deploying RL algorithms in environments where\nreal-time interaction is impractical, risky, or costly. In the offline setting,\nlearning from a static dataset of past interactions allows for the development\nof robust and safe policies without the need for live data collection, which\ncan be fraught with challenges. Building on this foundational importance, we\npresent EAQ, Episodes Augmentation guided by Q-total loss, a novel approach for\noffline MARL framework utilizing diffusion models. EAQ integrates the Q-total\nfunction directly into the diffusion model as a guidance to maximize the global\nreturns in an episode, eliminating the need for separate training. Our focus\nprimarily lies on cooperative scenarios, where agents are required to act\ncollectively towards achieving a shared goal-essentially, maximizing global\nreturns. Consequently, we demonstrate that our episodes augmentation in a\ncollaborative manner significantly boosts offline MARL algorithm compared to\nthe original dataset, improving the normalized return by +17.3% and +12.9% for\nmedium and poor behavioral policies in SMAC simulator, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offline multi-agent reinforcement learning (MARL) is increasingly recognized\nas crucial for effectively deploying RL algorithms in environments where\nreal-time interaction is impractical, risky, or costly. In the offline setting,\nlearning from a static dataset of past interactions allows for the development\nof robust and safe policies without the need for live data collection, which\ncan be fraught with challenges. Building on this foundational importance, we\npresent EAQ, Episodes Augmentation guided by Q-total loss, a novel approach for\noffline MARL framework utilizing diffusion models. EAQ integrates the Q-total\nfunction directly into the diffusion model as a guidance to maximize the global\nreturns in an episode, eliminating the need for separate training. Our focus\nprimarily lies on cooperative scenarios, where agents are required to act\ncollectively towards achieving a shared goal-essentially, maximizing global\nreturns. Consequently, we demonstrate that our episodes augmentation in a\ncollaborative manner significantly boosts offline MARL algorithm compared to\nthe original dataset, improving the normalized return by +17.3% and +12.9% for\nmedium and poor behavioral policies in SMAC simulator, respectively."
                },
                "authors": [
                    {
                        "name": "Jihwan Oh"
                    },
                    {
                        "name": "Sungnyun Kim"
                    },
                    {
                        "name": "Gahee Kim"
                    },
                    {
                        "name": "Sunghwan Kim"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "Accepted by SPIGM Workshop at ICML 2024 (Structured Probabilistic\n  Inference & Generative Modeling)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13092v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14962v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14962v5",
                "updated": "2024-08-23T14:14:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    14,
                    14,
                    21,
                    4,
                    236,
                    0
                ],
                "published": "2024-07-20T18:48:35Z",
                "published_parsed": [
                    2024,
                    7,
                    20,
                    18,
                    48,
                    35,
                    5,
                    202,
                    0
                ],
                "title": "Recent Advances in Generative AI and Large Language Models: Current\n  Status, Challenges, and Perspectives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Advances in Generative AI and Large Language Models: Current\n  Status, Challenges, and Perspectives"
                },
                "summary": "The emergence of Generative Artificial Intelligence (AI) and Large Language\nModels (LLMs) has marked a new era of Natural Language Processing (NLP),\nintroducing unprecedented capabilities that are revolutionizing various\ndomains. This paper explores the current state of these cutting-edge\ntechnologies, demonstrating their remarkable advancements and wide-ranging\napplications. Our paper contributes to providing a holistic perspective on the\ntechnical foundations, practical applications, and emerging challenges within\nthe evolving landscape of Generative AI and LLMs. We believe that understanding\nthe generative capabilities of AI systems and the specific context of LLMs is\ncrucial for researchers, practitioners, and policymakers to collaboratively\nshape the responsible and ethical integration of these technologies into\nvarious domains. Furthermore, we identify and address main research gaps,\nproviding valuable insights to guide future research endeavors within the AI\nresearch community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of Generative Artificial Intelligence (AI) and Large Language\nModels (LLMs) has marked a new era of Natural Language Processing (NLP),\nintroducing unprecedented capabilities that are revolutionizing various\ndomains. This paper explores the current state of these cutting-edge\ntechnologies, demonstrating their remarkable advancements and wide-ranging\napplications. Our paper contributes to providing a holistic perspective on the\ntechnical foundations, practical applications, and emerging challenges within\nthe evolving landscape of Generative AI and LLMs. We believe that understanding\nthe generative capabilities of AI systems and the specific context of LLMs is\ncrucial for researchers, practitioners, and policymakers to collaboratively\nshape the responsible and ethical integration of these technologies into\nvarious domains. Furthermore, we identify and address main research gaps,\nproviding valuable insights to guide future research endeavors within the AI\nresearch community."
                },
                "authors": [
                    {
                        "name": "Desta Haileselassie Hagos"
                    },
                    {
                        "name": "Rick Battle"
                    },
                    {
                        "name": "Danda B. Rawat"
                    }
                ],
                "author_detail": {
                    "name": "Danda B. Rawat"
                },
                "author": "Danda B. Rawat",
                "arxiv_comment": "This version is accepted for publication in the Journal of IEEE\n  Transactions on Artificial Intelligence (TAI)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14962v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14962v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.09105v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.09105v5",
                "updated": "2024-08-23T14:11:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    14,
                    11,
                    5,
                    4,
                    236,
                    0
                ],
                "published": "2024-07-12T09:10:37Z",
                "published_parsed": [
                    2024,
                    7,
                    12,
                    9,
                    10,
                    37,
                    4,
                    194,
                    0
                ],
                "title": "Enhancing Training Efficiency Using Packing with Flash Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Training Efficiency Using Packing with Flash Attention"
                },
                "summary": "Padding is often used in tuning LLM models by adding special tokens to\nshorter training examples to match the length of the longest sequence in each\nbatch. While this ensures uniformity for batch processing, it introduces\ninefficiencies by including irrelevant padding tokens in the computation and\nwastes GPU resources. Hugging Face SFT trainer has always offered the option to\nuse packing to combine multiple training examples, allowing for maximal\nutilization of GPU resources. However, up till now, it did not offer proper\nmasking of each packed training example. This capability has now been added to\nHugging Face Transformers 4.44. We analyse this new feature and show the\nbenefits across different variations of packing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Padding is often used in tuning LLM models by adding special tokens to\nshorter training examples to match the length of the longest sequence in each\nbatch. While this ensures uniformity for batch processing, it introduces\ninefficiencies by including irrelevant padding tokens in the computation and\nwastes GPU resources. Hugging Face SFT trainer has always offered the option to\nuse packing to combine multiple training examples, allowing for maximal\nutilization of GPU resources. However, up till now, it did not offer proper\nmasking of each packed training example. This capability has now been added to\nHugging Face Transformers 4.44. We analyse this new feature and show the\nbenefits across different variations of packing."
                },
                "authors": [
                    {
                        "name": "Achintya Kundu"
                    },
                    {
                        "name": "Rhui Dih Lee"
                    },
                    {
                        "name": "Laura Wynter"
                    },
                    {
                        "name": "Raghu Kiran Ganti"
                    },
                    {
                        "name": "Mayank Mishra"
                    }
                ],
                "author_detail": {
                    "name": "Mayank Mishra"
                },
                "author": "Mayank Mishra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.09105v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.09105v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13073v1",
                "updated": "2024-08-23T13:56:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    13,
                    56,
                    0,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T13:56:00Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    13,
                    56,
                    0,
                    4,
                    236,
                    0
                ],
                "title": "IntelliCare: Improving Healthcare Analysis with Variance-Controlled\n  Patient-Level Knowledge from Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IntelliCare: Improving Healthcare Analysis with Variance-Controlled\n  Patient-Level Knowledge from Large Language Models"
                },
                "summary": "While pioneering deep learning methods have made great strides in analyzing\nelectronic health record (EHR) data, they often struggle to fully capture the\nsemantics of diverse medical codes from limited data. The integration of\nexternal knowledge from Large Language Models (LLMs) presents a promising\navenue for improving healthcare predictions. However, LLM analyses may exhibit\nsignificant variance due to ambiguity problems and inconsistency issues,\nhindering their effective utilization. To address these challenges, we propose\nIntelliCare, a novel framework that leverages LLMs to provide high-quality\npatient-level external knowledge and enhance existing EHR models. Concretely,\nIntelliCare identifies patient cohorts and employs task-relevant statistical\ninformation to augment LLM understanding and generation, effectively mitigating\nthe ambiguity problem. Additionally, it refines LLM-derived knowledge through a\nhybrid approach, generating multiple analyses and calibrating them using both\nthe EHR model and perplexity measures. Experimental evaluations on three\nclinical prediction tasks across two large-scale EHR datasets demonstrate that\nIntelliCare delivers significant performance improvements to existing methods,\nhighlighting its potential in advancing personalized healthcare predictions and\ndecision support systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While pioneering deep learning methods have made great strides in analyzing\nelectronic health record (EHR) data, they often struggle to fully capture the\nsemantics of diverse medical codes from limited data. The integration of\nexternal knowledge from Large Language Models (LLMs) presents a promising\navenue for improving healthcare predictions. However, LLM analyses may exhibit\nsignificant variance due to ambiguity problems and inconsistency issues,\nhindering their effective utilization. To address these challenges, we propose\nIntelliCare, a novel framework that leverages LLMs to provide high-quality\npatient-level external knowledge and enhance existing EHR models. Concretely,\nIntelliCare identifies patient cohorts and employs task-relevant statistical\ninformation to augment LLM understanding and generation, effectively mitigating\nthe ambiguity problem. Additionally, it refines LLM-derived knowledge through a\nhybrid approach, generating multiple analyses and calibrating them using both\nthe EHR model and perplexity measures. Experimental evaluations on three\nclinical prediction tasks across two large-scale EHR datasets demonstrate that\nIntelliCare delivers significant performance improvements to existing methods,\nhighlighting its potential in advancing personalized healthcare predictions and\ndecision support systems."
                },
                "authors": [
                    {
                        "name": "Zhihao Yu"
                    },
                    {
                        "name": "Yujie Jin"
                    },
                    {
                        "name": "Yongxin Xu"
                    },
                    {
                        "name": "Xu Chu"
                    },
                    {
                        "name": "Yasha Wang"
                    },
                    {
                        "name": "Junfeng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Junfeng Zhao"
                },
                "author": "Junfeng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13071v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13071v1",
                "updated": "2024-08-23T13:55:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    13,
                    55,
                    36,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T13:55:36Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    13,
                    55,
                    36,
                    4,
                    236,
                    0
                ],
                "title": "Guiding IoT-Based Healthcare Alert Systems with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guiding IoT-Based Healthcare Alert Systems with Large Language Models"
                },
                "summary": "Healthcare alert systems (HAS) are undergoing rapid evolution, propelled by\nadvancements in artificial intelligence (AI), Internet of Things (IoT)\ntechnologies, and increasing health consciousness. Despite significant\nprogress, a fundamental challenge remains: balancing the accuracy of\npersonalized health alerts with stringent privacy protection in HAS\nenvironments constrained by resources. To address this issue, we introduce a\nuniform framework, LLM-HAS, which incorporates Large Language Models (LLM) into\nHAS to significantly boost the accuracy, ensure user privacy, and enhance\npersonalized health service, while also improving the subjective quality of\nexperience (QoE) for users. Our innovative framework leverages a Mixture of\nExperts (MoE) approach, augmented with LLM, to analyze users' personalized\npreferences and potential health risks from additional textual job\ndescriptions. This analysis guides the selection of specialized Deep\nReinforcement Learning (DDPG) experts, tasked with making precise health\nalerts. Moreover, LLM-HAS can process Conversational User Feedback, which not\nonly allows fine-tuning of DDPG but also deepen user engagement, thereby\nenhancing both the accuracy and personalization of health management\nstrategies. Simulation results validate the effectiveness of the LLM-HAS\nframework, highlighting its potential as a groundbreaking approach for\nemploying generative AI (GAI) to provide highly accurate and reliable alerts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Healthcare alert systems (HAS) are undergoing rapid evolution, propelled by\nadvancements in artificial intelligence (AI), Internet of Things (IoT)\ntechnologies, and increasing health consciousness. Despite significant\nprogress, a fundamental challenge remains: balancing the accuracy of\npersonalized health alerts with stringent privacy protection in HAS\nenvironments constrained by resources. To address this issue, we introduce a\nuniform framework, LLM-HAS, which incorporates Large Language Models (LLM) into\nHAS to significantly boost the accuracy, ensure user privacy, and enhance\npersonalized health service, while also improving the subjective quality of\nexperience (QoE) for users. Our innovative framework leverages a Mixture of\nExperts (MoE) approach, augmented with LLM, to analyze users' personalized\npreferences and potential health risks from additional textual job\ndescriptions. This analysis guides the selection of specialized Deep\nReinforcement Learning (DDPG) experts, tasked with making precise health\nalerts. Moreover, LLM-HAS can process Conversational User Feedback, which not\nonly allows fine-tuning of DDPG but also deepen user engagement, thereby\nenhancing both the accuracy and personalization of health management\nstrategies. Simulation results validate the effectiveness of the LLM-HAS\nframework, highlighting its potential as a groundbreaking approach for\nemploying generative AI (GAI) to provide highly accurate and reliable alerts."
                },
                "authors": [
                    {
                        "name": "Yulan Gao"
                    },
                    {
                        "name": "Ziqiang Ye"
                    },
                    {
                        "name": "Ming Xiao"
                    },
                    {
                        "name": "Yue Xiao"
                    },
                    {
                        "name": "Dong In Kim"
                    }
                ],
                "author_detail": {
                    "name": "Dong In Kim"
                },
                "author": "Dong In Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13071v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13071v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13061v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13061v1",
                "updated": "2024-08-23T13:37:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    13,
                    37,
                    54,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T13:37:54Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    13,
                    37,
                    54,
                    4,
                    236,
                    0
                ],
                "title": "General Intelligent Imaging and Uncertainty Quantification by\n  Deterministic Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General Intelligent Imaging and Uncertainty Quantification by\n  Deterministic Diffusion Model"
                },
                "summary": "Computational imaging is crucial in many disciplines from autonomous driving\nto life sciences. However, traditional model-driven and iterative methods\nconsume large computational power and lack scalability for imaging. Deep\nlearning (DL) is effective in processing local-to-local patterns, but it\nstruggles with handling universal global-to-local (nonlocal) patterns under\ncurrent frameworks. To bridge this gap, we propose a novel DL framework that\nemploys a progressive denoising strategy, named the deterministic diffusion\nmodel (DDM), to facilitate general computational imaging at a low cost. We\nexperimentally demonstrate the efficient and faithful image reconstruction\ncapabilities of DDM from nonlocal patterns, such as speckles from multimode\nfiber and intensity patterns of second harmonic generation, surpassing the\ncapability of previous state-of-the-art DL algorithms. By embedding Bayesian\ninference into DDM, we establish a theoretical framework and provide\nexperimental proof of its uncertainty quantification. This advancement ensures\nthe predictive reliability of DDM, avoiding misjudgment in high-stakes\nscenarios. This versatile and integrable DDM framework can readily extend and\nimprove the efficacy of existing DL-based imaging applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational imaging is crucial in many disciplines from autonomous driving\nto life sciences. However, traditional model-driven and iterative methods\nconsume large computational power and lack scalability for imaging. Deep\nlearning (DL) is effective in processing local-to-local patterns, but it\nstruggles with handling universal global-to-local (nonlocal) patterns under\ncurrent frameworks. To bridge this gap, we propose a novel DL framework that\nemploys a progressive denoising strategy, named the deterministic diffusion\nmodel (DDM), to facilitate general computational imaging at a low cost. We\nexperimentally demonstrate the efficient and faithful image reconstruction\ncapabilities of DDM from nonlocal patterns, such as speckles from multimode\nfiber and intensity patterns of second harmonic generation, surpassing the\ncapability of previous state-of-the-art DL algorithms. By embedding Bayesian\ninference into DDM, we establish a theoretical framework and provide\nexperimental proof of its uncertainty quantification. This advancement ensures\nthe predictive reliability of DDM, avoiding misjudgment in high-stakes\nscenarios. This versatile and integrable DDM framework can readily extend and\nimprove the efficacy of existing DL-based imaging applications."
                },
                "authors": [
                    {
                        "name": "Weiru Fan"
                    },
                    {
                        "name": "Xiaobin Tang"
                    },
                    {
                        "name": "Yiyi Liao"
                    },
                    {
                        "name": "Da-Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Da-Wei Wang"
                },
                "author": "Da-Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13061v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13061v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13045v1",
                "updated": "2024-08-23T13:06:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    13,
                    6,
                    43,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T13:06:43Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    13,
                    6,
                    43,
                    4,
                    236,
                    0
                ],
                "title": "Adaptive complexity of log-concave sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive complexity of log-concave sampling"
                },
                "summary": "In large-data applications, such as the inference process of diffusion\nmodels, it is desirable to design sampling algorithms with a high degree of\nparallelization. In this work, we study the adaptive complexity of sampling,\nwhich is the minimal number of sequential rounds required to achieve sampling\ngiven polynomially many queries executed in parallel at each round. For\nunconstrained sampling, we examine distributions that are log-smooth or\nlog-Lipschitz and log strongly or non-strongly concave. We show that an almost\nlinear iteration algorithm cannot return a sample with a specific exponentially\nsmall accuracy under total variation distance. For box-constrained sampling, we\nshow that an almost linear iteration algorithm cannot return a sample with\nsup-polynomially small accuracy under total variation distance for log-concave\ndistributions. Our proof relies upon novel analysis with the characterization\nof the output for the hardness potentials based on the chain-like structure\nwith random partition and classical smoothing techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-data applications, such as the inference process of diffusion\nmodels, it is desirable to design sampling algorithms with a high degree of\nparallelization. In this work, we study the adaptive complexity of sampling,\nwhich is the minimal number of sequential rounds required to achieve sampling\ngiven polynomially many queries executed in parallel at each round. For\nunconstrained sampling, we examine distributions that are log-smooth or\nlog-Lipschitz and log strongly or non-strongly concave. We show that an almost\nlinear iteration algorithm cannot return a sample with a specific exponentially\nsmall accuracy under total variation distance. For box-constrained sampling, we\nshow that an almost linear iteration algorithm cannot return a sample with\nsup-polynomially small accuracy under total variation distance for log-concave\ndistributions. Our proof relies upon novel analysis with the characterization\nof the output for the hardness potentials based on the chain-like structure\nwith random partition and classical smoothing techniques."
                },
                "authors": [
                    {
                        "name": "Huanjian Zhou"
                    },
                    {
                        "name": "Baoxiang Wang"
                    },
                    {
                        "name": "Masashi Sugiyama"
                    }
                ],
                "author_detail": {
                    "name": "Masashi Sugiyama"
                },
                "author": "Masashi Sugiyama",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.05610v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.05610v2",
                "updated": "2024-08-23T12:44:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    12,
                    44,
                    8,
                    4,
                    236,
                    0
                ],
                "published": "2024-04-08T15:35:03Z",
                "published_parsed": [
                    2024,
                    4,
                    8,
                    15,
                    35,
                    3,
                    0,
                    99,
                    0
                ],
                "title": "KaMPIng: Flexible and (Near) Zero-Overhead C++ Bindings for MPI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KaMPIng: Flexible and (Near) Zero-Overhead C++ Bindings for MPI"
                },
                "summary": "The Message-Passing Interface (MPI) and C++ form the backbone of\nhigh-performance computing, but MPI only provides C and Fortran bindings. While\nthis offers great language interoperability, high-level programming languages\nlike C++ make software development quicker and less error-prone.\n  We propose novel C++ language bindings that cover all abstraction levels from\nlow-level MPI calls to convenient STL-style bindings, where most parameters are\ninferred from a small subset of parameters, by bringing named parameters to\nC++. This enables rapid prototyping and fine-tuning runtime behavior and memory\nmanagement. A flexible type system and additional safety guarantees help to\nprevent programming errors.\n  By exploiting C++'s template metaprogramming capabilities, this has (near)\nzero overhead, as only required code paths are generated at compile time.\n  We demonstrate that our library is a strong foundation for a future\ndistributed standard library using multiple application benchmarks, ranging\nfrom text-book sorting algorithms to phylogenetic interference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Message-Passing Interface (MPI) and C++ form the backbone of\nhigh-performance computing, but MPI only provides C and Fortran bindings. While\nthis offers great language interoperability, high-level programming languages\nlike C++ make software development quicker and less error-prone.\n  We propose novel C++ language bindings that cover all abstraction levels from\nlow-level MPI calls to convenient STL-style bindings, where most parameters are\ninferred from a small subset of parameters, by bringing named parameters to\nC++. This enables rapid prototyping and fine-tuning runtime behavior and memory\nmanagement. A flexible type system and additional safety guarantees help to\nprevent programming errors.\n  By exploiting C++'s template metaprogramming capabilities, this has (near)\nzero overhead, as only required code paths are generated at compile time.\n  We demonstrate that our library is a strong foundation for a future\ndistributed standard library using multiple application benchmarks, ranging\nfrom text-book sorting algorithms to phylogenetic interference."
                },
                "authors": [
                    {
                        "name": "Tim Niklas Uhl"
                    },
                    {
                        "name": "Matthias Schimek"
                    },
                    {
                        "name": "Lukas H√ºbner"
                    },
                    {
                        "name": "Demian Hespe"
                    },
                    {
                        "name": "Florian Kurpicz"
                    },
                    {
                        "name": "Christoph Stelz"
                    },
                    {
                        "name": "Peter Sanders"
                    }
                ],
                "author_detail": {
                    "name": "Peter Sanders"
                },
                "author": "Peter Sanders",
                "arxiv_comment": "To appear at SC24, November 17-22, 2024, Atlanta, Georgia, USA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.05610v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.05610v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13030v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13030v1",
                "updated": "2024-08-23T12:38:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    12,
                    38,
                    22,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T12:38:22Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    12,
                    38,
                    22,
                    4,
                    236,
                    0
                ],
                "title": "Real Log Canonical Thresholds at Non-singular Points",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real Log Canonical Thresholds at Non-singular Points"
                },
                "summary": "Recent advances have clarified theoretical learning accuracy in Bayesian\ninference, revealing that the asymptotic behavior of metrics such as\ngeneralization loss and free energy, assessing predictive accuracy, is dictated\nby a rational number unique to each statistical model, termed the learning\ncoefficient (real log canonical threshold). For models meeting regularity\nconditions, their learning coefficients are known. However, for singular models\nnot meeting these conditions, exact values of learning coefficients are\nprovided for specific models like reduced-rank regression, but a broadly\napplicable calculation method for these learning coefficients in singular\nmodels remains elusive.\n  This paper extends the application range of the previous work and provides an\napproach that can be applied to many points within the set of realizable\nparameters. Specifically, it provides a formula for calculating the real log\ncanonical threshold at many non-singular points within the set of realizable\nparameters. If this calculation can be performed, it is possible to obtain an\nupper bound for the learning coefficient of the statistical model. Thus, this\napproach can also be used to easily obtain an upper bound for the learning\ncoefficients of statistical models. As an application example, it provides an\nupper bound for the learning coefficient of a mixed binomial model, and\ncalculates the learning coefficient for a specific case of reduced-rank\nregression, confirming that the results are consistent with previous research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances have clarified theoretical learning accuracy in Bayesian\ninference, revealing that the asymptotic behavior of metrics such as\ngeneralization loss and free energy, assessing predictive accuracy, is dictated\nby a rational number unique to each statistical model, termed the learning\ncoefficient (real log canonical threshold). For models meeting regularity\nconditions, their learning coefficients are known. However, for singular models\nnot meeting these conditions, exact values of learning coefficients are\nprovided for specific models like reduced-rank regression, but a broadly\napplicable calculation method for these learning coefficients in singular\nmodels remains elusive.\n  This paper extends the application range of the previous work and provides an\napproach that can be applied to many points within the set of realizable\nparameters. Specifically, it provides a formula for calculating the real log\ncanonical threshold at many non-singular points within the set of realizable\nparameters. If this calculation can be performed, it is possible to obtain an\nupper bound for the learning coefficient of the statistical model. Thus, this\napproach can also be used to easily obtain an upper bound for the learning\ncoefficients of statistical models. As an application example, it provides an\nupper bound for the learning coefficient of a mixed binomial model, and\ncalculates the learning coefficient for a specific case of reduced-rank\nregression, confirming that the results are consistent with previous research."
                },
                "authors": [
                    {
                        "name": "Yuki Kurumadani"
                    }
                ],
                "author_detail": {
                    "name": "Yuki Kurumadani"
                },
                "author": "Yuki Kurumadani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13030v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13030v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13028v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13028v1",
                "updated": "2024-08-23T12:32:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    12,
                    32,
                    12,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T12:32:12Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    12,
                    32,
                    12,
                    4,
                    236,
                    0
                ],
                "title": "In-Context Learning with Reinforcement Learning for Incomplete Utterance\n  Rewriting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Learning with Reinforcement Learning for Incomplete Utterance\n  Rewriting"
                },
                "summary": "In-context learning (ICL) of large language models (LLMs) has attracted\nincreasing attention in the community where LLMs make predictions only based on\ninstructions augmented with a few examples. Existing example selection methods\nfor ICL utilize sparse or dense retrievers and derive effective performance.\nHowever, these methods do not utilize direct feedback of LLM to train the\nretriever and the examples selected can not necessarily improve the analogy\nability of LLM. To tackle this, we propose our policy-based reinforcement\nlearning framework for example selection (RLS), which consists of a language\nmodel (LM) selector and an LLM generator. The LM selector encodes the candidate\nexamples into dense representations and selects the top-k examples into the\ndemonstration for LLM. The outputs of LLM are adopted to compute the reward and\npolicy gradient to optimize the LM selector. We conduct experiments on\ndifferent datasets and significantly outperform existing example selection\nmethods. Moreover, our approach shows advantages over supervised finetuning\n(SFT) models in few shot setting. Further experiments show the balance of\nabundance and the similarity with the test case of examples is important for\nICL performance of LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) of large language models (LLMs) has attracted\nincreasing attention in the community where LLMs make predictions only based on\ninstructions augmented with a few examples. Existing example selection methods\nfor ICL utilize sparse or dense retrievers and derive effective performance.\nHowever, these methods do not utilize direct feedback of LLM to train the\nretriever and the examples selected can not necessarily improve the analogy\nability of LLM. To tackle this, we propose our policy-based reinforcement\nlearning framework for example selection (RLS), which consists of a language\nmodel (LM) selector and an LLM generator. The LM selector encodes the candidate\nexamples into dense representations and selects the top-k examples into the\ndemonstration for LLM. The outputs of LLM are adopted to compute the reward and\npolicy gradient to optimize the LM selector. We conduct experiments on\ndifferent datasets and significantly outperform existing example selection\nmethods. Moreover, our approach shows advantages over supervised finetuning\n(SFT) models in few shot setting. Further experiments show the balance of\nabundance and the similarity with the test case of examples is important for\nICL performance of LLM."
                },
                "authors": [
                    {
                        "name": "Haowei Du"
                    },
                    {
                        "name": "Dongyan Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongyan Zhao"
                },
                "author": "Dongyan Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13028v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13028v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13026v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13026v1",
                "updated": "2024-08-23T12:29:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    12,
                    29,
                    5,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T12:29:05Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    12,
                    29,
                    5,
                    4,
                    236,
                    0
                ],
                "title": "Thermodynamic inference of correlations in nonequilibrium collective\n  dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thermodynamic inference of correlations in nonequilibrium collective\n  dynamics"
                },
                "summary": "The theory of stochastic thermodynamics has revealed many useful fluctuation\nrelations, with the thermodynamic uncertainty relation (TUR) being a theorem of\nmajor interest. When many nonequilibrium currents interact with each other, a\nnaive application of the TUR to an individual current can result in an apparent\nviolation of the TUR bound. Here, we explore how such an apparent violation can\nbe used to put a lower bound on the strength of correlations as well as the\nnumber of interacting currents in collective dynamics. Our proposed protocol\nallows for the inference of hidden correlations in experiment, for example when\na team of molecular motors pulls on the same cargo but only one or a subset of\nthem is fluorescently tagged. By solving analytically and numerically several\nmodels of many-body nonequilibrium dynamics, we ascertain under which\nconditions this strategy can be applied and the inferred bound on correlations\nbecomes tight.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The theory of stochastic thermodynamics has revealed many useful fluctuation\nrelations, with the thermodynamic uncertainty relation (TUR) being a theorem of\nmajor interest. When many nonequilibrium currents interact with each other, a\nnaive application of the TUR to an individual current can result in an apparent\nviolation of the TUR bound. Here, we explore how such an apparent violation can\nbe used to put a lower bound on the strength of correlations as well as the\nnumber of interacting currents in collective dynamics. Our proposed protocol\nallows for the inference of hidden correlations in experiment, for example when\na team of molecular motors pulls on the same cargo but only one or a subset of\nthem is fluorescently tagged. By solving analytically and numerically several\nmodels of many-body nonequilibrium dynamics, we ascertain under which\nconditions this strategy can be applied and the inferred bound on correlations\nbecomes tight."
                },
                "authors": [
                    {
                        "name": "Michalis Chatzittofi"
                    },
                    {
                        "name": "Ramin Golestanian"
                    },
                    {
                        "name": "Jaime Agudo-Canalejo"
                    }
                ],
                "author_detail": {
                    "name": "Jaime Agudo-Canalejo"
                },
                "author": "Jaime Agudo-Canalejo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13026v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13026v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02392v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02392v3",
                "updated": "2024-08-23T12:27:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    12,
                    27,
                    6,
                    4,
                    236,
                    0
                ],
                "published": "2024-07-02T16:10:55Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    16,
                    10,
                    55,
                    1,
                    184,
                    0
                ],
                "title": "TokenPacker: Efficient Visual Projector for Multimodal LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenPacker: Efficient Visual Projector for Multimodal LLM"
                },
                "summary": "The visual projector serves as an essential bridge between the visual encoder\nand the Large Language Model (LLM) in a Multimodal LLM (MLLM). Typically, MLLMs\nadopt a simple MLP to preserve all visual contexts via one-to-one\ntransformation. However, the visual tokens are redundant and can be\nconsiderably increased when dealing with high-resolution images, impairing the\nefficiency of MLLMs significantly. Some recent works have introduced resampler\nor abstractor to reduce the number of resulting visual tokens. Unfortunately,\nthey fail to capture finer details and undermine the visual reasoning\ncapabilities of MLLMs. In this work, we propose a novel visual projector, which\nadopts a coarse-to-fine scheme to inject the enriched characteristics to\ngenerate the condensed visual tokens. In specific, we first interpolate the\nvisual features as a low-resolution point query, providing the overall visual\nrepresentation as the foundation. Then, we introduce a region-to-point\ninjection module that utilizes high-resolution, multi-level region-based cues\nas fine-grained reference keys and values, allowing them to be fully absorbed\nwithin the corresponding local context region. This step effectively updates\nthe coarse point query, transforming it into an enriched one for the subsequent\nLLM reasoning. Extensive experiments demonstrate that our approach compresses\nthe visual tokens by 75%~89%, while achieves comparable or even better\nperformance across diverse benchmarks with significantly higher efficiency. The\nsource codes can be found at https://github.com/CircleRadon/TokenPacker.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The visual projector serves as an essential bridge between the visual encoder\nand the Large Language Model (LLM) in a Multimodal LLM (MLLM). Typically, MLLMs\nadopt a simple MLP to preserve all visual contexts via one-to-one\ntransformation. However, the visual tokens are redundant and can be\nconsiderably increased when dealing with high-resolution images, impairing the\nefficiency of MLLMs significantly. Some recent works have introduced resampler\nor abstractor to reduce the number of resulting visual tokens. Unfortunately,\nthey fail to capture finer details and undermine the visual reasoning\ncapabilities of MLLMs. In this work, we propose a novel visual projector, which\nadopts a coarse-to-fine scheme to inject the enriched characteristics to\ngenerate the condensed visual tokens. In specific, we first interpolate the\nvisual features as a low-resolution point query, providing the overall visual\nrepresentation as the foundation. Then, we introduce a region-to-point\ninjection module that utilizes high-resolution, multi-level region-based cues\nas fine-grained reference keys and values, allowing them to be fully absorbed\nwithin the corresponding local context region. This step effectively updates\nthe coarse point query, transforming it into an enriched one for the subsequent\nLLM reasoning. Extensive experiments demonstrate that our approach compresses\nthe visual tokens by 75%~89%, while achieves comparable or even better\nperformance across diverse benchmarks with significantly higher efficiency. The\nsource codes can be found at https://github.com/CircleRadon/TokenPacker."
                },
                "authors": [
                    {
                        "name": "Wentong Li"
                    },
                    {
                        "name": "Yuqian Yuan"
                    },
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "Dongqi Tang"
                    },
                    {
                        "name": "Song Wang"
                    },
                    {
                        "name": "Jie Qin"
                    },
                    {
                        "name": "Jianke Zhu"
                    },
                    {
                        "name": "Lei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zhang"
                },
                "author": "Lei Zhang",
                "arxiv_comment": "16 pages, Codes:https://github.com/CircleRadon/TokenPacker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02392v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02392v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09895v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09895v2",
                "updated": "2024-08-23T12:14:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    12,
                    14,
                    18,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-19T11:09:12Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    11,
                    9,
                    12,
                    0,
                    232,
                    0
                ],
                "title": "Performance Law of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Law of Large Language Models"
                },
                "summary": "Guided by the belief of the scaling law, large language models (LLMs) have\nachieved impressive performance in recent years. However, scaling law only\ngives a qualitative estimation of loss, which is influenced by various factors\nsuch as model architectures, data distributions, tokenizers, and computation\nprecision. Thus, estimating the real performance of LLMs with different\ntraining settings rather than loss may be quite useful in practical\ndevelopment. In this article, we present an empirical equation named\n\"Performance Law\" to directly predict the MMLU score of an LLM, which is a\nwidely used metric to indicate the general capability of LLMs in real-world\nconversations and applications. Based on only a few key hyperparameters of the\nLLM architecture and the size of training data, we obtain a quite accurate MMLU\nprediction of various LLMs with diverse sizes and architectures developed by\ndifferent organizations in different years. Performance law can be used to\nguide the choice of LLM architecture and the effective allocation of\ncomputational resources without extensive experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guided by the belief of the scaling law, large language models (LLMs) have\nachieved impressive performance in recent years. However, scaling law only\ngives a qualitative estimation of loss, which is influenced by various factors\nsuch as model architectures, data distributions, tokenizers, and computation\nprecision. Thus, estimating the real performance of LLMs with different\ntraining settings rather than loss may be quite useful in practical\ndevelopment. In this article, we present an empirical equation named\n\"Performance Law\" to directly predict the MMLU score of an LLM, which is a\nwidely used metric to indicate the general capability of LLMs in real-world\nconversations and applications. Based on only a few key hyperparameters of the\nLLM architecture and the size of training data, we obtain a quite accurate MMLU\nprediction of various LLMs with diverse sizes and architectures developed by\ndifferent organizations in different years. Performance law can be used to\nguide the choice of LLM architecture and the effective allocation of\ncomputational resources without extensive experiments."
                },
                "authors": [
                    {
                        "name": "Chuhan Wu"
                    },
                    {
                        "name": "Ruiming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Ruiming Tang"
                },
                "author": "Ruiming Tang",
                "arxiv_comment": "Personal opinions of the authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09895v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09895v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13010v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13010v1",
                "updated": "2024-08-23T11:57:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    11,
                    57,
                    2,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T11:57:02Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    11,
                    57,
                    2,
                    4,
                    236,
                    0
                ],
                "title": "A Web-Based Solution for Federated Learning with LLM-Based Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Web-Based Solution for Federated Learning with LLM-Based Automation"
                },
                "summary": "Federated Learning (FL) offers a promising approach for collaborative machine\nlearning across distributed devices. However, its adoption is hindered by the\ncomplexity of building reliable communication architectures and the need for\nexpertise in both machine learning and network programming. This paper presents\na comprehensive solution that simplifies the orchestration of FL tasks while\nintegrating intent-based automation. We develop a user-friendly web application\nsupporting the federated averaging (FedAvg) algorithm, enabling users to\nconfigure parameters through an intuitive interface. The backend solution\nefficiently manages communication between the parameter server and edge nodes.\nWe also implement model compression and scheduling algorithms to optimize FL\nperformance. Furthermore, we explore intent-based automation in FL using a\nfine-tuned Language Model (LLM) trained on a tailored dataset, allowing users\nto conduct FL tasks using high-level prompts. We observe that the LLM-based\nautomated solution achieves comparable test accuracy to the standard web-based\nsolution while reducing transferred bytes by up to 64% and CPU time by up to\n46% for FL tasks. Also, we leverage the neural architecture search (NAS) and\nhyperparameter optimization (HPO) using LLM to improve the performance. We\nobserve that by using this approach test accuracy can be improved by 10-20% for\nthe carried out FL tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) offers a promising approach for collaborative machine\nlearning across distributed devices. However, its adoption is hindered by the\ncomplexity of building reliable communication architectures and the need for\nexpertise in both machine learning and network programming. This paper presents\na comprehensive solution that simplifies the orchestration of FL tasks while\nintegrating intent-based automation. We develop a user-friendly web application\nsupporting the federated averaging (FedAvg) algorithm, enabling users to\nconfigure parameters through an intuitive interface. The backend solution\nefficiently manages communication between the parameter server and edge nodes.\nWe also implement model compression and scheduling algorithms to optimize FL\nperformance. Furthermore, we explore intent-based automation in FL using a\nfine-tuned Language Model (LLM) trained on a tailored dataset, allowing users\nto conduct FL tasks using high-level prompts. We observe that the LLM-based\nautomated solution achieves comparable test accuracy to the standard web-based\nsolution while reducing transferred bytes by up to 64% and CPU time by up to\n46% for FL tasks. Also, we leverage the neural architecture search (NAS) and\nhyperparameter optimization (HPO) using LLM to improve the performance. We\nobserve that by using this approach test accuracy can be improved by 10-20% for\nthe carried out FL tasks."
                },
                "authors": [
                    {
                        "name": "Chamith Mawela"
                    },
                    {
                        "name": "Chaouki Ben Issaid"
                    },
                    {
                        "name": "Mehdi Bennis"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Bennis"
                },
                "author": "Mehdi Bennis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13010v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13010v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13006v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13006v1",
                "updated": "2024-08-23T11:49:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    11,
                    49,
                    1,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T11:49:01Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    11,
                    49,
                    1,
                    4,
                    236,
                    0
                ],
                "title": "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks:\n  Explainable Metrics and Diverse Prompt Templates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks:\n  Explainable Metrics and Diverse Prompt Templates"
                },
                "summary": "Alignment approaches such as RLHF and DPO are actively investigated to align\nlarge language models (LLMs) with human preferences. Commercial large language\nmodels (LLMs) like GPT-4 have been recently employed to evaluate and compare\ndifferent LLM alignment approaches. These models act as surrogates for human\nevaluators due to their promising abilities to approximate human preferences\nwith remarkably faster feedback and lower costs. This methodology is referred\nto as LLM-as-a-judge. However, concerns regarding its reliability have emerged,\nattributed to LLM judges' biases and inconsistent decision-making. Previous\nresearch has sought to develop robust evaluation frameworks for assessing the\nreliability of LLM judges and their alignment with human preferences. However,\nthe employed evaluation metrics often lack adequate explainability and fail to\naddress the internal inconsistency of LLMs. Additionally, existing studies\ninadequately explore the impact of various prompt templates when applying\nLLM-as-a-judge methods, which leads to potentially inconsistent comparisons\nbetween different alignment algorithms. In this work, we systematically\nevaluate LLM judges on alignment tasks (e.g. summarization) by defining\nevaluation metrics with improved theoretical interpretability and disentangling\nreliability metrics with LLM internal inconsistency. We develop a framework to\nevaluate, compare, and visualize the reliability and alignment of LLM judges to\nprovide informative observations that help choose LLM judges for alignment\ntasks. Our results indicate a significant impact of prompt templates on LLM\njudge performance, as well as a mediocre alignment level between the tested LLM\njudges and human evaluators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alignment approaches such as RLHF and DPO are actively investigated to align\nlarge language models (LLMs) with human preferences. Commercial large language\nmodels (LLMs) like GPT-4 have been recently employed to evaluate and compare\ndifferent LLM alignment approaches. These models act as surrogates for human\nevaluators due to their promising abilities to approximate human preferences\nwith remarkably faster feedback and lower costs. This methodology is referred\nto as LLM-as-a-judge. However, concerns regarding its reliability have emerged,\nattributed to LLM judges' biases and inconsistent decision-making. Previous\nresearch has sought to develop robust evaluation frameworks for assessing the\nreliability of LLM judges and their alignment with human preferences. However,\nthe employed evaluation metrics often lack adequate explainability and fail to\naddress the internal inconsistency of LLMs. Additionally, existing studies\ninadequately explore the impact of various prompt templates when applying\nLLM-as-a-judge methods, which leads to potentially inconsistent comparisons\nbetween different alignment algorithms. In this work, we systematically\nevaluate LLM judges on alignment tasks (e.g. summarization) by defining\nevaluation metrics with improved theoretical interpretability and disentangling\nreliability metrics with LLM internal inconsistency. We develop a framework to\nevaluate, compare, and visualize the reliability and alignment of LLM judges to\nprovide informative observations that help choose LLM judges for alignment\ntasks. Our results indicate a significant impact of prompt templates on LLM\njudge performance, as well as a mediocre alignment level between the tested LLM\njudges and human evaluators."
                },
                "authors": [
                    {
                        "name": "Hui Wei"
                    },
                    {
                        "name": "Shenghua He"
                    },
                    {
                        "name": "Tian Xia"
                    },
                    {
                        "name": "Andy Wong"
                    },
                    {
                        "name": "Jingyang Lin"
                    },
                    {
                        "name": "Mei Han"
                    }
                ],
                "author_detail": {
                    "name": "Mei Han"
                },
                "author": "Mei Han",
                "arxiv_comment": "Preprint, under review. 17 pages, 7 figures, 16 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13006v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13006v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13001v1",
                "updated": "2024-08-23T11:43:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    11,
                    43,
                    0,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T11:43:00Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    11,
                    43,
                    0,
                    4,
                    236,
                    0
                ],
                "title": "CRUXEval-X: A Benchmark for Multilingual Code Reasoning, Understanding\n  and Execution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CRUXEval-X: A Benchmark for Multilingual Code Reasoning, Understanding\n  and Execution"
                },
                "summary": "Code benchmarks such as HumanEval are widely adopted to evaluate Large\nLanguage Models' (LLMs) coding capabilities. However, there is an unignorable\nprogramming language bias in existing code benchmarks -- over 95% code\ngeneration benchmarks are dominated by Python, leaving the LLMs' capabilities\nin other programming languages such as Java and C/C++ unknown. Moreover, coding\ntask bias is also crucial. Most benchmarks focus on code generation capability,\nwhile benchmarks for code reasoning (given input, reasoning output; and given\noutput, reasoning input), an essential coding capability, are insufficient.\nYet, constructing multi-lingual benchmarks can be expensive and\nlabor-intensive, and codes in contest websites such as Leetcode suffer from\ndata contamination during training. To fill this gap, we propose CRUXEVAL-X, a\nmulti-lingual code reasoning benchmark that contains 19 programming languages.\nIt comprises at least 600 subjects for each language, along with 19K\ncontent-consistent tests in total. In particular, the construction pipeline of\nCRUXEVAL-X works in a fully automated and test-guided manner, which iteratively\ngenerates and repairs based on execution feedback. Also, to cross language\nbarriers (e.g., dynamic/static type systems in Python/C++), we formulated\nvarious transition rules between language pairs to facilitate translation. Our\nintensive evaluation of 24 representative LLMs reveals the correlation between\nlanguage pairs. For example, TypeScript and JavaScript show a significant\npositive correlation, while Racket has less correlation with other languages.\nMore interestingly, even a model trained solely on Python can achieve at most\n34.4% Pass@1 in other languages, revealing the cross-language generalization of\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code benchmarks such as HumanEval are widely adopted to evaluate Large\nLanguage Models' (LLMs) coding capabilities. However, there is an unignorable\nprogramming language bias in existing code benchmarks -- over 95% code\ngeneration benchmarks are dominated by Python, leaving the LLMs' capabilities\nin other programming languages such as Java and C/C++ unknown. Moreover, coding\ntask bias is also crucial. Most benchmarks focus on code generation capability,\nwhile benchmarks for code reasoning (given input, reasoning output; and given\noutput, reasoning input), an essential coding capability, are insufficient.\nYet, constructing multi-lingual benchmarks can be expensive and\nlabor-intensive, and codes in contest websites such as Leetcode suffer from\ndata contamination during training. To fill this gap, we propose CRUXEVAL-X, a\nmulti-lingual code reasoning benchmark that contains 19 programming languages.\nIt comprises at least 600 subjects for each language, along with 19K\ncontent-consistent tests in total. In particular, the construction pipeline of\nCRUXEVAL-X works in a fully automated and test-guided manner, which iteratively\ngenerates and repairs based on execution feedback. Also, to cross language\nbarriers (e.g., dynamic/static type systems in Python/C++), we formulated\nvarious transition rules between language pairs to facilitate translation. Our\nintensive evaluation of 24 representative LLMs reveals the correlation between\nlanguage pairs. For example, TypeScript and JavaScript show a significant\npositive correlation, while Racket has less correlation with other languages.\nMore interestingly, even a model trained solely on Python can achieve at most\n34.4% Pass@1 in other languages, revealing the cross-language generalization of\nLLMs."
                },
                "authors": [
                    {
                        "name": "Ruiyang Xu"
                    },
                    {
                        "name": "Jialun Cao"
                    },
                    {
                        "name": "Yaojie Lu"
                    },
                    {
                        "name": "Hongyu Lin"
                    },
                    {
                        "name": "Xianpei Han"
                    },
                    {
                        "name": "Ben He"
                    },
                    {
                        "name": "Shing-Chi Cheung"
                    },
                    {
                        "name": "Le Sun"
                    }
                ],
                "author_detail": {
                    "name": "Le Sun"
                },
                "author": "Le Sun",
                "arxiv_comment": "13pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12989v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12989v1",
                "updated": "2024-08-23T11:14:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    11,
                    14,
                    20,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T11:14:20Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    11,
                    14,
                    20,
                    4,
                    236,
                    0
                ],
                "title": "RIFF: Inducing Rules for Fraud Detection from Decision Trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RIFF: Inducing Rules for Fraud Detection from Decision Trees"
                },
                "summary": "Financial fraud is the cause of multi-billion dollar losses annually.\nTraditionally, fraud detection systems rely on rules due to their transparency\nand interpretability, key features in domains where decisions need to be\nexplained. However, rule systems require significant input from domain experts\nto create and tune, an issue that rule induction algorithms attempt to mitigate\nby inferring rules directly from data. We explore the application of these\nalgorithms to fraud detection, where rule systems are constrained to have a low\nfalse positive rate (FPR) or alert rate, by proposing RIFF, a rule induction\nalgorithm that distills a low FPR rule set directly from decision trees. Our\nexperiments show that the induced rules are often able to maintain or improve\nperformance of the original models for low FPR tasks, while substantially\nreducing their complexity and outperforming rules hand-tuned by experts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Financial fraud is the cause of multi-billion dollar losses annually.\nTraditionally, fraud detection systems rely on rules due to their transparency\nand interpretability, key features in domains where decisions need to be\nexplained. However, rule systems require significant input from domain experts\nto create and tune, an issue that rule induction algorithms attempt to mitigate\nby inferring rules directly from data. We explore the application of these\nalgorithms to fraud detection, where rule systems are constrained to have a low\nfalse positive rate (FPR) or alert rate, by proposing RIFF, a rule induction\nalgorithm that distills a low FPR rule set directly from decision trees. Our\nexperiments show that the induced rules are often able to maintain or improve\nperformance of the original models for low FPR tasks, while substantially\nreducing their complexity and outperforming rules hand-tuned by experts."
                },
                "authors": [
                    {
                        "name": "Jo√£o Lucas Martins"
                    },
                    {
                        "name": "Jo√£o Bravo"
                    },
                    {
                        "name": "Ana Sofia Gomes"
                    },
                    {
                        "name": "Carlos Soares"
                    },
                    {
                        "name": "Pedro Bizarro"
                    }
                ],
                "author_detail": {
                    "name": "Pedro Bizarro"
                },
                "author": "Pedro Bizarro",
                "arxiv_comment": "Published as a conference paper at RuleML+RR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12989v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12989v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12982v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12982v1",
                "updated": "2024-08-23T10:59:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    10,
                    59,
                    14,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T10:59:14Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    10,
                    59,
                    14,
                    4,
                    236,
                    0
                ],
                "title": "Inference-Adaptive Neural Steering for Real-Time Area-Based Sound Source\n  Separation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Adaptive Neural Steering for Real-Time Area-Based Sound Source\n  Separation"
                },
                "summary": "We propose a novel Neural Steering technique that adapts the target area of a\nspatial-aware multi-microphone sound source separation algorithm during\ninference without the necessity of retraining the deep neural network (DNN). To\nachieve this, we first train a DNN aiming to retain speech within a target\nregion, defined by an angular span, while suppressing sound sources stemming\nfrom other directions. Afterward, a phase shift is applied to the microphone\nsignals, allowing us to shift the center of the target area during inference at\nnegligible additional cost in computational complexity. Further, we show that\nthe proposed approach performs well in a wide variety of acoustic scenarios,\nincluding several speakers inside and outside the target area and additional\nnoise. More precisely, the proposed approach performs on par with DNNs trained\nexplicitly for the steered target area in terms of DNSMOS and SI-SDR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel Neural Steering technique that adapts the target area of a\nspatial-aware multi-microphone sound source separation algorithm during\ninference without the necessity of retraining the deep neural network (DNN). To\nachieve this, we first train a DNN aiming to retain speech within a target\nregion, defined by an angular span, while suppressing sound sources stemming\nfrom other directions. Afterward, a phase shift is applied to the microphone\nsignals, allowing us to shift the center of the target area during inference at\nnegligible additional cost in computational complexity. Further, we show that\nthe proposed approach performs well in a wide variety of acoustic scenarios,\nincluding several speakers inside and outside the target area and additional\nnoise. More precisely, the proposed approach performs on par with DNNs trained\nexplicitly for the steered target area in terms of DNSMOS and SI-SDR."
                },
                "authors": [
                    {
                        "name": "Martin Strauss"
                    },
                    {
                        "name": "Wolfgang Mack"
                    },
                    {
                        "name": "Mar√≠a Luis Valero"
                    },
                    {
                        "name": "Okan K√∂p√ºkl√º"
                    }
                ],
                "author_detail": {
                    "name": "Okan K√∂p√ºkl√º"
                },
                "author": "Okan K√∂p√ºkl√º",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12982v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12979v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12979v1",
                "updated": "2024-08-23T10:52:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    10,
                    52,
                    57,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T10:52:57Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    10,
                    52,
                    57,
                    4,
                    236,
                    0
                ],
                "title": "Internal and External Knowledge Interactive Refinement Framework for\n  Knowledge-Intensive Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Internal and External Knowledge Interactive Refinement Framework for\n  Knowledge-Intensive Question Answering"
                },
                "summary": "Recent works have attempted to integrate external knowledge into LLMs to\naddress the limitations and potential factual errors in LLM-generated content.\nHowever, how to retrieve the correct knowledge from the large amount of\nexternal knowledge imposes a challenge. To this end, we empirically observe\nthat LLMs have already encoded rich knowledge in their pretrained parameters\nand utilizing these internal knowledge improves the retrieval of external\nknowledge when applying them to knowledge-intensive tasks. In this paper, we\npropose a new internal and external knowledge interactive refinement paradigm\ndubbed IEKR to utilize internal knowledge in LLM to help retrieve relevant\nknowledge from the external knowledge base, as well as exploit the external\nknowledge to refine the hallucination of generated internal knowledge. By\nsimply adding a prompt like 'Tell me something about' to the LLMs, we try to\nreview related explicit knowledge and insert them with the query into the\nretriever for external retrieval. The external knowledge is utilized to\ncomplement the internal knowledge into input of LLM for answers. We conduct\nexperiments on 3 benchmark datasets in knowledge-intensive question answering\ntask with different LLMs and domains, achieving the new state-of-the-art.\nFurther analysis shows the effectiveness of different modules in our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent works have attempted to integrate external knowledge into LLMs to\naddress the limitations and potential factual errors in LLM-generated content.\nHowever, how to retrieve the correct knowledge from the large amount of\nexternal knowledge imposes a challenge. To this end, we empirically observe\nthat LLMs have already encoded rich knowledge in their pretrained parameters\nand utilizing these internal knowledge improves the retrieval of external\nknowledge when applying them to knowledge-intensive tasks. In this paper, we\npropose a new internal and external knowledge interactive refinement paradigm\ndubbed IEKR to utilize internal knowledge in LLM to help retrieve relevant\nknowledge from the external knowledge base, as well as exploit the external\nknowledge to refine the hallucination of generated internal knowledge. By\nsimply adding a prompt like 'Tell me something about' to the LLMs, we try to\nreview related explicit knowledge and insert them with the query into the\nretriever for external retrieval. The external knowledge is utilized to\ncomplement the internal knowledge into input of LLM for answers. We conduct\nexperiments on 3 benchmark datasets in knowledge-intensive question answering\ntask with different LLMs and domains, achieving the new state-of-the-art.\nFurther analysis shows the effectiveness of different modules in our approach."
                },
                "authors": [
                    {
                        "name": "Haowei Du"
                    },
                    {
                        "name": "Dongyan Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongyan Zhao"
                },
                "author": "Dongyan Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12979v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12979v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.09067v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.09067v2",
                "updated": "2024-08-23T10:42:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    10,
                    42,
                    45,
                    4,
                    236,
                    0
                ],
                "published": "2023-08-17T15:54:38Z",
                "published_parsed": [
                    2023,
                    8,
                    17,
                    15,
                    54,
                    38,
                    3,
                    229,
                    0
                ],
                "title": "Contrasting Linguistic Patterns in Human and LLM-Generated Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrasting Linguistic Patterns in Human and LLM-Generated Text"
                },
                "summary": "We conduct a quantitative analysis contrasting human-written English news\ntext with comparable large language model (LLM) output from six different LLMs\nthat cover three different families and four sizes in total. Our analysis spans\nseveral measurable linguistic dimensions, including morphological, syntactic,\npsychometric, and sociolinguistic aspects. The results reveal various\nmeasurable differences between human and AI-generated texts. Human texts\nexhibit more scattered sentence length distributions, more variety of\nvocabulary, a distinct use of dependency and constituent types, shorter\nconstituents, and more optimized dependency distances. Humans tend to exhibit\nstronger negative emotions (such as fear and disgust) and less joy compared to\ntext generated by LLMs, with the toxicity of these models increasing as their\nsize grows. LLM outputs use more numbers, symbols and auxiliaries (suggesting\nobjective language) than human texts, as well as more pronouns. The sexist bias\nprevalent in human text is also expressed by LLMs, and even magnified in all of\nthem but one. Differences between LLMs and humans are larger than between LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We conduct a quantitative analysis contrasting human-written English news\ntext with comparable large language model (LLM) output from six different LLMs\nthat cover three different families and four sizes in total. Our analysis spans\nseveral measurable linguistic dimensions, including morphological, syntactic,\npsychometric, and sociolinguistic aspects. The results reveal various\nmeasurable differences between human and AI-generated texts. Human texts\nexhibit more scattered sentence length distributions, more variety of\nvocabulary, a distinct use of dependency and constituent types, shorter\nconstituents, and more optimized dependency distances. Humans tend to exhibit\nstronger negative emotions (such as fear and disgust) and less joy compared to\ntext generated by LLMs, with the toxicity of these models increasing as their\nsize grows. LLM outputs use more numbers, symbols and auxiliaries (suggesting\nobjective language) than human texts, as well as more pronouns. The sexist bias\nprevalent in human text is also expressed by LLMs, and even magnified in all of\nthem but one. Differences between LLMs and humans are larger than between LLMs."
                },
                "authors": [
                    {
                        "name": "Alberto Mu√±oz-Ortiz"
                    },
                    {
                        "name": "Carlos G√≥mez-Rodr√≠guez"
                    },
                    {
                        "name": "David Vilares"
                    }
                ],
                "author_detail": {
                    "name": "David Vilares"
                },
                "author": "David Vilares",
                "arxiv_doi": "10.1007/s10462-024-10903-2",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10462-024-10903-2",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2308.09067v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.09067v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published at Artificial Intelligence Review vol. 57",
                "arxiv_journal_ref": "Artificial Intelligence Review 57, 265 (2024)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12963v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12963v1",
                "updated": "2024-08-23T10:18:39Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    10,
                    18,
                    39,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T10:18:39Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    10,
                    18,
                    39,
                    4,
                    236,
                    0
                ],
                "title": "Open Llama2 Model for the Lithuanian Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open Llama2 Model for the Lithuanian Language"
                },
                "summary": "In this paper, we propose and describe the first open Llama2 large language\nmodels (LLMs) for the Lithuanian language, including an accompanying\nquestion/answer (Q/A) dataset and translations of popular LLM benchmarks. We\nprovide a brief review of open regional LLMs and detailed information on the\nproposed LLMs and their training process. We also conduct an empirical\nevaluation, comparing the perplexities of the proposed LLMs with those of other\nmodern open LLMs. In addition, benchmarking the proposed LLMs against language\nunderstanding tasks reveals that high-quality pretraining datasets may be\nessential for achieving models that perform efficiently on these benchmarks.\nThe full realisations of the described LLMs are available in the accompanying\nopen repository~\\url{https://huggingface.co/neurotechnology}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose and describe the first open Llama2 large language\nmodels (LLMs) for the Lithuanian language, including an accompanying\nquestion/answer (Q/A) dataset and translations of popular LLM benchmarks. We\nprovide a brief review of open regional LLMs and detailed information on the\nproposed LLMs and their training process. We also conduct an empirical\nevaluation, comparing the perplexities of the proposed LLMs with those of other\nmodern open LLMs. In addition, benchmarking the proposed LLMs against language\nunderstanding tasks reveals that high-quality pretraining datasets may be\nessential for achieving models that perform efficiently on these benchmarks.\nThe full realisations of the described LLMs are available in the accompanying\nopen repository~\\url{https://huggingface.co/neurotechnology}."
                },
                "authors": [
                    {
                        "name": "Art≈´ras Nakvosas"
                    },
                    {
                        "name": "Povilas Daniu≈°is"
                    },
                    {
                        "name": "Vytas Muleviƒçius"
                    }
                ],
                "author_detail": {
                    "name": "Vytas Muleviƒçius"
                },
                "author": "Vytas Muleviƒçius",
                "arxiv_comment": "12 pages, 8 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12963v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12963v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12959v1",
                "updated": "2024-08-23T10:10:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    10,
                    10,
                    1,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T10:10:01Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    10,
                    10,
                    1,
                    4,
                    236,
                    0
                ],
                "title": "Multimodal Contrastive In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Contrastive In-Context Learning"
                },
                "summary": "The rapid growth of Large Language Models (LLMs) usage has highlighted the\nimportance of gradient-free in-context learning (ICL). However, interpreting\ntheir inner workings remains challenging. This paper introduces a novel\nmultimodal contrastive in-context learning framework to enhance our\nunderstanding of ICL in LLMs. First, we present a contrastive learning-based\ninterpretation of ICL in real-world settings, marking the distance of the\nkey-value representation as the differentiator in ICL. Second, we develop an\nanalytical framework to address biases in multimodal input formatting for\nreal-world datasets. We demonstrate the effectiveness of ICL examples where\nbaseline performance is poor, even when they are represented in unseen formats.\nLastly, we propose an on-the-fly approach for ICL (Anchored-by-Text ICL) that\ndemonstrates effectiveness in detecting hateful memes, a task where typical ICL\nstruggles due to resource limitations. Extensive experiments on multimodal\ndatasets reveal that our approach significantly improves ICL performance across\nvarious scenarios, such as challenging tasks and resource-constrained\nenvironments. Moreover, it provides valuable insights into the mechanisms of\nin-context learning in LLMs. Our findings have important implications for\ndeveloping more interpretable, efficient, and robust multimodal AI systems,\nespecially in challenging tasks and resource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of Large Language Models (LLMs) usage has highlighted the\nimportance of gradient-free in-context learning (ICL). However, interpreting\ntheir inner workings remains challenging. This paper introduces a novel\nmultimodal contrastive in-context learning framework to enhance our\nunderstanding of ICL in LLMs. First, we present a contrastive learning-based\ninterpretation of ICL in real-world settings, marking the distance of the\nkey-value representation as the differentiator in ICL. Second, we develop an\nanalytical framework to address biases in multimodal input formatting for\nreal-world datasets. We demonstrate the effectiveness of ICL examples where\nbaseline performance is poor, even when they are represented in unseen formats.\nLastly, we propose an on-the-fly approach for ICL (Anchored-by-Text ICL) that\ndemonstrates effectiveness in detecting hateful memes, a task where typical ICL\nstruggles due to resource limitations. Extensive experiments on multimodal\ndatasets reveal that our approach significantly improves ICL performance across\nvarious scenarios, such as challenging tasks and resource-constrained\nenvironments. Moreover, it provides valuable insights into the mechanisms of\nin-context learning in LLMs. Our findings have important implications for\ndeveloping more interpretable, efficient, and robust multimodal AI systems,\nespecially in challenging tasks and resource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Yosuke Miyanishi"
                    },
                    {
                        "name": "Minh Le Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Minh Le Nguyen"
                },
                "author": "Minh Le Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12948v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12948v1",
                "updated": "2024-08-23T09:57:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    57,
                    37,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T09:57:37Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    57,
                    37,
                    4,
                    236,
                    0
                ],
                "title": "E-code: Mastering Efficient Code Generation through Pretrained Models\n  and Expert Encoder Group",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E-code: Mastering Efficient Code Generation through Pretrained Models\n  and Expert Encoder Group"
                },
                "summary": "Context: With the waning of Moore's Law, the software industry is placing\nincreasing importance on finding alternative solutions for continuous\nperformance enhancement. The significance and research results of software\nperformance optimization have been on the rise in recent years, especially with\nthe advancement propelled by Large Language Models(LLMs). However, traditional\nstrategies for rectifying performance flaws have shown significant limitations\nat the competitive code efficiency optimization level, and research on this\ntopic is surprisingly scarce. Objective: This study aims to address the\nresearch gap in this domain, offering practical solutions to the various\nchallenges encountered. Specifically, we have overcome the constraints of\ntraditional performance error rectification strategies and developed a Language\nModel (LM) tailored for the competitive code efficiency optimization realm.\nMethod: We introduced E-code, an advanced program synthesis LM. Inspired by the\nrecent success of expert LMs, we designed an innovative structure called the\nExpert Encoder Group. This structure employs multiple expert encoders to\nextract features tailored for different input types. We assessed the\nperformance of E-code against other leading models on a competitive dataset and\nconducted in-depth ablation experiments. Results: Upon systematic evaluation,\nE-code achieved a 54.98% improvement in code efficiency, significantly\noutperforming other advanced models. In the ablation experiments, we further\nvalidated the significance of the expert encoder group and other components\nwithin E-code. Conclusion: The research findings indicate that the expert\nencoder group can effectively handle various inputs in efficiency optimization\ntasks, significantly enhancing the model's performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: With the waning of Moore's Law, the software industry is placing\nincreasing importance on finding alternative solutions for continuous\nperformance enhancement. The significance and research results of software\nperformance optimization have been on the rise in recent years, especially with\nthe advancement propelled by Large Language Models(LLMs). However, traditional\nstrategies for rectifying performance flaws have shown significant limitations\nat the competitive code efficiency optimization level, and research on this\ntopic is surprisingly scarce. Objective: This study aims to address the\nresearch gap in this domain, offering practical solutions to the various\nchallenges encountered. Specifically, we have overcome the constraints of\ntraditional performance error rectification strategies and developed a Language\nModel (LM) tailored for the competitive code efficiency optimization realm.\nMethod: We introduced E-code, an advanced program synthesis LM. Inspired by the\nrecent success of expert LMs, we designed an innovative structure called the\nExpert Encoder Group. This structure employs multiple expert encoders to\nextract features tailored for different input types. We assessed the\nperformance of E-code against other leading models on a competitive dataset and\nconducted in-depth ablation experiments. Results: Upon systematic evaluation,\nE-code achieved a 54.98% improvement in code efficiency, significantly\noutperforming other advanced models. In the ablation experiments, we further\nvalidated the significance of the expert encoder group and other components\nwithin E-code. Conclusion: The research findings indicate that the expert\nencoder group can effectively handle various inputs in efficiency optimization\ntasks, significantly enhancing the model's performance."
                },
                "authors": [
                    {
                        "name": "Yue Pan"
                    },
                    {
                        "name": "Chen Lyu"
                    },
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Lantian Li"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Xiuting Shao"
                    }
                ],
                "author_detail": {
                    "name": "Xiuting Shao"
                },
                "author": "Xiuting Shao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12948v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12948v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12942v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12942v1",
                "updated": "2024-08-23T09:46:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    46,
                    15,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T09:46:15Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    46,
                    15,
                    4,
                    236,
                    0
                ],
                "title": "Causal-Guided Active Learning for Debiasing Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal-Guided Active Learning for Debiasing Large Language Models"
                },
                "summary": "Although achieving promising performance, recent analyses show that current\ngenerative large language models (LLMs) may still capture dataset biases and\nutilize them for generation, leading to poor generalizability and harmfulness\nof LLMs. However, due to the diversity of dataset biases and the\nover-optimization problem, previous prior-knowledge-based debiasing methods and\nfine-tuning-based debiasing methods may not be suitable for current LLMs. To\naddress this issue, we explore combining active learning with the causal\nmechanisms and propose a casual-guided active learning (CAL) framework, which\nutilizes LLMs itself to automatically and autonomously identify informative\nbiased samples and induce the bias patterns. Then a cost-effective and\nefficient in-context learning based method is employed to prevent LLMs from\nutilizing dataset biases during generation. Experimental results show that CAL\ncan effectively recognize typical biased instances and induce various bias\npatterns for debiasing LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although achieving promising performance, recent analyses show that current\ngenerative large language models (LLMs) may still capture dataset biases and\nutilize them for generation, leading to poor generalizability and harmfulness\nof LLMs. However, due to the diversity of dataset biases and the\nover-optimization problem, previous prior-knowledge-based debiasing methods and\nfine-tuning-based debiasing methods may not be suitable for current LLMs. To\naddress this issue, we explore combining active learning with the causal\nmechanisms and propose a casual-guided active learning (CAL) framework, which\nutilizes LLMs itself to automatically and autonomously identify informative\nbiased samples and induce the bias patterns. Then a cost-effective and\nefficient in-context learning based method is employed to prevent LLMs from\nutilizing dataset biases during generation. Experimental results show that CAL\ncan effectively recognize typical biased instances and induce various bias\npatterns for debiasing LLMs."
                },
                "authors": [
                    {
                        "name": "Zhouhao Sun"
                    },
                    {
                        "name": "Li Du"
                    },
                    {
                        "name": "Xiao Ding"
                    },
                    {
                        "name": "Yixuan Ma"
                    },
                    {
                        "name": "Kaitao Qiu"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Bing Qin"
                    }
                ],
                "author_detail": {
                    "name": "Bing Qin"
                },
                "author": "Bing Qin",
                "arxiv_comment": "ACL main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12942v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12942v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12935v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12935v1",
                "updated": "2024-08-23T09:33:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    33,
                    48,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T09:33:48Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    33,
                    48,
                    4,
                    236,
                    0
                ],
                "title": "Trustworthy, Responsible, and Safe AI: A Comprehensive Architectural\n  Framework for AI Safety with Challenges and Mitigations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trustworthy, Responsible, and Safe AI: A Comprehensive Architectural\n  Framework for AI Safety with Challenges and Mitigations"
                },
                "summary": "AI Safety is an emerging area of critical importance to the safe adoption and\ndeployment of AI systems. With the rapid proliferation of AI and especially\nwith the recent advancement of Generative AI (or GAI), the technology ecosystem\nbehind the design, development, adoption, and deployment of AI systems has\ndrastically changed, broadening the scope of AI Safety to address impacts on\npublic safety and national security. In this paper, we propose a novel\narchitectural framework for understanding and analyzing AI Safety; defining its\ncharacteristics from three perspectives: Trustworthy AI, Responsible AI, and\nSafe AI. We provide an extensive review of current research and advancements in\nAI safety from these perspectives, highlighting their key challenges and\nmitigation approaches. Through examples from state-of-the-art technologies,\nparticularly Large Language Models (LLMs), we present innovative mechanism,\nmethodologies, and techniques for designing and testing AI safety. Our goal is\nto promote advancement in AI safety research, and ultimately enhance people's\ntrust in digital transformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Safety is an emerging area of critical importance to the safe adoption and\ndeployment of AI systems. With the rapid proliferation of AI and especially\nwith the recent advancement of Generative AI (or GAI), the technology ecosystem\nbehind the design, development, adoption, and deployment of AI systems has\ndrastically changed, broadening the scope of AI Safety to address impacts on\npublic safety and national security. In this paper, we propose a novel\narchitectural framework for understanding and analyzing AI Safety; defining its\ncharacteristics from three perspectives: Trustworthy AI, Responsible AI, and\nSafe AI. We provide an extensive review of current research and advancements in\nAI safety from these perspectives, highlighting their key challenges and\nmitigation approaches. Through examples from state-of-the-art technologies,\nparticularly Large Language Models (LLMs), we present innovative mechanism,\nmethodologies, and techniques for designing and testing AI safety. Our goal is\nto promote advancement in AI safety research, and ultimately enhance people's\ntrust in digital transformation."
                },
                "authors": [
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Ziyao Liu"
                    },
                    {
                        "name": "Weifeng Jiang"
                    },
                    {
                        "name": "Goh Si Qi"
                    },
                    {
                        "name": "KwoK-Yan Lam"
                    }
                ],
                "author_detail": {
                    "name": "KwoK-Yan Lam"
                },
                "author": "KwoK-Yan Lam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12935v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12935v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20962v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20962v3",
                "updated": "2024-08-23T09:24:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    24,
                    22,
                    4,
                    236,
                    0
                ],
                "published": "2024-05-31T16:07:33Z",
                "published_parsed": [
                    2024,
                    5,
                    31,
                    16,
                    7,
                    33,
                    4,
                    152,
                    0
                ],
                "title": "Large Language Models are Zero-Shot Next Location Predictors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are Zero-Shot Next Location Predictors"
                },
                "summary": "Predicting the locations an individual will visit in the future is crucial\nfor solving many societal issues like disease diffusion and reduction of\npollution. However, next-location predictors require a significant amount of\nindividual-level information that may be scarce or unavailable in some\nscenarios (e.g., cold-start). Large Language Models (LLMs) have shown good\ngeneralization and reasoning capabilities and are rich in geographical\nknowledge, allowing us to believe that these models can act as zero-shot\nnext-location predictors. We tested more than 15 LLMs on three real-world\nmobility datasets and we found that LLMs can obtain accuracies up to 36.2%, a\nsignificant relative improvement of almost 640% when compared to other models\nspecifically designed for human mobility. We also test for data contamination\nand explored the possibility of using LLMs as text-based explainers for\nnext-location prediction, showing that, regardless of the model size, LLMs can\nexplain their decision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting the locations an individual will visit in the future is crucial\nfor solving many societal issues like disease diffusion and reduction of\npollution. However, next-location predictors require a significant amount of\nindividual-level information that may be scarce or unavailable in some\nscenarios (e.g., cold-start). Large Language Models (LLMs) have shown good\ngeneralization and reasoning capabilities and are rich in geographical\nknowledge, allowing us to believe that these models can act as zero-shot\nnext-location predictors. We tested more than 15 LLMs on three real-world\nmobility datasets and we found that LLMs can obtain accuracies up to 36.2%, a\nsignificant relative improvement of almost 640% when compared to other models\nspecifically designed for human mobility. We also test for data contamination\nand explored the possibility of using LLMs as text-based explainers for\nnext-location prediction, showing that, regardless of the model size, LLMs can\nexplain their decision."
                },
                "authors": [
                    {
                        "name": "Ciro Beneduce"
                    },
                    {
                        "name": "Bruno Lepri"
                    },
                    {
                        "name": "Massimiliano Luca"
                    }
                ],
                "author_detail": {
                    "name": "Massimiliano Luca"
                },
                "author": "Massimiliano Luca",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20962v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20962v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12930v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12930v1",
                "updated": "2024-08-23T09:19:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    19,
                    34,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T09:19:34Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    19,
                    34,
                    4,
                    236,
                    0
                ],
                "title": "Animal Identification with Independent Foreground and Background\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Animal Identification with Independent Foreground and Background\n  Modeling"
                },
                "summary": "We propose a method that robustly exploits background and foreground in\nvisual identification of individual animals. Experiments show that their\nautomatic separation, made easy with methods like Segment Anything, together\nwith independent foreground and background-related modeling, improves results.\nThe two predictions are combined in a principled way, thanks to novel\nPer-Instance Temperature Scaling that helps the classifier to deal with\nappearance ambiguities in training and to produce calibrated outputs in the\ninference phase. For identity prediction from the background, we propose novel\nspatial and temporal models. On two problems, the relative error w.r.t. the\nbaseline was reduced by 22.3% and 8.8%, respectively. For cases where objects\nappear in new locations, an example of background drift, accuracy doubles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a method that robustly exploits background and foreground in\nvisual identification of individual animals. Experiments show that their\nautomatic separation, made easy with methods like Segment Anything, together\nwith independent foreground and background-related modeling, improves results.\nThe two predictions are combined in a principled way, thanks to novel\nPer-Instance Temperature Scaling that helps the classifier to deal with\nappearance ambiguities in training and to produce calibrated outputs in the\ninference phase. For identity prediction from the background, we propose novel\nspatial and temporal models. On two problems, the relative error w.r.t. the\nbaseline was reduced by 22.3% and 8.8%, respectively. For cases where objects\nappear in new locations, an example of background drift, accuracy doubles."
                },
                "authors": [
                    {
                        "name": "Lukas Picek"
                    },
                    {
                        "name": "Lukas Neumann"
                    },
                    {
                        "name": "Jiri Matas"
                    }
                ],
                "author_detail": {
                    "name": "Jiri Matas"
                },
                "author": "Jiri Matas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12930v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12930v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12928v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12928v1",
                "updated": "2024-08-23T09:14:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    14,
                    58,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T09:14:58Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    14,
                    58,
                    4,
                    236,
                    0
                ],
                "title": "ParGo: Bridging Vision-Language with Partial and Global Views",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParGo: Bridging Vision-Language with Partial and Global Views"
                },
                "summary": "This work presents ParGo, a novel Partial-Global projector designed to\nconnect the vision and language modalities for Multimodal Large Language Models\n(MLLMs). Unlike previous works that rely on global attention-based projectors,\nour ParGo bridges the representation gap between the separately pre-trained\nvision encoders and the LLMs by integrating global and partial views, which\nalleviates the overemphasis on prominent regions. To facilitate the effective\ntraining of ParGo, we collect a large-scale detail-captioned image-text dataset\nnamed ParGoCap-1M-PT, consisting of 1 million images paired with high-quality\ncaptions. Extensive experiments on several MLLM benchmarks demonstrate the\neffectiveness of our ParGo, highlighting its superiority in aligning vision and\nlanguage modalities. Compared to conventional Q-Former projector, our ParGo\nachieves an improvement of 259.96 in MME benchmark. Furthermore, our\nexperiments reveal that ParGo significantly outperforms other projectors,\nparticularly in tasks that emphasize detail perception ability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents ParGo, a novel Partial-Global projector designed to\nconnect the vision and language modalities for Multimodal Large Language Models\n(MLLMs). Unlike previous works that rely on global attention-based projectors,\nour ParGo bridges the representation gap between the separately pre-trained\nvision encoders and the LLMs by integrating global and partial views, which\nalleviates the overemphasis on prominent regions. To facilitate the effective\ntraining of ParGo, we collect a large-scale detail-captioned image-text dataset\nnamed ParGoCap-1M-PT, consisting of 1 million images paired with high-quality\ncaptions. Extensive experiments on several MLLM benchmarks demonstrate the\neffectiveness of our ParGo, highlighting its superiority in aligning vision and\nlanguage modalities. Compared to conventional Q-Former projector, our ParGo\nachieves an improvement of 259.96 in MME benchmark. Furthermore, our\nexperiments reveal that ParGo significantly outperforms other projectors,\nparticularly in tasks that emphasize detail perception ability."
                },
                "authors": [
                    {
                        "name": "An-Lan Wang"
                    },
                    {
                        "name": "Bin Shan"
                    },
                    {
                        "name": "Wei Shi"
                    },
                    {
                        "name": "Kun-Yu Lin"
                    },
                    {
                        "name": "Xiang Fei"
                    },
                    {
                        "name": "Guozhi Tang"
                    },
                    {
                        "name": "Lei Liao"
                    },
                    {
                        "name": "Jingqun Tang"
                    },
                    {
                        "name": "Can Huang"
                    },
                    {
                        "name": "Wei-Shi Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Wei-Shi Zheng"
                },
                "author": "Wei-Shi Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12928v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12928v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14609v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14609v3",
                "updated": "2024-08-23T08:40:23Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    8,
                    40,
                    23,
                    4,
                    236,
                    0
                ],
                "published": "2024-02-22T14:57:44Z",
                "published_parsed": [
                    2024,
                    2,
                    22,
                    14,
                    57,
                    44,
                    3,
                    53,
                    0
                ],
                "title": "Federated Neural Graph Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Neural Graph Databases"
                },
                "summary": "The increasing demand for large-scale language models (LLMs) has highlighted\nthe importance of efficient data retrieval mechanisms. Neural graph databases\n(NGDBs) have emerged as a promising approach to storing and querying\ngraph-structured data in neural space, enabling the retrieval of relevant\ninformation for LLMs. However, existing NGDBs are typically designed to operate\non a single graph, limiting their ability to reason across multiple graphs.\nFurthermore, the lack of support for multi-source graph data in existing NGDBs\nhinders their ability to capture the complexity and diversity of real-world\ndata. In many applications, data is distributed across multiple sources, and\nthe ability to reason across these sources is crucial for making informed\ndecisions. This limitation is particularly problematic when dealing with\nsensitive graph data, as directly sharing and aggregating such data poses\nsignificant privacy risks. As a result, many applications that rely on NGDBs\nare forced to choose between compromising data privacy or sacrificing the\nability to reason across multiple graphs. To address these limitations, we\npropose Federated Neural Graph Database (FedNGDB), a novel framework that\nenables reasoning over multi-source graph-based data while preserving privacy.\nFedNGDB leverages federated learning to collaboratively learn graph\nrepresentations across multiple sources, enriching relationships between\nentities and improving the overall quality of the graph data. Unlike existing\nmethods, FedNGDB can handle complex graph structures and relationships, making\nit suitable for various downstream tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for large-scale language models (LLMs) has highlighted\nthe importance of efficient data retrieval mechanisms. Neural graph databases\n(NGDBs) have emerged as a promising approach to storing and querying\ngraph-structured data in neural space, enabling the retrieval of relevant\ninformation for LLMs. However, existing NGDBs are typically designed to operate\non a single graph, limiting their ability to reason across multiple graphs.\nFurthermore, the lack of support for multi-source graph data in existing NGDBs\nhinders their ability to capture the complexity and diversity of real-world\ndata. In many applications, data is distributed across multiple sources, and\nthe ability to reason across these sources is crucial for making informed\ndecisions. This limitation is particularly problematic when dealing with\nsensitive graph data, as directly sharing and aggregating such data poses\nsignificant privacy risks. As a result, many applications that rely on NGDBs\nare forced to choose between compromising data privacy or sacrificing the\nability to reason across multiple graphs. To address these limitations, we\npropose Federated Neural Graph Database (FedNGDB), a novel framework that\nenables reasoning over multi-source graph-based data while preserving privacy.\nFedNGDB leverages federated learning to collaboratively learn graph\nrepresentations across multiple sources, enriching relationships between\nentities and improving the overall quality of the graph data. Unlike existing\nmethods, FedNGDB can handle complex graph structures and relationships, making\nit suitable for various downstream tasks."
                },
                "authors": [
                    {
                        "name": "Qi Hu"
                    },
                    {
                        "name": "Weifeng Jiang"
                    },
                    {
                        "name": "Haoran Li"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Jiaxin Bai"
                    },
                    {
                        "name": "Qianren Mao"
                    },
                    {
                        "name": "Yangqiu Song"
                    },
                    {
                        "name": "Lixin Fan"
                    },
                    {
                        "name": "Jianxin Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianxin Li"
                },
                "author": "Jianxin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14609v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14609v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06571v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06571v5",
                "updated": "2024-08-23T08:17:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    8,
                    17,
                    58,
                    4,
                    236,
                    0
                ],
                "published": "2024-06-03T16:43:04Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    16,
                    43,
                    4,
                    0,
                    155,
                    0
                ],
                "title": "SUBLLM: A Novel Efficient Architecture with Token Sequence Subsampling\n  for LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SUBLLM: A Novel Efficient Architecture with Token Sequence Subsampling\n  for LLM"
                },
                "summary": "While Large Language Models (LLMs) have achieved remarkable success in\nvarious fields, the efficiency of training and inference remains a major\nchallenge. To address this issue, we propose SUBLLM, short for\nSubsampling-Upsampling-Bypass Large Language Model, an innovative architecture\nthat extends the core decoder-only framework by incorporating subsampling,\nupsampling, and bypass modules. The subsampling modules are responsible for\nshortening the sequence, while the upsampling modules restore the sequence\nlength, and the bypass modules enhance convergence. In comparison to LLaMA, the\nproposed SUBLLM exhibits significant enhancements in both training and\ninference speeds as well as memory usage, while maintaining competitive\nfew-shot performance. During training, SUBLLM increases speeds by 26% and cuts\nmemory by 10GB per GPU. In inference, it boosts speeds by up to 37% and reduces\nmemory by 1GB per GPU. The training and inference speeds can be enhanced by 34%\nand 52% respectively when the context window is expanded to 8192. Our code is\navailable at https://github.com/XiaoMi/subllm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) have achieved remarkable success in\nvarious fields, the efficiency of training and inference remains a major\nchallenge. To address this issue, we propose SUBLLM, short for\nSubsampling-Upsampling-Bypass Large Language Model, an innovative architecture\nthat extends the core decoder-only framework by incorporating subsampling,\nupsampling, and bypass modules. The subsampling modules are responsible for\nshortening the sequence, while the upsampling modules restore the sequence\nlength, and the bypass modules enhance convergence. In comparison to LLaMA, the\nproposed SUBLLM exhibits significant enhancements in both training and\ninference speeds as well as memory usage, while maintaining competitive\nfew-shot performance. During training, SUBLLM increases speeds by 26% and cuts\nmemory by 10GB per GPU. In inference, it boosts speeds by up to 37% and reduces\nmemory by 1GB per GPU. The training and inference speeds can be enhanced by 34%\nand 52% respectively when the context window is expanded to 8192. Our code is\navailable at https://github.com/XiaoMi/subllm."
                },
                "authors": [
                    {
                        "name": "Quandong Wang"
                    },
                    {
                        "name": "Yuxuan Yuan"
                    },
                    {
                        "name": "Xiaoyu Yang"
                    },
                    {
                        "name": "Ruike Zhang"
                    },
                    {
                        "name": "Kang Zhao"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jian Luan"
                    },
                    {
                        "name": "Daniel Povey"
                    },
                    {
                        "name": "Bin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bin Wang"
                },
                "author": "Bin Wang",
                "arxiv_comment": "10 pages, 5 figures, accepted by ECAI 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06571v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06571v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11557v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11557v2",
                "updated": "2024-08-23T08:16:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    8,
                    16,
                    50,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-21T12:09:37Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    12,
                    9,
                    37,
                    2,
                    234,
                    0
                ],
                "title": "A Quick, trustworthy spectral detection Q&A system based on the SDAAP\n  Dataset and large language model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Quick, trustworthy spectral detection Q&A system based on the SDAAP\n  Dataset and large language model"
                },
                "summary": "Large Language Model (LLM) has demonstrated significant success in a range of\nnatural language processing (NLP) tasks within general domain. The emergence of\nLLM has introduced innovative methodologies across diverse fields, including\nthe natural sciences. Researchers aim to implement automated, concurrent\nprocess driven by LLM to supplant conventional manual, repetitive and\nlabor-intensive work. In the domain of spectral analysis and detection, it is\nimperative for researchers to autonomously acquire pertinent knowledge across\nvarious research objects, which encompasses the spectroscopic techniques and\nthe chemometric methods that are employed in experiments and analysis.\nParadoxically, despite the recognition of spectroscopic detection as an\neffective analytical method, the fundamental process of knowledge retrieval\nremains both time-intensive and repetitive. In response to this challenge, we\nfirst introduced the Spectral Detection and Analysis Based Paper(SDAAP)\ndataset, which is the first open-source textual knowledge dataset for spectral\nanalysis and detection and contains annotated literature data as well as\ncorresponding knowledge instruction data. Subsequently, we also designed an\nautomated Q\\&A framework based on the SDAAP dataset, which can retrieve\nrelevant knowledge and generate high-quality responses by extracting entities\nin the input as retrieval parameters. It is worth noting that: within this\nframework, LLM is only used as a tool to provide generalizability, while RAG\ntechnique is used to accurately capture the source of the knowledge.This\napproach not only improves the quality of the generated responses, but also\nensures the traceability of the knowledge. Experimental results show that our\nframework generates responses with more reliable expertise compared to the\nbaseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) has demonstrated significant success in a range of\nnatural language processing (NLP) tasks within general domain. The emergence of\nLLM has introduced innovative methodologies across diverse fields, including\nthe natural sciences. Researchers aim to implement automated, concurrent\nprocess driven by LLM to supplant conventional manual, repetitive and\nlabor-intensive work. In the domain of spectral analysis and detection, it is\nimperative for researchers to autonomously acquire pertinent knowledge across\nvarious research objects, which encompasses the spectroscopic techniques and\nthe chemometric methods that are employed in experiments and analysis.\nParadoxically, despite the recognition of spectroscopic detection as an\neffective analytical method, the fundamental process of knowledge retrieval\nremains both time-intensive and repetitive. In response to this challenge, we\nfirst introduced the Spectral Detection and Analysis Based Paper(SDAAP)\ndataset, which is the first open-source textual knowledge dataset for spectral\nanalysis and detection and contains annotated literature data as well as\ncorresponding knowledge instruction data. Subsequently, we also designed an\nautomated Q\\&A framework based on the SDAAP dataset, which can retrieve\nrelevant knowledge and generate high-quality responses by extracting entities\nin the input as retrieval parameters. It is worth noting that: within this\nframework, LLM is only used as a tool to provide generalizability, while RAG\ntechnique is used to accurately capture the source of the knowledge.This\napproach not only improves the quality of the generated responses, but also\nensures the traceability of the knowledge. Experimental results show that our\nframework generates responses with more reliable expertise compared to the\nbaseline."
                },
                "authors": [
                    {
                        "name": "Jiheng Liang"
                    },
                    {
                        "name": "Ziru Yu"
                    },
                    {
                        "name": "Zujie Xie"
                    },
                    {
                        "name": "Xiangyang Yu"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyang Yu"
                },
                "author": "Xiangyang Yu",
                "arxiv_comment": "16 pages,10 figures,3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11557v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11557v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12369v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12369v2",
                "updated": "2024-08-23T08:11:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    8,
                    11,
                    9,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-22T13:13:06Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    13,
                    13,
                    6,
                    3,
                    235,
                    0
                ],
                "title": "RoundTable: Leveraging Dynamic Schema and Contextual Autocomplete for\n  Enhanced Query Precision in Tabular Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoundTable: Leveraging Dynamic Schema and Contextual Autocomplete for\n  Enhanced Query Precision in Tabular Question Answering"
                },
                "summary": "With advancements in Large Language Models (LLMs), a major use case that has\nemerged is querying databases in plain English, translating user questions into\nexecutable database queries, which has improved significantly. However,\nreal-world datasets often feature a vast array of attributes and complex\nvalues, complicating the LLMs task of accurately identifying relevant columns\nor values from natural language queries. Traditional methods cannot fully relay\nthe datasets size and complexity to the LLM. To address these challenges, we\npropose a novel framework that leverages Full-Text Search (FTS) on the input\ntable. This approach not only enables precise detection of specific values and\ncolumns but also narrows the search space for language models, thereby\nenhancing query accuracy. Additionally, it supports a custom auto-complete\nfeature that suggests queries based on the data in the table. This integration\nsignificantly refines the interaction between the user and complex datasets,\noffering a sophisticated solution to the limitations faced by current table\nquerying capabilities. This work is accompanied by an application for both Mac\nand Windows platforms, which readers can try out themselves on their own data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With advancements in Large Language Models (LLMs), a major use case that has\nemerged is querying databases in plain English, translating user questions into\nexecutable database queries, which has improved significantly. However,\nreal-world datasets often feature a vast array of attributes and complex\nvalues, complicating the LLMs task of accurately identifying relevant columns\nor values from natural language queries. Traditional methods cannot fully relay\nthe datasets size and complexity to the LLM. To address these challenges, we\npropose a novel framework that leverages Full-Text Search (FTS) on the input\ntable. This approach not only enables precise detection of specific values and\ncolumns but also narrows the search space for language models, thereby\nenhancing query accuracy. Additionally, it supports a custom auto-complete\nfeature that suggests queries based on the data in the table. This integration\nsignificantly refines the interaction between the user and complex datasets,\noffering a sophisticated solution to the limitations faced by current table\nquerying capabilities. This work is accompanied by an application for both Mac\nand Windows platforms, which readers can try out themselves on their own data."
                },
                "authors": [
                    {
                        "name": "Pratyush Kumar"
                    },
                    {
                        "name": "Kuber Vijaykumar Bellad"
                    },
                    {
                        "name": "Bharat Vadlamudi"
                    },
                    {
                        "name": "Aman Chadha"
                    }
                ],
                "author_detail": {
                    "name": "Aman Chadha"
                },
                "author": "Aman Chadha",
                "arxiv_comment": "13 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12369v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12369v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.11672v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.11672v2",
                "updated": "2024-08-23T07:54:39Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    7,
                    54,
                    39,
                    4,
                    236,
                    0
                ],
                "published": "2023-07-21T16:18:58Z",
                "published_parsed": [
                    2023,
                    7,
                    21,
                    16,
                    18,
                    58,
                    4,
                    202,
                    0
                ],
                "title": "Robust Feature Inference: A Test-time Defense Strategy using Spectral\n  Projections",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Feature Inference: A Test-time Defense Strategy using Spectral\n  Projections"
                },
                "summary": "Test-time defenses are used to improve the robustness of deep neural networks\nto adversarial examples during inference. However, existing methods either\nrequire an additional trained classifier to detect and correct the adversarial\nsamples, or perform additional complex optimization on the model parameters or\nthe input to adapt to the adversarial samples at test-time, resulting in a\nsignificant increase in the inference time compared to the base model. In this\nwork, we propose a novel test-time defense strategy called Robust Feature\nInference (RFI) that is easy to integrate with any existing (robust) training\nprocedure without additional test-time computation. Based on the notion of\nrobustness of features that we present, the key idea is to project the trained\nmodels to the most robust feature space, thereby reducing the vulnerability to\nadversarial attacks in non-robust directions. We theoretically characterize the\nsubspace of the eigenspectrum of the feature covariance that is the most robust\nfor a generalized additive model. Our extensive experiments on CIFAR-10,\nCIFAR-100, tiny ImageNet and ImageNet datasets for several robustness\nbenchmarks, including the state-of-the-art methods in RobustBench show that RFI\nimproves robustness across adaptive and transfer attacks consistently. We also\ncompare RFI with adaptive test-time defenses to demonstrate the effectiveness\nof our proposed approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time defenses are used to improve the robustness of deep neural networks\nto adversarial examples during inference. However, existing methods either\nrequire an additional trained classifier to detect and correct the adversarial\nsamples, or perform additional complex optimization on the model parameters or\nthe input to adapt to the adversarial samples at test-time, resulting in a\nsignificant increase in the inference time compared to the base model. In this\nwork, we propose a novel test-time defense strategy called Robust Feature\nInference (RFI) that is easy to integrate with any existing (robust) training\nprocedure without additional test-time computation. Based on the notion of\nrobustness of features that we present, the key idea is to project the trained\nmodels to the most robust feature space, thereby reducing the vulnerability to\nadversarial attacks in non-robust directions. We theoretically characterize the\nsubspace of the eigenspectrum of the feature covariance that is the most robust\nfor a generalized additive model. Our extensive experiments on CIFAR-10,\nCIFAR-100, tiny ImageNet and ImageNet datasets for several robustness\nbenchmarks, including the state-of-the-art methods in RobustBench show that RFI\nimproves robustness across adaptive and transfer attacks consistently. We also\ncompare RFI with adaptive test-time defenses to demonstrate the effectiveness\nof our proposed approach."
                },
                "authors": [
                    {
                        "name": "Anurag Singh"
                    },
                    {
                        "name": "Mahalakshmi Sabanayagam"
                    },
                    {
                        "name": "Krikamol Muandet"
                    },
                    {
                        "name": "Debarghya Ghoshdastidar"
                    }
                ],
                "author_detail": {
                    "name": "Debarghya Ghoshdastidar"
                },
                "author": "Debarghya Ghoshdastidar",
                "arxiv_comment": "Published in TMLR (28 pages, 6 figures, 20 tables)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.11672v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.11672v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.07637v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.07637v4",
                "updated": "2024-08-23T07:46:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    7,
                    46,
                    17,
                    4,
                    236,
                    0
                ],
                "published": "2023-10-11T16:33:29Z",
                "published_parsed": [
                    2023,
                    10,
                    11,
                    16,
                    33,
                    29,
                    2,
                    284,
                    0
                ],
                "title": "OpsEval: A Comprehensive IT Operations Benchmark Suite for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpsEval: A Comprehensive IT Operations Benchmark Suite for Large\n  Language Models"
                },
                "summary": "Information Technology (IT) Operations (Ops), particularly Artificial\nIntelligence for IT Operations (AIOps), is the guarantee for maintaining the\norderly and stable operation of existing information systems. According to\nGartner's prediction, the use of AI technology for automated IT operations has\nbecome a new trend. Large language models (LLMs) that have exhibited remarkable\ncapabilities in NLP-related tasks, are showing great potential in the field of\nAIOps, such as in aspects of root cause analysis of failures, generation of\noperations and maintenance scripts, and summarizing of alert information.\nNevertheless, the performance of current LLMs in Ops tasks is yet to be\ndetermined. In this paper, we present OpsEval, a comprehensive task-oriented\nOps benchmark designed for LLMs. For the first time, OpsEval assesses LLMs'\nproficiency in various crucial scenarios at different ability levels. The\nbenchmark includes 7184 multi-choice questions and 1736 question-answering (QA)\nformats in English and Chinese. By conducting a comprehensive performance\nevaluation of the current leading large language models, we show how various\nLLM techniques can affect the performance of Ops, and discussed findings\nrelated to various topics, including model quantification, QA evaluation, and\nhallucination issues. To ensure the credibility of our evaluation, we invite\ndozens of domain experts to manually review our questions. At the same time, we\nhave open-sourced 20% of the test QA to assist current researchers in\npreliminary evaluations of their OpsLLM models. The remaining 80% of the data,\nwhich is not disclosed, is used to eliminate the issue of the test set leakage.\nAdditionally, we have constructed an online leaderboard that is updated in\nreal-time and will continue to be updated, ensuring that any newly emerging\nLLMs will be evaluated promptly. Both our dataset and leaderboard have been\nmade public.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information Technology (IT) Operations (Ops), particularly Artificial\nIntelligence for IT Operations (AIOps), is the guarantee for maintaining the\norderly and stable operation of existing information systems. According to\nGartner's prediction, the use of AI technology for automated IT operations has\nbecome a new trend. Large language models (LLMs) that have exhibited remarkable\ncapabilities in NLP-related tasks, are showing great potential in the field of\nAIOps, such as in aspects of root cause analysis of failures, generation of\noperations and maintenance scripts, and summarizing of alert information.\nNevertheless, the performance of current LLMs in Ops tasks is yet to be\ndetermined. In this paper, we present OpsEval, a comprehensive task-oriented\nOps benchmark designed for LLMs. For the first time, OpsEval assesses LLMs'\nproficiency in various crucial scenarios at different ability levels. The\nbenchmark includes 7184 multi-choice questions and 1736 question-answering (QA)\nformats in English and Chinese. By conducting a comprehensive performance\nevaluation of the current leading large language models, we show how various\nLLM techniques can affect the performance of Ops, and discussed findings\nrelated to various topics, including model quantification, QA evaluation, and\nhallucination issues. To ensure the credibility of our evaluation, we invite\ndozens of domain experts to manually review our questions. At the same time, we\nhave open-sourced 20% of the test QA to assist current researchers in\npreliminary evaluations of their OpsLLM models. The remaining 80% of the data,\nwhich is not disclosed, is used to eliminate the issue of the test set leakage.\nAdditionally, we have constructed an online leaderboard that is updated in\nreal-time and will continue to be updated, ensuring that any newly emerging\nLLMs will be evaluated promptly. Both our dataset and leaderboard have been\nmade public."
                },
                "authors": [
                    {
                        "name": "Yuhe Liu"
                    },
                    {
                        "name": "Changhua Pei"
                    },
                    {
                        "name": "Longlong Xu"
                    },
                    {
                        "name": "Bohan Chen"
                    },
                    {
                        "name": "Mingze Sun"
                    },
                    {
                        "name": "Zhirui Zhang"
                    },
                    {
                        "name": "Yongqian Sun"
                    },
                    {
                        "name": "Shenglin Zhang"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Haiming Zhang"
                    },
                    {
                        "name": "Jianhui Li"
                    },
                    {
                        "name": "Gaogang Xie"
                    },
                    {
                        "name": "Xidao Wen"
                    },
                    {
                        "name": "Xiaohui Nie"
                    },
                    {
                        "name": "Minghua Ma"
                    },
                    {
                        "name": "Dan Pei"
                    }
                ],
                "author_detail": {
                    "name": "Dan Pei"
                },
                "author": "Dan Pei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.07637v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.07637v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2203.11702v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2203.11702v3",
                "updated": "2024-08-23T07:46:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    7,
                    46,
                    0,
                    4,
                    236,
                    0
                ],
                "published": "2022-03-22T13:12:27Z",
                "published_parsed": [
                    2022,
                    3,
                    22,
                    13,
                    12,
                    27,
                    1,
                    81,
                    0
                ],
                "title": "BERT-ASC: Auxiliary-Sentence Construction for Implicit Aspect Learning\n  in Sentiment Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BERT-ASC: Auxiliary-Sentence Construction for Implicit Aspect Learning\n  in Sentiment Analysis"
                },
                "summary": "Aspect-based sentiment analysis (ABSA) aims to associate a text with a set of\naspects and infer their respective sentimental polarities. State-of-the-art\napproaches are built on fine-tuning pre-trained language models, focusing on\nlearning aspect-specific representations from the corpus. However, aspects are\noften expressed implicitly, making implicit mapping challenging without\nsufficient labeled examples, which may be scarce in real-world scenarios. This\npaper proposes a unified framework to address aspect categorization and\naspect-based sentiment subtasks. We introduce a mechanism to construct an\nauxiliary-sentence for the implicit aspect using the corpus's semantic\ninformation. We then encourage BERT to learn aspect-specific representation in\nresponse to this auxiliary-sentence, not the aspect itself. We evaluate our\napproach on real benchmark datasets for both ABSA and Targeted-ABSA tasks. Our\nexperiments show that it consistently achieves state-of-the-art performance in\naspect categorization and aspect-based sentiment across all datasets, with\nconsiderable improvement margins. The BERT-ASC code is available at\nhttps://github.com/amurtadha/BERT-ASC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aspect-based sentiment analysis (ABSA) aims to associate a text with a set of\naspects and infer their respective sentimental polarities. State-of-the-art\napproaches are built on fine-tuning pre-trained language models, focusing on\nlearning aspect-specific representations from the corpus. However, aspects are\noften expressed implicitly, making implicit mapping challenging without\nsufficient labeled examples, which may be scarce in real-world scenarios. This\npaper proposes a unified framework to address aspect categorization and\naspect-based sentiment subtasks. We introduce a mechanism to construct an\nauxiliary-sentence for the implicit aspect using the corpus's semantic\ninformation. We then encourage BERT to learn aspect-specific representation in\nresponse to this auxiliary-sentence, not the aspect itself. We evaluate our\napproach on real benchmark datasets for both ABSA and Targeted-ABSA tasks. Our\nexperiments show that it consistently achieves state-of-the-art performance in\naspect categorization and aspect-based sentiment across all datasets, with\nconsiderable improvement margins. The BERT-ASC code is available at\nhttps://github.com/amurtadha/BERT-ASC."
                },
                "authors": [
                    {
                        "name": "Murtadha Ahmed"
                    },
                    {
                        "name": "Bo Wen"
                    },
                    {
                        "name": "Shengfeng Pan"
                    },
                    {
                        "name": "Jianlin Su"
                    },
                    {
                        "name": "Luo Ao"
                    },
                    {
                        "name": "Yunfeng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yunfeng Liu"
                },
                "author": "Yunfeng Liu",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2203.11702v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2203.11702v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14874v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14874v2",
                "updated": "2024-08-23T07:31:42Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    7,
                    31,
                    42,
                    4,
                    236,
                    0
                ],
                "published": "2024-02-21T17:20:38Z",
                "published_parsed": [
                    2024,
                    2,
                    21,
                    17,
                    20,
                    38,
                    2,
                    52,
                    0
                ],
                "title": "Distillation Contrastive Decoding: Improving LLMs Reasoning with\n  Contrastive Decoding and Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distillation Contrastive Decoding: Improving LLMs Reasoning with\n  Contrastive Decoding and Distillation"
                },
                "summary": "We propose a straightforward approach called Distillation Contrastive\nDecoding (DCD) to enhance the reasoning capabilities of Large Language Models\n(LLMs) during inference. In contrast to previous approaches that relied on\nsmaller amateur models or analysis of hidden state differences, DCD employs\nContrastive Chain-of-thought Prompting and advanced distillation techniques,\nincluding Dropout and Quantization. This approach effectively addresses the\nlimitations of Contrastive Decoding (CD), which typically requires both an\nexpert and an amateur model, thus increasing computational resource demands. By\nintegrating contrastive prompts with distillation, DCD obviates the need for an\namateur model and reduces memory usage. Our evaluations demonstrate that DCD\nsignificantly enhances LLM performance across a range of reasoning benchmarks,\nsurpassing both CD and existing methods in the GSM8K and StrategyQA datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a straightforward approach called Distillation Contrastive\nDecoding (DCD) to enhance the reasoning capabilities of Large Language Models\n(LLMs) during inference. In contrast to previous approaches that relied on\nsmaller amateur models or analysis of hidden state differences, DCD employs\nContrastive Chain-of-thought Prompting and advanced distillation techniques,\nincluding Dropout and Quantization. This approach effectively addresses the\nlimitations of Contrastive Decoding (CD), which typically requires both an\nexpert and an amateur model, thus increasing computational resource demands. By\nintegrating contrastive prompts with distillation, DCD obviates the need for an\namateur model and reduces memory usage. Our evaluations demonstrate that DCD\nsignificantly enhances LLM performance across a range of reasoning benchmarks,\nsurpassing both CD and existing methods in the GSM8K and StrategyQA datasets."
                },
                "authors": [
                    {
                        "name": "Phuc Phan"
                    },
                    {
                        "name": "Hieu Tran"
                    },
                    {
                        "name": "Long Phan"
                    }
                ],
                "author_detail": {
                    "name": "Long Phan"
                },
                "author": "Long Phan",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14874v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14874v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12867v1",
                "updated": "2024-08-23T06:48:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    6,
                    48,
                    46,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T06:48:46Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    6,
                    48,
                    46,
                    4,
                    236,
                    0
                ],
                "title": "Semantic Alignment for Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Alignment for Multimodal Large Language Models"
                },
                "summary": "Research on Multi-modal Large Language Models (MLLMs) towards the multi-image\ncross-modal instruction has received increasing attention and made significant\nprogress, particularly in scenarios involving closely resembling images (e.g.,\nchange captioning). Existing MLLMs typically follow a two-step process in their\npipelines: first, extracting visual tokens independently for each input image,\nand then aligning these visual tokens from different images with the Large\nLanguage Model (LLM) in its textual feature space. However, the independent\nextraction of visual tokens for each image may result in different semantics\nbeing prioritized for different images in the first step, leading to a lack of\npreservation of linking information among images for subsequent LLM analysis.\nThis issue becomes more serious in scenarios where significant variations exist\namong the images (e.g., visual storytelling). To address this challenge, we\nintroduce Semantic Alignment for Multi-modal large language models (SAM). By\ninvolving the bidirectional semantic guidance between different images in the\nvisual-token extraction process, SAM aims to enhance the preservation of\nlinking information for coherent analysis and align the semantics of different\nimages before feeding them into LLM. As the test bed, we propose a large-scale\ndataset named MmLINK consisting of 69K samples. Different from most existing\ndatasets for MLLMs fine-tuning, our MmLINK dataset comprises multi-modal\ninstructions with significantly diverse images. Extensive experiments on the\ngroup captioning task and the storytelling task prove the effectiveness of our\nSAM model, surpassing the state-of-the-art methods by a large margin (+37% for\ngroup captioning and +22% for storytelling on CIDEr score). Project page:\nhttps://mccartney01.github.io/SAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research on Multi-modal Large Language Models (MLLMs) towards the multi-image\ncross-modal instruction has received increasing attention and made significant\nprogress, particularly in scenarios involving closely resembling images (e.g.,\nchange captioning). Existing MLLMs typically follow a two-step process in their\npipelines: first, extracting visual tokens independently for each input image,\nand then aligning these visual tokens from different images with the Large\nLanguage Model (LLM) in its textual feature space. However, the independent\nextraction of visual tokens for each image may result in different semantics\nbeing prioritized for different images in the first step, leading to a lack of\npreservation of linking information among images for subsequent LLM analysis.\nThis issue becomes more serious in scenarios where significant variations exist\namong the images (e.g., visual storytelling). To address this challenge, we\nintroduce Semantic Alignment for Multi-modal large language models (SAM). By\ninvolving the bidirectional semantic guidance between different images in the\nvisual-token extraction process, SAM aims to enhance the preservation of\nlinking information for coherent analysis and align the semantics of different\nimages before feeding them into LLM. As the test bed, we propose a large-scale\ndataset named MmLINK consisting of 69K samples. Different from most existing\ndatasets for MLLMs fine-tuning, our MmLINK dataset comprises multi-modal\ninstructions with significantly diverse images. Extensive experiments on the\ngroup captioning task and the storytelling task prove the effectiveness of our\nSAM model, surpassing the state-of-the-art methods by a large margin (+37% for\ngroup captioning and +22% for storytelling on CIDEr score). Project page:\nhttps://mccartney01.github.io/SAM."
                },
                "authors": [
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Mengze Li"
                    },
                    {
                        "name": "Jingyuan Chen"
                    },
                    {
                        "name": "Wei Ji"
                    },
                    {
                        "name": "Wang Lin"
                    },
                    {
                        "name": "Jinyang Gao"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Zhou Zhao"
                    },
                    {
                        "name": "Fei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Wu"
                },
                "author": "Fei Wu",
                "arxiv_comment": "Accepted by MM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12194v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12194v2",
                "updated": "2024-08-23T06:46:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    6,
                    46,
                    41,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-22T08:16:07Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    8,
                    16,
                    7,
                    3,
                    235,
                    0
                ],
                "title": "Large Language Models as Foundations for Next-Gen Dense Retrieval: A\n  Comprehensive Empirical Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models as Foundations for Next-Gen Dense Retrieval: A\n  Comprehensive Empirical Assessment"
                },
                "summary": "Pretrained language models like BERT and T5 serve as crucial backbone\nencoders for dense retrieval. However, these models often exhibit limited\ngeneralization capabilities and face challenges in improving in domain\naccuracy. Recent research has explored using large language models (LLMs) as\nretrievers, achieving SOTA performance across various tasks. Despite these\nadvancements, the specific benefits of LLMs over traditional retrievers and the\nimpact of different LLM configurations, such as parameter sizes, pretraining\nduration, and alignment processes on retrieval tasks remain unclear. In this\nwork, we conduct a comprehensive empirical study on a wide range of retrieval\ntasks, including in domain accuracy, data efficiency, zero shot generalization,\nlengthy retrieval, instruction based retrieval, and multi task learning. We\nevaluate over 15 different backbone LLMs and non LLMs. Our findings reveal that\nlarger models and extensive pretraining consistently enhance in domain accuracy\nand data efficiency. Additionally, larger models demonstrate significant\npotential in zero shot generalization, lengthy retrieval, instruction based\nretrieval, and multi task learning. These results underscore the advantages of\nLLMs as versatile and effective backbone encoders in dense retrieval, providing\nvaluable insights for future research and development in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pretrained language models like BERT and T5 serve as crucial backbone\nencoders for dense retrieval. However, these models often exhibit limited\ngeneralization capabilities and face challenges in improving in domain\naccuracy. Recent research has explored using large language models (LLMs) as\nretrievers, achieving SOTA performance across various tasks. Despite these\nadvancements, the specific benefits of LLMs over traditional retrievers and the\nimpact of different LLM configurations, such as parameter sizes, pretraining\nduration, and alignment processes on retrieval tasks remain unclear. In this\nwork, we conduct a comprehensive empirical study on a wide range of retrieval\ntasks, including in domain accuracy, data efficiency, zero shot generalization,\nlengthy retrieval, instruction based retrieval, and multi task learning. We\nevaluate over 15 different backbone LLMs and non LLMs. Our findings reveal that\nlarger models and extensive pretraining consistently enhance in domain accuracy\nand data efficiency. Additionally, larger models demonstrate significant\npotential in zero shot generalization, lengthy retrieval, instruction based\nretrieval, and multi task learning. These results underscore the advantages of\nLLMs as versatile and effective backbone encoders in dense retrieval, providing\nvaluable insights for future research and development in this field."
                },
                "authors": [
                    {
                        "name": "Kun Luo"
                    },
                    {
                        "name": "Minghao Qin"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "arxiv_comment": "Submitted to EMNLP24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12194v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12194v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15624v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15624v2",
                "updated": "2024-08-23T06:44:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    6,
                    44,
                    36,
                    4,
                    236,
                    0
                ],
                "published": "2024-03-22T21:28:19Z",
                "published_parsed": [
                    2024,
                    3,
                    22,
                    21,
                    28,
                    19,
                    4,
                    82,
                    0
                ],
                "title": "Semantic Gaussians: Open-Vocabulary Scene Understanding with 3D Gaussian\n  Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Gaussians: Open-Vocabulary Scene Understanding with 3D Gaussian\n  Splatting"
                },
                "summary": "Open-vocabulary 3D scene understanding presents a significant challenge in\ncomputer vision, with wide-ranging applications in embodied agents and\naugmented reality systems. Existing methods adopt neurel rendering methods as\n3D representations and jointly optimize color and semantic features to achieve\nrendering and scene understanding simultaneously. In this paper, we introduce\nSemantic Gaussians, a novel open-vocabulary scene understanding approach based\non 3D Gaussian Splatting. Our key idea is to distill knowledge from 2D\npre-trained models to 3D Gaussians. Unlike existing methods, we design a\nversatile projection approach that maps various 2D semantic features from\npre-trained image encoders into a novel semantic component of 3D Gaussians,\nwhich is based on spatial relationship and need no additional training. We\nfurther build a 3D semantic network that directly predicts the semantic\ncomponent from raw 3D Gaussians for fast inference. The quantitative results on\nScanNet segmentation and LERF object localization demonstates the superior\nperformance of our method. Additionally, we explore several applications of\nSemantic Gaussians including object part segmentation, instance segmentation,\nscene editing, and spatiotemporal segmentation with better qualitative results\nover 2D and 3D baselines, highlighting its versatility and effectiveness on\nsupporting diverse downstream tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-vocabulary 3D scene understanding presents a significant challenge in\ncomputer vision, with wide-ranging applications in embodied agents and\naugmented reality systems. Existing methods adopt neurel rendering methods as\n3D representations and jointly optimize color and semantic features to achieve\nrendering and scene understanding simultaneously. In this paper, we introduce\nSemantic Gaussians, a novel open-vocabulary scene understanding approach based\non 3D Gaussian Splatting. Our key idea is to distill knowledge from 2D\npre-trained models to 3D Gaussians. Unlike existing methods, we design a\nversatile projection approach that maps various 2D semantic features from\npre-trained image encoders into a novel semantic component of 3D Gaussians,\nwhich is based on spatial relationship and need no additional training. We\nfurther build a 3D semantic network that directly predicts the semantic\ncomponent from raw 3D Gaussians for fast inference. The quantitative results on\nScanNet segmentation and LERF object localization demonstates the superior\nperformance of our method. Additionally, we explore several applications of\nSemantic Gaussians including object part segmentation, instance segmentation,\nscene editing, and spatiotemporal segmentation with better qualitative results\nover 2D and 3D baselines, highlighting its versatility and effectiveness on\nsupporting diverse downstream tasks."
                },
                "authors": [
                    {
                        "name": "Jun Guo"
                    },
                    {
                        "name": "Xiaojian Ma"
                    },
                    {
                        "name": "Yue Fan"
                    },
                    {
                        "name": "Huaping Liu"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "arxiv_comment": "Project page: see https://semantic-gaussians.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15624v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15624v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12857v1",
                "updated": "2024-08-23T05:54:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    5,
                    54,
                    53,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T05:54:53Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    5,
                    54,
                    53,
                    4,
                    236,
                    0
                ],
                "title": "Memory-Efficient LLM Training with Online Subspace Descent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Efficient LLM Training with Online Subspace Descent"
                },
                "summary": "Recently, a wide range of memory-efficient LLM training algorithms have\ngained substantial popularity. These methods leverage the low-rank structure of\ngradients to project optimizer states into a subspace using projection matrix\nfound by singular value decomposition (SVD). However, convergence of these\nalgorithms is highly dependent on the update rules of their projection matrix.\nIn this work, we provide the \\emph{first} convergence guarantee for arbitrary\nupdate rules of projection matrix. This guarantee is generally applicable to\noptimizers that can be analyzed with Hamiltonian Descent, including most common\nones, such as LION, Adam. Inspired by our theoretical understanding, we propose\nOnline Subspace Descent, a new family of subspace descent optimizer without\nSVD. Instead of updating the projection matrix with eigenvectors, Online\nSubspace Descent updates the projection matrix with online PCA. Online Subspace\nDescent is flexible and introduces only minimum overhead to training. We show\nthat for the task of pretraining LLaMA models ranging from 60M to 7B parameters\non the C4 dataset, Online Subspace Descent achieves lower perplexity and better\ndownstream tasks performance than state-of-the-art low-rank training methods\nacross different settings and narrows the gap with full-rank baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, a wide range of memory-efficient LLM training algorithms have\ngained substantial popularity. These methods leverage the low-rank structure of\ngradients to project optimizer states into a subspace using projection matrix\nfound by singular value decomposition (SVD). However, convergence of these\nalgorithms is highly dependent on the update rules of their projection matrix.\nIn this work, we provide the \\emph{first} convergence guarantee for arbitrary\nupdate rules of projection matrix. This guarantee is generally applicable to\noptimizers that can be analyzed with Hamiltonian Descent, including most common\nones, such as LION, Adam. Inspired by our theoretical understanding, we propose\nOnline Subspace Descent, a new family of subspace descent optimizer without\nSVD. Instead of updating the projection matrix with eigenvectors, Online\nSubspace Descent updates the projection matrix with online PCA. Online Subspace\nDescent is flexible and introduces only minimum overhead to training. We show\nthat for the task of pretraining LLaMA models ranging from 60M to 7B parameters\non the C4 dataset, Online Subspace Descent achieves lower perplexity and better\ndownstream tasks performance than state-of-the-art low-rank training methods\nacross different settings and narrows the gap with full-rank baselines."
                },
                "authors": [
                    {
                        "name": "Kaizhao Liang"
                    },
                    {
                        "name": "Bo Liu"
                    },
                    {
                        "name": "Lizhang Chen"
                    },
                    {
                        "name": "Qiang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Liu"
                },
                "author": "Qiang Liu",
                "arxiv_comment": "Code is available at\n  https://github.com/kyleliang919/Online-Subspace-Descent",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12844v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12844v1",
                "updated": "2024-08-23T05:25:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    5,
                    25,
                    11,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T05:25:11Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    5,
                    25,
                    11,
                    4,
                    236,
                    0
                ],
                "title": "Predicting Affective States from Screen Text Sentiment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting Affective States from Screen Text Sentiment"
                },
                "summary": "The proliferation of mobile sensing technologies has enabled the study of\nvarious physiological and behavioural phenomena through unobtrusive data\ncollection from smartphone sensors. This approach offers real-time insights\ninto individuals' physical and mental states, creating opportunities for\npersonalised treatment and interventions. However, the potential of analysing\nthe textual content viewed on smartphones to predict affective states remains\nunderexplored. To better understand how the screen text that users are exposed\nto and interact with can influence their affects, we investigated a subset of\ndata obtained from a digital phenotyping study of Australian university\nstudents conducted in 2023. We employed linear regression, zero-shot, and\nmulti-shot prompting using a large language model (LLM) to analyse\nrelationships between screen text and affective states. Our findings indicate\nthat multi-shot prompting substantially outperforms both linear regression and\nzero-shot prompting, highlighting the importance of context in affect\nprediction. We discuss the value of incorporating textual and sentiment data\nfor improving affect prediction, providing a basis for future advancements in\nunderstanding smartphone use and wellbeing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of mobile sensing technologies has enabled the study of\nvarious physiological and behavioural phenomena through unobtrusive data\ncollection from smartphone sensors. This approach offers real-time insights\ninto individuals' physical and mental states, creating opportunities for\npersonalised treatment and interventions. However, the potential of analysing\nthe textual content viewed on smartphones to predict affective states remains\nunderexplored. To better understand how the screen text that users are exposed\nto and interact with can influence their affects, we investigated a subset of\ndata obtained from a digital phenotyping study of Australian university\nstudents conducted in 2023. We employed linear regression, zero-shot, and\nmulti-shot prompting using a large language model (LLM) to analyse\nrelationships between screen text and affective states. Our findings indicate\nthat multi-shot prompting substantially outperforms both linear regression and\nzero-shot prompting, highlighting the importance of context in affect\nprediction. We discuss the value of incorporating textual and sentiment data\nfor improving affect prediction, providing a basis for future advancements in\nunderstanding smartphone use and wellbeing."
                },
                "authors": [
                    {
                        "name": "Songyan Teng"
                    },
                    {
                        "name": "Tianyi Zhang"
                    },
                    {
                        "name": "Simon D'Alfonso"
                    },
                    {
                        "name": "Vassilis Kostakos"
                    }
                ],
                "author_detail": {
                    "name": "Vassilis Kostakos"
                },
                "author": "Vassilis Kostakos",
                "arxiv_doi": "10.1145/3675094.3678489",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3675094.3678489",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.12844v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12844v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "7 pages",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12840v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12840v1",
                "updated": "2024-08-23T05:11:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    5,
                    11,
                    22,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T05:11:22Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    5,
                    11,
                    22,
                    4,
                    236,
                    0
                ],
                "title": "HGNAS: Hardware-Aware Graph Neural Architecture Search for Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HGNAS: Hardware-Aware Graph Neural Architecture Search for Edge Devices"
                },
                "summary": "Graph Neural Networks (GNNs) are becoming increasingly popular for\ngraph-based learning tasks such as point cloud processing due to their\nstate-of-the-art (SOTA) performance. Nevertheless, the research community has\nprimarily focused on improving model expressiveness, lacking consideration of\nhow to design efficient GNN models for edge scenarios with real-time\nrequirements and limited resources. Examining existing GNN models reveals\nvaried execution across platforms and frequent Out-Of-Memory (OOM) problems,\nhighlighting the need for hardware-aware GNN design. To address this challenge,\nthis work proposes a novel hardware-aware graph neural architecture search\nframework tailored for resource constraint edge devices, namely HGNAS. To\nachieve hardware awareness, HGNAS integrates an efficient GNN hardware\nperformance predictor that evaluates the latency and peak memory usage of GNNs\nin milliseconds. Meanwhile, we study GNN memory usage during inference and\noffer a peak memory estimation method, enhancing the robustness of architecture\nevaluations when combined with predictor outcomes. Furthermore, HGNAS\nconstructs a fine-grained design space to enable the exploration of extreme\nperformance architectures by decoupling the GNN paradigm. In addition, the\nmulti-stage hierarchical search strategy is leveraged to facilitate the\nnavigation of huge candidates, which can reduce the single search time to a few\nGPU hours. To the best of our knowledge, HGNAS is the first automated GNN\ndesign framework for edge devices, and also the first work to achieve hardware\nawareness of GNNs across different platforms. Extensive experiments across\nvarious applications and edge devices have proven the superiority of HGNAS. It\ncan achieve up to a 10.6x speedup and an 82.5% peak memory reduction with\nnegligible accuracy loss compared to DGCNN on ModelNet40.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) are becoming increasingly popular for\ngraph-based learning tasks such as point cloud processing due to their\nstate-of-the-art (SOTA) performance. Nevertheless, the research community has\nprimarily focused on improving model expressiveness, lacking consideration of\nhow to design efficient GNN models for edge scenarios with real-time\nrequirements and limited resources. Examining existing GNN models reveals\nvaried execution across platforms and frequent Out-Of-Memory (OOM) problems,\nhighlighting the need for hardware-aware GNN design. To address this challenge,\nthis work proposes a novel hardware-aware graph neural architecture search\nframework tailored for resource constraint edge devices, namely HGNAS. To\nachieve hardware awareness, HGNAS integrates an efficient GNN hardware\nperformance predictor that evaluates the latency and peak memory usage of GNNs\nin milliseconds. Meanwhile, we study GNN memory usage during inference and\noffer a peak memory estimation method, enhancing the robustness of architecture\nevaluations when combined with predictor outcomes. Furthermore, HGNAS\nconstructs a fine-grained design space to enable the exploration of extreme\nperformance architectures by decoupling the GNN paradigm. In addition, the\nmulti-stage hierarchical search strategy is leveraged to facilitate the\nnavigation of huge candidates, which can reduce the single search time to a few\nGPU hours. To the best of our knowledge, HGNAS is the first automated GNN\ndesign framework for edge devices, and also the first work to achieve hardware\nawareness of GNNs across different platforms. Extensive experiments across\nvarious applications and edge devices have proven the superiority of HGNAS. It\ncan achieve up to a 10.6x speedup and an 82.5% peak memory reduction with\nnegligible accuracy loss compared to DGCNN on ModelNet40."
                },
                "authors": [
                    {
                        "name": "Ao Zhou"
                    },
                    {
                        "name": "Jianlei Yang"
                    },
                    {
                        "name": "Yingjie Qi"
                    },
                    {
                        "name": "Tong Qiao"
                    },
                    {
                        "name": "Yumeng Shi"
                    },
                    {
                        "name": "Cenlin Duan"
                    },
                    {
                        "name": "Weisheng Zhao"
                    },
                    {
                        "name": "Chunming Hu"
                    }
                ],
                "author_detail": {
                    "name": "Chunming Hu"
                },
                "author": "Chunming Hu",
                "arxiv_comment": "Accepted by IEEE Transactions on Computers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12840v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12840v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.18093v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.18093v2",
                "updated": "2024-08-23T05:03:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    5,
                    3,
                    44,
                    4,
                    236,
                    0
                ],
                "published": "2024-02-28T06:28:15Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    6,
                    28,
                    15,
                    2,
                    59,
                    0
                ],
                "title": "ChatSpamDetector: Leveraging Large Language Models for Effective\n  Phishing Email Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatSpamDetector: Leveraging Large Language Models for Effective\n  Phishing Email Detection"
                },
                "summary": "The proliferation of phishing sites and emails poses significant challenges\nto existing cybersecurity efforts. Despite advances in malicious email filters\nand email security protocols, problems with oversight and false positives\npersist. Users often struggle to understand why emails are flagged as\npotentially fraudulent, risking the possibility of missing important\ncommunications or mistakenly trusting deceptive phishing emails. This study\nintroduces ChatSpamDetector, a system that uses large language models (LLMs) to\ndetect phishing emails. By converting email data into a prompt suitable for LLM\nanalysis, the system provides a highly accurate determination of whether an\nemail is phishing or not. Importantly, it offers detailed reasoning for its\nphishing determinations, assisting users in making informed decisions about how\nto handle suspicious emails. We conducted an evaluation using a comprehensive\nphishing email dataset and compared our system to several LLMs and baseline\nsystems. We confirmed that our system using GPT-4 has superior detection\ncapabilities with an accuracy of 99.70%. Advanced contextual interpretation by\nLLMs enables the identification of various phishing tactics and impersonations,\nmaking them a potentially powerful tool in the fight against email-based\nphishing threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of phishing sites and emails poses significant challenges\nto existing cybersecurity efforts. Despite advances in malicious email filters\nand email security protocols, problems with oversight and false positives\npersist. Users often struggle to understand why emails are flagged as\npotentially fraudulent, risking the possibility of missing important\ncommunications or mistakenly trusting deceptive phishing emails. This study\nintroduces ChatSpamDetector, a system that uses large language models (LLMs) to\ndetect phishing emails. By converting email data into a prompt suitable for LLM\nanalysis, the system provides a highly accurate determination of whether an\nemail is phishing or not. Importantly, it offers detailed reasoning for its\nphishing determinations, assisting users in making informed decisions about how\nto handle suspicious emails. We conducted an evaluation using a comprehensive\nphishing email dataset and compared our system to several LLMs and baseline\nsystems. We confirmed that our system using GPT-4 has superior detection\ncapabilities with an accuracy of 99.70%. Advanced contextual interpretation by\nLLMs enables the identification of various phishing tactics and impersonations,\nmaking them a potentially powerful tool in the fight against email-based\nphishing threats."
                },
                "authors": [
                    {
                        "name": "Takashi Koide"
                    },
                    {
                        "name": "Naoki Fukushi"
                    },
                    {
                        "name": "Hiroki Nakano"
                    },
                    {
                        "name": "Daiki Chiba"
                    }
                ],
                "author_detail": {
                    "name": "Daiki Chiba"
                },
                "author": "Daiki Chiba",
                "arxiv_comment": "Accepted at SecureComm 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.18093v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.18093v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12834v1",
                "updated": "2024-08-23T04:44:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    4,
                    44,
                    5,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T04:44:05Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    4,
                    44,
                    5,
                    4,
                    236,
                    0
                ],
                "title": "CLLMFS: A Contrastive Learning enhanced Large Language Model Framework\n  for Few-Shot Named Entity Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLLMFS: A Contrastive Learning enhanced Large Language Model Framework\n  for Few-Shot Named Entity Recognition"
                },
                "summary": "Few-shot Named Entity Recognition (NER), the task of identifying named\nentities with only a limited amount of labeled data, has gained increasing\nsignificance in natural language processing. While existing methodologies have\nshown some effectiveness, such as enriching label semantics through various\nprompting modes or employing metric learning techniques, their performance\nexhibits limited robustness across diverse domains due to the lack of rich\nknowledge in their pre-trained models. To address this issue, we propose\nCLLMFS, a Contrastive Learning enhanced Large Language Model (LLM) Framework\nfor Few-Shot Named Entity Recognition, achieving promising results with limited\ntraining data. Considering the impact of LLM's internal representations on\ndownstream tasks, CLLMFS integrates Low-Rank Adaptation (LoRA) and contrastive\nlearning mechanisms specifically tailored for few-shot NER. By enhancing the\nmodel's internal representations, CLLMFS effectively improves both entity\nboundary awareness ability and entity recognition accuracy. Our method has\nachieved state-of-the-art performance improvements on F1-score ranging from\n2.58\\% to 97.74\\% over existing best-performing methods across several\nrecognized benchmarks. Furthermore, through cross-domain NER experiments\nconducted on multiple datasets, we have further validated the robust\ngeneralization capability of our method. Our code will be released in the near\nfuture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot Named Entity Recognition (NER), the task of identifying named\nentities with only a limited amount of labeled data, has gained increasing\nsignificance in natural language processing. While existing methodologies have\nshown some effectiveness, such as enriching label semantics through various\nprompting modes or employing metric learning techniques, their performance\nexhibits limited robustness across diverse domains due to the lack of rich\nknowledge in their pre-trained models. To address this issue, we propose\nCLLMFS, a Contrastive Learning enhanced Large Language Model (LLM) Framework\nfor Few-Shot Named Entity Recognition, achieving promising results with limited\ntraining data. Considering the impact of LLM's internal representations on\ndownstream tasks, CLLMFS integrates Low-Rank Adaptation (LoRA) and contrastive\nlearning mechanisms specifically tailored for few-shot NER. By enhancing the\nmodel's internal representations, CLLMFS effectively improves both entity\nboundary awareness ability and entity recognition accuracy. Our method has\nachieved state-of-the-art performance improvements on F1-score ranging from\n2.58\\% to 97.74\\% over existing best-performing methods across several\nrecognized benchmarks. Furthermore, through cross-domain NER experiments\nconducted on multiple datasets, we have further validated the robust\ngeneralization capability of our method. Our code will be released in the near\nfuture."
                },
                "authors": [
                    {
                        "name": "Yafeng Zhang"
                    },
                    {
                        "name": "Zilan Yu"
                    },
                    {
                        "name": "Yuang Huang"
                    },
                    {
                        "name": "Jing Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Tang"
                },
                "author": "Jing Tang",
                "arxiv_comment": "27TH EUROPEAN CONFERENCE ON ARTIFICIAL INTELLIGENCE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12832v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12832v1",
                "updated": "2024-08-23T04:28:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    4,
                    28,
                    56,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T04:28:56Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    4,
                    28,
                    56,
                    4,
                    236,
                    0
                ],
                "title": "LIMP: Large Language Model Enhanced Intent-aware Mobility Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LIMP: Large Language Model Enhanced Intent-aware Mobility Prediction"
                },
                "summary": "Human mobility prediction is essential for applications like urban planning\nand transportation management, yet it remains challenging due to the complex,\noften implicit, intentions behind human behavior. Existing models predominantly\nfocus on spatiotemporal patterns, paying less attention to the underlying\nintentions that govern movements. Recent advancements in large language models\n(LLMs) offer a promising alternative research angle for integrating commonsense\nreasoning into mobility prediction. However, it is a non-trivial problem\nbecause LLMs are not natively built for mobility intention inference, and they\nalso face scalability issues and integration difficulties with spatiotemporal\nmodels. To address these challenges, we propose a novel LIMP (LLMs for\nIntent-ware Mobility Prediction) framework. Specifically, LIMP introduces an\n\"Analyze-Abstract-Infer\" (A2I) agentic workflow to unleash LLM's commonsense\nreasoning power for mobility intention inference. Besides, we design an\nefficient fine-tuning scheme to transfer reasoning power from commercial LLM to\nsmaller-scale, open-source language model, ensuring LIMP's scalability to\nmillions of mobility records. Moreover, we propose a transformer-based\nintention-aware mobility prediction model to effectively harness the intention\ninference ability of LLM. Evaluated on two real-world datasets, LIMP\nsignificantly outperforms baseline models, demonstrating improved accuracy in\nnext-location prediction and effective intention inference. The\ninterpretability of intention-aware mobility prediction highlights our LIMP\nframework's potential for real-world applications. Codes and data can be found\nin https://github.com/tsinghua-fib-lab/LIMP .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human mobility prediction is essential for applications like urban planning\nand transportation management, yet it remains challenging due to the complex,\noften implicit, intentions behind human behavior. Existing models predominantly\nfocus on spatiotemporal patterns, paying less attention to the underlying\nintentions that govern movements. Recent advancements in large language models\n(LLMs) offer a promising alternative research angle for integrating commonsense\nreasoning into mobility prediction. However, it is a non-trivial problem\nbecause LLMs are not natively built for mobility intention inference, and they\nalso face scalability issues and integration difficulties with spatiotemporal\nmodels. To address these challenges, we propose a novel LIMP (LLMs for\nIntent-ware Mobility Prediction) framework. Specifically, LIMP introduces an\n\"Analyze-Abstract-Infer\" (A2I) agentic workflow to unleash LLM's commonsense\nreasoning power for mobility intention inference. Besides, we design an\nefficient fine-tuning scheme to transfer reasoning power from commercial LLM to\nsmaller-scale, open-source language model, ensuring LIMP's scalability to\nmillions of mobility records. Moreover, we propose a transformer-based\nintention-aware mobility prediction model to effectively harness the intention\ninference ability of LLM. Evaluated on two real-world datasets, LIMP\nsignificantly outperforms baseline models, demonstrating improved accuracy in\nnext-location prediction and effective intention inference. The\ninterpretability of intention-aware mobility prediction highlights our LIMP\nframework's potential for real-world applications. Codes and data can be found\nin https://github.com/tsinghua-fib-lab/LIMP ."
                },
                "authors": [
                    {
                        "name": "Songwei Li"
                    },
                    {
                        "name": "Jie Feng"
                    },
                    {
                        "name": "Jiawei Chi"
                    },
                    {
                        "name": "Xinyuan Hu"
                    },
                    {
                        "name": "Xiaomeng Zhao"
                    },
                    {
                        "name": "Fengli Xu"
                    }
                ],
                "author_detail": {
                    "name": "Fengli Xu"
                },
                "author": "Fengli Xu",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12832v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12832v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12830v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12830v1",
                "updated": "2024-08-23T04:25:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    4,
                    25,
                    9,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T04:25:09Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    4,
                    25,
                    9,
                    4,
                    236,
                    0
                ],
                "title": "SAMBO-RL: Shifts-aware Model-based Offline Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAMBO-RL: Shifts-aware Model-based Offline Reinforcement Learning"
                },
                "summary": "Model-based Offline Reinforcement Learning trains policies based on offline\ndatasets and model dynamics, without direct real-world environment\ninteractions. However, this method is inherently challenged by distribution\nshift. Previous approaches have primarily focused on tackling this issue\ndirectly leveraging off-policy mechanisms and heuristic uncertainty in model\ndynamics, but they resulted in inconsistent objectives and lacked a unified\ntheoretical foundation. This paper offers a comprehensive analysis that\ndisentangles the problem into two key components: model bias and policy shift.\nWe provide both theoretical insights and empirical evidence to demonstrate how\nthese factors lead to inaccuracies in value function estimation and impose\nimplicit restrictions on policy learning. To address these challenges, we\nderive adjustment terms for model bias and policy shift within a unified\nprobabilistic inference framework. These adjustments are seamlessly integrated\ninto the vanilla reward function to create a novel Shifts-aware Reward (SAR),\naiming at refining value learning and facilitating policy training.\nFurthermore, we introduce Shifts-aware Model-based Offline Reinforcement\nLearning (SAMBO-RL), a practical framework that efficiently trains classifiers\nto approximate the SAR for policy optimization. Empirically, we show that SAR\neffectively mitigates distribution shift, and SAMBO-RL demonstrates superior\nperformance across various benchmarks, underscoring its practical effectiveness\nand validating our theoretical analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-based Offline Reinforcement Learning trains policies based on offline\ndatasets and model dynamics, without direct real-world environment\ninteractions. However, this method is inherently challenged by distribution\nshift. Previous approaches have primarily focused on tackling this issue\ndirectly leveraging off-policy mechanisms and heuristic uncertainty in model\ndynamics, but they resulted in inconsistent objectives and lacked a unified\ntheoretical foundation. This paper offers a comprehensive analysis that\ndisentangles the problem into two key components: model bias and policy shift.\nWe provide both theoretical insights and empirical evidence to demonstrate how\nthese factors lead to inaccuracies in value function estimation and impose\nimplicit restrictions on policy learning. To address these challenges, we\nderive adjustment terms for model bias and policy shift within a unified\nprobabilistic inference framework. These adjustments are seamlessly integrated\ninto the vanilla reward function to create a novel Shifts-aware Reward (SAR),\naiming at refining value learning and facilitating policy training.\nFurthermore, we introduce Shifts-aware Model-based Offline Reinforcement\nLearning (SAMBO-RL), a practical framework that efficiently trains classifiers\nto approximate the SAR for policy optimization. Empirically, we show that SAR\neffectively mitigates distribution shift, and SAMBO-RL demonstrates superior\nperformance across various benchmarks, underscoring its practical effectiveness\nand validating our theoretical analysis."
                },
                "authors": [
                    {
                        "name": "Wang Luo"
                    },
                    {
                        "name": "Haoran Li"
                    },
                    {
                        "name": "Zicheng Zhang"
                    },
                    {
                        "name": "Congying Han"
                    },
                    {
                        "name": "Jiayu Lv"
                    },
                    {
                        "name": "Tiande Guo"
                    }
                ],
                "author_detail": {
                    "name": "Tiande Guo"
                },
                "author": "Tiande Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12830v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12830v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11824v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11824v2",
                "updated": "2024-08-23T04:13:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    4,
                    13,
                    48,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-05T06:31:39Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    6,
                    31,
                    39,
                    0,
                    218,
                    0
                ],
                "title": "AppAgent v2: Advanced Agent for Flexible Mobile Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AppAgent v2: Advanced Agent for Flexible Mobile Interactions"
                },
                "summary": "With the advancement of Multimodal Large Language Models (MLLM), LLM-driven\nvisual agents are increasingly impacting software interfaces, particularly\nthose with graphical user interfaces. This work introduces a novel LLM-based\nmultimodal agent framework for mobile devices. This framework, capable of\nnavigating mobile devices, emulates human-like interactions. Our agent\nconstructs a flexible action space that enhances adaptability across various\napplications including parser, text and vision descriptions. The agent operates\nthrough two main phases: exploration and deployment. During the exploration\nphase, functionalities of user interface elements are documented either through\nagent-driven or manual explorations into a customized structured knowledge\nbase. In the deployment phase, RAG technology enables efficient retrieval and\nupdate from this knowledge base, thereby empowering the agent to perform tasks\neffectively and accurately. This includes performing complex, multi-step\noperations across various applications, thereby demonstrating the framework's\nadaptability and precision in handling customized task workflows. Our\nexperimental results across various benchmarks demonstrate the framework's\nsuperior performance, confirming its effectiveness in real-world scenarios. Our\ncode will be open source soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancement of Multimodal Large Language Models (MLLM), LLM-driven\nvisual agents are increasingly impacting software interfaces, particularly\nthose with graphical user interfaces. This work introduces a novel LLM-based\nmultimodal agent framework for mobile devices. This framework, capable of\nnavigating mobile devices, emulates human-like interactions. Our agent\nconstructs a flexible action space that enhances adaptability across various\napplications including parser, text and vision descriptions. The agent operates\nthrough two main phases: exploration and deployment. During the exploration\nphase, functionalities of user interface elements are documented either through\nagent-driven or manual explorations into a customized structured knowledge\nbase. In the deployment phase, RAG technology enables efficient retrieval and\nupdate from this knowledge base, thereby empowering the agent to perform tasks\neffectively and accurately. This includes performing complex, multi-step\noperations across various applications, thereby demonstrating the framework's\nadaptability and precision in handling customized task workflows. Our\nexperimental results across various benchmarks demonstrate the framework's\nsuperior performance, confirming its effectiveness in real-world scenarios. Our\ncode will be open source soon."
                },
                "authors": [
                    {
                        "name": "Yanda Li"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Wanqi Yang"
                    },
                    {
                        "name": "Bin Fu"
                    },
                    {
                        "name": "Pei Cheng"
                    },
                    {
                        "name": "Xin Chen"
                    },
                    {
                        "name": "Ling Chen"
                    },
                    {
                        "name": "Yunchao Wei"
                    }
                ],
                "author_detail": {
                    "name": "Yunchao Wei"
                },
                "author": "Yunchao Wei",
                "arxiv_comment": "Pre-print version, some content needs to be supplemented",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11824v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11824v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00276v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00276v4",
                "updated": "2024-08-23T04:06:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    4,
                    6,
                    41,
                    4,
                    236,
                    0
                ],
                "published": "2024-03-30T08:02:16Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    8,
                    2,
                    16,
                    5,
                    90,
                    0
                ],
                "title": "Instruction-Driven Game Engines on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-Driven Game Engines on Large Language Models"
                },
                "summary": "The Instruction-Driven Game Engine (IDGE) project aims to democratize game\ndevelopment by enabling a large language model (LLM) to follow free-form game\nrules and autonomously generate game-play processes. The IDGE allows users to\ncreate games by issuing simple natural language instructions, which\nsignificantly lowers the barrier for game development. We approach the learning\nprocess for IDGEs as a Next State Prediction task, wherein the model\nautoregressively predicts in-game states given player actions. It is a\nchallenging task because the computation of in-game states must be precise;\notherwise, slight errors could disrupt the game-play. To address this, we train\nthe IDGE in a curriculum manner that progressively increases the model's\nexposure to complex scenarios. Our initial progress lies in developing an IDGE\nfor Poker, a universally cherished card game. The engine we've designed not\nonly supports a wide range of poker variants but also allows for high\ncustomization of rules through natural language inputs. Furthermore, it also\nfavors rapid prototyping of new games from minimal samples, proposing an\ninnovative paradigm in game development that relies on minimal prompt and data\nengineering. This work lays the groundwork for future advancements in\ninstruction-driven game creation, potentially transforming how games are\ndesigned and played.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Instruction-Driven Game Engine (IDGE) project aims to democratize game\ndevelopment by enabling a large language model (LLM) to follow free-form game\nrules and autonomously generate game-play processes. The IDGE allows users to\ncreate games by issuing simple natural language instructions, which\nsignificantly lowers the barrier for game development. We approach the learning\nprocess for IDGEs as a Next State Prediction task, wherein the model\nautoregressively predicts in-game states given player actions. It is a\nchallenging task because the computation of in-game states must be precise;\notherwise, slight errors could disrupt the game-play. To address this, we train\nthe IDGE in a curriculum manner that progressively increases the model's\nexposure to complex scenarios. Our initial progress lies in developing an IDGE\nfor Poker, a universally cherished card game. The engine we've designed not\nonly supports a wide range of poker variants but also allows for high\ncustomization of rules through natural language inputs. Furthermore, it also\nfavors rapid prototyping of new games from minimal samples, proposing an\ninnovative paradigm in game development that relies on minimal prompt and data\nengineering. This work lays the groundwork for future advancements in\ninstruction-driven game creation, potentially transforming how games are\ndesigned and played."
                },
                "authors": [
                    {
                        "name": "Hongqiu Wu"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Xingyuan Liu"
                    },
                    {
                        "name": "Hai Zhao"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00276v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00276v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05842v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05842v2",
                "updated": "2024-08-23T03:57:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    3,
                    57,
                    24,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-11T18:32:29Z",
                "published_parsed": [
                    2024,
                    8,
                    11,
                    18,
                    32,
                    29,
                    6,
                    224,
                    0
                ],
                "title": "Scaling Virtual World with Delta-Engine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Virtual World with Delta-Engine"
                },
                "summary": "In this paper, we focus on the \\emph{virtual world}, a cyberspace where\npeople can live in. An ideal virtual world shares great similarity with our\nreal world. One of the crucial aspects is its evolving nature, reflected by\nindividuals' capability to grow and thereby influence the objective world. Such\ndynamics is unpredictable and beyond the reach of existing systems. For this,\nwe propose a special engine called \\textbf{\\emph{Delta-Engine}} to drive this\nvirtual world. $\\Delta$ associates the world's evolution to the engine's\nscalability. It consists of a base engine and a neural proxy. The base engine\nprograms the prototype of the virtual world; given a trigger, the neural proxy\ngenerates new snippets on the base engine through \\emph{incremental\nprediction}.\n  This paper presents a full-stack introduction to the delta-engine. The key\nfeature of the delta-engine is its scalability to unknown elements within the\nworld, Technically, it derives from the prefect co-work of the neural proxy and\nthe base engine, and the alignment with high-quality data. We introduce an\nengine-oriented fine-tuning method that embeds the base engine into the proxy.\nWe then discuss the human-LLM collaborative design to produce novel and\ninteresting data efficiently. Eventually, we propose three evaluation\nprinciples to comprehensively assess the performance of a delta engine: naive\nevaluation, incremental evaluation, and adversarial evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we focus on the \\emph{virtual world}, a cyberspace where\npeople can live in. An ideal virtual world shares great similarity with our\nreal world. One of the crucial aspects is its evolving nature, reflected by\nindividuals' capability to grow and thereby influence the objective world. Such\ndynamics is unpredictable and beyond the reach of existing systems. For this,\nwe propose a special engine called \\textbf{\\emph{Delta-Engine}} to drive this\nvirtual world. $\\Delta$ associates the world's evolution to the engine's\nscalability. It consists of a base engine and a neural proxy. The base engine\nprograms the prototype of the virtual world; given a trigger, the neural proxy\ngenerates new snippets on the base engine through \\emph{incremental\nprediction}.\n  This paper presents a full-stack introduction to the delta-engine. The key\nfeature of the delta-engine is its scalability to unknown elements within the\nworld, Technically, it derives from the prefect co-work of the neural proxy and\nthe base engine, and the alignment with high-quality data. We introduce an\nengine-oriented fine-tuning method that embeds the base engine into the proxy.\nWe then discuss the human-LLM collaborative design to produce novel and\ninteresting data efficiently. Eventually, we propose three evaluation\nprinciples to comprehensively assess the performance of a delta engine: naive\nevaluation, incremental evaluation, and adversarial evaluation."
                },
                "authors": [
                    {
                        "name": "Hongqiu Wu"
                    },
                    {
                        "name": "Zekai Xu"
                    },
                    {
                        "name": "Tianyang Xu"
                    },
                    {
                        "name": "Jiale Hong"
                    },
                    {
                        "name": "Weiqi Wu"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Hai Zhao"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Zhezhi He"
                    }
                ],
                "author_detail": {
                    "name": "Zhezhi He"
                },
                "author": "Zhezhi He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05842v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05842v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12821v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12821v1",
                "updated": "2024-08-23T03:45:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    3,
                    45,
                    31,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T03:45:31Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    3,
                    45,
                    31,
                    4,
                    236,
                    0
                ],
                "title": "Examining the Commitments and Difficulties Inherent in Multimodal\n  Foundation Models for Street View Imagery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Examining the Commitments and Difficulties Inherent in Multimodal\n  Foundation Models for Street View Imagery"
                },
                "summary": "The emergence of Large Language Models (LLMs) and multimodal foundation\nmodels (FMs) has generated heightened interest in their applications that\nintegrate vision and language. This paper investigates the capabilities of\nChatGPT-4V and Gemini Pro for Street View Imagery, Built Environment, and\nInterior by evaluating their performance across various tasks. The assessments\ninclude street furniture identification, pedestrian and car counts, and road\nwidth measurement in Street View Imagery; building function classification,\nbuilding age analysis, building height analysis, and building structure\nclassification in the Built Environment; and interior room classification,\ninterior design style analysis, interior furniture counts, and interior length\nmeasurement in Interior. The results reveal proficiency in length measurement,\nstyle analysis, question answering, and basic image understanding, but\nhighlight limitations in detailed recognition and counting tasks. While\nzero-shot learning shows potential, performance varies depending on the problem\ndomains and image complexities. This study provides new insights into the\nstrengths and weaknesses of multimodal foundation models for practical\nchallenges in Street View Imagery, Built Environment, and Interior. Overall,\nthe findings demonstrate foundational multimodal intelligence, emphasizing the\npotential of FMs to drive forward interdisciplinary applications at the\nintersection of computer vision and language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of Large Language Models (LLMs) and multimodal foundation\nmodels (FMs) has generated heightened interest in their applications that\nintegrate vision and language. This paper investigates the capabilities of\nChatGPT-4V and Gemini Pro for Street View Imagery, Built Environment, and\nInterior by evaluating their performance across various tasks. The assessments\ninclude street furniture identification, pedestrian and car counts, and road\nwidth measurement in Street View Imagery; building function classification,\nbuilding age analysis, building height analysis, and building structure\nclassification in the Built Environment; and interior room classification,\ninterior design style analysis, interior furniture counts, and interior length\nmeasurement in Interior. The results reveal proficiency in length measurement,\nstyle analysis, question answering, and basic image understanding, but\nhighlight limitations in detailed recognition and counting tasks. While\nzero-shot learning shows potential, performance varies depending on the problem\ndomains and image complexities. This study provides new insights into the\nstrengths and weaknesses of multimodal foundation models for practical\nchallenges in Street View Imagery, Built Environment, and Interior. Overall,\nthe findings demonstrate foundational multimodal intelligence, emphasizing the\npotential of FMs to drive forward interdisciplinary applications at the\nintersection of computer vision and language."
                },
                "authors": [
                    {
                        "name": "Zhenyuan Yang"
                    },
                    {
                        "name": "Xuhui Lin"
                    },
                    {
                        "name": "Qinyi He"
                    },
                    {
                        "name": "Ziye Huang"
                    },
                    {
                        "name": "Zhengliang Liu"
                    },
                    {
                        "name": "Hanqi Jiang"
                    },
                    {
                        "name": "Peng Shu"
                    },
                    {
                        "name": "Zihao Wu"
                    },
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Stephen Law"
                    },
                    {
                        "name": "Gengchen Mai"
                    },
                    {
                        "name": "Tianming Liu"
                    },
                    {
                        "name": "Tao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tao Yang"
                },
                "author": "Tao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12821v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12821v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11609v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11609v2",
                "updated": "2024-08-23T03:40:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    3,
                    40,
                    44,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-21T13:34:29Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    13,
                    34,
                    29,
                    2,
                    234,
                    0
                ],
                "title": "Xinyu: An Efficient LLM-based System for Commentary Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Xinyu: An Efficient LLM-based System for Commentary Generation"
                },
                "summary": "Commentary provides readers with a deep understanding of events by presenting\ndiverse arguments and evidence. However, creating commentary is a\ntime-consuming task, even for skilled commentators. Large language models\n(LLMs) have simplified the process of natural language generation, but their\ndirect application in commentary creation still faces challenges due to unique\ntask requirements. These requirements can be categorized into two levels: 1)\nfundamental requirements, which include creating well-structured and logically\nconsistent narratives, and 2) advanced requirements, which involve generating\nquality arguments and providing convincing evidence. In this paper, we\nintroduce Xinyu, an efficient LLM-based system designed to assist commentators\nin generating Chinese commentaries. To meet the fundamental requirements, we\ndeconstruct the generation process into sequential steps, proposing targeted\nstrategies and supervised fine-tuning (SFT) for each step. To address the\nadvanced requirements, we present an argument ranking model for arguments and\nestablish a comprehensive evidence database that includes up-to-date events and\nclassic books, thereby strengthening the substantiation of the evidence with\nretrieval augmented generation (RAG) technology. To evaluate the generated\ncommentaries more fairly, corresponding to the two-level requirements, we\nintroduce a comprehensive evaluation metric that considers five distinct\nperspectives in commentary generation. Our experiments confirm the\neffectiveness of our proposed system. We also observe a significant increase in\nthe efficiency of commentators in real-world scenarios, with the average time\nspent on creating a commentary dropping from 4 hours to 20 minutes.\nImportantly, such an increase in efficiency does not compromise the quality of\nthe commentaries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Commentary provides readers with a deep understanding of events by presenting\ndiverse arguments and evidence. However, creating commentary is a\ntime-consuming task, even for skilled commentators. Large language models\n(LLMs) have simplified the process of natural language generation, but their\ndirect application in commentary creation still faces challenges due to unique\ntask requirements. These requirements can be categorized into two levels: 1)\nfundamental requirements, which include creating well-structured and logically\nconsistent narratives, and 2) advanced requirements, which involve generating\nquality arguments and providing convincing evidence. In this paper, we\nintroduce Xinyu, an efficient LLM-based system designed to assist commentators\nin generating Chinese commentaries. To meet the fundamental requirements, we\ndeconstruct the generation process into sequential steps, proposing targeted\nstrategies and supervised fine-tuning (SFT) for each step. To address the\nadvanced requirements, we present an argument ranking model for arguments and\nestablish a comprehensive evidence database that includes up-to-date events and\nclassic books, thereby strengthening the substantiation of the evidence with\nretrieval augmented generation (RAG) technology. To evaluate the generated\ncommentaries more fairly, corresponding to the two-level requirements, we\nintroduce a comprehensive evaluation metric that considers five distinct\nperspectives in commentary generation. Our experiments confirm the\neffectiveness of our proposed system. We also observe a significant increase in\nthe efficiency of commentators in real-world scenarios, with the average time\nspent on creating a commentary dropping from 4 hours to 20 minutes.\nImportantly, such an increase in efficiency does not compromise the quality of\nthe commentaries."
                },
                "authors": [
                    {
                        "name": "Yiquan Wu"
                    },
                    {
                        "name": "Bo Tang"
                    },
                    {
                        "name": "Chenyang Xi"
                    },
                    {
                        "name": "Yu Yu"
                    },
                    {
                        "name": "Pengyu Wang"
                    },
                    {
                        "name": "Yifei Liu"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Haiying Deng"
                    },
                    {
                        "name": "Zhiyu Li"
                    },
                    {
                        "name": "Feiyu Xiong"
                    },
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Zhonghao Wang"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Yi Luo"
                    },
                    {
                        "name": "Mingchuan Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mingchuan Yang"
                },
                "author": "Mingchuan Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11609v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11609v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.00380v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.00380v2",
                "updated": "2024-08-23T03:39:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    3,
                    39,
                    57,
                    4,
                    236,
                    0
                ],
                "published": "2024-06-01T09:36:16Z",
                "published_parsed": [
                    2024,
                    6,
                    1,
                    9,
                    36,
                    16,
                    5,
                    153,
                    0
                ],
                "title": "The Best of Both Worlds: Toward an Honest and Helpful Large Language\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Best of Both Worlds: Toward an Honest and Helpful Large Language\n  Model"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success across various\nindustries due to their exceptional generative capabilities. However, for safe\nand effective real-world deployments, ensuring honesty and helpfulness is\ncritical. This paper addresses the question: Can we prioritize the helpfulness\nof LLMs while preserving their honesty? To begin with, we establish exhaustive\nprinciples aimed at guaranteeing the honesty of LLM. Additionally, we introduce\na novel dataset, referred to as HoneSet, comprising 930 queries spanning six\ncategories meticulously crafted to assess an LLM's capacity for maintaining\nhonesty. Subsequently, we present two approaches to augmenting honesty and\nhelpfulness in LLMs: a training-free enhancement and a fine-tuning-based\nimprovement. The training-free approach, which is based on curiosity-driven\nprompting, empowers LLMs to articulate internal confusion and uncertainty\nregarding queries, thereby optimizing their responses. Conversely, the\nfine-tuning-based method employs a two-stage process inspired by curriculum\nlearning: initially instructing LLMs to discern between honest and dishonest\nresponses, then refining their training to enhance helpfulness. Experiments\nconducted on nine prominent LLMs demonstrate a significant improvement in\nalignment with honesty across all models through the implementation of our\nproposed enhancements. Particularly noteworthy is the 65.3% enhancement\nobserved in Llama3-8b and the remarkable 124.7% improvement in Mistral-7b, as\nmeasured by the H$^{2}$ (honest and helpful) assessment. We believe that our\nwork can pave the way for developing more trustworthy LLMs for real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success across various\nindustries due to their exceptional generative capabilities. However, for safe\nand effective real-world deployments, ensuring honesty and helpfulness is\ncritical. This paper addresses the question: Can we prioritize the helpfulness\nof LLMs while preserving their honesty? To begin with, we establish exhaustive\nprinciples aimed at guaranteeing the honesty of LLM. Additionally, we introduce\na novel dataset, referred to as HoneSet, comprising 930 queries spanning six\ncategories meticulously crafted to assess an LLM's capacity for maintaining\nhonesty. Subsequently, we present two approaches to augmenting honesty and\nhelpfulness in LLMs: a training-free enhancement and a fine-tuning-based\nimprovement. The training-free approach, which is based on curiosity-driven\nprompting, empowers LLMs to articulate internal confusion and uncertainty\nregarding queries, thereby optimizing their responses. Conversely, the\nfine-tuning-based method employs a two-stage process inspired by curriculum\nlearning: initially instructing LLMs to discern between honest and dishonest\nresponses, then refining their training to enhance helpfulness. Experiments\nconducted on nine prominent LLMs demonstrate a significant improvement in\nalignment with honesty across all models through the implementation of our\nproposed enhancements. Particularly noteworthy is the 65.3% enhancement\nobserved in Llama3-8b and the remarkable 124.7% improvement in Mistral-7b, as\nmeasured by the H$^{2}$ (honest and helpful) assessment. We believe that our\nwork can pave the way for developing more trustworthy LLMs for real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Chujie Gao"
                    },
                    {
                        "name": "Qihui Zhang"
                    },
                    {
                        "name": "Dongping Chen"
                    },
                    {
                        "name": "Yue Huang"
                    },
                    {
                        "name": "Siyuan Wu"
                    },
                    {
                        "name": "Zhengyan Fu"
                    },
                    {
                        "name": "Yao Wan"
                    },
                    {
                        "name": "Xiangliang Zhang"
                    },
                    {
                        "name": "Lichao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Lichao Sun"
                },
                "author": "Lichao Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.00380v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.00380v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12812v1",
                "updated": "2024-08-23T03:16:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    3,
                    16,
                    26,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T03:16:26Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    3,
                    16,
                    26,
                    4,
                    236,
                    0
                ],
                "title": "Grounding Fallacies Misrepresenting Scientific Publications in Evidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounding Fallacies Misrepresenting Scientific Publications in Evidence"
                },
                "summary": "Health-related misinformation claims often falsely cite a credible biomedical\npublication as evidence, which superficially appears to support the false\nclaim. The publication does not really support the claim, but a reader could\nbelieve it thanks to the use of logical fallacies. Here, we aim to detect and\nto highlight such fallacies, which requires carefully assessing the exact\ncontent of the misrepresented publications. To achieve this, we introduce\nMissciPlus, an extension of the fallacy detection dataset Missci. MissciPlus\nbuilds on Missci by grounding the applied fallacies in real-world passages from\nmisrepresented studies. This creates a realistic test-bed for detecting and\nverbalizing these fallacies under real-world input conditions, and enables\nnovel passage-retrieval tasks. MissciPlus is the first logical fallacy dataset\nwhich pairs the real-world misrepresented evidence with incorrect claims,\nidentical to the input to evidence-based fact-checking models. With MissciPlus,\nwe i) benchmark retrieval models in identifying passages that support claims\nonly when fallacies are applied, ii) evaluate how well LLMs articulate\nfallacious reasoning from misrepresented scientific passages, and iii) assess\nthe effectiveness of fact-checking models in refuting claims that misrepresent\nbiomedical research. Our findings show that current fact-checking models\nstruggle to use relevant passages from misrepresented publications to refute\nmisinformation. Moreover, these passages can mislead LLMs into accepting false\nclaims as true.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Health-related misinformation claims often falsely cite a credible biomedical\npublication as evidence, which superficially appears to support the false\nclaim. The publication does not really support the claim, but a reader could\nbelieve it thanks to the use of logical fallacies. Here, we aim to detect and\nto highlight such fallacies, which requires carefully assessing the exact\ncontent of the misrepresented publications. To achieve this, we introduce\nMissciPlus, an extension of the fallacy detection dataset Missci. MissciPlus\nbuilds on Missci by grounding the applied fallacies in real-world passages from\nmisrepresented studies. This creates a realistic test-bed for detecting and\nverbalizing these fallacies under real-world input conditions, and enables\nnovel passage-retrieval tasks. MissciPlus is the first logical fallacy dataset\nwhich pairs the real-world misrepresented evidence with incorrect claims,\nidentical to the input to evidence-based fact-checking models. With MissciPlus,\nwe i) benchmark retrieval models in identifying passages that support claims\nonly when fallacies are applied, ii) evaluate how well LLMs articulate\nfallacious reasoning from misrepresented scientific passages, and iii) assess\nthe effectiveness of fact-checking models in refuting claims that misrepresent\nbiomedical research. Our findings show that current fact-checking models\nstruggle to use relevant passages from misrepresented publications to refute\nmisinformation. Moreover, these passages can mislead LLMs into accepting false\nclaims as true."
                },
                "authors": [
                    {
                        "name": "Max Glockner"
                    },
                    {
                        "name": "Yufang Hou"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Iryna Gurevych"
                    }
                ],
                "author_detail": {
                    "name": "Iryna Gurevych"
                },
                "author": "Iryna Gurevych",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12806v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12806v1",
                "updated": "2024-08-23T02:56:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    2,
                    56,
                    13,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T02:56:13Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    2,
                    56,
                    13,
                    4,
                    236,
                    0
                ],
                "title": "Is Generative AI the Next Tactical Cyber Weapon For Threat Actors?\n  Unforeseen Implications of AI Generated Cyber Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Generative AI the Next Tactical Cyber Weapon For Threat Actors?\n  Unforeseen Implications of AI Generated Cyber Attacks"
                },
                "summary": "In an era where digital threats are increasingly sophisticated, the\nintersection of Artificial Intelligence and cybersecurity presents both\npromising defenses and potent dangers. This paper delves into the escalating\nthreat posed by the misuse of AI, specifically through the use of Large\nLanguage Models (LLMs). This study details various techniques like the switch\nmethod and character play method, which can be exploited by cybercriminals to\ngenerate and automate cyber attacks. Through a series of controlled\nexperiments, the paper demonstrates how these models can be manipulated to\nbypass ethical and privacy safeguards to effectively generate cyber attacks\nsuch as social engineering, malicious code, payload generation, and spyware. By\ntesting these AI generated attacks on live systems, the study assesses their\neffectiveness and the vulnerabilities they exploit, offering a practical\nperspective on the risks AI poses to critical infrastructure. We also introduce\nOccupy AI, a customized, finetuned LLM specifically engineered to automate and\nexecute cyberattacks. This specialized AI driven tool is adept at crafting\nsteps and generating executable code for a variety of cyber threats, including\nphishing, malware injection, and system exploitation. The results underscore\nthe urgency for ethical AI practices, robust cybersecurity measures, and\nregulatory oversight to mitigate AI related threats. This paper aims to elevate\nawareness within the cybersecurity community about the evolving digital threat\nlandscape, advocating for proactive defense strategies and responsible AI\ndevelopment to protect against emerging cyber threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In an era where digital threats are increasingly sophisticated, the\nintersection of Artificial Intelligence and cybersecurity presents both\npromising defenses and potent dangers. This paper delves into the escalating\nthreat posed by the misuse of AI, specifically through the use of Large\nLanguage Models (LLMs). This study details various techniques like the switch\nmethod and character play method, which can be exploited by cybercriminals to\ngenerate and automate cyber attacks. Through a series of controlled\nexperiments, the paper demonstrates how these models can be manipulated to\nbypass ethical and privacy safeguards to effectively generate cyber attacks\nsuch as social engineering, malicious code, payload generation, and spyware. By\ntesting these AI generated attacks on live systems, the study assesses their\neffectiveness and the vulnerabilities they exploit, offering a practical\nperspective on the risks AI poses to critical infrastructure. We also introduce\nOccupy AI, a customized, finetuned LLM specifically engineered to automate and\nexecute cyberattacks. This specialized AI driven tool is adept at crafting\nsteps and generating executable code for a variety of cyber threats, including\nphishing, malware injection, and system exploitation. The results underscore\nthe urgency for ethical AI practices, robust cybersecurity measures, and\nregulatory oversight to mitigate AI related threats. This paper aims to elevate\nawareness within the cybersecurity community about the evolving digital threat\nlandscape, advocating for proactive defense strategies and responsible AI\ndevelopment to protect against emerging cyber threats."
                },
                "authors": [
                    {
                        "name": "Yusuf Usman"
                    },
                    {
                        "name": "Aadesh Upadhyay"
                    },
                    {
                        "name": "Prashnna Gyawali"
                    },
                    {
                        "name": "Robin Chataut"
                    }
                ],
                "author_detail": {
                    "name": "Robin Chataut"
                },
                "author": "Robin Chataut",
                "arxiv_comment": "Journal Paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12806v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12806v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "Primary 03C90, Secondary 03-02,",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12803v1",
                "updated": "2024-08-23T02:44:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    2,
                    44,
                    8,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T02:44:08Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    2,
                    44,
                    8,
                    4,
                    236,
                    0
                ],
                "title": "Multi-Treatment Multi-Task Uplift Modeling for Enhancing User Growth",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Treatment Multi-Task Uplift Modeling for Enhancing User Growth"
                },
                "summary": "As a key component in boosting online user growth, uplift modeling aims to\nmeasure individual user responses (e.g., whether to play the game) to various\ntreatments, such as gaming bonuses, thereby enhancing business outcomes.\nHowever, previous research typically considers a single-task, single-treatment\nsetting, where only one treatment exists and the overall treatment effect is\nmeasured by a single type of user response. In this paper, we propose a\nMulti-Treatment Multi-Task (MTMT) uplift network to estimate treatment effects\nin a multi-task scenario. We identify the multi-treatment problem as a causal\ninference problem with a tiered response, comprising a base effect (from\noffering a treatment) and an incremental effect (from offering a specific type\nof treatment), where the base effect can be numerically much larger than the\nincremental effect. Specifically, MTMT separately encodes user features and\ntreatments. The user feature encoder uses a multi-gate mixture of experts\n(MMOE) network to encode relevant user features, explicitly learning inter-task\nrelations. The resultant embeddings are used to measure natural responses per\ntask. Furthermore, we introduce a treatment-user feature interaction module to\nmodel correlations between each treatment and user feature. Consequently, we\nseparately measure the base and incremental treatment effect for each task\nbased on the produced treatment-aware representations. Experimental results\nbased on an offline public dataset and an online proprietary dataset\ndemonstrate the effectiveness of MTMT in single/multi-treatment and\nsingle/multi-task settings. Additionally, MTMT has been deployed in our gaming\nplatform to improve user experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a key component in boosting online user growth, uplift modeling aims to\nmeasure individual user responses (e.g., whether to play the game) to various\ntreatments, such as gaming bonuses, thereby enhancing business outcomes.\nHowever, previous research typically considers a single-task, single-treatment\nsetting, where only one treatment exists and the overall treatment effect is\nmeasured by a single type of user response. In this paper, we propose a\nMulti-Treatment Multi-Task (MTMT) uplift network to estimate treatment effects\nin a multi-task scenario. We identify the multi-treatment problem as a causal\ninference problem with a tiered response, comprising a base effect (from\noffering a treatment) and an incremental effect (from offering a specific type\nof treatment), where the base effect can be numerically much larger than the\nincremental effect. Specifically, MTMT separately encodes user features and\ntreatments. The user feature encoder uses a multi-gate mixture of experts\n(MMOE) network to encode relevant user features, explicitly learning inter-task\nrelations. The resultant embeddings are used to measure natural responses per\ntask. Furthermore, we introduce a treatment-user feature interaction module to\nmodel correlations between each treatment and user feature. Consequently, we\nseparately measure the base and incremental treatment effect for each task\nbased on the produced treatment-aware representations. Experimental results\nbased on an offline public dataset and an online proprietary dataset\ndemonstrate the effectiveness of MTMT in single/multi-treatment and\nsingle/multi-task settings. Additionally, MTMT has been deployed in our gaming\nplatform to improve user experience."
                },
                "authors": [
                    {
                        "name": "Yuxiang Wei"
                    },
                    {
                        "name": "Zhaoxin Qiu"
                    },
                    {
                        "name": "Yingjie Li"
                    },
                    {
                        "name": "Yuke Sun"
                    },
                    {
                        "name": "Xiaoling Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoling Li"
                },
                "author": "Xiaoling Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13958v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13958v4",
                "updated": "2024-08-23T02:29:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    2,
                    29,
                    15,
                    4,
                    236,
                    0
                ],
                "published": "2024-07-19T00:35:22Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    0,
                    35,
                    22,
                    4,
                    201,
                    0
                ],
                "title": "Flexible max-stable processes for fast and efficient inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flexible max-stable processes for fast and efficient inference"
                },
                "summary": "Max-stable processes serve as the fundamental distributional family in\nextreme value theory. However, likelihood-based inference methods for\nmax-stable processes still heavily rely on composite likelihoods, rendering\nthem intractable in high dimensions due to their intractable densities. In this\npaper, we introduce a fast and efficient inference method for max-stable\nprocesses based on their angular densities for a class of max-stable processes\nwhose angular densities do not put mass on the boundary space of the simplex.\nThis class can also be used to construct r-Pareto processes. We demonstrate the\nefficiency of the proposed method through two new max-stable processes: the\ntruncated extremal-t process and the skewed Brown-Resnick process. The skewed\nBrown-Resnick process contains the popular Brown-Resnick model as a special\ncase and possesses nonstationary extremal dependence structures. The proposed\nmethod is shown to be computationally efficient and can be applied to large\ndatasets. We showcase the new max-stable processes on simulated and real data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Max-stable processes serve as the fundamental distributional family in\nextreme value theory. However, likelihood-based inference methods for\nmax-stable processes still heavily rely on composite likelihoods, rendering\nthem intractable in high dimensions due to their intractable densities. In this\npaper, we introduce a fast and efficient inference method for max-stable\nprocesses based on their angular densities for a class of max-stable processes\nwhose angular densities do not put mass on the boundary space of the simplex.\nThis class can also be used to construct r-Pareto processes. We demonstrate the\nefficiency of the proposed method through two new max-stable processes: the\ntruncated extremal-t process and the skewed Brown-Resnick process. The skewed\nBrown-Resnick process contains the popular Brown-Resnick model as a special\ncase and possesses nonstationary extremal dependence structures. The proposed\nmethod is shown to be computationally efficient and can be applied to large\ndatasets. We showcase the new max-stable processes on simulated and real data."
                },
                "authors": [
                    {
                        "name": "Peng Zhong"
                    },
                    {
                        "name": "Scott A. Sisson"
                    },
                    {
                        "name": "Boris Beranger"
                    }
                ],
                "author_detail": {
                    "name": "Boris Beranger"
                },
                "author": "Boris Beranger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13958v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13958v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12798v1",
                "updated": "2024-08-23T02:21:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    2,
                    21,
                    21,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T02:21:21Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    2,
                    21,
                    21,
                    4,
                    236,
                    0
                ],
                "title": "BackdoorLLM: A Comprehensive Benchmark for Backdoor Attacks on Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BackdoorLLM: A Comprehensive Benchmark for Backdoor Attacks on Large\n  Language Models"
                },
                "summary": "Generative Large Language Models (LLMs) have made significant strides across\nvarious tasks, but they remain vulnerable to backdoor attacks, where specific\ntriggers in the prompt cause the LLM to generate adversary-desired responses.\nWhile most backdoor research has focused on vision or text classification\ntasks, backdoor attacks in text generation have been largely overlooked. In\nthis work, we introduce \\textit{BackdoorLLM}, the first comprehensive benchmark\nfor studying backdoor attacks on LLMs. \\textit{BackdoorLLM} features: 1) a\nrepository of backdoor benchmarks with a standardized training pipeline, 2)\ndiverse attack strategies, including data poisoning, weight poisoning, hidden\nstate attacks, and chain-of-thought attacks, 3) extensive evaluations with over\n200 experiments on 8 attacks across 7 scenarios and 6 model architectures, and\n4) key insights into the effectiveness and limitations of backdoors in LLMs. We\nhope \\textit{BackdoorLLM} will raise awareness of backdoor threats and\ncontribute to advancing AI safety. The code is available at\n\\url{https://github.com/bboylyg/BackdoorLLM}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Large Language Models (LLMs) have made significant strides across\nvarious tasks, but they remain vulnerable to backdoor attacks, where specific\ntriggers in the prompt cause the LLM to generate adversary-desired responses.\nWhile most backdoor research has focused on vision or text classification\ntasks, backdoor attacks in text generation have been largely overlooked. In\nthis work, we introduce \\textit{BackdoorLLM}, the first comprehensive benchmark\nfor studying backdoor attacks on LLMs. \\textit{BackdoorLLM} features: 1) a\nrepository of backdoor benchmarks with a standardized training pipeline, 2)\ndiverse attack strategies, including data poisoning, weight poisoning, hidden\nstate attacks, and chain-of-thought attacks, 3) extensive evaluations with over\n200 experiments on 8 attacks across 7 scenarios and 6 model architectures, and\n4) key insights into the effectiveness and limitations of backdoors in LLMs. We\nhope \\textit{BackdoorLLM} will raise awareness of backdoor threats and\ncontribute to advancing AI safety. The code is available at\n\\url{https://github.com/bboylyg/BackdoorLLM}."
                },
                "authors": [
                    {
                        "name": "Yige Li"
                    },
                    {
                        "name": "Hanxun Huang"
                    },
                    {
                        "name": "Yunhan Zhao"
                    },
                    {
                        "name": "Xingjun Ma"
                    },
                    {
                        "name": "Jun Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jun Sun"
                },
                "author": "Jun Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16686v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16686v2",
                "updated": "2024-08-23T01:52:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    1,
                    52,
                    10,
                    4,
                    236,
                    0
                ],
                "published": "2024-07-23T17:50:45Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    50,
                    45,
                    1,
                    205,
                    0
                ],
                "title": "Can Large Language Models Automatically Jailbreak GPT-4V?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Automatically Jailbreak GPT-4V?"
                },
                "summary": "GPT-4V has attracted considerable attention due to its extraordinary capacity\nfor integrating and processing multimodal information. At the same time, its\nability of face recognition raises new safety concerns of privacy leakage.\nDespite researchers' efforts in safety alignment through RLHF or preprocessing\nfilters, vulnerabilities might still be exploited. In our study, we introduce\nAutoJailbreak, an innovative automatic jailbreak technique inspired by prompt\noptimization. We leverage Large Language Models (LLMs) for red-teaming to\nrefine the jailbreak prompt and employ weak-to-strong in-context learning\nprompts to boost efficiency. Furthermore, we present an effective search method\nthat incorporates early stopping to minimize optimization time and token\nexpenditure. Our experiments demonstrate that AutoJailbreak significantly\nsurpasses conventional methods, achieving an Attack Success Rate (ASR)\nexceeding 95.3\\%. This research sheds light on strengthening GPT-4V security,\nunderscoring the potential for LLMs to be exploited in compromising GPT-4V\nintegrity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT-4V has attracted considerable attention due to its extraordinary capacity\nfor integrating and processing multimodal information. At the same time, its\nability of face recognition raises new safety concerns of privacy leakage.\nDespite researchers' efforts in safety alignment through RLHF or preprocessing\nfilters, vulnerabilities might still be exploited. In our study, we introduce\nAutoJailbreak, an innovative automatic jailbreak technique inspired by prompt\noptimization. We leverage Large Language Models (LLMs) for red-teaming to\nrefine the jailbreak prompt and employ weak-to-strong in-context learning\nprompts to boost efficiency. Furthermore, we present an effective search method\nthat incorporates early stopping to minimize optimization time and token\nexpenditure. Our experiments demonstrate that AutoJailbreak significantly\nsurpasses conventional methods, achieving an Attack Success Rate (ASR)\nexceeding 95.3\\%. This research sheds light on strengthening GPT-4V security,\nunderscoring the potential for LLMs to be exploited in compromising GPT-4V\nintegrity."
                },
                "authors": [
                    {
                        "name": "Yuanwei Wu"
                    },
                    {
                        "name": "Yue Huang"
                    },
                    {
                        "name": "Yixin Liu"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Pan Zhou"
                    },
                    {
                        "name": "Lichao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Lichao Sun"
                },
                "author": "Lichao Sun",
                "arxiv_comment": "TrustNLP@NAACL2024 (Fourth Workshop on Trustworthy Natural Language\n  Processing)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16686v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16686v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12789v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12789v1",
                "updated": "2024-08-23T01:44:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    1,
                    44,
                    10,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T01:44:10Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    1,
                    44,
                    10,
                    4,
                    236,
                    0
                ],
                "title": "Context-Aware Temporal Embedding of Objects in Video Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-Aware Temporal Embedding of Objects in Video Data"
                },
                "summary": "In video analysis, understanding the temporal context is crucial for\nrecognizing object interactions, event patterns, and contextual changes over\ntime. The proposed model leverages adjacency and semantic similarities between\nobjects from neighboring video frames to construct context-aware temporal\nobject embeddings. Unlike traditional methods that rely solely on visual\nappearance, our temporal embedding model considers the contextual relationships\nbetween objects, creating a meaningful embedding space where temporally\nconnected object's vectors are positioned in proximity. Empirical studies\ndemonstrate that our context-aware temporal embeddings can be used in\nconjunction with conventional visual embeddings to enhance the effectiveness of\ndownstream applications. Moreover, the embeddings can be used to narrate a\nvideo using a Large Language Model (LLM). This paper describes the intricate\ndetails of the proposed objective function to generate context-aware temporal\nobject embeddings for video data and showcases the potential applications of\nthe generated embeddings in video analysis and object classification tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In video analysis, understanding the temporal context is crucial for\nrecognizing object interactions, event patterns, and contextual changes over\ntime. The proposed model leverages adjacency and semantic similarities between\nobjects from neighboring video frames to construct context-aware temporal\nobject embeddings. Unlike traditional methods that rely solely on visual\nappearance, our temporal embedding model considers the contextual relationships\nbetween objects, creating a meaningful embedding space where temporally\nconnected object's vectors are positioned in proximity. Empirical studies\ndemonstrate that our context-aware temporal embeddings can be used in\nconjunction with conventional visual embeddings to enhance the effectiveness of\ndownstream applications. Moreover, the embeddings can be used to narrate a\nvideo using a Large Language Model (LLM). This paper describes the intricate\ndetails of the proposed objective function to generate context-aware temporal\nobject embeddings for video data and showcases the potential applications of\nthe generated embeddings in video analysis and object classification tasks."
                },
                "authors": [
                    {
                        "name": "Ahnaf Farhan"
                    },
                    {
                        "name": "M. Shahriar Hossain"
                    }
                ],
                "author_detail": {
                    "name": "M. Shahriar Hossain"
                },
                "author": "M. Shahriar Hossain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12789v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12789v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12787v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12787v1",
                "updated": "2024-08-23T01:37:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    1,
                    37,
                    29,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T01:37:29Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    1,
                    37,
                    29,
                    4,
                    236,
                    0
                ],
                "title": "LLM-PBE: Assessing Data Privacy in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-PBE: Assessing Data Privacy in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have become integral to numerous domains,\nsignificantly advancing applications in data management, mining, and analysis.\nTheir profound capabilities in processing and interpreting complex language\ndata, however, bring to light pressing concerns regarding data privacy,\nespecially the risk of unintentional training data leakage. Despite the\ncritical nature of this issue, there has been no existing literature to offer a\ncomprehensive assessment of data privacy risks in LLMs. Addressing this gap,\nour paper introduces LLM-PBE, a toolkit crafted specifically for the systematic\nevaluation of data privacy risks in LLMs. LLM-PBE is designed to analyze\nprivacy across the entire lifecycle of LLMs, incorporating diverse attack and\ndefense strategies, and handling various data types and metrics. Through\ndetailed experimentation with multiple LLMs, LLM-PBE facilitates an in-depth\nexploration of data privacy concerns, shedding light on influential factors\nsuch as model size, data characteristics, and evolving temporal dimensions.\nThis study not only enriches the understanding of privacy issues in LLMs but\nalso serves as a vital resource for future research in the field. Aimed at\nenhancing the breadth of knowledge in this area, the findings, resources, and\nour full technical report are made available at https://llm-pbe.github.io/,\nproviding an open platform for academic and practical advancements in LLM\nprivacy assessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become integral to numerous domains,\nsignificantly advancing applications in data management, mining, and analysis.\nTheir profound capabilities in processing and interpreting complex language\ndata, however, bring to light pressing concerns regarding data privacy,\nespecially the risk of unintentional training data leakage. Despite the\ncritical nature of this issue, there has been no existing literature to offer a\ncomprehensive assessment of data privacy risks in LLMs. Addressing this gap,\nour paper introduces LLM-PBE, a toolkit crafted specifically for the systematic\nevaluation of data privacy risks in LLMs. LLM-PBE is designed to analyze\nprivacy across the entire lifecycle of LLMs, incorporating diverse attack and\ndefense strategies, and handling various data types and metrics. Through\ndetailed experimentation with multiple LLMs, LLM-PBE facilitates an in-depth\nexploration of data privacy concerns, shedding light on influential factors\nsuch as model size, data characteristics, and evolving temporal dimensions.\nThis study not only enriches the understanding of privacy issues in LLMs but\nalso serves as a vital resource for future research in the field. Aimed at\nenhancing the breadth of knowledge in this area, the findings, resources, and\nour full technical report are made available at https://llm-pbe.github.io/,\nproviding an open platform for academic and practical advancements in LLM\nprivacy assessment."
                },
                "authors": [
                    {
                        "name": "Qinbin Li"
                    },
                    {
                        "name": "Junyuan Hong"
                    },
                    {
                        "name": "Chulin Xie"
                    },
                    {
                        "name": "Jeffrey Tan"
                    },
                    {
                        "name": "Rachel Xin"
                    },
                    {
                        "name": "Junyi Hou"
                    },
                    {
                        "name": "Xavier Yin"
                    },
                    {
                        "name": "Zhun Wang"
                    },
                    {
                        "name": "Dan Hendrycks"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Bingsheng He"
                    },
                    {
                        "name": "Dawn Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawn Song"
                },
                "author": "Dawn Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12787v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12787v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12247v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12247v2",
                "updated": "2024-08-23T01:25:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    1,
                    25,
                    26,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-22T09:36:15Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    9,
                    36,
                    15,
                    3,
                    235,
                    0
                ],
                "title": "Enhanced Fine-Tuning of Lightweight Domain-Specific Q&A Model Based on\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Fine-Tuning of Lightweight Domain-Specific Q&A Model Based on\n  Large Language Models"
                },
                "summary": "Large language models (LLMs) excel at general question-answering (Q&A) but\noften fall short in specialized domains due to a lack of domain-specific\nknowledge. Commercial companies face the dual challenges of privacy protection\nand resource constraints when involving LLMs for fine-tuning. This paper\npropose a novel framework, Self-Evolution, designed to address these issues by\nleveraging lightweight open-source LLMs through multiple iterative fine-tuning\nrounds. To enhance the efficiency of iterative fine-tuning, Self-Evolution\nemploy a strategy that filters and reinforces the knowledge with higher value\nduring the iterative process. We employed Self-Evolution on Qwen1.5-7B-Chat\nusing 4,000 documents containing rich domain knowledge from China Mobile,\nachieving a performance score 174% higher on domain-specific question-answering\nevaluations than Qwen1.5-7B-Chat and even 22% higher than Qwen1.5-72B-Chat.\nSelf-Evolution has been deployed in China Mobile's daily operation and\nmaintenance for 117 days, and it improves the efficiency of locating alarms,\nfixing problems, and finding related reports, with an average efficiency\nimprovement of over 18.6%. In addition, we release Self-Evolution framework\ncode in https://github.com/Zero-Pointer/Self-Evolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at general question-answering (Q&A) but\noften fall short in specialized domains due to a lack of domain-specific\nknowledge. Commercial companies face the dual challenges of privacy protection\nand resource constraints when involving LLMs for fine-tuning. This paper\npropose a novel framework, Self-Evolution, designed to address these issues by\nleveraging lightweight open-source LLMs through multiple iterative fine-tuning\nrounds. To enhance the efficiency of iterative fine-tuning, Self-Evolution\nemploy a strategy that filters and reinforces the knowledge with higher value\nduring the iterative process. We employed Self-Evolution on Qwen1.5-7B-Chat\nusing 4,000 documents containing rich domain knowledge from China Mobile,\nachieving a performance score 174% higher on domain-specific question-answering\nevaluations than Qwen1.5-7B-Chat and even 22% higher than Qwen1.5-72B-Chat.\nSelf-Evolution has been deployed in China Mobile's daily operation and\nmaintenance for 117 days, and it improves the efficiency of locating alarms,\nfixing problems, and finding related reports, with an average efficiency\nimprovement of over 18.6%. In addition, we release Self-Evolution framework\ncode in https://github.com/Zero-Pointer/Self-Evolution."
                },
                "authors": [
                    {
                        "name": "Shenglin Zhang"
                    },
                    {
                        "name": "Pengtian Zhu"
                    },
                    {
                        "name": "Minghua Ma"
                    },
                    {
                        "name": "Jiagang Wang"
                    },
                    {
                        "name": "Yongqian Sun"
                    },
                    {
                        "name": "Dongwen Li"
                    },
                    {
                        "name": "Jingyu Wang"
                    },
                    {
                        "name": "Qianying Guo"
                    },
                    {
                        "name": "Xiaolei Hua"
                    },
                    {
                        "name": "Lin Zhu"
                    },
                    {
                        "name": "Dan Pei"
                    }
                ],
                "author_detail": {
                    "name": "Dan Pei"
                },
                "author": "Dan Pei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12247v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12247v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08554v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08554v2",
                "updated": "2024-08-23T01:09:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    1,
                    9,
                    8,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-16T06:39:08Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    39,
                    8,
                    4,
                    229,
                    0
                ],
                "title": "ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration for Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have revolutionized natural language processing\ntasks. However, their practical application is constrained by substantial\nmemory and computational demands. Post-training quantization (PTQ) is\nconsidered an effective method to accelerate LLM inference. Despite its growing\npopularity in LLM model compression, PTQ deployment faces two major challenges.\nFirst, low-bit quantization leads to performance degradation. Second,\nrestricted by the limited integer computing unit type on GPUs, quantized matrix\noperations with different precisions cannot be effectively accelerated. To\naddress these issues, we introduce a novel arbitrary-bit quantization algorithm\nand inference framework, ABQ-LLM. It achieves superior performance across\nvarious quantization settings and enables efficient arbitrary-precision\nquantized inference on the GPU. ABQ-LLM introduces several key innovations: (1)\na distribution correction method for transformer blocks to mitigate\ndistribution differences caused by full quantization of weights and\nactivations, improving performance at low bit-widths. (2) the bit balance\nstrategy to counteract performance degradation from asymmetric distribution\nissues at very low bit-widths (e.g., 2-bit). (3) an innovative quantization\nacceleration framework that reconstructs the quantization matrix multiplication\nof arbitrary precision combinations based on BTC (Binary TensorCore)\nequivalents, gets rid of the limitations of INT4/INT8 computing units. ABQ-LLM\ncan convert each component bit width gain into actual acceleration gain,\nmaximizing performance under mixed precision(e.g., W6A6, W2A8). Based on W2*A8\nquantization configuration on LLaMA-7B model, it achieved a WikiText2\nperplexity of 7.59 (2.17$\\downarrow $ vs 9.76 in AffineQuant). Compared to\nSmoothQuant, we realized 1.6$\\times$ acceleration improvement and 2.7$\\times$\nmemory compression gain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized natural language processing\ntasks. However, their practical application is constrained by substantial\nmemory and computational demands. Post-training quantization (PTQ) is\nconsidered an effective method to accelerate LLM inference. Despite its growing\npopularity in LLM model compression, PTQ deployment faces two major challenges.\nFirst, low-bit quantization leads to performance degradation. Second,\nrestricted by the limited integer computing unit type on GPUs, quantized matrix\noperations with different precisions cannot be effectively accelerated. To\naddress these issues, we introduce a novel arbitrary-bit quantization algorithm\nand inference framework, ABQ-LLM. It achieves superior performance across\nvarious quantization settings and enables efficient arbitrary-precision\nquantized inference on the GPU. ABQ-LLM introduces several key innovations: (1)\na distribution correction method for transformer blocks to mitigate\ndistribution differences caused by full quantization of weights and\nactivations, improving performance at low bit-widths. (2) the bit balance\nstrategy to counteract performance degradation from asymmetric distribution\nissues at very low bit-widths (e.g., 2-bit). (3) an innovative quantization\nacceleration framework that reconstructs the quantization matrix multiplication\nof arbitrary precision combinations based on BTC (Binary TensorCore)\nequivalents, gets rid of the limitations of INT4/INT8 computing units. ABQ-LLM\ncan convert each component bit width gain into actual acceleration gain,\nmaximizing performance under mixed precision(e.g., W6A6, W2A8). Based on W2*A8\nquantization configuration on LLaMA-7B model, it achieved a WikiText2\nperplexity of 7.59 (2.17$\\downarrow $ vs 9.76 in AffineQuant). Compared to\nSmoothQuant, we realized 1.6$\\times$ acceleration improvement and 2.7$\\times$\nmemory compression gain."
                },
                "authors": [
                    {
                        "name": "Chao Zeng"
                    },
                    {
                        "name": "Songwei Liu"
                    },
                    {
                        "name": "Yusheng Xie"
                    },
                    {
                        "name": "Hong Liu"
                    },
                    {
                        "name": "Xiaojian Wang"
                    },
                    {
                        "name": "Miao Wei"
                    },
                    {
                        "name": "Shu Yang"
                    },
                    {
                        "name": "Fangmin Chen"
                    },
                    {
                        "name": "Xing Mei"
                    }
                ],
                "author_detail": {
                    "name": "Xing Mei"
                },
                "author": "Xing Mei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08554v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08554v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.08702v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.08702v4",
                "updated": "2024-08-23T01:03:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    1,
                    3,
                    13,
                    4,
                    236,
                    0
                ],
                "published": "2023-12-14T07:38:12Z",
                "published_parsed": [
                    2023,
                    12,
                    14,
                    7,
                    38,
                    12,
                    3,
                    348,
                    0
                ],
                "title": "Rational Sensibility: LLM Enhanced Empathetic Response Generation Guided\n  by Self-presentation Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rational Sensibility: LLM Enhanced Empathetic Response Generation Guided\n  by Self-presentation Theory"
                },
                "summary": "The development of Large Language Models (LLMs) provides human-centered\nArtificial General Intelligence (AGI) with a glimmer of hope. Empathy serves as\na key emotional attribute of humanity, playing an irreplaceable role in\nhuman-centered AGI. Despite numerous researches aim to improve the cognitive\nempathy of models by incorporating external knowledge, there has been limited\nattention on the sensibility and rationality of the conversation itself, which\nare vital components of the empathy. However, the rationality information\nwithin the conversation is restricted, and previous methods of extending\nknowledge are subject to semantic conflict and single-role view. In this paper,\nwe design an innovative encoder module inspired by self-presentation theory in\nsociology, which specifically processes sensibility and rationality sentences\nin dialogues. And we employ a LLM as a rational brain to decipher profound\nlogical information preserved within the conversation, which assists our model\nin assessing the balance between sensibility and rationality to produce\nhigh-quality empathetic response. Experimental results demonstrate that our\nmodel outperforms other methods in both automatic and human evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of Large Language Models (LLMs) provides human-centered\nArtificial General Intelligence (AGI) with a glimmer of hope. Empathy serves as\na key emotional attribute of humanity, playing an irreplaceable role in\nhuman-centered AGI. Despite numerous researches aim to improve the cognitive\nempathy of models by incorporating external knowledge, there has been limited\nattention on the sensibility and rationality of the conversation itself, which\nare vital components of the empathy. However, the rationality information\nwithin the conversation is restricted, and previous methods of extending\nknowledge are subject to semantic conflict and single-role view. In this paper,\nwe design an innovative encoder module inspired by self-presentation theory in\nsociology, which specifically processes sensibility and rationality sentences\nin dialogues. And we employ a LLM as a rational brain to decipher profound\nlogical information preserved within the conversation, which assists our model\nin assessing the balance between sensibility and rationality to produce\nhigh-quality empathetic response. Experimental results demonstrate that our\nmodel outperforms other methods in both automatic and human evaluations."
                },
                "authors": [
                    {
                        "name": "Linzhuang Sun"
                    },
                    {
                        "name": "Yao Dong"
                    },
                    {
                        "name": "Nan Xu"
                    },
                    {
                        "name": "Jingxuan Wei"
                    },
                    {
                        "name": "Bihui Yu"
                    },
                    {
                        "name": "Yin Luo"
                    }
                ],
                "author_detail": {
                    "name": "Yin Luo"
                },
                "author": "Yin Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.08702v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.08702v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12780v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12780v1",
                "updated": "2024-08-23T00:59:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    0,
                    59,
                    38,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T00:59:38Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    0,
                    59,
                    38,
                    4,
                    236,
                    0
                ],
                "title": "Quality or Quantity? On Data Scale and Diversity in Adapting Large\n  Language Models for Low-Resource Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quality or Quantity? On Data Scale and Diversity in Adapting Large\n  Language Models for Low-Resource Translation"
                },
                "summary": "Despite the recent popularity of Large Language Models (LLMs) in Machine\nTranslation (MT), their performance in low-resource translation still lags\nsignificantly behind Neural Machine Translation (NMT) models. In this paper, we\nexplore what it would take to adapt LLMs for low-resource settings. In\nparticular, we re-examine the role of two factors: a) the importance and\napplication of parallel data, and b) diversity in Supervised Fine-Tuning (SFT).\nRecently, parallel data has been shown to be less important for MT using LLMs\nthan in previous MT research. Similarly, diversity during SFT has been shown to\npromote significant transfer in LLMs across languages and tasks. However, for\nlow-resource LLM-MT, we show that the opposite is true for both of these\nconsiderations: a) parallel data is critical during both pretraining and SFT,\nand b) diversity tends to cause interference, not transfer. Our experiments,\nconducted with 3 LLMs across 2 low-resourced language groups - indigenous\nAmerican and North-East Indian - reveal consistent patterns in both cases,\nunderscoring the generalizability of our findings. We believe these insights\nwill be valuable for scaling to massively multilingual LLM-MT models that can\neffectively serve lower-resource languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the recent popularity of Large Language Models (LLMs) in Machine\nTranslation (MT), their performance in low-resource translation still lags\nsignificantly behind Neural Machine Translation (NMT) models. In this paper, we\nexplore what it would take to adapt LLMs for low-resource settings. In\nparticular, we re-examine the role of two factors: a) the importance and\napplication of parallel data, and b) diversity in Supervised Fine-Tuning (SFT).\nRecently, parallel data has been shown to be less important for MT using LLMs\nthan in previous MT research. Similarly, diversity during SFT has been shown to\npromote significant transfer in LLMs across languages and tasks. However, for\nlow-resource LLM-MT, we show that the opposite is true for both of these\nconsiderations: a) parallel data is critical during both pretraining and SFT,\nand b) diversity tends to cause interference, not transfer. Our experiments,\nconducted with 3 LLMs across 2 low-resourced language groups - indigenous\nAmerican and North-East Indian - reveal consistent patterns in both cases,\nunderscoring the generalizability of our findings. We believe these insights\nwill be valuable for scaling to massively multilingual LLM-MT models that can\neffectively serve lower-resource languages."
                },
                "authors": [
                    {
                        "name": "Vivek Iyer"
                    },
                    {
                        "name": "Bhavitvya Malik"
                    },
                    {
                        "name": "Pavel Stepachev"
                    },
                    {
                        "name": "Pinzhen Chen"
                    },
                    {
                        "name": "Barry Haddow"
                    },
                    {
                        "name": "Alexandra Birch"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Birch"
                },
                "author": "Alexandra Birch",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12780v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12780v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12779v1",
                "updated": "2024-08-23T00:57:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    0,
                    57,
                    37,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T00:57:37Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    0,
                    57,
                    37,
                    4,
                    236,
                    0
                ],
                "title": "Investigating LLM Applications in E-Commerce",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating LLM Applications in E-Commerce"
                },
                "summary": "The emergence of Large Language Models (LLMs) has revolutionized natural\nlanguage processing in various applications especially in e-commerce. One\ncrucial step before the application of such LLMs in these fields is to\nunderstand and compare the performance in different use cases in such tasks.\nThis paper explored the efficacy of LLMs in the e-commerce domain, focusing on\ninstruction-tuning an open source LLM model with public e-commerce datasets of\nvarying sizes and comparing the performance with the conventional models\nprevalent in industrial applications. We conducted a comprehensive comparison\nbetween LLMs and traditional pre-trained language models across specific tasks\nintrinsic to the e-commerce domain, namely classification, generation,\nsummarization, and named entity recognition (NER). Furthermore, we examined the\neffectiveness of the current niche industrial application of very large LLM,\nusing in-context learning, in e-commerce specific tasks. Our findings indicate\nthat few-shot inference with very large LLMs often does not outperform\nfine-tuning smaller pre-trained models, underscoring the importance of\ntask-specific model optimization.Additionally, we investigated different\ntraining methodologies such as single-task training, mixed-task training, and\nLoRA merging both within domain/tasks and between different tasks. Through\nrigorous experimentation and analysis, this paper offers valuable insights into\nthe potential effectiveness of LLMs to advance natural language processing\ncapabilities within the e-commerce industry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of Large Language Models (LLMs) has revolutionized natural\nlanguage processing in various applications especially in e-commerce. One\ncrucial step before the application of such LLMs in these fields is to\nunderstand and compare the performance in different use cases in such tasks.\nThis paper explored the efficacy of LLMs in the e-commerce domain, focusing on\ninstruction-tuning an open source LLM model with public e-commerce datasets of\nvarying sizes and comparing the performance with the conventional models\nprevalent in industrial applications. We conducted a comprehensive comparison\nbetween LLMs and traditional pre-trained language models across specific tasks\nintrinsic to the e-commerce domain, namely classification, generation,\nsummarization, and named entity recognition (NER). Furthermore, we examined the\neffectiveness of the current niche industrial application of very large LLM,\nusing in-context learning, in e-commerce specific tasks. Our findings indicate\nthat few-shot inference with very large LLMs often does not outperform\nfine-tuning smaller pre-trained models, underscoring the importance of\ntask-specific model optimization.Additionally, we investigated different\ntraining methodologies such as single-task training, mixed-task training, and\nLoRA merging both within domain/tasks and between different tasks. Through\nrigorous experimentation and analysis, this paper offers valuable insights into\nthe potential effectiveness of LLMs to advance natural language processing\ncapabilities within the e-commerce industry."
                },
                "authors": [
                    {
                        "name": "Chester Palen-Michel"
                    },
                    {
                        "name": "Ruixiang Wang"
                    },
                    {
                        "name": "Yipeng Zhang"
                    },
                    {
                        "name": "David Yu"
                    },
                    {
                        "name": "Canran Xu"
                    },
                    {
                        "name": "Zhe Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Wu"
                },
                "author": "Zhe Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10436v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10436v2",
                "updated": "2024-08-23T00:55:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    0,
                    55,
                    56,
                    4,
                    236,
                    0
                ],
                "published": "2024-06-14T22:58:37Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    22,
                    58,
                    37,
                    4,
                    166,
                    0
                ],
                "title": "Effect of the spatial curvature on light bending and time delay in\n  curved Einstein-Straus--de Sitter spacetime",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effect of the spatial curvature on light bending and time delay in\n  curved Einstein-Straus--de Sitter spacetime"
                },
                "summary": "A method of general applicability has been developed, whereby the null\ngeodesic equations of the Einstein-Straus-de Sitter metric can be integrated\nsimultaneously in terms of the curvature constant $k$. The purpose is to\ngeneralize the computation of light deflection and time delay by a spherical\nmass distribution. Assuming a flat Universe with most recent measurements of\nthe Hubble constant $H_0$ and the cosmological constant $\\Lambda$, five time\ndelays between the four bright images of the lensed quasar SDSS J1004+4112 have\nbeen forecasted and compared to others in the field. In addition, we have\nreviewed the question of the possible contribution of a positive $\\Lambda$ to\nreduce the light bending, and concluded that the changes are seemingly too\nsmall to be appreciable on cosmological scales. The same conclusion has been\nreached regarding the time delay. Having addressed the question of the effect\nof the spatial curvature in both closed and open Universe, we have found that\nthe strong lensing is slightly affected by the expected small curvature density\n$\\Omega_{k0}$ of the current Universe within its error bar\n$|\\Omega_{k0}|\\lessapprox 0.001$, in such a way that it may safely be\nneglected. However, it's only if $\\Omega_{k0}$ gets quite larger that the\neffect being noticeable. While it is only theoretically possible for\n$\\Omega_{k0}$ to be higher, it's worthwhile to stress that this should impact\nthe light bending and time delay, causing them to decrease or increase\ndepending upon whether the spatial curvature is positive or negative.\nFurthermore, one can infer that the observed light deflection and time delay\nindependently, that are found to be significantly deviated from those of the\nflat Universe, may serve as a useful means to provide constraints on $\\Omega\n_{k0}$, thus making the approach employed in this work more promising than\nothers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A method of general applicability has been developed, whereby the null\ngeodesic equations of the Einstein-Straus-de Sitter metric can be integrated\nsimultaneously in terms of the curvature constant $k$. The purpose is to\ngeneralize the computation of light deflection and time delay by a spherical\nmass distribution. Assuming a flat Universe with most recent measurements of\nthe Hubble constant $H_0$ and the cosmological constant $\\Lambda$, five time\ndelays between the four bright images of the lensed quasar SDSS J1004+4112 have\nbeen forecasted and compared to others in the field. In addition, we have\nreviewed the question of the possible contribution of a positive $\\Lambda$ to\nreduce the light bending, and concluded that the changes are seemingly too\nsmall to be appreciable on cosmological scales. The same conclusion has been\nreached regarding the time delay. Having addressed the question of the effect\nof the spatial curvature in both closed and open Universe, we have found that\nthe strong lensing is slightly affected by the expected small curvature density\n$\\Omega_{k0}$ of the current Universe within its error bar\n$|\\Omega_{k0}|\\lessapprox 0.001$, in such a way that it may safely be\nneglected. However, it's only if $\\Omega_{k0}$ gets quite larger that the\neffect being noticeable. While it is only theoretically possible for\n$\\Omega_{k0}$ to be higher, it's worthwhile to stress that this should impact\nthe light bending and time delay, causing them to decrease or increase\ndepending upon whether the spatial curvature is positive or negative.\nFurthermore, one can infer that the observed light deflection and time delay\nindependently, that are found to be significantly deviated from those of the\nflat Universe, may serve as a useful means to provide constraints on $\\Omega\n_{k0}$, thus making the approach employed in this work more promising than\nothers."
                },
                "authors": [
                    {
                        "name": "Mourad Guenouche"
                    }
                ],
                "author_detail": {
                    "name": "Mourad Guenouche"
                },
                "author": "Mourad Guenouche",
                "arxiv_comment": "22 pages, 6 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10436v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10436v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12777v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12777v1",
                "updated": "2024-08-23T00:54:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    0,
                    54,
                    28,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T00:54:28Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    0,
                    54,
                    28,
                    4,
                    236,
                    0
                ],
                "title": "Environment-Centric Active Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Environment-Centric Active Inference"
                },
                "summary": "To handle unintended changes in the environment by agents, we propose an\nenvironment-centric active inference EC-AIF in which the Markov Blanket of\nactive inference is defined starting from the environment. In normal active\ninference, the Markov Blanket is defined starting from the agent. That is,\nfirst the agent was defined as the entity that performs the \"action\" such as a\nrobot or a person, then the environment was defined as other people or objects\nthat are directly affected by the agent's \"action,\" and the boundary between\nthe agent and the environment was defined as the Markov Blanket. This\nagent-centric definition does not allow the agent to respond to unintended\nchanges in the environment caused by factors outside of the defined\nenvironment. In the proposed EC-AIF, there is no entity corresponding to an\nagent. The environment includes all observable things, including people and\nthings conventionally considered to be the environment, as well as entities\nthat perform \"actions\" such as robots and people. Accordingly, all states,\nincluding robots and people, are included in inference targets, eliminating\nunintended changes in the environment. The EC-AIF was applied to a robot arm\nand validated with an object transport task by the robot arm. The results\nshowed that the robot arm successfully transported objects while responding to\nchanges in the target position of the object and to changes in the orientation\nof another robot arm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To handle unintended changes in the environment by agents, we propose an\nenvironment-centric active inference EC-AIF in which the Markov Blanket of\nactive inference is defined starting from the environment. In normal active\ninference, the Markov Blanket is defined starting from the agent. That is,\nfirst the agent was defined as the entity that performs the \"action\" such as a\nrobot or a person, then the environment was defined as other people or objects\nthat are directly affected by the agent's \"action,\" and the boundary between\nthe agent and the environment was defined as the Markov Blanket. This\nagent-centric definition does not allow the agent to respond to unintended\nchanges in the environment caused by factors outside of the defined\nenvironment. In the proposed EC-AIF, there is no entity corresponding to an\nagent. The environment includes all observable things, including people and\nthings conventionally considered to be the environment, as well as entities\nthat perform \"actions\" such as robots and people. Accordingly, all states,\nincluding robots and people, are included in inference targets, eliminating\nunintended changes in the environment. The EC-AIF was applied to a robot arm\nand validated with an object transport task by the robot arm. The results\nshowed that the robot arm successfully transported objects while responding to\nchanges in the target position of the object and to changes in the orientation\nof another robot arm."
                },
                "authors": [
                    {
                        "name": "Kanako Esaki"
                    },
                    {
                        "name": "Tadayuki Matsumura"
                    },
                    {
                        "name": "Takeshi Kato"
                    },
                    {
                        "name": "Shunsuke Minusa"
                    },
                    {
                        "name": "Yang Shao"
                    },
                    {
                        "name": "Hiroyuki Mizuno"
                    }
                ],
                "author_detail": {
                    "name": "Hiroyuki Mizuno"
                },
                "author": "Hiroyuki Mizuno",
                "arxiv_comment": "14 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12777v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12777v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12775v1",
                "updated": "2024-08-23T00:49:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    0,
                    49,
                    36,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T00:49:36Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    0,
                    49,
                    36,
                    4,
                    236,
                    0
                ],
                "title": "Intelligent OPC Engineer Assistant for Semiconductor Manufacturing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent OPC Engineer Assistant for Semiconductor Manufacturing"
                },
                "summary": "Advancements in chip design and manufacturing have enabled the processing of\ncomplex tasks such as deep learning and natural language processing, paving the\nway for the development of artificial general intelligence (AGI). AI, on the\nother hand, can be leveraged to innovate and streamline semiconductor\ntechnology from planning and implementation to manufacturing. In this paper, we\npresent \\textit{Intelligent OPC Engineer Assistant}, an AI/LLM-powered\nmethodology designed to solve the core manufacturing-aware optimization problem\nknown as optical proximity correction (OPC). The methodology involves a\nreinforcement learning-based OPC recipe search and a customized multi-modal\nagent system for recipe summarization. Experiments demonstrate that our\nmethodology can efficiently build OPC recipes on various chip designs with\nspecially handled design topologies, a task that typically requires the\nfull-time effort of OPC engineers with years of experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in chip design and manufacturing have enabled the processing of\ncomplex tasks such as deep learning and natural language processing, paving the\nway for the development of artificial general intelligence (AGI). AI, on the\nother hand, can be leveraged to innovate and streamline semiconductor\ntechnology from planning and implementation to manufacturing. In this paper, we\npresent \\textit{Intelligent OPC Engineer Assistant}, an AI/LLM-powered\nmethodology designed to solve the core manufacturing-aware optimization problem\nknown as optical proximity correction (OPC). The methodology involves a\nreinforcement learning-based OPC recipe search and a customized multi-modal\nagent system for recipe summarization. Experiments demonstrate that our\nmethodology can efficiently build OPC recipes on various chip designs with\nspecially handled design topologies, a task that typically requires the\nfull-time effort of OPC engineers with years of experience."
                },
                "authors": [
                    {
                        "name": "Guojin Chen"
                    },
                    {
                        "name": "Haoyu Yang"
                    },
                    {
                        "name": "Haoxing Ren"
                    },
                    {
                        "name": "Bei Yu"
                    }
                ],
                "author_detail": {
                    "name": "Bei Yu"
                },
                "author": "Bei Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.08707v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.08707v4",
                "updated": "2024-08-23T00:31:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    0,
                    31,
                    17,
                    4,
                    236,
                    0
                ],
                "published": "2023-09-15T18:58:08Z",
                "published_parsed": [
                    2023,
                    9,
                    15,
                    18,
                    58,
                    8,
                    4,
                    258,
                    0
                ],
                "title": "Fixed-b Asymptotics for Panel Models with Two-Way Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fixed-b Asymptotics for Panel Models with Two-Way Clustering"
                },
                "summary": "This paper studies a cluster robust variance estimator proposed by Chiang,\nHansen and Sasaki (2024) for linear panels. First, we show algebraically that\nthis variance estimator (CHS estimator, hereafter) is a linear combination of\nthree common variance estimators: the one-way unit cluster estimator, the \"HAC\nof averages\" estimator, and the \"average of HACs\" estimator. Based on this\nfinding, we obtain a fixed-$b$ asymptotic result for the CHS estimator and\ncorresponding test statistics as the cross-section and time sample sizes\njointly go to infinity. Furthermore, we propose two simple bias-corrected\nversions of the variance estimator and derive the fixed-$b$ limits. In a\nsimulation study, we find that the two bias-corrected variance estimators along\nwith fixed-$b$ critical values provide improvements in finite sample coverage\nprobabilities. We illustrate the impact of bias-correction and use of the\nfixed-$b$ critical values on inference in an empirical example on the\nrelationship between industry profitability and market concentration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies a cluster robust variance estimator proposed by Chiang,\nHansen and Sasaki (2024) for linear panels. First, we show algebraically that\nthis variance estimator (CHS estimator, hereafter) is a linear combination of\nthree common variance estimators: the one-way unit cluster estimator, the \"HAC\nof averages\" estimator, and the \"average of HACs\" estimator. Based on this\nfinding, we obtain a fixed-$b$ asymptotic result for the CHS estimator and\ncorresponding test statistics as the cross-section and time sample sizes\njointly go to infinity. Furthermore, we propose two simple bias-corrected\nversions of the variance estimator and derive the fixed-$b$ limits. In a\nsimulation study, we find that the two bias-corrected variance estimators along\nwith fixed-$b$ critical values provide improvements in finite sample coverage\nprobabilities. We illustrate the impact of bias-correction and use of the\nfixed-$b$ critical values on inference in an empirical example on the\nrelationship between industry profitability and market concentration."
                },
                "authors": [
                    {
                        "name": "Kaicheng Chen"
                    },
                    {
                        "name": "Timothy J. Vogelsang"
                    }
                ],
                "author_detail": {
                    "name": "Timothy J. Vogelsang"
                },
                "author": "Timothy J. Vogelsang",
                "arxiv_doi": "10.1016/j.jeconom.2024.105831",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jeconom.2024.105831",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2309.08707v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.08707v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10981v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10981v2",
                "updated": "2024-08-23T00:17:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    0,
                    17,
                    2,
                    4,
                    236,
                    0
                ],
                "published": "2024-04-17T01:27:42Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    1,
                    27,
                    42,
                    2,
                    108,
                    0
                ],
                "title": "A Survey on Retrieval-Augmented Text Generation for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Retrieval-Augmented Text Generation for Large Language\n  Models"
                },
                "summary": "Retrieval-Augmented Generation (RAG) merges retrieval methods with deep\nlearning advancements to address the static limitations of large language\nmodels (LLMs) by enabling the dynamic integration of up-to-date external\ninformation. This methodology, focusing primarily on the text domain, provides\na cost-effective solution to the generation of plausible but possibly incorrect\nresponses by LLMs, thereby enhancing the accuracy and reliability of their\noutputs through the use of real-world data. As RAG grows in complexity and\nincorporates multiple concepts that can influence its performance, this paper\norganizes the RAG paradigm into four categories: pre-retrieval, retrieval,\npost-retrieval, and generation, offering a detailed perspective from the\nretrieval viewpoint. It outlines RAG's evolution and discusses the field's\nprogression through the analysis of significant studies. Additionally, the\npaper introduces evaluation methods for RAG, addressing the challenges faced\nand proposing future research directions. By offering an organized framework\nand categorization, the study aims to consolidate existing research on RAG,\nclarify its technological underpinnings, and highlight its potential to broaden\nthe adaptability and applications of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) merges retrieval methods with deep\nlearning advancements to address the static limitations of large language\nmodels (LLMs) by enabling the dynamic integration of up-to-date external\ninformation. This methodology, focusing primarily on the text domain, provides\na cost-effective solution to the generation of plausible but possibly incorrect\nresponses by LLMs, thereby enhancing the accuracy and reliability of their\noutputs through the use of real-world data. As RAG grows in complexity and\nincorporates multiple concepts that can influence its performance, this paper\norganizes the RAG paradigm into four categories: pre-retrieval, retrieval,\npost-retrieval, and generation, offering a detailed perspective from the\nretrieval viewpoint. It outlines RAG's evolution and discusses the field's\nprogression through the analysis of significant studies. Additionally, the\npaper introduces evaluation methods for RAG, addressing the challenges faced\nand proposing future research directions. By offering an organized framework\nand categorization, the study aims to consolidate existing research on RAG,\nclarify its technological underpinnings, and highlight its potential to broaden\nthe adaptability and applications of LLMs."
                },
                "authors": [
                    {
                        "name": "Yizheng Huang"
                    },
                    {
                        "name": "Jimmy Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jimmy Huang"
                },
                "author": "Jimmy Huang",
                "arxiv_comment": "Ongoing Work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10981v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10981v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18966v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18966v3",
                "updated": "2024-08-23T00:14:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    0,
                    14,
                    51,
                    4,
                    236,
                    0
                ],
                "published": "2024-06-27T07:56:44Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    7,
                    56,
                    44,
                    3,
                    179,
                    0
                ],
                "title": "UniGen: A Unified Framework for Textual Dataset Generation Using Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniGen: A Unified Framework for Textual Dataset Generation Using Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) such as GPT-4 and Llama3 have significantly\nimpacted various fields by enabling high-quality synthetic data generation and\nreducing dependence on expensive human-generated datasets. Despite this,\nchallenges remain in the areas of generalization, controllability, diversity,\nand truthfulness within the existing generative frameworks. To address these\nchallenges, this paper presents UniGen, a comprehensive LLM-powered framework\ndesigned to produce diverse, accurate, and highly controllable datasets. UniGen\nis adaptable, supporting all types of text datasets and enhancing the\ngenerative process through innovative mechanisms. To augment data diversity,\nUniGen incorporates an attribute-guided generation module and a group checking\nfeature. For accuracy, it employs a code-based mathematical assessment for\nlabel verification alongside a retrieval-augmented generation technique for\nfactual validation. The framework also allows for user-specified constraints,\nenabling customization of the data generation process to suit particular\nrequirements. Extensive experiments demonstrate the superior quality of data\ngenerated by UniGen, and each module within UniGen plays a critical role in\nthis enhancement. Additionally, UniGen is applied in two practical scenarios:\nbenchmarking LLMs and data augmentation. The results indicate that UniGen\neffectively supports dynamic and evolving benchmarking, and that data\naugmentation improves LLM capabilities in various domains, including\nagent-oriented abilities and reasoning skills.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) such as GPT-4 and Llama3 have significantly\nimpacted various fields by enabling high-quality synthetic data generation and\nreducing dependence on expensive human-generated datasets. Despite this,\nchallenges remain in the areas of generalization, controllability, diversity,\nand truthfulness within the existing generative frameworks. To address these\nchallenges, this paper presents UniGen, a comprehensive LLM-powered framework\ndesigned to produce diverse, accurate, and highly controllable datasets. UniGen\nis adaptable, supporting all types of text datasets and enhancing the\ngenerative process through innovative mechanisms. To augment data diversity,\nUniGen incorporates an attribute-guided generation module and a group checking\nfeature. For accuracy, it employs a code-based mathematical assessment for\nlabel verification alongside a retrieval-augmented generation technique for\nfactual validation. The framework also allows for user-specified constraints,\nenabling customization of the data generation process to suit particular\nrequirements. Extensive experiments demonstrate the superior quality of data\ngenerated by UniGen, and each module within UniGen plays a critical role in\nthis enhancement. Additionally, UniGen is applied in two practical scenarios:\nbenchmarking LLMs and data augmentation. The results indicate that UniGen\neffectively supports dynamic and evolving benchmarking, and that data\naugmentation improves LLM capabilities in various domains, including\nagent-oriented abilities and reasoning skills."
                },
                "authors": [
                    {
                        "name": "Siyuan Wu"
                    },
                    {
                        "name": "Yue Huang"
                    },
                    {
                        "name": "Chujie Gao"
                    },
                    {
                        "name": "Dongping Chen"
                    },
                    {
                        "name": "Qihui Zhang"
                    },
                    {
                        "name": "Yao Wan"
                    },
                    {
                        "name": "Tianyi Zhou"
                    },
                    {
                        "name": "Xiangliang Zhang"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Chaowei Xiao"
                    },
                    {
                        "name": "Lichao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Lichao Sun"
                },
                "author": "Lichao Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18966v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18966v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2408.13257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13257v1",
                "updated": "2024-08-23T17:59:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    59,
                    51,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T17:59:51Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    59,
                    51,
                    4,
                    236,
                    0
                ],
                "title": "MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution\n  Real-World Scenarios that are Difficult for Humans?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution\n  Real-World Scenarios that are Difficult for Humans?"
                },
                "summary": "Comprehensive evaluation of Multimodal Large Language Models (MLLMs) has\nrecently garnered widespread attention in the research community. However, we\nobserve that existing benchmarks present several common barriers that make it\ndifficult to measure the significant challenges that models face in the real\nworld, including: 1) small data scale leads to a large performance variance; 2)\nreliance on model-based annotations results in restricted data quality; 3)\ninsufficient task difficulty, especially caused by the limited image\nresolution. To tackle these issues, we introduce MME-RealWorld. Specifically,\nwe collect more than $300$K images from public datasets and the Internet,\nfiltering $13,366$ high-quality images for annotation. This involves the\nefforts of professional $25$ annotators and $7$ experts in MLLMs, contributing\nto $29,429$ question-answer pairs that cover $43$ subtasks across $5$\nreal-world scenarios, extremely challenging even for humans. As far as we know,\nMME-RealWorld is the largest manually annotated benchmark to date, featuring\nthe highest resolution and a targeted focus on real-world applications. We\nfurther conduct a thorough evaluation involving $28$ prominent MLLMs, such as\nGPT-4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet. Our results show that even the\nmost advanced models struggle with our benchmarks, where none of them reach\n$60\\%$ accuracy. The challenges of perceiving high-resolution images and\nunderstanding complex real-world scenarios remain urgent issues to be\naddressed. The data and evaluation code are released at\nhttps://mme-realworld.github.io/ .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comprehensive evaluation of Multimodal Large Language Models (MLLMs) has\nrecently garnered widespread attention in the research community. However, we\nobserve that existing benchmarks present several common barriers that make it\ndifficult to measure the significant challenges that models face in the real\nworld, including: 1) small data scale leads to a large performance variance; 2)\nreliance on model-based annotations results in restricted data quality; 3)\ninsufficient task difficulty, especially caused by the limited image\nresolution. To tackle these issues, we introduce MME-RealWorld. Specifically,\nwe collect more than $300$K images from public datasets and the Internet,\nfiltering $13,366$ high-quality images for annotation. This involves the\nefforts of professional $25$ annotators and $7$ experts in MLLMs, contributing\nto $29,429$ question-answer pairs that cover $43$ subtasks across $5$\nreal-world scenarios, extremely challenging even for humans. As far as we know,\nMME-RealWorld is the largest manually annotated benchmark to date, featuring\nthe highest resolution and a targeted focus on real-world applications. We\nfurther conduct a thorough evaluation involving $28$ prominent MLLMs, such as\nGPT-4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet. Our results show that even the\nmost advanced models struggle with our benchmarks, where none of them reach\n$60\\%$ accuracy. The challenges of perceiving high-resolution images and\nunderstanding complex real-world scenarios remain urgent issues to be\naddressed. The data and evaluation code are released at\nhttps://mme-realworld.github.io/ ."
                },
                "authors": [
                    {
                        "name": "Yi-Fan Zhang"
                    },
                    {
                        "name": "Huanyu Zhang"
                    },
                    {
                        "name": "Haochen Tian"
                    },
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Shuangqing Zhang"
                    },
                    {
                        "name": "Junfei Wu"
                    },
                    {
                        "name": "Feng Li"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Qingsong Wen"
                    },
                    {
                        "name": "Zhang Zhang"
                    },
                    {
                        "name": "Liang Wang"
                    },
                    {
                        "name": "Rong Jin"
                    },
                    {
                        "name": "Tieniu Tan"
                    }
                ],
                "author_detail": {
                    "name": "Tieniu Tan"
                },
                "author": "Tieniu Tan",
                "arxiv_comment": "Project Page:\n  $\\href{https://mme-realworld.github.io/}{\\text{https://mme-realworld.github.io/}}$",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08750v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08750v3",
                "updated": "2024-08-23T17:55:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    55,
                    12,
                    4,
                    236,
                    0
                ],
                "published": "2023-10-12T22:30:15Z",
                "published_parsed": [
                    2023,
                    10,
                    12,
                    22,
                    30,
                    15,
                    3,
                    285,
                    0
                ],
                "title": "Search-Adaptor: Embedding Customization for Information Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search-Adaptor: Embedding Customization for Information Retrieval"
                },
                "summary": "Embeddings extracted by pre-trained Large Language Models (LLMs) have\nsignificant potential to improve information retrieval and search. Beyond the\nzero-shot setup in which they are being conventionally used, being able to take\nadvantage of the information from the relevant query-corpus paired data can\nfurther boost the LLM capabilities. In this paper, we propose a novel method,\nSearch-Adaptor, for customizing LLMs for information retrieval in an efficient\nand robust way. Search-Adaptor modifies the embeddings generated by pre-trained\nLLMs, and can be integrated with any LLM, including those only available via\nprediction APIs. On multiple English, multilingual, and multimodal retrieval\ndatasets, we show consistent and significant performance benefits for\nSearch-Adaptor -- e.g., more than 5% improvements for Google Embedding APIs in\nnDCG@10 averaged over 14 BEIR datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embeddings extracted by pre-trained Large Language Models (LLMs) have\nsignificant potential to improve information retrieval and search. Beyond the\nzero-shot setup in which they are being conventionally used, being able to take\nadvantage of the information from the relevant query-corpus paired data can\nfurther boost the LLM capabilities. In this paper, we propose a novel method,\nSearch-Adaptor, for customizing LLMs for information retrieval in an efficient\nand robust way. Search-Adaptor modifies the embeddings generated by pre-trained\nLLMs, and can be integrated with any LLM, including those only available via\nprediction APIs. On multiple English, multilingual, and multimodal retrieval\ndatasets, we show consistent and significant performance benefits for\nSearch-Adaptor -- e.g., more than 5% improvements for Google Embedding APIs in\nnDCG@10 averaged over 14 BEIR datasets."
                },
                "authors": [
                    {
                        "name": "Jinsung Yoon"
                    },
                    {
                        "name": "Sercan O Arik"
                    },
                    {
                        "name": "Yanfei Chen"
                    },
                    {
                        "name": "Tomas Pfister"
                    }
                ],
                "author_detail": {
                    "name": "Tomas Pfister"
                },
                "author": "Tomas Pfister",
                "arxiv_comment": "Published in 2024 ACL Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.08750v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08750v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11049v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11049v3",
                "updated": "2024-08-23T17:54:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    54,
                    34,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-20T17:57:31Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    31,
                    1,
                    233,
                    0
                ],
                "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/."
                },
                "authors": [
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Vashisth Tiwari"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Jinyuan Shi"
                    },
                    {
                        "name": "Ian En-Hsu Yen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11049v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11049v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13251v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13251v1",
                "updated": "2024-08-23T17:48:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    48,
                    22,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T17:48:22Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    48,
                    22,
                    4,
                    236,
                    0
                ],
                "title": "Re-evaluation of Face Anti-spoofing Algorithm in Post COVID-19 Era Using\n  Mask Based Occlusion Attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re-evaluation of Face Anti-spoofing Algorithm in Post COVID-19 Era Using\n  Mask Based Occlusion Attack"
                },
                "summary": "Face anti-spoofing algorithms play a pivotal role in the robust deployment of\nface recognition systems against presentation attacks. Conventionally, full\nfacial images are required by such systems to correctly authenticate\nindividuals, but the widespread requirement of masks due to the current\nCOVID-19 pandemic has introduced new challenges for these biometric\nauthentication systems. Hence, in this work, we investigate the performance of\npresentation attack detection (PAD) algorithms under synthetic facial\nocclusions using masks and glasses. We have used five variants of masks to\ncover the lower part of the face with varying coverage areas (low-coverage,\nmedium-coverage, high-coverage, round coverage), and 3D cues. We have also used\ndifferent variants of glasses that cover the upper part of the face. We\nsystematically tested the performance of four PAD algorithms under these\nocclusion attacks using a benchmark dataset. We have specifically looked at\nfour different baseline PAD algorithms that focus on, texture, image quality,\nframe difference/motion, and abstract features through a convolutional neural\nnetwork (CNN). Additionally we have introduced a new hybrid model that uses CNN\nand local binary pattern textures. Our experiment shows that adding the\nocclusions significantly degrades the performance of all of the PAD algorithms.\nOur results show the vulnerability of face anti-spoofing algorithms with\nocclusions, which could be in the usage of such algorithms in the post-pandemic\nera.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Face anti-spoofing algorithms play a pivotal role in the robust deployment of\nface recognition systems against presentation attacks. Conventionally, full\nfacial images are required by such systems to correctly authenticate\nindividuals, but the widespread requirement of masks due to the current\nCOVID-19 pandemic has introduced new challenges for these biometric\nauthentication systems. Hence, in this work, we investigate the performance of\npresentation attack detection (PAD) algorithms under synthetic facial\nocclusions using masks and glasses. We have used five variants of masks to\ncover the lower part of the face with varying coverage areas (low-coverage,\nmedium-coverage, high-coverage, round coverage), and 3D cues. We have also used\ndifferent variants of glasses that cover the upper part of the face. We\nsystematically tested the performance of four PAD algorithms under these\nocclusion attacks using a benchmark dataset. We have specifically looked at\nfour different baseline PAD algorithms that focus on, texture, image quality,\nframe difference/motion, and abstract features through a convolutional neural\nnetwork (CNN). Additionally we have introduced a new hybrid model that uses CNN\nand local binary pattern textures. Our experiment shows that adding the\nocclusions significantly degrades the performance of all of the PAD algorithms.\nOur results show the vulnerability of face anti-spoofing algorithms with\nocclusions, which could be in the usage of such algorithms in the post-pandemic\nera."
                },
                "authors": [
                    {
                        "name": "Vaibhav Sundharam"
                    },
                    {
                        "name": "Abhijit Sarkar"
                    },
                    {
                        "name": "A. Lynn Abbott"
                    }
                ],
                "author_detail": {
                    "name": "A. Lynn Abbott"
                },
                "author": "A. Lynn Abbott",
                "arxiv_comment": "10 pages, This work was done in 2020",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13251v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13251v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13247v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13247v1",
                "updated": "2024-08-23T17:42:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    42,
                    6,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T17:42:06Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    42,
                    6,
                    4,
                    236,
                    0
                ],
                "title": "Data Exposure from LLM Apps: An In-depth Investigation of OpenAI's GPTs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Exposure from LLM Apps: An In-depth Investigation of OpenAI's GPTs"
                },
                "summary": "LLM app ecosystems are quickly maturing and supporting a wide range of use\ncases, which requires them to collect excessive user data. Given that the LLM\napps are developed by third-parties and that anecdotal evidence suggests LLM\nplatforms currently do not strictly enforce their policies, user data shared\nwith arbitrary third-parties poses a significant privacy risk. In this paper we\naim to bring transparency in data practices of LLM apps. As a case study, we\nstudy OpenAI's GPT app ecosystem. We develop an LLM-based framework to conduct\nthe static analysis of natural language-based source code of GPTs and their\nActions (external services) to characterize their data collection practices.\nOur findings indicate that Actions collect expansive data about users,\nincluding sensitive information prohibited by OpenAI, such as passwords. We\nfind that some Actions, including related to advertising and analytics, are\nembedded in multiple GPTs, which allow them to track user activities across\nGPTs. Additionally, co-occurrence of Actions exposes as much as 9.5x more data\nto them, than it is exposed to individual Actions. Lastly, we develop an\nLLM-based privacy policy analysis framework to automatically check the\nconsistency of data collection by Actions with disclosures in their privacy\npolicies. Our measurements indicate that the disclosures for most of the\ncollected data types are omitted in privacy policies, with only 5.8% of Actions\nclearly disclosing their data collection practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM app ecosystems are quickly maturing and supporting a wide range of use\ncases, which requires them to collect excessive user data. Given that the LLM\napps are developed by third-parties and that anecdotal evidence suggests LLM\nplatforms currently do not strictly enforce their policies, user data shared\nwith arbitrary third-parties poses a significant privacy risk. In this paper we\naim to bring transparency in data practices of LLM apps. As a case study, we\nstudy OpenAI's GPT app ecosystem. We develop an LLM-based framework to conduct\nthe static analysis of natural language-based source code of GPTs and their\nActions (external services) to characterize their data collection practices.\nOur findings indicate that Actions collect expansive data about users,\nincluding sensitive information prohibited by OpenAI, such as passwords. We\nfind that some Actions, including related to advertising and analytics, are\nembedded in multiple GPTs, which allow them to track user activities across\nGPTs. Additionally, co-occurrence of Actions exposes as much as 9.5x more data\nto them, than it is exposed to individual Actions. Lastly, we develop an\nLLM-based privacy policy analysis framework to automatically check the\nconsistency of data collection by Actions with disclosures in their privacy\npolicies. Our measurements indicate that the disclosures for most of the\ncollected data types are omitted in privacy policies, with only 5.8% of Actions\nclearly disclosing their data collection practices."
                },
                "authors": [
                    {
                        "name": "Evin Jaff"
                    },
                    {
                        "name": "Yuhao Wu"
                    },
                    {
                        "name": "Ning Zhang"
                    },
                    {
                        "name": "Umar Iqbal"
                    }
                ],
                "author_detail": {
                    "name": "Umar Iqbal"
                },
                "author": "Umar Iqbal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13247v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05148v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05148v2",
                "updated": "2024-08-23T17:40:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    40,
                    15,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-09T16:07:37Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    7,
                    37,
                    4,
                    222,
                    0
                ],
                "title": "Impacts of floating-point non-associativity on reproducibility for HPC\n  and deep learning applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impacts of floating-point non-associativity on reproducibility for HPC\n  and deep learning applications"
                },
                "summary": "Run-by-run variability in parallel programs caused by floating-point\nnon-associativity (FPNA) has been known to significantly affect reproducibility\nin iterative algorithms, due to accumulating errors. Non-reproducibility\nnegatively affects efficiency and effectiveness of correctness testing for\nstochastic programs. Recently, the sensitivity of deep learning (DL) training\nand inference pipelines to FPNA have been found to be extreme, and can prevent\ncertification for commercial applications, accurate assessment of robustness\nand sensitivity, and bug detection. New approaches in scientific computing\napplications have coupled DL models with high-performance computing (HPC)\nsimulations, leading to an aggravation of debugging and testing challenges.\nHere we perform an investigation of the statistical properties of FPNA within\nmodern parallel programming models, analyze performance and productivity\nimpacts of replacing atomic operations with deterministic alternatives on GPUs,\nand examine the recently-added deterministic options within the PyTorch\nframework within the context of GPU deployment, uncovering and quantifying the\nimpacts of input parameters triggering run-by-run variability and reporting on\nthe reliability and completeness of the documentation. Finally, we evaluate the\nstrategy of exploiting automatic determinism provided by deterministic\nhardware, using the Groq LPU$^{TM}$ accelerator for inference portions of the\nDL pipeline. We demonstrate the benefits that this strategy can provide within\nreproducibility and correctness efforts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Run-by-run variability in parallel programs caused by floating-point\nnon-associativity (FPNA) has been known to significantly affect reproducibility\nin iterative algorithms, due to accumulating errors. Non-reproducibility\nnegatively affects efficiency and effectiveness of correctness testing for\nstochastic programs. Recently, the sensitivity of deep learning (DL) training\nand inference pipelines to FPNA have been found to be extreme, and can prevent\ncertification for commercial applications, accurate assessment of robustness\nand sensitivity, and bug detection. New approaches in scientific computing\napplications have coupled DL models with high-performance computing (HPC)\nsimulations, leading to an aggravation of debugging and testing challenges.\nHere we perform an investigation of the statistical properties of FPNA within\nmodern parallel programming models, analyze performance and productivity\nimpacts of replacing atomic operations with deterministic alternatives on GPUs,\nand examine the recently-added deterministic options within the PyTorch\nframework within the context of GPU deployment, uncovering and quantifying the\nimpacts of input parameters triggering run-by-run variability and reporting on\nthe reliability and completeness of the documentation. Finally, we evaluate the\nstrategy of exploiting automatic determinism provided by deterministic\nhardware, using the Groq LPU$^{TM}$ accelerator for inference portions of the\nDL pipeline. We demonstrate the benefits that this strategy can provide within\nreproducibility and correctness efforts."
                },
                "authors": [
                    {
                        "name": "Sanjif Shanmugavelu"
                    },
                    {
                        "name": "Mathieu Taillefumier"
                    },
                    {
                        "name": "Christopher Culver"
                    },
                    {
                        "name": "Oscar Hernandez"
                    },
                    {
                        "name": "Mark Coletti"
                    },
                    {
                        "name": "Ada Sedova"
                    }
                ],
                "author_detail": {
                    "name": "Ada Sedova"
                },
                "author": "Ada Sedova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05148v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05148v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13233v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13233v1",
                "updated": "2024-08-23T17:16:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    16,
                    43,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T17:16:43Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    16,
                    43,
                    4,
                    236,
                    0
                ],
                "title": "Multi-Layer Transformers Gradient Can be Approximated in Almost Linear\n  Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Layer Transformers Gradient Can be Approximated in Almost Linear\n  Time"
                },
                "summary": "The quadratic computational complexity in the self-attention mechanism of\npopular transformer architectures poses significant challenges for training and\ninference, particularly in terms of efficiency and memory requirements. Towards\naddressing these challenges, this paper introduces a novel fast computation\nmethod for gradient calculation in multi-layer transformer models. Our approach\nenables the computation of gradients for the entire multi-layer transformer\nmodel in almost linear time $n^{1+o(1)}$, where $n$ is the input sequence\nlength. This breakthrough significantly reduces the computational bottleneck\nassociated with the traditional quadratic time complexity. Our theory holds for\nany loss function and maintains a bounded approximation error across the entire\nmodel. Furthermore, our analysis can hold when the multi-layer transformer\nmodel contains many practical sub-modules, such as residual connection, casual\nmask, and multi-head attention. By improving the efficiency of gradient\ncomputation in large language models, we hope that our work will facilitate the\nmore effective training and deployment of long-context language models based on\nour theoretical results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quadratic computational complexity in the self-attention mechanism of\npopular transformer architectures poses significant challenges for training and\ninference, particularly in terms of efficiency and memory requirements. Towards\naddressing these challenges, this paper introduces a novel fast computation\nmethod for gradient calculation in multi-layer transformer models. Our approach\nenables the computation of gradients for the entire multi-layer transformer\nmodel in almost linear time $n^{1+o(1)}$, where $n$ is the input sequence\nlength. This breakthrough significantly reduces the computational bottleneck\nassociated with the traditional quadratic time complexity. Our theory holds for\nany loss function and maintains a bounded approximation error across the entire\nmodel. Furthermore, our analysis can hold when the multi-layer transformer\nmodel contains many practical sub-modules, such as residual connection, casual\nmask, and multi-head attention. By improving the efficiency of gradient\ncomputation in large language models, we hope that our work will facilitate the\nmore effective training and deployment of long-context language models based on\nour theoretical results."
                },
                "authors": [
                    {
                        "name": "Yingyu Liang"
                    },
                    {
                        "name": "Zhizhou Sha"
                    },
                    {
                        "name": "Zhenmei Shi"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yufa Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yufa Zhou"
                },
                "author": "Yufa Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13233v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13233v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11903v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11903v2",
                "updated": "2024-08-23T17:15:39Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    15,
                    39,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-21T18:00:21Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    18,
                    0,
                    21,
                    2,
                    234,
                    0
                ],
                "title": "Ancient Wisdom, Modern Tools: Exploring Retrieval-Augmented LLMs for\n  Ancient Indian Philosophy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ancient Wisdom, Modern Tools: Exploring Retrieval-Augmented LLMs for\n  Ancient Indian Philosophy"
                },
                "summary": "LLMs have revolutionized the landscape of information retrieval and knowledge\ndissemination. However, their application in specialized areas is often\nhindered by factual inaccuracies and hallucinations, especially in long-tail\nknowledge distributions. We explore the potential of retrieval-augmented\ngeneration (RAG) models for long-form question answering (LFQA) in a\nspecialized knowledge domain. We present VedantaNY-10M, a dataset curated from\nextensive public discourses on the ancient Indian philosophy of Advaita\nVedanta. We develop and benchmark a RAG model against a standard, non-RAG LLM,\nfocusing on transcription, retrieval, and generation performance. Human\nevaluations by computational linguists and domain experts show that the RAG\nmodel significantly outperforms the standard model in producing factual and\ncomprehensive responses having fewer hallucinations. In addition, a\nkeyword-based hybrid retriever that emphasizes unique low-frequency terms\nfurther improves results. Our study provides insights into effectively\nintegrating modern large language models with ancient knowledge systems.\nProject page with dataset and code: https://sites.google.com/view/vedantany-10m",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have revolutionized the landscape of information retrieval and knowledge\ndissemination. However, their application in specialized areas is often\nhindered by factual inaccuracies and hallucinations, especially in long-tail\nknowledge distributions. We explore the potential of retrieval-augmented\ngeneration (RAG) models for long-form question answering (LFQA) in a\nspecialized knowledge domain. We present VedantaNY-10M, a dataset curated from\nextensive public discourses on the ancient Indian philosophy of Advaita\nVedanta. We develop and benchmark a RAG model against a standard, non-RAG LLM,\nfocusing on transcription, retrieval, and generation performance. Human\nevaluations by computational linguists and domain experts show that the RAG\nmodel significantly outperforms the standard model in producing factual and\ncomprehensive responses having fewer hallucinations. In addition, a\nkeyword-based hybrid retriever that emphasizes unique low-frequency terms\nfurther improves results. Our study provides insights into effectively\nintegrating modern large language models with ancient knowledge systems.\nProject page with dataset and code: https://sites.google.com/view/vedantany-10m"
                },
                "authors": [
                    {
                        "name": "Priyanka Mandikal"
                    }
                ],
                "author_detail": {
                    "name": "Priyanka Mandikal"
                },
                "author": "Priyanka Mandikal",
                "arxiv_comment": "Outstanding Paper at the Machine Learning for Ancient Languages\n  Workshop, 2024.ml4al-1.23, Association for Computational Linguistics (ACL)\n  2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11903v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11903v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15264v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15264v2",
                "updated": "2024-08-23T17:04:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    4,
                    38,
                    4,
                    236,
                    0
                ],
                "published": "2024-06-21T15:57:24Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    15,
                    57,
                    24,
                    4,
                    173,
                    0
                ],
                "title": "Towards Fine-Grained Citation Evaluation in Generated Text: A\n  Comparative Analysis of Faithfulness Metrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Fine-Grained Citation Evaluation in Generated Text: A\n  Comparative Analysis of Faithfulness Metrics"
                },
                "summary": "Large language models (LLMs) often produce unsupported or unverifiable\ncontent, known as \"hallucinations.\" To mitigate this, retrieval-augmented LLMs\nincorporate citations, grounding the content in verifiable sources. Despite\nsuch developments, manually assessing how well a citation supports the\nassociated statement remains a major challenge. Previous studies use\nfaithfulness metrics to estimate citation support automatically but are limited\nto binary classification, overlooking fine-grained citation support in\npractical scenarios. To investigate the effectiveness of faithfulness metrics\nin fine-grained scenarios, we propose a comparative evaluation framework that\nassesses the metric effectiveness in distinguishing citations between\nthree-category support levels: full, partial, and no support. Our framework\nemploys correlation analysis, classification evaluation, and retrieval\nevaluation to measure the alignment between metric scores and human judgments\ncomprehensively. Our results show no single metric consistently excels across\nall evaluations, revealing the complexity of assessing fine-grained support.\nBased on the findings, we provide practical recommendations for developing more\neffective metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often produce unsupported or unverifiable\ncontent, known as \"hallucinations.\" To mitigate this, retrieval-augmented LLMs\nincorporate citations, grounding the content in verifiable sources. Despite\nsuch developments, manually assessing how well a citation supports the\nassociated statement remains a major challenge. Previous studies use\nfaithfulness metrics to estimate citation support automatically but are limited\nto binary classification, overlooking fine-grained citation support in\npractical scenarios. To investigate the effectiveness of faithfulness metrics\nin fine-grained scenarios, we propose a comparative evaluation framework that\nassesses the metric effectiveness in distinguishing citations between\nthree-category support levels: full, partial, and no support. Our framework\nemploys correlation analysis, classification evaluation, and retrieval\nevaluation to measure the alignment between metric scores and human judgments\ncomprehensively. Our results show no single metric consistently excels across\nall evaluations, revealing the complexity of assessing fine-grained support.\nBased on the findings, we provide practical recommendations for developing more\neffective metrics."
                },
                "authors": [
                    {
                        "name": "Weijia Zhang"
                    },
                    {
                        "name": "Mohammad Aliannejadi"
                    },
                    {
                        "name": "Yifei Yuan"
                    },
                    {
                        "name": "Jiahuan Pei"
                    },
                    {
                        "name": "Jia-Hong Huang"
                    },
                    {
                        "name": "Evangelos Kanoulas"
                    }
                ],
                "author_detail": {
                    "name": "Evangelos Kanoulas"
                },
                "author": "Evangelos Kanoulas",
                "arxiv_comment": "Accepted by the 17th International Natural Language Generation\n  Conference (INLG 2024) as an oral presentation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15264v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15264v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07018v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07018v2",
                "updated": "2024-08-23T16:41:47Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    16,
                    41,
                    47,
                    4,
                    236,
                    0
                ],
                "published": "2024-07-09T16:38:48Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    16,
                    38,
                    48,
                    1,
                    191,
                    0
                ],
                "title": "End-To-End Causal Effect Estimation from Unstructured Natural Language\n  Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-To-End Causal Effect Estimation from Unstructured Natural Language\n  Data"
                },
                "summary": "Knowing the effect of an intervention is critical for human decision-making,\nbut current approaches for causal effect estimation rely on manual data\ncollection and structuring, regardless of the causal assumptions. This\nincreases both the cost and time-to-completion for studies. We show how large,\ndiverse observational text data can be mined with large language models (LLMs)\nto produce inexpensive causal effect estimates under appropriate causal\nassumptions. We introduce NATURAL, a novel family of causal effect estimators\nbuilt with LLMs that operate over datasets of unstructured text. Our estimators\nuse LLM conditional distributions (over variables of interest, given the text\ndata) to assist in the computation of classical estimators of causal effect. We\novercome a number of technical challenges to realize this idea, such as\nautomating data curation and using LLMs to impute missing information. We\nprepare six (two synthetic and four real) observational datasets, paired with\ncorresponding ground truth in the form of randomized trials, which we used to\nsystematically evaluate each step of our pipeline. NATURAL estimators\ndemonstrate remarkable performance, yielding causal effect estimates that fall\nwithin 3 percentage points of their ground truth counterparts, including on\nreal-world Phase 3/4 clinical trials. Our results suggest that unstructured\ntext data is a rich source of causal effect information, and NATURAL is a first\nstep towards an automated pipeline to tap this resource.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowing the effect of an intervention is critical for human decision-making,\nbut current approaches for causal effect estimation rely on manual data\ncollection and structuring, regardless of the causal assumptions. This\nincreases both the cost and time-to-completion for studies. We show how large,\ndiverse observational text data can be mined with large language models (LLMs)\nto produce inexpensive causal effect estimates under appropriate causal\nassumptions. We introduce NATURAL, a novel family of causal effect estimators\nbuilt with LLMs that operate over datasets of unstructured text. Our estimators\nuse LLM conditional distributions (over variables of interest, given the text\ndata) to assist in the computation of classical estimators of causal effect. We\novercome a number of technical challenges to realize this idea, such as\nautomating data curation and using LLMs to impute missing information. We\nprepare six (two synthetic and four real) observational datasets, paired with\ncorresponding ground truth in the form of randomized trials, which we used to\nsystematically evaluate each step of our pipeline. NATURAL estimators\ndemonstrate remarkable performance, yielding causal effect estimates that fall\nwithin 3 percentage points of their ground truth counterparts, including on\nreal-world Phase 3/4 clinical trials. Our results suggest that unstructured\ntext data is a rich source of causal effect information, and NATURAL is a first\nstep towards an automated pipeline to tap this resource."
                },
                "authors": [
                    {
                        "name": "Nikita Dhawan"
                    },
                    {
                        "name": "Leonardo Cotta"
                    },
                    {
                        "name": "Karen Ullrich"
                    },
                    {
                        "name": "Rahul G. Krishnan"
                    },
                    {
                        "name": "Chris J. Maddison"
                    }
                ],
                "author_detail": {
                    "name": "Chris J. Maddison"
                },
                "author": "Chris J. Maddison",
                "arxiv_comment": "28 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07018v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07018v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13204v1",
                "updated": "2024-08-23T16:33:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    16,
                    33,
                    58,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T16:33:58Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    16,
                    33,
                    58,
                    4,
                    236,
                    0
                ],
                "title": "DOMAINEVAL: An Auto-Constructed Benchmark for Multi-Domain Code\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DOMAINEVAL: An Auto-Constructed Benchmark for Multi-Domain Code\n  Generation"
                },
                "summary": "Code benchmarks such as HumanEval are widely adopted to evaluate the\ncapabilities of Large Language Models (LLMs), providing insights into their\nstrengths and weaknesses. However, current benchmarks primarily exercise LLMs'\ncapability on common coding tasks (e.g., bubble sort, greatest common divisor),\nleaving domain-specific coding tasks (e.g., computation, system, cryptography)\nunexplored. To fill this gap, we propose a multi-domain code benchmark,\nDOMAINEVAL, designed to evaluate LLMs' coding capabilities thoroughly. Our\npipeline works in a fully automated manner, enabling a push-bottom construction\nfrom code repositories into formatted subjects under study. Interesting\nfindings are observed by evaluating 12 representative LLMs against DOMAINEVAL.\nWe notice that LLMs are generally good at computation tasks while falling short\non cryptography and system coding tasks. The performance gap can be as much as\n68.94% (80.94% - 12.0%) in some LLMs. We also observe that generating more\nsamples can increase the overall performance of LLMs, while the domain bias may\neven increase. The contributions of this study include a code generation\nbenchmark dataset DOMAINEVAL, encompassing six popular domains, a fully\nautomated pipeline for constructing code benchmarks, and an identification of\nthe limitations of LLMs in code generation tasks based on their performance on\nDOMAINEVAL, providing directions for future research improvements. The\nleaderboard is available at https://domaineval.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code benchmarks such as HumanEval are widely adopted to evaluate the\ncapabilities of Large Language Models (LLMs), providing insights into their\nstrengths and weaknesses. However, current benchmarks primarily exercise LLMs'\ncapability on common coding tasks (e.g., bubble sort, greatest common divisor),\nleaving domain-specific coding tasks (e.g., computation, system, cryptography)\nunexplored. To fill this gap, we propose a multi-domain code benchmark,\nDOMAINEVAL, designed to evaluate LLMs' coding capabilities thoroughly. Our\npipeline works in a fully automated manner, enabling a push-bottom construction\nfrom code repositories into formatted subjects under study. Interesting\nfindings are observed by evaluating 12 representative LLMs against DOMAINEVAL.\nWe notice that LLMs are generally good at computation tasks while falling short\non cryptography and system coding tasks. The performance gap can be as much as\n68.94% (80.94% - 12.0%) in some LLMs. We also observe that generating more\nsamples can increase the overall performance of LLMs, while the domain bias may\neven increase. The contributions of this study include a code generation\nbenchmark dataset DOMAINEVAL, encompassing six popular domains, a fully\nautomated pipeline for constructing code benchmarks, and an identification of\nthe limitations of LLMs in code generation tasks based on their performance on\nDOMAINEVAL, providing directions for future research improvements. The\nleaderboard is available at https://domaineval.github.io/."
                },
                "authors": [
                    {
                        "name": "Qiming Zhu"
                    },
                    {
                        "name": "Jialun Cao"
                    },
                    {
                        "name": "Yaojie Lu"
                    },
                    {
                        "name": "Hongyu Lin"
                    },
                    {
                        "name": "Xianpei Han"
                    },
                    {
                        "name": "Le Sun"
                    },
                    {
                        "name": "Shing-Chi Cheung"
                    }
                ],
                "author_detail": {
                    "name": "Shing-Chi Cheung"
                },
                "author": "Shing-Chi Cheung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10128v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10128v2",
                "updated": "2024-08-23T16:15:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    16,
                    15,
                    30,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-19T16:15:09Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    16,
                    15,
                    9,
                    0,
                    232,
                    0
                ],
                "title": "Advancing Voice Cloning for Nepali: Leveraging Transfer Learning in a\n  Low-Resource Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Voice Cloning for Nepali: Leveraging Transfer Learning in a\n  Low-Resource Language"
                },
                "summary": "Voice cloning is a prominent feature in personalized speech interfaces. A\nneural vocal cloning system can mimic someone's voice using just a few audio\nsamples. Both speaker encoding and speaker adaptation are topics of research in\nthe field of voice cloning. Speaker adaptation relies on fine-tuning a\nmulti-speaker generative model, which involves training a separate model to\ninfer a new speaker embedding used for speaker encoding. Both methods can\nachieve excellent performance, even with a small number of cloning audios, in\nterms of the speech's naturalness and similarity to the original speaker.\nSpeaker encoding approaches are more appropriate for low-resource deployment\nsince they require significantly less memory and have a faster cloning time\nthan speaker adaption, which can offer slightly greater naturalness and\nsimilarity. The main goal is to create a vocal cloning system that produces\naudio output with a Nepali accent or that sounds like Nepali. For the further\nadvancement of TTS, the idea of transfer learning was effectively used to\naddress several issues that were encountered in the development of this system,\nincluding the poor audio quality and the lack of available data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Voice cloning is a prominent feature in personalized speech interfaces. A\nneural vocal cloning system can mimic someone's voice using just a few audio\nsamples. Both speaker encoding and speaker adaptation are topics of research in\nthe field of voice cloning. Speaker adaptation relies on fine-tuning a\nmulti-speaker generative model, which involves training a separate model to\ninfer a new speaker embedding used for speaker encoding. Both methods can\nachieve excellent performance, even with a small number of cloning audios, in\nterms of the speech's naturalness and similarity to the original speaker.\nSpeaker encoding approaches are more appropriate for low-resource deployment\nsince they require significantly less memory and have a faster cloning time\nthan speaker adaption, which can offer slightly greater naturalness and\nsimilarity. The main goal is to create a vocal cloning system that produces\naudio output with a Nepali accent or that sounds like Nepali. For the further\nadvancement of TTS, the idea of transfer learning was effectively used to\naddress several issues that were encountered in the development of this system,\nincluding the poor audio quality and the lack of available data."
                },
                "authors": [
                    {
                        "name": "Manjil Karki"
                    },
                    {
                        "name": "Pratik Shakya"
                    },
                    {
                        "name": "Sandesh Acharya"
                    },
                    {
                        "name": "Ravi Pandit"
                    },
                    {
                        "name": "Dinesh Gothe"
                    }
                ],
                "author_detail": {
                    "name": "Dinesh Gothe"
                },
                "author": "Dinesh Gothe",
                "arxiv_comment": "6 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10128v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10128v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "91F20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19094v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19094v2",
                "updated": "2024-08-23T16:06:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    16,
                    6,
                    27,
                    4,
                    236,
                    0
                ],
                "published": "2024-07-26T21:18:57Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    21,
                    18,
                    57,
                    4,
                    208,
                    0
                ],
                "title": "Solving Robotics Problems in Zero-Shot with Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving Robotics Problems in Zero-Shot with Vision-Language Models"
                },
                "summary": "We introduce Wonderful Team, a multi-agent visual LLM (VLLM) framework for\nsolving robotics problems in the zero-shot regime. By zero-shot we mean that,\nfor a novel environment, we feed a VLLM an image of the robot's environment and\na description of the task, and have the VLLM output the sequence of actions\nnecessary for the robot to complete the task. Prior work on VLLMs in robotics\nhas largely focused on settings where some part of the pipeline is fine-tuned,\nsuch as tuning an LLM on robot data or training a separate vision encoder for\nperception and action generation. Surprisingly, due to recent advances in the\ncapabilities of VLLMs, this type of fine-tuning may no longer be necessary for\nmany tasks. In this work, we show that with careful engineering, we can prompt\na single off-the-shelf VLLM to handle all aspects of a robotics task, from\nhigh-level planning to low-level location-extraction and action-execution.\nWonderful Team builds on recent advances in multi-agent LLMs to partition tasks\nacross an agent hierarchy, making it self-corrective and able to effectively\npartition and solve even long-horizon tasks. Extensive experiments on VIMABench\nand real-world robotic environments demonstrate the system's capability to\nhandle a variety of robotic tasks, including manipulation, visual\ngoal-reaching, and visual reasoning, all in a zero-shot manner. These results\nunderscore a key point: vision-language models have progressed rapidly in the\npast year, and should strongly be considered as a backbone for robotics\nproblems going forward.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Wonderful Team, a multi-agent visual LLM (VLLM) framework for\nsolving robotics problems in the zero-shot regime. By zero-shot we mean that,\nfor a novel environment, we feed a VLLM an image of the robot's environment and\na description of the task, and have the VLLM output the sequence of actions\nnecessary for the robot to complete the task. Prior work on VLLMs in robotics\nhas largely focused on settings where some part of the pipeline is fine-tuned,\nsuch as tuning an LLM on robot data or training a separate vision encoder for\nperception and action generation. Surprisingly, due to recent advances in the\ncapabilities of VLLMs, this type of fine-tuning may no longer be necessary for\nmany tasks. In this work, we show that with careful engineering, we can prompt\na single off-the-shelf VLLM to handle all aspects of a robotics task, from\nhigh-level planning to low-level location-extraction and action-execution.\nWonderful Team builds on recent advances in multi-agent LLMs to partition tasks\nacross an agent hierarchy, making it self-corrective and able to effectively\npartition and solve even long-horizon tasks. Extensive experiments on VIMABench\nand real-world robotic environments demonstrate the system's capability to\nhandle a variety of robotic tasks, including manipulation, visual\ngoal-reaching, and visual reasoning, all in a zero-shot manner. These results\nunderscore a key point: vision-language models have progressed rapidly in the\npast year, and should strongly be considered as a backbone for robotics\nproblems going forward."
                },
                "authors": [
                    {
                        "name": "Zidan Wang"
                    },
                    {
                        "name": "Rui Shen"
                    },
                    {
                        "name": "Bradly Stadie"
                    }
                ],
                "author_detail": {
                    "name": "Bradly Stadie"
                },
                "author": "Bradly Stadie",
                "arxiv_comment": "aka Wonderful Team",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19094v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19094v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13184v1",
                "updated": "2024-08-23T16:02:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    16,
                    2,
                    54,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T16:02:54Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    16,
                    2,
                    54,
                    4,
                    236,
                    0
                ],
                "title": "Can LLM be a Good Path Planner based on Prompt Engineering? Mitigating\n  the Hallucination for Path Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLM be a Good Path Planner based on Prompt Engineering? Mitigating\n  the Hallucination for Path Planning"
                },
                "summary": "Spatial reasoning in Large Language Models (LLMs) is the foundation for\nembodied intelligence. However, even in simple maze environments, LLMs still\nencounter challenges in long-term path-planning, primarily influenced by their\nspatial hallucination and context inconsistency hallucination by long-term\nreasoning. To address this challenge, this study proposes an innovative model,\nSpatial-to-Relational Transformation and Curriculum Q-Learning (S2RCQL). To\naddress the spatial hallucination of LLMs, we propose the Spatial-to-Relational\napproach, which transforms spatial prompts into entity relations and paths\nrepresenting entity relation chains. This approach fully taps the potential of\nLLMs in terms of sequential thinking. As a result, we design a path-planning\nalgorithm based on Q-learning to mitigate the context inconsistency\nhallucination, which enhances the reasoning ability of LLMs. Using the Q-value\nof state-action as auxiliary information for prompts, we correct the\nhallucinations of LLMs, thereby guiding LLMs to learn the optimal path.\nFinally, we propose a reverse curriculum learning technique based on LLMs to\nfurther mitigate the context inconsistency hallucination. LLMs can rapidly\naccumulate successful experiences by reducing task difficulty and leveraging\nthem to tackle more complex tasks. We performed comprehensive experiments based\non Baidu's self-developed LLM: ERNIE-Bot 4.0. The results showed that our\nS2RCQL achieved a 23%--40% improvement in both success and optimality rates\ncompared with advanced prompt engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial reasoning in Large Language Models (LLMs) is the foundation for\nembodied intelligence. However, even in simple maze environments, LLMs still\nencounter challenges in long-term path-planning, primarily influenced by their\nspatial hallucination and context inconsistency hallucination by long-term\nreasoning. To address this challenge, this study proposes an innovative model,\nSpatial-to-Relational Transformation and Curriculum Q-Learning (S2RCQL). To\naddress the spatial hallucination of LLMs, we propose the Spatial-to-Relational\napproach, which transforms spatial prompts into entity relations and paths\nrepresenting entity relation chains. This approach fully taps the potential of\nLLMs in terms of sequential thinking. As a result, we design a path-planning\nalgorithm based on Q-learning to mitigate the context inconsistency\nhallucination, which enhances the reasoning ability of LLMs. Using the Q-value\nof state-action as auxiliary information for prompts, we correct the\nhallucinations of LLMs, thereby guiding LLMs to learn the optimal path.\nFinally, we propose a reverse curriculum learning technique based on LLMs to\nfurther mitigate the context inconsistency hallucination. LLMs can rapidly\naccumulate successful experiences by reducing task difficulty and leveraging\nthem to tackle more complex tasks. We performed comprehensive experiments based\non Baidu's self-developed LLM: ERNIE-Bot 4.0. The results showed that our\nS2RCQL achieved a 23%--40% improvement in both success and optimality rates\ncompared with advanced prompt engineering."
                },
                "authors": [
                    {
                        "name": "Hourui Deng"
                    },
                    {
                        "name": "Hongjie Zhang"
                    },
                    {
                        "name": "Jie Ou"
                    },
                    {
                        "name": "Chaosheng Feng"
                    }
                ],
                "author_detail": {
                    "name": "Chaosheng Feng"
                },
                "author": "Chaosheng Feng",
                "arxiv_comment": "Submitted to ICASSP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13172v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13172v1",
                "updated": "2024-08-23T15:43:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    15,
                    43,
                    51,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T15:43:51Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    15,
                    43,
                    51,
                    4,
                    236,
                    0
                ],
                "title": "Towards Weaknesses and Attack Patterns Prediction for IoT Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Weaknesses and Attack Patterns Prediction for IoT Devices"
                },
                "summary": "As the adoption of Internet of Things (IoT) devices continues to rise in\nenterprise environments, the need for effective and efficient security measures\nbecomes increasingly critical. This paper presents a cost-efficient platform to\nfacilitate the pre-deployment security checks of IoT devices by predicting\npotential weaknesses and associated attack patterns. The platform employs a\nBidirectional Long Short-Term Memory (Bi-LSTM) network to analyse\ndevice-related textual data and predict weaknesses. At the same time, a\nGradient Boosting Machine (GBM) model predicts likely attack patterns that\ncould exploit these weaknesses. When evaluated on a dataset curated from the\nNational Vulnerability Database (NVD) and publicly accessible IoT data sources,\nthe system demonstrates high accuracy and reliability. The dataset created for\nthis solution is publicly accessible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the adoption of Internet of Things (IoT) devices continues to rise in\nenterprise environments, the need for effective and efficient security measures\nbecomes increasingly critical. This paper presents a cost-efficient platform to\nfacilitate the pre-deployment security checks of IoT devices by predicting\npotential weaknesses and associated attack patterns. The platform employs a\nBidirectional Long Short-Term Memory (Bi-LSTM) network to analyse\ndevice-related textual data and predict weaknesses. At the same time, a\nGradient Boosting Machine (GBM) model predicts likely attack patterns that\ncould exploit these weaknesses. When evaluated on a dataset curated from the\nNational Vulnerability Database (NVD) and publicly accessible IoT data sources,\nthe system demonstrates high accuracy and reliability. The dataset created for\nthis solution is publicly accessible."
                },
                "authors": [
                    {
                        "name": "Carlos A. Rivera A."
                    },
                    {
                        "name": "Arash Shaghaghi"
                    },
                    {
                        "name": "Gustavo Batista"
                    },
                    {
                        "name": "Salil S. Kanhere"
                    }
                ],
                "author_detail": {
                    "name": "Salil S. Kanhere"
                },
                "author": "Salil S. Kanhere",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13172v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13172v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04180v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04180v2",
                "updated": "2024-08-23T15:29:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    15,
                    29,
                    5,
                    4,
                    236,
                    0
                ],
                "published": "2023-12-07T10:06:34Z",
                "published_parsed": [
                    2023,
                    12,
                    7,
                    10,
                    6,
                    34,
                    3,
                    341,
                    0
                ],
                "title": "AI and Jobs: Has the Inflection Point Arrived? Evidence from an Online\n  Labor Platform",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI and Jobs: Has the Inflection Point Arrived? Evidence from an Online\n  Labor Platform"
                },
                "summary": "The emergence of Large Language Models (LLMs) has renewed the debate on the\nimportant issue of \"technology displacement\". While prior research has\ninvestigated the effect of information technology in general on human labor\nfrom a macro perspective, this paper complements the literature by examining\nthe impact of LLMs on freelancers from a micro perspective. Specifically, we\nleverage the release of ChatGPT to investigate how AI influences freelancers\nacross different online labor markets (OLMs). Employing the\nDifference-in-Differences method, we discovered two distinct scenarios\nfollowing ChatGPT's release: 1) the displacement effect of LLMs, featuring\nreduced work volume and earnings, as is exemplified by the translation &\nlocalization OLM; 2) the productivity effect of LLMs, featuring increased work\nvolume and earnings, as is exemplified by the web development OLM. To shed\nlight on the underlying mechanisms, we developed a Cournot-type competition\nmodel to highlight the existence of an inflection point for each occupation\nwhich separates the timeline of AI progress into a honeymoon phase and a\nsubstitution phase. Before AI performance crosses the inflection point, human\nlabor benefits each time AI improves, resulting in the honeymoon phase.\nHowever, after AI performance crosses the inflection point, additional AI\nenhancement hurts human labor. Further analyzing the progression from ChatGPT\n3.5 to 4.0, we found three effect scenarios (i.e., productivity to\nproductivity, displacement to displacement, and productivity to displacement),\nconsistent with the inflection point conjecture. Heterogeneous analyses reveal\nthat U.S. web developers tend to benefit more from the release of ChatGPT\ncompared to their counterparts in other regions, and somewhat surprisingly,\nexperienced translators seem more likely to exit the market than less\nexperienced translators after the release of ChatGPT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of Large Language Models (LLMs) has renewed the debate on the\nimportant issue of \"technology displacement\". While prior research has\ninvestigated the effect of information technology in general on human labor\nfrom a macro perspective, this paper complements the literature by examining\nthe impact of LLMs on freelancers from a micro perspective. Specifically, we\nleverage the release of ChatGPT to investigate how AI influences freelancers\nacross different online labor markets (OLMs). Employing the\nDifference-in-Differences method, we discovered two distinct scenarios\nfollowing ChatGPT's release: 1) the displacement effect of LLMs, featuring\nreduced work volume and earnings, as is exemplified by the translation &\nlocalization OLM; 2) the productivity effect of LLMs, featuring increased work\nvolume and earnings, as is exemplified by the web development OLM. To shed\nlight on the underlying mechanisms, we developed a Cournot-type competition\nmodel to highlight the existence of an inflection point for each occupation\nwhich separates the timeline of AI progress into a honeymoon phase and a\nsubstitution phase. Before AI performance crosses the inflection point, human\nlabor benefits each time AI improves, resulting in the honeymoon phase.\nHowever, after AI performance crosses the inflection point, additional AI\nenhancement hurts human labor. Further analyzing the progression from ChatGPT\n3.5 to 4.0, we found three effect scenarios (i.e., productivity to\nproductivity, displacement to displacement, and productivity to displacement),\nconsistent with the inflection point conjecture. Heterogeneous analyses reveal\nthat U.S. web developers tend to benefit more from the release of ChatGPT\ncompared to their counterparts in other regions, and somewhat surprisingly,\nexperienced translators seem more likely to exit the market than less\nexperienced translators after the release of ChatGPT."
                },
                "authors": [
                    {
                        "name": "Dandan Qiao"
                    },
                    {
                        "name": "Huaxia Rui"
                    },
                    {
                        "name": "Qian Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Qian Xiong"
                },
                "author": "Qian Xiong",
                "arxiv_comment": "41 pages, 3 figures, 13 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.04180v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04180v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14962v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14962v5",
                "updated": "2024-08-23T14:14:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    14,
                    14,
                    21,
                    4,
                    236,
                    0
                ],
                "published": "2024-07-20T18:48:35Z",
                "published_parsed": [
                    2024,
                    7,
                    20,
                    18,
                    48,
                    35,
                    5,
                    202,
                    0
                ],
                "title": "Recent Advances in Generative AI and Large Language Models: Current\n  Status, Challenges, and Perspectives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Advances in Generative AI and Large Language Models: Current\n  Status, Challenges, and Perspectives"
                },
                "summary": "The emergence of Generative Artificial Intelligence (AI) and Large Language\nModels (LLMs) has marked a new era of Natural Language Processing (NLP),\nintroducing unprecedented capabilities that are revolutionizing various\ndomains. This paper explores the current state of these cutting-edge\ntechnologies, demonstrating their remarkable advancements and wide-ranging\napplications. Our paper contributes to providing a holistic perspective on the\ntechnical foundations, practical applications, and emerging challenges within\nthe evolving landscape of Generative AI and LLMs. We believe that understanding\nthe generative capabilities of AI systems and the specific context of LLMs is\ncrucial for researchers, practitioners, and policymakers to collaboratively\nshape the responsible and ethical integration of these technologies into\nvarious domains. Furthermore, we identify and address main research gaps,\nproviding valuable insights to guide future research endeavors within the AI\nresearch community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of Generative Artificial Intelligence (AI) and Large Language\nModels (LLMs) has marked a new era of Natural Language Processing (NLP),\nintroducing unprecedented capabilities that are revolutionizing various\ndomains. This paper explores the current state of these cutting-edge\ntechnologies, demonstrating their remarkable advancements and wide-ranging\napplications. Our paper contributes to providing a holistic perspective on the\ntechnical foundations, practical applications, and emerging challenges within\nthe evolving landscape of Generative AI and LLMs. We believe that understanding\nthe generative capabilities of AI systems and the specific context of LLMs is\ncrucial for researchers, practitioners, and policymakers to collaboratively\nshape the responsible and ethical integration of these technologies into\nvarious domains. Furthermore, we identify and address main research gaps,\nproviding valuable insights to guide future research endeavors within the AI\nresearch community."
                },
                "authors": [
                    {
                        "name": "Desta Haileselassie Hagos"
                    },
                    {
                        "name": "Rick Battle"
                    },
                    {
                        "name": "Danda B. Rawat"
                    }
                ],
                "author_detail": {
                    "name": "Danda B. Rawat"
                },
                "author": "Danda B. Rawat",
                "arxiv_comment": "This version is accepted for publication in the Journal of IEEE\n  Transactions on Artificial Intelligence (TAI)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14962v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14962v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.09105v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.09105v5",
                "updated": "2024-08-23T14:11:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    14,
                    11,
                    5,
                    4,
                    236,
                    0
                ],
                "published": "2024-07-12T09:10:37Z",
                "published_parsed": [
                    2024,
                    7,
                    12,
                    9,
                    10,
                    37,
                    4,
                    194,
                    0
                ],
                "title": "Enhancing Training Efficiency Using Packing with Flash Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Training Efficiency Using Packing with Flash Attention"
                },
                "summary": "Padding is often used in tuning LLM models by adding special tokens to\nshorter training examples to match the length of the longest sequence in each\nbatch. While this ensures uniformity for batch processing, it introduces\ninefficiencies by including irrelevant padding tokens in the computation and\nwastes GPU resources. Hugging Face SFT trainer has always offered the option to\nuse packing to combine multiple training examples, allowing for maximal\nutilization of GPU resources. However, up till now, it did not offer proper\nmasking of each packed training example. This capability has now been added to\nHugging Face Transformers 4.44. We analyse this new feature and show the\nbenefits across different variations of packing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Padding is often used in tuning LLM models by adding special tokens to\nshorter training examples to match the length of the longest sequence in each\nbatch. While this ensures uniformity for batch processing, it introduces\ninefficiencies by including irrelevant padding tokens in the computation and\nwastes GPU resources. Hugging Face SFT trainer has always offered the option to\nuse packing to combine multiple training examples, allowing for maximal\nutilization of GPU resources. However, up till now, it did not offer proper\nmasking of each packed training example. This capability has now been added to\nHugging Face Transformers 4.44. We analyse this new feature and show the\nbenefits across different variations of packing."
                },
                "authors": [
                    {
                        "name": "Achintya Kundu"
                    },
                    {
                        "name": "Rhui Dih Lee"
                    },
                    {
                        "name": "Laura Wynter"
                    },
                    {
                        "name": "Raghu Kiran Ganti"
                    },
                    {
                        "name": "Mayank Mishra"
                    }
                ],
                "author_detail": {
                    "name": "Mayank Mishra"
                },
                "author": "Mayank Mishra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.09105v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.09105v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13073v1",
                "updated": "2024-08-23T13:56:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    13,
                    56,
                    0,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T13:56:00Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    13,
                    56,
                    0,
                    4,
                    236,
                    0
                ],
                "title": "IntelliCare: Improving Healthcare Analysis with Variance-Controlled\n  Patient-Level Knowledge from Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IntelliCare: Improving Healthcare Analysis with Variance-Controlled\n  Patient-Level Knowledge from Large Language Models"
                },
                "summary": "While pioneering deep learning methods have made great strides in analyzing\nelectronic health record (EHR) data, they often struggle to fully capture the\nsemantics of diverse medical codes from limited data. The integration of\nexternal knowledge from Large Language Models (LLMs) presents a promising\navenue for improving healthcare predictions. However, LLM analyses may exhibit\nsignificant variance due to ambiguity problems and inconsistency issues,\nhindering their effective utilization. To address these challenges, we propose\nIntelliCare, a novel framework that leverages LLMs to provide high-quality\npatient-level external knowledge and enhance existing EHR models. Concretely,\nIntelliCare identifies patient cohorts and employs task-relevant statistical\ninformation to augment LLM understanding and generation, effectively mitigating\nthe ambiguity problem. Additionally, it refines LLM-derived knowledge through a\nhybrid approach, generating multiple analyses and calibrating them using both\nthe EHR model and perplexity measures. Experimental evaluations on three\nclinical prediction tasks across two large-scale EHR datasets demonstrate that\nIntelliCare delivers significant performance improvements to existing methods,\nhighlighting its potential in advancing personalized healthcare predictions and\ndecision support systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While pioneering deep learning methods have made great strides in analyzing\nelectronic health record (EHR) data, they often struggle to fully capture the\nsemantics of diverse medical codes from limited data. The integration of\nexternal knowledge from Large Language Models (LLMs) presents a promising\navenue for improving healthcare predictions. However, LLM analyses may exhibit\nsignificant variance due to ambiguity problems and inconsistency issues,\nhindering their effective utilization. To address these challenges, we propose\nIntelliCare, a novel framework that leverages LLMs to provide high-quality\npatient-level external knowledge and enhance existing EHR models. Concretely,\nIntelliCare identifies patient cohorts and employs task-relevant statistical\ninformation to augment LLM understanding and generation, effectively mitigating\nthe ambiguity problem. Additionally, it refines LLM-derived knowledge through a\nhybrid approach, generating multiple analyses and calibrating them using both\nthe EHR model and perplexity measures. Experimental evaluations on three\nclinical prediction tasks across two large-scale EHR datasets demonstrate that\nIntelliCare delivers significant performance improvements to existing methods,\nhighlighting its potential in advancing personalized healthcare predictions and\ndecision support systems."
                },
                "authors": [
                    {
                        "name": "Zhihao Yu"
                    },
                    {
                        "name": "Yujie Jin"
                    },
                    {
                        "name": "Yongxin Xu"
                    },
                    {
                        "name": "Xu Chu"
                    },
                    {
                        "name": "Yasha Wang"
                    },
                    {
                        "name": "Junfeng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Junfeng Zhao"
                },
                "author": "Junfeng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13071v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13071v1",
                "updated": "2024-08-23T13:55:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    13,
                    55,
                    36,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T13:55:36Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    13,
                    55,
                    36,
                    4,
                    236,
                    0
                ],
                "title": "Guiding IoT-Based Healthcare Alert Systems with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guiding IoT-Based Healthcare Alert Systems with Large Language Models"
                },
                "summary": "Healthcare alert systems (HAS) are undergoing rapid evolution, propelled by\nadvancements in artificial intelligence (AI), Internet of Things (IoT)\ntechnologies, and increasing health consciousness. Despite significant\nprogress, a fundamental challenge remains: balancing the accuracy of\npersonalized health alerts with stringent privacy protection in HAS\nenvironments constrained by resources. To address this issue, we introduce a\nuniform framework, LLM-HAS, which incorporates Large Language Models (LLM) into\nHAS to significantly boost the accuracy, ensure user privacy, and enhance\npersonalized health service, while also improving the subjective quality of\nexperience (QoE) for users. Our innovative framework leverages a Mixture of\nExperts (MoE) approach, augmented with LLM, to analyze users' personalized\npreferences and potential health risks from additional textual job\ndescriptions. This analysis guides the selection of specialized Deep\nReinforcement Learning (DDPG) experts, tasked with making precise health\nalerts. Moreover, LLM-HAS can process Conversational User Feedback, which not\nonly allows fine-tuning of DDPG but also deepen user engagement, thereby\nenhancing both the accuracy and personalization of health management\nstrategies. Simulation results validate the effectiveness of the LLM-HAS\nframework, highlighting its potential as a groundbreaking approach for\nemploying generative AI (GAI) to provide highly accurate and reliable alerts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Healthcare alert systems (HAS) are undergoing rapid evolution, propelled by\nadvancements in artificial intelligence (AI), Internet of Things (IoT)\ntechnologies, and increasing health consciousness. Despite significant\nprogress, a fundamental challenge remains: balancing the accuracy of\npersonalized health alerts with stringent privacy protection in HAS\nenvironments constrained by resources. To address this issue, we introduce a\nuniform framework, LLM-HAS, which incorporates Large Language Models (LLM) into\nHAS to significantly boost the accuracy, ensure user privacy, and enhance\npersonalized health service, while also improving the subjective quality of\nexperience (QoE) for users. Our innovative framework leverages a Mixture of\nExperts (MoE) approach, augmented with LLM, to analyze users' personalized\npreferences and potential health risks from additional textual job\ndescriptions. This analysis guides the selection of specialized Deep\nReinforcement Learning (DDPG) experts, tasked with making precise health\nalerts. Moreover, LLM-HAS can process Conversational User Feedback, which not\nonly allows fine-tuning of DDPG but also deepen user engagement, thereby\nenhancing both the accuracy and personalization of health management\nstrategies. Simulation results validate the effectiveness of the LLM-HAS\nframework, highlighting its potential as a groundbreaking approach for\nemploying generative AI (GAI) to provide highly accurate and reliable alerts."
                },
                "authors": [
                    {
                        "name": "Yulan Gao"
                    },
                    {
                        "name": "Ziqiang Ye"
                    },
                    {
                        "name": "Ming Xiao"
                    },
                    {
                        "name": "Yue Xiao"
                    },
                    {
                        "name": "Dong In Kim"
                    }
                ],
                "author_detail": {
                    "name": "Dong In Kim"
                },
                "author": "Dong In Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13071v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13071v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13028v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13028v1",
                "updated": "2024-08-23T12:32:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    12,
                    32,
                    12,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T12:32:12Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    12,
                    32,
                    12,
                    4,
                    236,
                    0
                ],
                "title": "In-Context Learning with Reinforcement Learning for Incomplete Utterance\n  Rewriting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Learning with Reinforcement Learning for Incomplete Utterance\n  Rewriting"
                },
                "summary": "In-context learning (ICL) of large language models (LLMs) has attracted\nincreasing attention in the community where LLMs make predictions only based on\ninstructions augmented with a few examples. Existing example selection methods\nfor ICL utilize sparse or dense retrievers and derive effective performance.\nHowever, these methods do not utilize direct feedback of LLM to train the\nretriever and the examples selected can not necessarily improve the analogy\nability of LLM. To tackle this, we propose our policy-based reinforcement\nlearning framework for example selection (RLS), which consists of a language\nmodel (LM) selector and an LLM generator. The LM selector encodes the candidate\nexamples into dense representations and selects the top-k examples into the\ndemonstration for LLM. The outputs of LLM are adopted to compute the reward and\npolicy gradient to optimize the LM selector. We conduct experiments on\ndifferent datasets and significantly outperform existing example selection\nmethods. Moreover, our approach shows advantages over supervised finetuning\n(SFT) models in few shot setting. Further experiments show the balance of\nabundance and the similarity with the test case of examples is important for\nICL performance of LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) of large language models (LLMs) has attracted\nincreasing attention in the community where LLMs make predictions only based on\ninstructions augmented with a few examples. Existing example selection methods\nfor ICL utilize sparse or dense retrievers and derive effective performance.\nHowever, these methods do not utilize direct feedback of LLM to train the\nretriever and the examples selected can not necessarily improve the analogy\nability of LLM. To tackle this, we propose our policy-based reinforcement\nlearning framework for example selection (RLS), which consists of a language\nmodel (LM) selector and an LLM generator. The LM selector encodes the candidate\nexamples into dense representations and selects the top-k examples into the\ndemonstration for LLM. The outputs of LLM are adopted to compute the reward and\npolicy gradient to optimize the LM selector. We conduct experiments on\ndifferent datasets and significantly outperform existing example selection\nmethods. Moreover, our approach shows advantages over supervised finetuning\n(SFT) models in few shot setting. Further experiments show the balance of\nabundance and the similarity with the test case of examples is important for\nICL performance of LLM."
                },
                "authors": [
                    {
                        "name": "Haowei Du"
                    },
                    {
                        "name": "Dongyan Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongyan Zhao"
                },
                "author": "Dongyan Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13028v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13028v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02392v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02392v3",
                "updated": "2024-08-23T12:27:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    12,
                    27,
                    6,
                    4,
                    236,
                    0
                ],
                "published": "2024-07-02T16:10:55Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    16,
                    10,
                    55,
                    1,
                    184,
                    0
                ],
                "title": "TokenPacker: Efficient Visual Projector for Multimodal LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenPacker: Efficient Visual Projector for Multimodal LLM"
                },
                "summary": "The visual projector serves as an essential bridge between the visual encoder\nand the Large Language Model (LLM) in a Multimodal LLM (MLLM). Typically, MLLMs\nadopt a simple MLP to preserve all visual contexts via one-to-one\ntransformation. However, the visual tokens are redundant and can be\nconsiderably increased when dealing with high-resolution images, impairing the\nefficiency of MLLMs significantly. Some recent works have introduced resampler\nor abstractor to reduce the number of resulting visual tokens. Unfortunately,\nthey fail to capture finer details and undermine the visual reasoning\ncapabilities of MLLMs. In this work, we propose a novel visual projector, which\nadopts a coarse-to-fine scheme to inject the enriched characteristics to\ngenerate the condensed visual tokens. In specific, we first interpolate the\nvisual features as a low-resolution point query, providing the overall visual\nrepresentation as the foundation. Then, we introduce a region-to-point\ninjection module that utilizes high-resolution, multi-level region-based cues\nas fine-grained reference keys and values, allowing them to be fully absorbed\nwithin the corresponding local context region. This step effectively updates\nthe coarse point query, transforming it into an enriched one for the subsequent\nLLM reasoning. Extensive experiments demonstrate that our approach compresses\nthe visual tokens by 75%~89%, while achieves comparable or even better\nperformance across diverse benchmarks with significantly higher efficiency. The\nsource codes can be found at https://github.com/CircleRadon/TokenPacker.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The visual projector serves as an essential bridge between the visual encoder\nand the Large Language Model (LLM) in a Multimodal LLM (MLLM). Typically, MLLMs\nadopt a simple MLP to preserve all visual contexts via one-to-one\ntransformation. However, the visual tokens are redundant and can be\nconsiderably increased when dealing with high-resolution images, impairing the\nefficiency of MLLMs significantly. Some recent works have introduced resampler\nor abstractor to reduce the number of resulting visual tokens. Unfortunately,\nthey fail to capture finer details and undermine the visual reasoning\ncapabilities of MLLMs. In this work, we propose a novel visual projector, which\nadopts a coarse-to-fine scheme to inject the enriched characteristics to\ngenerate the condensed visual tokens. In specific, we first interpolate the\nvisual features as a low-resolution point query, providing the overall visual\nrepresentation as the foundation. Then, we introduce a region-to-point\ninjection module that utilizes high-resolution, multi-level region-based cues\nas fine-grained reference keys and values, allowing them to be fully absorbed\nwithin the corresponding local context region. This step effectively updates\nthe coarse point query, transforming it into an enriched one for the subsequent\nLLM reasoning. Extensive experiments demonstrate that our approach compresses\nthe visual tokens by 75%~89%, while achieves comparable or even better\nperformance across diverse benchmarks with significantly higher efficiency. The\nsource codes can be found at https://github.com/CircleRadon/TokenPacker."
                },
                "authors": [
                    {
                        "name": "Wentong Li"
                    },
                    {
                        "name": "Yuqian Yuan"
                    },
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "Dongqi Tang"
                    },
                    {
                        "name": "Song Wang"
                    },
                    {
                        "name": "Jie Qin"
                    },
                    {
                        "name": "Jianke Zhu"
                    },
                    {
                        "name": "Lei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zhang"
                },
                "author": "Lei Zhang",
                "arxiv_comment": "16 pages, Codes:https://github.com/CircleRadon/TokenPacker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02392v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02392v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09895v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09895v2",
                "updated": "2024-08-23T12:14:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    12,
                    14,
                    18,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-19T11:09:12Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    11,
                    9,
                    12,
                    0,
                    232,
                    0
                ],
                "title": "Performance Law of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Law of Large Language Models"
                },
                "summary": "Guided by the belief of the scaling law, large language models (LLMs) have\nachieved impressive performance in recent years. However, scaling law only\ngives a qualitative estimation of loss, which is influenced by various factors\nsuch as model architectures, data distributions, tokenizers, and computation\nprecision. Thus, estimating the real performance of LLMs with different\ntraining settings rather than loss may be quite useful in practical\ndevelopment. In this article, we present an empirical equation named\n\"Performance Law\" to directly predict the MMLU score of an LLM, which is a\nwidely used metric to indicate the general capability of LLMs in real-world\nconversations and applications. Based on only a few key hyperparameters of the\nLLM architecture and the size of training data, we obtain a quite accurate MMLU\nprediction of various LLMs with diverse sizes and architectures developed by\ndifferent organizations in different years. Performance law can be used to\nguide the choice of LLM architecture and the effective allocation of\ncomputational resources without extensive experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guided by the belief of the scaling law, large language models (LLMs) have\nachieved impressive performance in recent years. However, scaling law only\ngives a qualitative estimation of loss, which is influenced by various factors\nsuch as model architectures, data distributions, tokenizers, and computation\nprecision. Thus, estimating the real performance of LLMs with different\ntraining settings rather than loss may be quite useful in practical\ndevelopment. In this article, we present an empirical equation named\n\"Performance Law\" to directly predict the MMLU score of an LLM, which is a\nwidely used metric to indicate the general capability of LLMs in real-world\nconversations and applications. Based on only a few key hyperparameters of the\nLLM architecture and the size of training data, we obtain a quite accurate MMLU\nprediction of various LLMs with diverse sizes and architectures developed by\ndifferent organizations in different years. Performance law can be used to\nguide the choice of LLM architecture and the effective allocation of\ncomputational resources without extensive experiments."
                },
                "authors": [
                    {
                        "name": "Chuhan Wu"
                    },
                    {
                        "name": "Ruiming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Ruiming Tang"
                },
                "author": "Ruiming Tang",
                "arxiv_comment": "Personal opinions of the authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09895v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09895v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13010v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13010v1",
                "updated": "2024-08-23T11:57:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    11,
                    57,
                    2,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T11:57:02Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    11,
                    57,
                    2,
                    4,
                    236,
                    0
                ],
                "title": "A Web-Based Solution for Federated Learning with LLM-Based Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Web-Based Solution for Federated Learning with LLM-Based Automation"
                },
                "summary": "Federated Learning (FL) offers a promising approach for collaborative machine\nlearning across distributed devices. However, its adoption is hindered by the\ncomplexity of building reliable communication architectures and the need for\nexpertise in both machine learning and network programming. This paper presents\na comprehensive solution that simplifies the orchestration of FL tasks while\nintegrating intent-based automation. We develop a user-friendly web application\nsupporting the federated averaging (FedAvg) algorithm, enabling users to\nconfigure parameters through an intuitive interface. The backend solution\nefficiently manages communication between the parameter server and edge nodes.\nWe also implement model compression and scheduling algorithms to optimize FL\nperformance. Furthermore, we explore intent-based automation in FL using a\nfine-tuned Language Model (LLM) trained on a tailored dataset, allowing users\nto conduct FL tasks using high-level prompts. We observe that the LLM-based\nautomated solution achieves comparable test accuracy to the standard web-based\nsolution while reducing transferred bytes by up to 64% and CPU time by up to\n46% for FL tasks. Also, we leverage the neural architecture search (NAS) and\nhyperparameter optimization (HPO) using LLM to improve the performance. We\nobserve that by using this approach test accuracy can be improved by 10-20% for\nthe carried out FL tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) offers a promising approach for collaborative machine\nlearning across distributed devices. However, its adoption is hindered by the\ncomplexity of building reliable communication architectures and the need for\nexpertise in both machine learning and network programming. This paper presents\na comprehensive solution that simplifies the orchestration of FL tasks while\nintegrating intent-based automation. We develop a user-friendly web application\nsupporting the federated averaging (FedAvg) algorithm, enabling users to\nconfigure parameters through an intuitive interface. The backend solution\nefficiently manages communication between the parameter server and edge nodes.\nWe also implement model compression and scheduling algorithms to optimize FL\nperformance. Furthermore, we explore intent-based automation in FL using a\nfine-tuned Language Model (LLM) trained on a tailored dataset, allowing users\nto conduct FL tasks using high-level prompts. We observe that the LLM-based\nautomated solution achieves comparable test accuracy to the standard web-based\nsolution while reducing transferred bytes by up to 64% and CPU time by up to\n46% for FL tasks. Also, we leverage the neural architecture search (NAS) and\nhyperparameter optimization (HPO) using LLM to improve the performance. We\nobserve that by using this approach test accuracy can be improved by 10-20% for\nthe carried out FL tasks."
                },
                "authors": [
                    {
                        "name": "Chamith Mawela"
                    },
                    {
                        "name": "Chaouki Ben Issaid"
                    },
                    {
                        "name": "Mehdi Bennis"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Bennis"
                },
                "author": "Mehdi Bennis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13010v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13010v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13006v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13006v1",
                "updated": "2024-08-23T11:49:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    11,
                    49,
                    1,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T11:49:01Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    11,
                    49,
                    1,
                    4,
                    236,
                    0
                ],
                "title": "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks:\n  Explainable Metrics and Diverse Prompt Templates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks:\n  Explainable Metrics and Diverse Prompt Templates"
                },
                "summary": "Alignment approaches such as RLHF and DPO are actively investigated to align\nlarge language models (LLMs) with human preferences. Commercial large language\nmodels (LLMs) like GPT-4 have been recently employed to evaluate and compare\ndifferent LLM alignment approaches. These models act as surrogates for human\nevaluators due to their promising abilities to approximate human preferences\nwith remarkably faster feedback and lower costs. This methodology is referred\nto as LLM-as-a-judge. However, concerns regarding its reliability have emerged,\nattributed to LLM judges' biases and inconsistent decision-making. Previous\nresearch has sought to develop robust evaluation frameworks for assessing the\nreliability of LLM judges and their alignment with human preferences. However,\nthe employed evaluation metrics often lack adequate explainability and fail to\naddress the internal inconsistency of LLMs. Additionally, existing studies\ninadequately explore the impact of various prompt templates when applying\nLLM-as-a-judge methods, which leads to potentially inconsistent comparisons\nbetween different alignment algorithms. In this work, we systematically\nevaluate LLM judges on alignment tasks (e.g. summarization) by defining\nevaluation metrics with improved theoretical interpretability and disentangling\nreliability metrics with LLM internal inconsistency. We develop a framework to\nevaluate, compare, and visualize the reliability and alignment of LLM judges to\nprovide informative observations that help choose LLM judges for alignment\ntasks. Our results indicate a significant impact of prompt templates on LLM\njudge performance, as well as a mediocre alignment level between the tested LLM\njudges and human evaluators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alignment approaches such as RLHF and DPO are actively investigated to align\nlarge language models (LLMs) with human preferences. Commercial large language\nmodels (LLMs) like GPT-4 have been recently employed to evaluate and compare\ndifferent LLM alignment approaches. These models act as surrogates for human\nevaluators due to their promising abilities to approximate human preferences\nwith remarkably faster feedback and lower costs. This methodology is referred\nto as LLM-as-a-judge. However, concerns regarding its reliability have emerged,\nattributed to LLM judges' biases and inconsistent decision-making. Previous\nresearch has sought to develop robust evaluation frameworks for assessing the\nreliability of LLM judges and their alignment with human preferences. However,\nthe employed evaluation metrics often lack adequate explainability and fail to\naddress the internal inconsistency of LLMs. Additionally, existing studies\ninadequately explore the impact of various prompt templates when applying\nLLM-as-a-judge methods, which leads to potentially inconsistent comparisons\nbetween different alignment algorithms. In this work, we systematically\nevaluate LLM judges on alignment tasks (e.g. summarization) by defining\nevaluation metrics with improved theoretical interpretability and disentangling\nreliability metrics with LLM internal inconsistency. We develop a framework to\nevaluate, compare, and visualize the reliability and alignment of LLM judges to\nprovide informative observations that help choose LLM judges for alignment\ntasks. Our results indicate a significant impact of prompt templates on LLM\njudge performance, as well as a mediocre alignment level between the tested LLM\njudges and human evaluators."
                },
                "authors": [
                    {
                        "name": "Hui Wei"
                    },
                    {
                        "name": "Shenghua He"
                    },
                    {
                        "name": "Tian Xia"
                    },
                    {
                        "name": "Andy Wong"
                    },
                    {
                        "name": "Jingyang Lin"
                    },
                    {
                        "name": "Mei Han"
                    }
                ],
                "author_detail": {
                    "name": "Mei Han"
                },
                "author": "Mei Han",
                "arxiv_comment": "Preprint, under review. 17 pages, 7 figures, 16 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13006v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13006v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13001v1",
                "updated": "2024-08-23T11:43:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    11,
                    43,
                    0,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T11:43:00Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    11,
                    43,
                    0,
                    4,
                    236,
                    0
                ],
                "title": "CRUXEval-X: A Benchmark for Multilingual Code Reasoning, Understanding\n  and Execution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CRUXEval-X: A Benchmark for Multilingual Code Reasoning, Understanding\n  and Execution"
                },
                "summary": "Code benchmarks such as HumanEval are widely adopted to evaluate Large\nLanguage Models' (LLMs) coding capabilities. However, there is an unignorable\nprogramming language bias in existing code benchmarks -- over 95% code\ngeneration benchmarks are dominated by Python, leaving the LLMs' capabilities\nin other programming languages such as Java and C/C++ unknown. Moreover, coding\ntask bias is also crucial. Most benchmarks focus on code generation capability,\nwhile benchmarks for code reasoning (given input, reasoning output; and given\noutput, reasoning input), an essential coding capability, are insufficient.\nYet, constructing multi-lingual benchmarks can be expensive and\nlabor-intensive, and codes in contest websites such as Leetcode suffer from\ndata contamination during training. To fill this gap, we propose CRUXEVAL-X, a\nmulti-lingual code reasoning benchmark that contains 19 programming languages.\nIt comprises at least 600 subjects for each language, along with 19K\ncontent-consistent tests in total. In particular, the construction pipeline of\nCRUXEVAL-X works in a fully automated and test-guided manner, which iteratively\ngenerates and repairs based on execution feedback. Also, to cross language\nbarriers (e.g., dynamic/static type systems in Python/C++), we formulated\nvarious transition rules between language pairs to facilitate translation. Our\nintensive evaluation of 24 representative LLMs reveals the correlation between\nlanguage pairs. For example, TypeScript and JavaScript show a significant\npositive correlation, while Racket has less correlation with other languages.\nMore interestingly, even a model trained solely on Python can achieve at most\n34.4% Pass@1 in other languages, revealing the cross-language generalization of\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code benchmarks such as HumanEval are widely adopted to evaluate Large\nLanguage Models' (LLMs) coding capabilities. However, there is an unignorable\nprogramming language bias in existing code benchmarks -- over 95% code\ngeneration benchmarks are dominated by Python, leaving the LLMs' capabilities\nin other programming languages such as Java and C/C++ unknown. Moreover, coding\ntask bias is also crucial. Most benchmarks focus on code generation capability,\nwhile benchmarks for code reasoning (given input, reasoning output; and given\noutput, reasoning input), an essential coding capability, are insufficient.\nYet, constructing multi-lingual benchmarks can be expensive and\nlabor-intensive, and codes in contest websites such as Leetcode suffer from\ndata contamination during training. To fill this gap, we propose CRUXEVAL-X, a\nmulti-lingual code reasoning benchmark that contains 19 programming languages.\nIt comprises at least 600 subjects for each language, along with 19K\ncontent-consistent tests in total. In particular, the construction pipeline of\nCRUXEVAL-X works in a fully automated and test-guided manner, which iteratively\ngenerates and repairs based on execution feedback. Also, to cross language\nbarriers (e.g., dynamic/static type systems in Python/C++), we formulated\nvarious transition rules between language pairs to facilitate translation. Our\nintensive evaluation of 24 representative LLMs reveals the correlation between\nlanguage pairs. For example, TypeScript and JavaScript show a significant\npositive correlation, while Racket has less correlation with other languages.\nMore interestingly, even a model trained solely on Python can achieve at most\n34.4% Pass@1 in other languages, revealing the cross-language generalization of\nLLMs."
                },
                "authors": [
                    {
                        "name": "Ruiyang Xu"
                    },
                    {
                        "name": "Jialun Cao"
                    },
                    {
                        "name": "Yaojie Lu"
                    },
                    {
                        "name": "Hongyu Lin"
                    },
                    {
                        "name": "Xianpei Han"
                    },
                    {
                        "name": "Ben He"
                    },
                    {
                        "name": "Shing-Chi Cheung"
                    },
                    {
                        "name": "Le Sun"
                    }
                ],
                "author_detail": {
                    "name": "Le Sun"
                },
                "author": "Le Sun",
                "arxiv_comment": "13pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03249v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03249v2",
                "updated": "2024-08-23T11:02:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    11,
                    2,
                    49,
                    4,
                    236,
                    0
                ],
                "published": "2024-06-05T13:26:25Z",
                "published_parsed": [
                    2024,
                    6,
                    5,
                    13,
                    26,
                    25,
                    2,
                    157,
                    0
                ],
                "title": "Near-field Beam training for Extremely Large-scale MIMO Based on Deep\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Near-field Beam training for Extremely Large-scale MIMO Based on Deep\n  Learning"
                },
                "summary": "Extremely Large-scale Array (ELAA) is considered a frontier technology for\nfuture communication systems, pivotal in improving wireless systems' rate and\nspectral efficiency. As ELAA employs a multitude of antennas operating at\nhigher frequencies, users are typically situated in the near-field region where\nthe spherical wavefront propagates. The near-field beam training in ELAA\nrequires both angle and distance information, which inevitably leads to a\nsignificant increase in the beam training overhead. To address this problem, we\npropose a near-field beam training method based on deep learning. We use a\nconvolutional neural network (CNN) to efficiently learn channel characteristics\nfrom historical data by strategically selecting padding and kernel sizes. The\nnegative value of the user average achievable rate is utilized as the loss\nfunction to optimize the beamformer. This method maximizes multi-user networks'\nachievable rate without predefined beam codebooks. Upon deployment, the model\nrequires solely the pre-estimated channel state information (CSI) to derive the\noptimal beamforming vector. The simulation results demonstrate that the\nproposed scheme achieves a more stable beamforming gain and significantly\nimproves performance compared to the traditional beam training method.\nFurthermore, owing to the inherent traits of deep learning methodologies, this\napproach substantially diminishes the near-field beam training overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extremely Large-scale Array (ELAA) is considered a frontier technology for\nfuture communication systems, pivotal in improving wireless systems' rate and\nspectral efficiency. As ELAA employs a multitude of antennas operating at\nhigher frequencies, users are typically situated in the near-field region where\nthe spherical wavefront propagates. The near-field beam training in ELAA\nrequires both angle and distance information, which inevitably leads to a\nsignificant increase in the beam training overhead. To address this problem, we\npropose a near-field beam training method based on deep learning. We use a\nconvolutional neural network (CNN) to efficiently learn channel characteristics\nfrom historical data by strategically selecting padding and kernel sizes. The\nnegative value of the user average achievable rate is utilized as the loss\nfunction to optimize the beamformer. This method maximizes multi-user networks'\nachievable rate without predefined beam codebooks. Upon deployment, the model\nrequires solely the pre-estimated channel state information (CSI) to derive the\noptimal beamforming vector. The simulation results demonstrate that the\nproposed scheme achieves a more stable beamforming gain and significantly\nimproves performance compared to the traditional beam training method.\nFurthermore, owing to the inherent traits of deep learning methodologies, this\napproach substantially diminishes the near-field beam training overhead."
                },
                "authors": [
                    {
                        "name": "Jiali Nie"
                    },
                    {
                        "name": "Yuanhao Cui"
                    },
                    {
                        "name": "Zhaohui Yang"
                    },
                    {
                        "name": "Weijie Yuan"
                    },
                    {
                        "name": "Xiaojun Jing"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojun Jing"
                },
                "author": "Xiaojun Jing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03249v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03249v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12979v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12979v1",
                "updated": "2024-08-23T10:52:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    10,
                    52,
                    57,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T10:52:57Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    10,
                    52,
                    57,
                    4,
                    236,
                    0
                ],
                "title": "Internal and External Knowledge Interactive Refinement Framework for\n  Knowledge-Intensive Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Internal and External Knowledge Interactive Refinement Framework for\n  Knowledge-Intensive Question Answering"
                },
                "summary": "Recent works have attempted to integrate external knowledge into LLMs to\naddress the limitations and potential factual errors in LLM-generated content.\nHowever, how to retrieve the correct knowledge from the large amount of\nexternal knowledge imposes a challenge. To this end, we empirically observe\nthat LLMs have already encoded rich knowledge in their pretrained parameters\nand utilizing these internal knowledge improves the retrieval of external\nknowledge when applying them to knowledge-intensive tasks. In this paper, we\npropose a new internal and external knowledge interactive refinement paradigm\ndubbed IEKR to utilize internal knowledge in LLM to help retrieve relevant\nknowledge from the external knowledge base, as well as exploit the external\nknowledge to refine the hallucination of generated internal knowledge. By\nsimply adding a prompt like 'Tell me something about' to the LLMs, we try to\nreview related explicit knowledge and insert them with the query into the\nretriever for external retrieval. The external knowledge is utilized to\ncomplement the internal knowledge into input of LLM for answers. We conduct\nexperiments on 3 benchmark datasets in knowledge-intensive question answering\ntask with different LLMs and domains, achieving the new state-of-the-art.\nFurther analysis shows the effectiveness of different modules in our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent works have attempted to integrate external knowledge into LLMs to\naddress the limitations and potential factual errors in LLM-generated content.\nHowever, how to retrieve the correct knowledge from the large amount of\nexternal knowledge imposes a challenge. To this end, we empirically observe\nthat LLMs have already encoded rich knowledge in their pretrained parameters\nand utilizing these internal knowledge improves the retrieval of external\nknowledge when applying them to knowledge-intensive tasks. In this paper, we\npropose a new internal and external knowledge interactive refinement paradigm\ndubbed IEKR to utilize internal knowledge in LLM to help retrieve relevant\nknowledge from the external knowledge base, as well as exploit the external\nknowledge to refine the hallucination of generated internal knowledge. By\nsimply adding a prompt like 'Tell me something about' to the LLMs, we try to\nreview related explicit knowledge and insert them with the query into the\nretriever for external retrieval. The external knowledge is utilized to\ncomplement the internal knowledge into input of LLM for answers. We conduct\nexperiments on 3 benchmark datasets in knowledge-intensive question answering\ntask with different LLMs and domains, achieving the new state-of-the-art.\nFurther analysis shows the effectiveness of different modules in our approach."
                },
                "authors": [
                    {
                        "name": "Haowei Du"
                    },
                    {
                        "name": "Dongyan Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongyan Zhao"
                },
                "author": "Dongyan Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12979v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12979v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12978v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12978v1",
                "updated": "2024-08-23T10:50:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    10,
                    50,
                    29,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T10:50:29Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    10,
                    50,
                    29,
                    4,
                    236,
                    0
                ],
                "title": "Energy-Efficient Spiking Recurrent Neural Network for Gesture\n  Recognition on Embedded GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy-Efficient Spiking Recurrent Neural Network for Gesture\n  Recognition on Embedded GPUs"
                },
                "summary": "Implementing AI algorithms on event-based embedded devices enables real-time\nprocessing of data, minimizes latency, and enhances power efficiency in edge\ncomputing. This research explores the deployment of a spiking recurrent neural\nnetwork (SRNN) with liquid time constant neurons for gesture recognition. We\nfocus on the energy efficiency and computational efficacy of NVIDIA Jetson Nano\nembedded GPU platforms. The embedded GPU showcases a 14-fold increase in power\nefficiency relative to a conventional GPU, making a compelling argument for its\nuse in energy-constrained applications. The study's empirical findings also\nhighlight that batch processing significantly boosts frame rates across various\nbatch sizes while maintaining accuracy levels well above the baseline. These\ninsights validate the SRNN with liquid time constant neurons as a robust model\nfor interpreting temporal-spatial data in gesture recognition, striking a\ncritical balance between processing speed and power frugality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implementing AI algorithms on event-based embedded devices enables real-time\nprocessing of data, minimizes latency, and enhances power efficiency in edge\ncomputing. This research explores the deployment of a spiking recurrent neural\nnetwork (SRNN) with liquid time constant neurons for gesture recognition. We\nfocus on the energy efficiency and computational efficacy of NVIDIA Jetson Nano\nembedded GPU platforms. The embedded GPU showcases a 14-fold increase in power\nefficiency relative to a conventional GPU, making a compelling argument for its\nuse in energy-constrained applications. The study's empirical findings also\nhighlight that batch processing significantly boosts frame rates across various\nbatch sizes while maintaining accuracy levels well above the baseline. These\ninsights validate the SRNN with liquid time constant neurons as a robust model\nfor interpreting temporal-spatial data in gesture recognition, striking a\ncritical balance between processing speed and power frugality."
                },
                "authors": [
                    {
                        "name": "Marzieh Hassanshahi Varposhti"
                    },
                    {
                        "name": "Mahyar Shahsavari"
                    },
                    {
                        "name": "Marcel van Gerven"
                    }
                ],
                "author_detail": {
                    "name": "Marcel van Gerven"
                },
                "author": "Marcel van Gerven",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12978v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12978v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.09067v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.09067v2",
                "updated": "2024-08-23T10:42:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    10,
                    42,
                    45,
                    4,
                    236,
                    0
                ],
                "published": "2023-08-17T15:54:38Z",
                "published_parsed": [
                    2023,
                    8,
                    17,
                    15,
                    54,
                    38,
                    3,
                    229,
                    0
                ],
                "title": "Contrasting Linguistic Patterns in Human and LLM-Generated Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrasting Linguistic Patterns in Human and LLM-Generated Text"
                },
                "summary": "We conduct a quantitative analysis contrasting human-written English news\ntext with comparable large language model (LLM) output from six different LLMs\nthat cover three different families and four sizes in total. Our analysis spans\nseveral measurable linguistic dimensions, including morphological, syntactic,\npsychometric, and sociolinguistic aspects. The results reveal various\nmeasurable differences between human and AI-generated texts. Human texts\nexhibit more scattered sentence length distributions, more variety of\nvocabulary, a distinct use of dependency and constituent types, shorter\nconstituents, and more optimized dependency distances. Humans tend to exhibit\nstronger negative emotions (such as fear and disgust) and less joy compared to\ntext generated by LLMs, with the toxicity of these models increasing as their\nsize grows. LLM outputs use more numbers, symbols and auxiliaries (suggesting\nobjective language) than human texts, as well as more pronouns. The sexist bias\nprevalent in human text is also expressed by LLMs, and even magnified in all of\nthem but one. Differences between LLMs and humans are larger than between LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We conduct a quantitative analysis contrasting human-written English news\ntext with comparable large language model (LLM) output from six different LLMs\nthat cover three different families and four sizes in total. Our analysis spans\nseveral measurable linguistic dimensions, including morphological, syntactic,\npsychometric, and sociolinguistic aspects. The results reveal various\nmeasurable differences between human and AI-generated texts. Human texts\nexhibit more scattered sentence length distributions, more variety of\nvocabulary, a distinct use of dependency and constituent types, shorter\nconstituents, and more optimized dependency distances. Humans tend to exhibit\nstronger negative emotions (such as fear and disgust) and less joy compared to\ntext generated by LLMs, with the toxicity of these models increasing as their\nsize grows. LLM outputs use more numbers, symbols and auxiliaries (suggesting\nobjective language) than human texts, as well as more pronouns. The sexist bias\nprevalent in human text is also expressed by LLMs, and even magnified in all of\nthem but one. Differences between LLMs and humans are larger than between LLMs."
                },
                "authors": [
                    {
                        "name": "Alberto Mu√±oz-Ortiz"
                    },
                    {
                        "name": "Carlos G√≥mez-Rodr√≠guez"
                    },
                    {
                        "name": "David Vilares"
                    }
                ],
                "author_detail": {
                    "name": "David Vilares"
                },
                "author": "David Vilares",
                "arxiv_doi": "10.1007/s10462-024-10903-2",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10462-024-10903-2",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2308.09067v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.09067v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published at Artificial Intelligence Review vol. 57",
                "arxiv_journal_ref": "Artificial Intelligence Review 57, 265 (2024)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12963v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12963v1",
                "updated": "2024-08-23T10:18:39Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    10,
                    18,
                    39,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T10:18:39Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    10,
                    18,
                    39,
                    4,
                    236,
                    0
                ],
                "title": "Open Llama2 Model for the Lithuanian Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open Llama2 Model for the Lithuanian Language"
                },
                "summary": "In this paper, we propose and describe the first open Llama2 large language\nmodels (LLMs) for the Lithuanian language, including an accompanying\nquestion/answer (Q/A) dataset and translations of popular LLM benchmarks. We\nprovide a brief review of open regional LLMs and detailed information on the\nproposed LLMs and their training process. We also conduct an empirical\nevaluation, comparing the perplexities of the proposed LLMs with those of other\nmodern open LLMs. In addition, benchmarking the proposed LLMs against language\nunderstanding tasks reveals that high-quality pretraining datasets may be\nessential for achieving models that perform efficiently on these benchmarks.\nThe full realisations of the described LLMs are available in the accompanying\nopen repository~\\url{https://huggingface.co/neurotechnology}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose and describe the first open Llama2 large language\nmodels (LLMs) for the Lithuanian language, including an accompanying\nquestion/answer (Q/A) dataset and translations of popular LLM benchmarks. We\nprovide a brief review of open regional LLMs and detailed information on the\nproposed LLMs and their training process. We also conduct an empirical\nevaluation, comparing the perplexities of the proposed LLMs with those of other\nmodern open LLMs. In addition, benchmarking the proposed LLMs against language\nunderstanding tasks reveals that high-quality pretraining datasets may be\nessential for achieving models that perform efficiently on these benchmarks.\nThe full realisations of the described LLMs are available in the accompanying\nopen repository~\\url{https://huggingface.co/neurotechnology}."
                },
                "authors": [
                    {
                        "name": "Art≈´ras Nakvosas"
                    },
                    {
                        "name": "Povilas Daniu≈°is"
                    },
                    {
                        "name": "Vytas Muleviƒçius"
                    }
                ],
                "author_detail": {
                    "name": "Vytas Muleviƒçius"
                },
                "author": "Vytas Muleviƒçius",
                "arxiv_comment": "12 pages, 8 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12963v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12963v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12959v1",
                "updated": "2024-08-23T10:10:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    10,
                    10,
                    1,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T10:10:01Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    10,
                    10,
                    1,
                    4,
                    236,
                    0
                ],
                "title": "Multimodal Contrastive In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Contrastive In-Context Learning"
                },
                "summary": "The rapid growth of Large Language Models (LLMs) usage has highlighted the\nimportance of gradient-free in-context learning (ICL). However, interpreting\ntheir inner workings remains challenging. This paper introduces a novel\nmultimodal contrastive in-context learning framework to enhance our\nunderstanding of ICL in LLMs. First, we present a contrastive learning-based\ninterpretation of ICL in real-world settings, marking the distance of the\nkey-value representation as the differentiator in ICL. Second, we develop an\nanalytical framework to address biases in multimodal input formatting for\nreal-world datasets. We demonstrate the effectiveness of ICL examples where\nbaseline performance is poor, even when they are represented in unseen formats.\nLastly, we propose an on-the-fly approach for ICL (Anchored-by-Text ICL) that\ndemonstrates effectiveness in detecting hateful memes, a task where typical ICL\nstruggles due to resource limitations. Extensive experiments on multimodal\ndatasets reveal that our approach significantly improves ICL performance across\nvarious scenarios, such as challenging tasks and resource-constrained\nenvironments. Moreover, it provides valuable insights into the mechanisms of\nin-context learning in LLMs. Our findings have important implications for\ndeveloping more interpretable, efficient, and robust multimodal AI systems,\nespecially in challenging tasks and resource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of Large Language Models (LLMs) usage has highlighted the\nimportance of gradient-free in-context learning (ICL). However, interpreting\ntheir inner workings remains challenging. This paper introduces a novel\nmultimodal contrastive in-context learning framework to enhance our\nunderstanding of ICL in LLMs. First, we present a contrastive learning-based\ninterpretation of ICL in real-world settings, marking the distance of the\nkey-value representation as the differentiator in ICL. Second, we develop an\nanalytical framework to address biases in multimodal input formatting for\nreal-world datasets. We demonstrate the effectiveness of ICL examples where\nbaseline performance is poor, even when they are represented in unseen formats.\nLastly, we propose an on-the-fly approach for ICL (Anchored-by-Text ICL) that\ndemonstrates effectiveness in detecting hateful memes, a task where typical ICL\nstruggles due to resource limitations. Extensive experiments on multimodal\ndatasets reveal that our approach significantly improves ICL performance across\nvarious scenarios, such as challenging tasks and resource-constrained\nenvironments. Moreover, it provides valuable insights into the mechanisms of\nin-context learning in LLMs. Our findings have important implications for\ndeveloping more interpretable, efficient, and robust multimodal AI systems,\nespecially in challenging tasks and resource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Yosuke Miyanishi"
                    },
                    {
                        "name": "Minh Le Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Minh Le Nguyen"
                },
                "author": "Minh Le Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12948v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12948v1",
                "updated": "2024-08-23T09:57:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    57,
                    37,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T09:57:37Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    57,
                    37,
                    4,
                    236,
                    0
                ],
                "title": "E-code: Mastering Efficient Code Generation through Pretrained Models\n  and Expert Encoder Group",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E-code: Mastering Efficient Code Generation through Pretrained Models\n  and Expert Encoder Group"
                },
                "summary": "Context: With the waning of Moore's Law, the software industry is placing\nincreasing importance on finding alternative solutions for continuous\nperformance enhancement. The significance and research results of software\nperformance optimization have been on the rise in recent years, especially with\nthe advancement propelled by Large Language Models(LLMs). However, traditional\nstrategies for rectifying performance flaws have shown significant limitations\nat the competitive code efficiency optimization level, and research on this\ntopic is surprisingly scarce. Objective: This study aims to address the\nresearch gap in this domain, offering practical solutions to the various\nchallenges encountered. Specifically, we have overcome the constraints of\ntraditional performance error rectification strategies and developed a Language\nModel (LM) tailored for the competitive code efficiency optimization realm.\nMethod: We introduced E-code, an advanced program synthesis LM. Inspired by the\nrecent success of expert LMs, we designed an innovative structure called the\nExpert Encoder Group. This structure employs multiple expert encoders to\nextract features tailored for different input types. We assessed the\nperformance of E-code against other leading models on a competitive dataset and\nconducted in-depth ablation experiments. Results: Upon systematic evaluation,\nE-code achieved a 54.98% improvement in code efficiency, significantly\noutperforming other advanced models. In the ablation experiments, we further\nvalidated the significance of the expert encoder group and other components\nwithin E-code. Conclusion: The research findings indicate that the expert\nencoder group can effectively handle various inputs in efficiency optimization\ntasks, significantly enhancing the model's performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: With the waning of Moore's Law, the software industry is placing\nincreasing importance on finding alternative solutions for continuous\nperformance enhancement. The significance and research results of software\nperformance optimization have been on the rise in recent years, especially with\nthe advancement propelled by Large Language Models(LLMs). However, traditional\nstrategies for rectifying performance flaws have shown significant limitations\nat the competitive code efficiency optimization level, and research on this\ntopic is surprisingly scarce. Objective: This study aims to address the\nresearch gap in this domain, offering practical solutions to the various\nchallenges encountered. Specifically, we have overcome the constraints of\ntraditional performance error rectification strategies and developed a Language\nModel (LM) tailored for the competitive code efficiency optimization realm.\nMethod: We introduced E-code, an advanced program synthesis LM. Inspired by the\nrecent success of expert LMs, we designed an innovative structure called the\nExpert Encoder Group. This structure employs multiple expert encoders to\nextract features tailored for different input types. We assessed the\nperformance of E-code against other leading models on a competitive dataset and\nconducted in-depth ablation experiments. Results: Upon systematic evaluation,\nE-code achieved a 54.98% improvement in code efficiency, significantly\noutperforming other advanced models. In the ablation experiments, we further\nvalidated the significance of the expert encoder group and other components\nwithin E-code. Conclusion: The research findings indicate that the expert\nencoder group can effectively handle various inputs in efficiency optimization\ntasks, significantly enhancing the model's performance."
                },
                "authors": [
                    {
                        "name": "Yue Pan"
                    },
                    {
                        "name": "Chen Lyu"
                    },
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Lantian Li"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Xiuting Shao"
                    }
                ],
                "author_detail": {
                    "name": "Xiuting Shao"
                },
                "author": "Xiuting Shao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12948v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12948v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12945v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12945v1",
                "updated": "2024-08-23T09:51:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    51,
                    55,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T09:51:55Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    51,
                    55,
                    4,
                    236,
                    0
                ],
                "title": "Find the Assembly Mistakes: Error Segmentation for Industrial\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Find the Assembly Mistakes: Error Segmentation for Industrial\n  Applications"
                },
                "summary": "Recognizing errors in assembly and maintenance procedures is valuable for\nindustrial applications, since it can increase worker efficiency and prevent\nunplanned down-time. Although assembly state recognition is gaining attention,\nnone of the current works investigate assembly error localization. Therefore,\nwe propose StateDiffNet, which localizes assembly errors based on detecting the\ndifferences between a (correct) intended assembly state and a test image from a\nsimilar viewpoint. StateDiffNet is trained on synthetically generated image\npairs, providing full control over the type of meaningful change that should be\ndetected. The proposed approach is the first to correctly localize assembly\nerrors taken from real ego-centric video data for both states and error types\nthat are never presented during training. Furthermore, the deployment of change\ndetection to this industrial application provides valuable insights and\nconsiderations into the mechanisms of state-of-the-art change detection\nalgorithms. The code and data generation pipeline are publicly available at:\nhttps://timschoonbeek.github.io/error_seg.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recognizing errors in assembly and maintenance procedures is valuable for\nindustrial applications, since it can increase worker efficiency and prevent\nunplanned down-time. Although assembly state recognition is gaining attention,\nnone of the current works investigate assembly error localization. Therefore,\nwe propose StateDiffNet, which localizes assembly errors based on detecting the\ndifferences between a (correct) intended assembly state and a test image from a\nsimilar viewpoint. StateDiffNet is trained on synthetically generated image\npairs, providing full control over the type of meaningful change that should be\ndetected. The proposed approach is the first to correctly localize assembly\nerrors taken from real ego-centric video data for both states and error types\nthat are never presented during training. Furthermore, the deployment of change\ndetection to this industrial application provides valuable insights and\nconsiderations into the mechanisms of state-of-the-art change detection\nalgorithms. The code and data generation pipeline are publicly available at:\nhttps://timschoonbeek.github.io/error_seg."
                },
                "authors": [
                    {
                        "name": "Dan Lehman"
                    },
                    {
                        "name": "Tim J. Schoonbeek"
                    },
                    {
                        "name": "Shao-Hsuan Hung"
                    },
                    {
                        "name": "Jacek Kustra"
                    },
                    {
                        "name": "Peter H. N. de With"
                    },
                    {
                        "name": "Fons van der Sommen"
                    }
                ],
                "author_detail": {
                    "name": "Fons van der Sommen"
                },
                "author": "Fons van der Sommen",
                "arxiv_comment": "23 pages (14 main paper, 2 references, 7 supplementary), 15 figures\n  (8 main paper, 7 supplementary). Accepted at ECCV Vision-based InduStrial\n  InspectiON (VISION) workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12945v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12945v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12942v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12942v1",
                "updated": "2024-08-23T09:46:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    46,
                    15,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T09:46:15Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    46,
                    15,
                    4,
                    236,
                    0
                ],
                "title": "Causal-Guided Active Learning for Debiasing Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal-Guided Active Learning for Debiasing Large Language Models"
                },
                "summary": "Although achieving promising performance, recent analyses show that current\ngenerative large language models (LLMs) may still capture dataset biases and\nutilize them for generation, leading to poor generalizability and harmfulness\nof LLMs. However, due to the diversity of dataset biases and the\nover-optimization problem, previous prior-knowledge-based debiasing methods and\nfine-tuning-based debiasing methods may not be suitable for current LLMs. To\naddress this issue, we explore combining active learning with the causal\nmechanisms and propose a casual-guided active learning (CAL) framework, which\nutilizes LLMs itself to automatically and autonomously identify informative\nbiased samples and induce the bias patterns. Then a cost-effective and\nefficient in-context learning based method is employed to prevent LLMs from\nutilizing dataset biases during generation. Experimental results show that CAL\ncan effectively recognize typical biased instances and induce various bias\npatterns for debiasing LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although achieving promising performance, recent analyses show that current\ngenerative large language models (LLMs) may still capture dataset biases and\nutilize them for generation, leading to poor generalizability and harmfulness\nof LLMs. However, due to the diversity of dataset biases and the\nover-optimization problem, previous prior-knowledge-based debiasing methods and\nfine-tuning-based debiasing methods may not be suitable for current LLMs. To\naddress this issue, we explore combining active learning with the causal\nmechanisms and propose a casual-guided active learning (CAL) framework, which\nutilizes LLMs itself to automatically and autonomously identify informative\nbiased samples and induce the bias patterns. Then a cost-effective and\nefficient in-context learning based method is employed to prevent LLMs from\nutilizing dataset biases during generation. Experimental results show that CAL\ncan effectively recognize typical biased instances and induce various bias\npatterns for debiasing LLMs."
                },
                "authors": [
                    {
                        "name": "Zhouhao Sun"
                    },
                    {
                        "name": "Li Du"
                    },
                    {
                        "name": "Xiao Ding"
                    },
                    {
                        "name": "Yixuan Ma"
                    },
                    {
                        "name": "Kaitao Qiu"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Bing Qin"
                    }
                ],
                "author_detail": {
                    "name": "Bing Qin"
                },
                "author": "Bing Qin",
                "arxiv_comment": "ACL main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12942v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12942v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12935v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12935v1",
                "updated": "2024-08-23T09:33:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    33,
                    48,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T09:33:48Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    33,
                    48,
                    4,
                    236,
                    0
                ],
                "title": "Trustworthy, Responsible, and Safe AI: A Comprehensive Architectural\n  Framework for AI Safety with Challenges and Mitigations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trustworthy, Responsible, and Safe AI: A Comprehensive Architectural\n  Framework for AI Safety with Challenges and Mitigations"
                },
                "summary": "AI Safety is an emerging area of critical importance to the safe adoption and\ndeployment of AI systems. With the rapid proliferation of AI and especially\nwith the recent advancement of Generative AI (or GAI), the technology ecosystem\nbehind the design, development, adoption, and deployment of AI systems has\ndrastically changed, broadening the scope of AI Safety to address impacts on\npublic safety and national security. In this paper, we propose a novel\narchitectural framework for understanding and analyzing AI Safety; defining its\ncharacteristics from three perspectives: Trustworthy AI, Responsible AI, and\nSafe AI. We provide an extensive review of current research and advancements in\nAI safety from these perspectives, highlighting their key challenges and\nmitigation approaches. Through examples from state-of-the-art technologies,\nparticularly Large Language Models (LLMs), we present innovative mechanism,\nmethodologies, and techniques for designing and testing AI safety. Our goal is\nto promote advancement in AI safety research, and ultimately enhance people's\ntrust in digital transformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Safety is an emerging area of critical importance to the safe adoption and\ndeployment of AI systems. With the rapid proliferation of AI and especially\nwith the recent advancement of Generative AI (or GAI), the technology ecosystem\nbehind the design, development, adoption, and deployment of AI systems has\ndrastically changed, broadening the scope of AI Safety to address impacts on\npublic safety and national security. In this paper, we propose a novel\narchitectural framework for understanding and analyzing AI Safety; defining its\ncharacteristics from three perspectives: Trustworthy AI, Responsible AI, and\nSafe AI. We provide an extensive review of current research and advancements in\nAI safety from these perspectives, highlighting their key challenges and\nmitigation approaches. Through examples from state-of-the-art technologies,\nparticularly Large Language Models (LLMs), we present innovative mechanism,\nmethodologies, and techniques for designing and testing AI safety. Our goal is\nto promote advancement in AI safety research, and ultimately enhance people's\ntrust in digital transformation."
                },
                "authors": [
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Ziyao Liu"
                    },
                    {
                        "name": "Weifeng Jiang"
                    },
                    {
                        "name": "Goh Si Qi"
                    },
                    {
                        "name": "KwoK-Yan Lam"
                    }
                ],
                "author_detail": {
                    "name": "KwoK-Yan Lam"
                },
                "author": "KwoK-Yan Lam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12935v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12935v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20962v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20962v3",
                "updated": "2024-08-23T09:24:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    24,
                    22,
                    4,
                    236,
                    0
                ],
                "published": "2024-05-31T16:07:33Z",
                "published_parsed": [
                    2024,
                    5,
                    31,
                    16,
                    7,
                    33,
                    4,
                    152,
                    0
                ],
                "title": "Large Language Models are Zero-Shot Next Location Predictors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are Zero-Shot Next Location Predictors"
                },
                "summary": "Predicting the locations an individual will visit in the future is crucial\nfor solving many societal issues like disease diffusion and reduction of\npollution. However, next-location predictors require a significant amount of\nindividual-level information that may be scarce or unavailable in some\nscenarios (e.g., cold-start). Large Language Models (LLMs) have shown good\ngeneralization and reasoning capabilities and are rich in geographical\nknowledge, allowing us to believe that these models can act as zero-shot\nnext-location predictors. We tested more than 15 LLMs on three real-world\nmobility datasets and we found that LLMs can obtain accuracies up to 36.2%, a\nsignificant relative improvement of almost 640% when compared to other models\nspecifically designed for human mobility. We also test for data contamination\nand explored the possibility of using LLMs as text-based explainers for\nnext-location prediction, showing that, regardless of the model size, LLMs can\nexplain their decision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting the locations an individual will visit in the future is crucial\nfor solving many societal issues like disease diffusion and reduction of\npollution. However, next-location predictors require a significant amount of\nindividual-level information that may be scarce or unavailable in some\nscenarios (e.g., cold-start). Large Language Models (LLMs) have shown good\ngeneralization and reasoning capabilities and are rich in geographical\nknowledge, allowing us to believe that these models can act as zero-shot\nnext-location predictors. We tested more than 15 LLMs on three real-world\nmobility datasets and we found that LLMs can obtain accuracies up to 36.2%, a\nsignificant relative improvement of almost 640% when compared to other models\nspecifically designed for human mobility. We also test for data contamination\nand explored the possibility of using LLMs as text-based explainers for\nnext-location prediction, showing that, regardless of the model size, LLMs can\nexplain their decision."
                },
                "authors": [
                    {
                        "name": "Ciro Beneduce"
                    },
                    {
                        "name": "Bruno Lepri"
                    },
                    {
                        "name": "Massimiliano Luca"
                    }
                ],
                "author_detail": {
                    "name": "Massimiliano Luca"
                },
                "author": "Massimiliano Luca",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20962v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20962v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2203.15885v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2203.15885v2",
                "updated": "2024-08-23T09:21:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    21,
                    0,
                    4,
                    236,
                    0
                ],
                "published": "2022-03-29T20:10:56Z",
                "published_parsed": [
                    2022,
                    3,
                    29,
                    20,
                    10,
                    56,
                    1,
                    88,
                    0
                ],
                "title": "Split Conformal Prediction and Non-Exchangeable Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Split Conformal Prediction and Non-Exchangeable Data"
                },
                "summary": "Split conformal prediction (CP) is arguably the most popular CP method for\nuncertainty quantification, enjoying both academic interest and widespread\ndeployment. However, the original theoretical analysis of split CP makes the\ncrucial assumption of data exchangeability, which hinders many real-world\napplications. In this paper, we present a novel theoretical framework based on\nconcentration inequalities and decoupling properties of the data, proving that\nsplit CP remains valid for many non-exchangeable processes by adding a small\ncoverage penalty. Through experiments with both real and synthetic data, we\nshow that our theoretical results translate to good empirical performance under\nnon-exchangeability, e.g., for time series and spatiotemporal data. Compared to\nrecent conformal algorithms designed to counter specific exchangeability\nviolations, we show that split CP is competitive in terms of coverage and\ninterval size, with the benefit of being extremely simple and orders of\nmagnitude faster than alternatives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Split conformal prediction (CP) is arguably the most popular CP method for\nuncertainty quantification, enjoying both academic interest and widespread\ndeployment. However, the original theoretical analysis of split CP makes the\ncrucial assumption of data exchangeability, which hinders many real-world\napplications. In this paper, we present a novel theoretical framework based on\nconcentration inequalities and decoupling properties of the data, proving that\nsplit CP remains valid for many non-exchangeable processes by adding a small\ncoverage penalty. Through experiments with both real and synthetic data, we\nshow that our theoretical results translate to good empirical performance under\nnon-exchangeability, e.g., for time series and spatiotemporal data. Compared to\nrecent conformal algorithms designed to counter specific exchangeability\nviolations, we show that split CP is competitive in terms of coverage and\ninterval size, with the benefit of being extremely simple and orders of\nmagnitude faster than alternatives."
                },
                "authors": [
                    {
                        "name": "Roberto I. Oliveira"
                    },
                    {
                        "name": "Paulo Orenstein"
                    },
                    {
                        "name": "Thiago Ramos"
                    },
                    {
                        "name": "Jo√£o Vitor Romano"
                    }
                ],
                "author_detail": {
                    "name": "Jo√£o Vitor Romano"
                },
                "author": "Jo√£o Vitor Romano",
                "arxiv_journal_ref": "Journal of Machine Learning Research, 25(225), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2203.15885v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2203.15885v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12928v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12928v1",
                "updated": "2024-08-23T09:14:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    14,
                    58,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T09:14:58Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    14,
                    58,
                    4,
                    236,
                    0
                ],
                "title": "ParGo: Bridging Vision-Language with Partial and Global Views",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParGo: Bridging Vision-Language with Partial and Global Views"
                },
                "summary": "This work presents ParGo, a novel Partial-Global projector designed to\nconnect the vision and language modalities for Multimodal Large Language Models\n(MLLMs). Unlike previous works that rely on global attention-based projectors,\nour ParGo bridges the representation gap between the separately pre-trained\nvision encoders and the LLMs by integrating global and partial views, which\nalleviates the overemphasis on prominent regions. To facilitate the effective\ntraining of ParGo, we collect a large-scale detail-captioned image-text dataset\nnamed ParGoCap-1M-PT, consisting of 1 million images paired with high-quality\ncaptions. Extensive experiments on several MLLM benchmarks demonstrate the\neffectiveness of our ParGo, highlighting its superiority in aligning vision and\nlanguage modalities. Compared to conventional Q-Former projector, our ParGo\nachieves an improvement of 259.96 in MME benchmark. Furthermore, our\nexperiments reveal that ParGo significantly outperforms other projectors,\nparticularly in tasks that emphasize detail perception ability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents ParGo, a novel Partial-Global projector designed to\nconnect the vision and language modalities for Multimodal Large Language Models\n(MLLMs). Unlike previous works that rely on global attention-based projectors,\nour ParGo bridges the representation gap between the separately pre-trained\nvision encoders and the LLMs by integrating global and partial views, which\nalleviates the overemphasis on prominent regions. To facilitate the effective\ntraining of ParGo, we collect a large-scale detail-captioned image-text dataset\nnamed ParGoCap-1M-PT, consisting of 1 million images paired with high-quality\ncaptions. Extensive experiments on several MLLM benchmarks demonstrate the\neffectiveness of our ParGo, highlighting its superiority in aligning vision and\nlanguage modalities. Compared to conventional Q-Former projector, our ParGo\nachieves an improvement of 259.96 in MME benchmark. Furthermore, our\nexperiments reveal that ParGo significantly outperforms other projectors,\nparticularly in tasks that emphasize detail perception ability."
                },
                "authors": [
                    {
                        "name": "An-Lan Wang"
                    },
                    {
                        "name": "Bin Shan"
                    },
                    {
                        "name": "Wei Shi"
                    },
                    {
                        "name": "Kun-Yu Lin"
                    },
                    {
                        "name": "Xiang Fei"
                    },
                    {
                        "name": "Guozhi Tang"
                    },
                    {
                        "name": "Lei Liao"
                    },
                    {
                        "name": "Jingqun Tang"
                    },
                    {
                        "name": "Can Huang"
                    },
                    {
                        "name": "Wei-Shi Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Wei-Shi Zheng"
                },
                "author": "Wei-Shi Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12928v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12928v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14609v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14609v3",
                "updated": "2024-08-23T08:40:23Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    8,
                    40,
                    23,
                    4,
                    236,
                    0
                ],
                "published": "2024-02-22T14:57:44Z",
                "published_parsed": [
                    2024,
                    2,
                    22,
                    14,
                    57,
                    44,
                    3,
                    53,
                    0
                ],
                "title": "Federated Neural Graph Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Neural Graph Databases"
                },
                "summary": "The increasing demand for large-scale language models (LLMs) has highlighted\nthe importance of efficient data retrieval mechanisms. Neural graph databases\n(NGDBs) have emerged as a promising approach to storing and querying\ngraph-structured data in neural space, enabling the retrieval of relevant\ninformation for LLMs. However, existing NGDBs are typically designed to operate\non a single graph, limiting their ability to reason across multiple graphs.\nFurthermore, the lack of support for multi-source graph data in existing NGDBs\nhinders their ability to capture the complexity and diversity of real-world\ndata. In many applications, data is distributed across multiple sources, and\nthe ability to reason across these sources is crucial for making informed\ndecisions. This limitation is particularly problematic when dealing with\nsensitive graph data, as directly sharing and aggregating such data poses\nsignificant privacy risks. As a result, many applications that rely on NGDBs\nare forced to choose between compromising data privacy or sacrificing the\nability to reason across multiple graphs. To address these limitations, we\npropose Federated Neural Graph Database (FedNGDB), a novel framework that\nenables reasoning over multi-source graph-based data while preserving privacy.\nFedNGDB leverages federated learning to collaboratively learn graph\nrepresentations across multiple sources, enriching relationships between\nentities and improving the overall quality of the graph data. Unlike existing\nmethods, FedNGDB can handle complex graph structures and relationships, making\nit suitable for various downstream tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for large-scale language models (LLMs) has highlighted\nthe importance of efficient data retrieval mechanisms. Neural graph databases\n(NGDBs) have emerged as a promising approach to storing and querying\ngraph-structured data in neural space, enabling the retrieval of relevant\ninformation for LLMs. However, existing NGDBs are typically designed to operate\non a single graph, limiting their ability to reason across multiple graphs.\nFurthermore, the lack of support for multi-source graph data in existing NGDBs\nhinders their ability to capture the complexity and diversity of real-world\ndata. In many applications, data is distributed across multiple sources, and\nthe ability to reason across these sources is crucial for making informed\ndecisions. This limitation is particularly problematic when dealing with\nsensitive graph data, as directly sharing and aggregating such data poses\nsignificant privacy risks. As a result, many applications that rely on NGDBs\nare forced to choose between compromising data privacy or sacrificing the\nability to reason across multiple graphs. To address these limitations, we\npropose Federated Neural Graph Database (FedNGDB), a novel framework that\nenables reasoning over multi-source graph-based data while preserving privacy.\nFedNGDB leverages federated learning to collaboratively learn graph\nrepresentations across multiple sources, enriching relationships between\nentities and improving the overall quality of the graph data. Unlike existing\nmethods, FedNGDB can handle complex graph structures and relationships, making\nit suitable for various downstream tasks."
                },
                "authors": [
                    {
                        "name": "Qi Hu"
                    },
                    {
                        "name": "Weifeng Jiang"
                    },
                    {
                        "name": "Haoran Li"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Jiaxin Bai"
                    },
                    {
                        "name": "Qianren Mao"
                    },
                    {
                        "name": "Yangqiu Song"
                    },
                    {
                        "name": "Lixin Fan"
                    },
                    {
                        "name": "Jianxin Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianxin Li"
                },
                "author": "Jianxin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14609v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14609v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08334v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08334v4",
                "updated": "2024-08-23T08:36:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    8,
                    36,
                    41,
                    4,
                    236,
                    0
                ],
                "published": "2024-07-11T09:35:08Z",
                "published_parsed": [
                    2024,
                    7,
                    11,
                    9,
                    35,
                    8,
                    3,
                    193,
                    0
                ],
                "title": "ADMM Based Semi-Structured Pattern Pruning Framework For Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ADMM Based Semi-Structured Pattern Pruning Framework For Transformer"
                },
                "summary": "NLP(natural language processsing) has achieved great success through the\ntransformer model.However, the model has hundreds of millions or billions\nparameters,which is huge burden for its deployment on personal computer or\nsmall scale of server.To deal with it, we either make the model's weight matrix\nrelatively sparser, or compress attention layer. Pattern pruning ,one of the\nmost important pruning methods, permits selecting fixed number of parameters in\neach divided pattern block and prunes it. However, the effect of pattern\npruning is strictly limited by the sparsity within a region of weights in each\nlayer. In this paper,we first introduced Alternating Direction Method of\nMultipliers(ADMM) based pattern pruning framework to reshape the distribution\nof activation map. Specifically, we propose to formulate the pattern pruning on\ntransformer as a constrained optimization and use ADMM to optimize the problem.\nIn this way, the initial dense feature maps is transformed to rather regionally\nsparsified ones.Therefore, we can then achieve higher compression ratio with\nbetter performance based on pattern pruning method. Additionally, this paper\nprovides a theoretical derivations of the ADMM with local sparsity. Finally, we\nalso extend the proposed ADMM based framework with SR-STE to demonstrate its\ngeneralization and to avoid gradient vanishing problem. We conduct extensive\nexperiments on classification tasks over GLUE datasets. Significantly, we\nachieve 50% percent compression ratio while maintaining overall score 80.1 on\nGLUE dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NLP(natural language processsing) has achieved great success through the\ntransformer model.However, the model has hundreds of millions or billions\nparameters,which is huge burden for its deployment on personal computer or\nsmall scale of server.To deal with it, we either make the model's weight matrix\nrelatively sparser, or compress attention layer. Pattern pruning ,one of the\nmost important pruning methods, permits selecting fixed number of parameters in\neach divided pattern block and prunes it. However, the effect of pattern\npruning is strictly limited by the sparsity within a region of weights in each\nlayer. In this paper,we first introduced Alternating Direction Method of\nMultipliers(ADMM) based pattern pruning framework to reshape the distribution\nof activation map. Specifically, we propose to formulate the pattern pruning on\ntransformer as a constrained optimization and use ADMM to optimize the problem.\nIn this way, the initial dense feature maps is transformed to rather regionally\nsparsified ones.Therefore, we can then achieve higher compression ratio with\nbetter performance based on pattern pruning method. Additionally, this paper\nprovides a theoretical derivations of the ADMM with local sparsity. Finally, we\nalso extend the proposed ADMM based framework with SR-STE to demonstrate its\ngeneralization and to avoid gradient vanishing problem. We conduct extensive\nexperiments on classification tasks over GLUE datasets. Significantly, we\nachieve 50% percent compression ratio while maintaining overall score 80.1 on\nGLUE dataset."
                },
                "authors": [
                    {
                        "name": "TianChen Wang"
                    }
                ],
                "author_detail": {
                    "name": "TianChen Wang"
                },
                "author": "TianChen Wang",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08334v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08334v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06571v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06571v5",
                "updated": "2024-08-23T08:17:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    8,
                    17,
                    58,
                    4,
                    236,
                    0
                ],
                "published": "2024-06-03T16:43:04Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    16,
                    43,
                    4,
                    0,
                    155,
                    0
                ],
                "title": "SUBLLM: A Novel Efficient Architecture with Token Sequence Subsampling\n  for LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SUBLLM: A Novel Efficient Architecture with Token Sequence Subsampling\n  for LLM"
                },
                "summary": "While Large Language Models (LLMs) have achieved remarkable success in\nvarious fields, the efficiency of training and inference remains a major\nchallenge. To address this issue, we propose SUBLLM, short for\nSubsampling-Upsampling-Bypass Large Language Model, an innovative architecture\nthat extends the core decoder-only framework by incorporating subsampling,\nupsampling, and bypass modules. The subsampling modules are responsible for\nshortening the sequence, while the upsampling modules restore the sequence\nlength, and the bypass modules enhance convergence. In comparison to LLaMA, the\nproposed SUBLLM exhibits significant enhancements in both training and\ninference speeds as well as memory usage, while maintaining competitive\nfew-shot performance. During training, SUBLLM increases speeds by 26% and cuts\nmemory by 10GB per GPU. In inference, it boosts speeds by up to 37% and reduces\nmemory by 1GB per GPU. The training and inference speeds can be enhanced by 34%\nand 52% respectively when the context window is expanded to 8192. Our code is\navailable at https://github.com/XiaoMi/subllm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) have achieved remarkable success in\nvarious fields, the efficiency of training and inference remains a major\nchallenge. To address this issue, we propose SUBLLM, short for\nSubsampling-Upsampling-Bypass Large Language Model, an innovative architecture\nthat extends the core decoder-only framework by incorporating subsampling,\nupsampling, and bypass modules. The subsampling modules are responsible for\nshortening the sequence, while the upsampling modules restore the sequence\nlength, and the bypass modules enhance convergence. In comparison to LLaMA, the\nproposed SUBLLM exhibits significant enhancements in both training and\ninference speeds as well as memory usage, while maintaining competitive\nfew-shot performance. During training, SUBLLM increases speeds by 26% and cuts\nmemory by 10GB per GPU. In inference, it boosts speeds by up to 37% and reduces\nmemory by 1GB per GPU. The training and inference speeds can be enhanced by 34%\nand 52% respectively when the context window is expanded to 8192. Our code is\navailable at https://github.com/XiaoMi/subllm."
                },
                "authors": [
                    {
                        "name": "Quandong Wang"
                    },
                    {
                        "name": "Yuxuan Yuan"
                    },
                    {
                        "name": "Xiaoyu Yang"
                    },
                    {
                        "name": "Ruike Zhang"
                    },
                    {
                        "name": "Kang Zhao"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jian Luan"
                    },
                    {
                        "name": "Daniel Povey"
                    },
                    {
                        "name": "Bin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bin Wang"
                },
                "author": "Bin Wang",
                "arxiv_comment": "10 pages, 5 figures, accepted by ECAI 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06571v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06571v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11557v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11557v2",
                "updated": "2024-08-23T08:16:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    8,
                    16,
                    50,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-21T12:09:37Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    12,
                    9,
                    37,
                    2,
                    234,
                    0
                ],
                "title": "A Quick, trustworthy spectral detection Q&A system based on the SDAAP\n  Dataset and large language model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Quick, trustworthy spectral detection Q&A system based on the SDAAP\n  Dataset and large language model"
                },
                "summary": "Large Language Model (LLM) has demonstrated significant success in a range of\nnatural language processing (NLP) tasks within general domain. The emergence of\nLLM has introduced innovative methodologies across diverse fields, including\nthe natural sciences. Researchers aim to implement automated, concurrent\nprocess driven by LLM to supplant conventional manual, repetitive and\nlabor-intensive work. In the domain of spectral analysis and detection, it is\nimperative for researchers to autonomously acquire pertinent knowledge across\nvarious research objects, which encompasses the spectroscopic techniques and\nthe chemometric methods that are employed in experiments and analysis.\nParadoxically, despite the recognition of spectroscopic detection as an\neffective analytical method, the fundamental process of knowledge retrieval\nremains both time-intensive and repetitive. In response to this challenge, we\nfirst introduced the Spectral Detection and Analysis Based Paper(SDAAP)\ndataset, which is the first open-source textual knowledge dataset for spectral\nanalysis and detection and contains annotated literature data as well as\ncorresponding knowledge instruction data. Subsequently, we also designed an\nautomated Q\\&A framework based on the SDAAP dataset, which can retrieve\nrelevant knowledge and generate high-quality responses by extracting entities\nin the input as retrieval parameters. It is worth noting that: within this\nframework, LLM is only used as a tool to provide generalizability, while RAG\ntechnique is used to accurately capture the source of the knowledge.This\napproach not only improves the quality of the generated responses, but also\nensures the traceability of the knowledge. Experimental results show that our\nframework generates responses with more reliable expertise compared to the\nbaseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) has demonstrated significant success in a range of\nnatural language processing (NLP) tasks within general domain. The emergence of\nLLM has introduced innovative methodologies across diverse fields, including\nthe natural sciences. Researchers aim to implement automated, concurrent\nprocess driven by LLM to supplant conventional manual, repetitive and\nlabor-intensive work. In the domain of spectral analysis and detection, it is\nimperative for researchers to autonomously acquire pertinent knowledge across\nvarious research objects, which encompasses the spectroscopic techniques and\nthe chemometric methods that are employed in experiments and analysis.\nParadoxically, despite the recognition of spectroscopic detection as an\neffective analytical method, the fundamental process of knowledge retrieval\nremains both time-intensive and repetitive. In response to this challenge, we\nfirst introduced the Spectral Detection and Analysis Based Paper(SDAAP)\ndataset, which is the first open-source textual knowledge dataset for spectral\nanalysis and detection and contains annotated literature data as well as\ncorresponding knowledge instruction data. Subsequently, we also designed an\nautomated Q\\&A framework based on the SDAAP dataset, which can retrieve\nrelevant knowledge and generate high-quality responses by extracting entities\nin the input as retrieval parameters. It is worth noting that: within this\nframework, LLM is only used as a tool to provide generalizability, while RAG\ntechnique is used to accurately capture the source of the knowledge.This\napproach not only improves the quality of the generated responses, but also\nensures the traceability of the knowledge. Experimental results show that our\nframework generates responses with more reliable expertise compared to the\nbaseline."
                },
                "authors": [
                    {
                        "name": "Jiheng Liang"
                    },
                    {
                        "name": "Ziru Yu"
                    },
                    {
                        "name": "Zujie Xie"
                    },
                    {
                        "name": "Xiangyang Yu"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyang Yu"
                },
                "author": "Xiangyang Yu",
                "arxiv_comment": "16 pages,10 figures,3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11557v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11557v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12369v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12369v2",
                "updated": "2024-08-23T08:11:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    8,
                    11,
                    9,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-22T13:13:06Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    13,
                    13,
                    6,
                    3,
                    235,
                    0
                ],
                "title": "RoundTable: Leveraging Dynamic Schema and Contextual Autocomplete for\n  Enhanced Query Precision in Tabular Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoundTable: Leveraging Dynamic Schema and Contextual Autocomplete for\n  Enhanced Query Precision in Tabular Question Answering"
                },
                "summary": "With advancements in Large Language Models (LLMs), a major use case that has\nemerged is querying databases in plain English, translating user questions into\nexecutable database queries, which has improved significantly. However,\nreal-world datasets often feature a vast array of attributes and complex\nvalues, complicating the LLMs task of accurately identifying relevant columns\nor values from natural language queries. Traditional methods cannot fully relay\nthe datasets size and complexity to the LLM. To address these challenges, we\npropose a novel framework that leverages Full-Text Search (FTS) on the input\ntable. This approach not only enables precise detection of specific values and\ncolumns but also narrows the search space for language models, thereby\nenhancing query accuracy. Additionally, it supports a custom auto-complete\nfeature that suggests queries based on the data in the table. This integration\nsignificantly refines the interaction between the user and complex datasets,\noffering a sophisticated solution to the limitations faced by current table\nquerying capabilities. This work is accompanied by an application for both Mac\nand Windows platforms, which readers can try out themselves on their own data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With advancements in Large Language Models (LLMs), a major use case that has\nemerged is querying databases in plain English, translating user questions into\nexecutable database queries, which has improved significantly. However,\nreal-world datasets often feature a vast array of attributes and complex\nvalues, complicating the LLMs task of accurately identifying relevant columns\nor values from natural language queries. Traditional methods cannot fully relay\nthe datasets size and complexity to the LLM. To address these challenges, we\npropose a novel framework that leverages Full-Text Search (FTS) on the input\ntable. This approach not only enables precise detection of specific values and\ncolumns but also narrows the search space for language models, thereby\nenhancing query accuracy. Additionally, it supports a custom auto-complete\nfeature that suggests queries based on the data in the table. This integration\nsignificantly refines the interaction between the user and complex datasets,\noffering a sophisticated solution to the limitations faced by current table\nquerying capabilities. This work is accompanied by an application for both Mac\nand Windows platforms, which readers can try out themselves on their own data."
                },
                "authors": [
                    {
                        "name": "Pratyush Kumar"
                    },
                    {
                        "name": "Kuber Vijaykumar Bellad"
                    },
                    {
                        "name": "Bharat Vadlamudi"
                    },
                    {
                        "name": "Aman Chadha"
                    }
                ],
                "author_detail": {
                    "name": "Aman Chadha"
                },
                "author": "Aman Chadha",
                "arxiv_comment": "13 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12369v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12369v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.07637v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.07637v4",
                "updated": "2024-08-23T07:46:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    7,
                    46,
                    17,
                    4,
                    236,
                    0
                ],
                "published": "2023-10-11T16:33:29Z",
                "published_parsed": [
                    2023,
                    10,
                    11,
                    16,
                    33,
                    29,
                    2,
                    284,
                    0
                ],
                "title": "OpsEval: A Comprehensive IT Operations Benchmark Suite for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpsEval: A Comprehensive IT Operations Benchmark Suite for Large\n  Language Models"
                },
                "summary": "Information Technology (IT) Operations (Ops), particularly Artificial\nIntelligence for IT Operations (AIOps), is the guarantee for maintaining the\norderly and stable operation of existing information systems. According to\nGartner's prediction, the use of AI technology for automated IT operations has\nbecome a new trend. Large language models (LLMs) that have exhibited remarkable\ncapabilities in NLP-related tasks, are showing great potential in the field of\nAIOps, such as in aspects of root cause analysis of failures, generation of\noperations and maintenance scripts, and summarizing of alert information.\nNevertheless, the performance of current LLMs in Ops tasks is yet to be\ndetermined. In this paper, we present OpsEval, a comprehensive task-oriented\nOps benchmark designed for LLMs. For the first time, OpsEval assesses LLMs'\nproficiency in various crucial scenarios at different ability levels. The\nbenchmark includes 7184 multi-choice questions and 1736 question-answering (QA)\nformats in English and Chinese. By conducting a comprehensive performance\nevaluation of the current leading large language models, we show how various\nLLM techniques can affect the performance of Ops, and discussed findings\nrelated to various topics, including model quantification, QA evaluation, and\nhallucination issues. To ensure the credibility of our evaluation, we invite\ndozens of domain experts to manually review our questions. At the same time, we\nhave open-sourced 20% of the test QA to assist current researchers in\npreliminary evaluations of their OpsLLM models. The remaining 80% of the data,\nwhich is not disclosed, is used to eliminate the issue of the test set leakage.\nAdditionally, we have constructed an online leaderboard that is updated in\nreal-time and will continue to be updated, ensuring that any newly emerging\nLLMs will be evaluated promptly. Both our dataset and leaderboard have been\nmade public.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information Technology (IT) Operations (Ops), particularly Artificial\nIntelligence for IT Operations (AIOps), is the guarantee for maintaining the\norderly and stable operation of existing information systems. According to\nGartner's prediction, the use of AI technology for automated IT operations has\nbecome a new trend. Large language models (LLMs) that have exhibited remarkable\ncapabilities in NLP-related tasks, are showing great potential in the field of\nAIOps, such as in aspects of root cause analysis of failures, generation of\noperations and maintenance scripts, and summarizing of alert information.\nNevertheless, the performance of current LLMs in Ops tasks is yet to be\ndetermined. In this paper, we present OpsEval, a comprehensive task-oriented\nOps benchmark designed for LLMs. For the first time, OpsEval assesses LLMs'\nproficiency in various crucial scenarios at different ability levels. The\nbenchmark includes 7184 multi-choice questions and 1736 question-answering (QA)\nformats in English and Chinese. By conducting a comprehensive performance\nevaluation of the current leading large language models, we show how various\nLLM techniques can affect the performance of Ops, and discussed findings\nrelated to various topics, including model quantification, QA evaluation, and\nhallucination issues. To ensure the credibility of our evaluation, we invite\ndozens of domain experts to manually review our questions. At the same time, we\nhave open-sourced 20% of the test QA to assist current researchers in\npreliminary evaluations of their OpsLLM models. The remaining 80% of the data,\nwhich is not disclosed, is used to eliminate the issue of the test set leakage.\nAdditionally, we have constructed an online leaderboard that is updated in\nreal-time and will continue to be updated, ensuring that any newly emerging\nLLMs will be evaluated promptly. Both our dataset and leaderboard have been\nmade public."
                },
                "authors": [
                    {
                        "name": "Yuhe Liu"
                    },
                    {
                        "name": "Changhua Pei"
                    },
                    {
                        "name": "Longlong Xu"
                    },
                    {
                        "name": "Bohan Chen"
                    },
                    {
                        "name": "Mingze Sun"
                    },
                    {
                        "name": "Zhirui Zhang"
                    },
                    {
                        "name": "Yongqian Sun"
                    },
                    {
                        "name": "Shenglin Zhang"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Haiming Zhang"
                    },
                    {
                        "name": "Jianhui Li"
                    },
                    {
                        "name": "Gaogang Xie"
                    },
                    {
                        "name": "Xidao Wen"
                    },
                    {
                        "name": "Xiaohui Nie"
                    },
                    {
                        "name": "Minghua Ma"
                    },
                    {
                        "name": "Dan Pei"
                    }
                ],
                "author_detail": {
                    "name": "Dan Pei"
                },
                "author": "Dan Pei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.07637v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.07637v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14874v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14874v2",
                "updated": "2024-08-23T07:31:42Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    7,
                    31,
                    42,
                    4,
                    236,
                    0
                ],
                "published": "2024-02-21T17:20:38Z",
                "published_parsed": [
                    2024,
                    2,
                    21,
                    17,
                    20,
                    38,
                    2,
                    52,
                    0
                ],
                "title": "Distillation Contrastive Decoding: Improving LLMs Reasoning with\n  Contrastive Decoding and Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distillation Contrastive Decoding: Improving LLMs Reasoning with\n  Contrastive Decoding and Distillation"
                },
                "summary": "We propose a straightforward approach called Distillation Contrastive\nDecoding (DCD) to enhance the reasoning capabilities of Large Language Models\n(LLMs) during inference. In contrast to previous approaches that relied on\nsmaller amateur models or analysis of hidden state differences, DCD employs\nContrastive Chain-of-thought Prompting and advanced distillation techniques,\nincluding Dropout and Quantization. This approach effectively addresses the\nlimitations of Contrastive Decoding (CD), which typically requires both an\nexpert and an amateur model, thus increasing computational resource demands. By\nintegrating contrastive prompts with distillation, DCD obviates the need for an\namateur model and reduces memory usage. Our evaluations demonstrate that DCD\nsignificantly enhances LLM performance across a range of reasoning benchmarks,\nsurpassing both CD and existing methods in the GSM8K and StrategyQA datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a straightforward approach called Distillation Contrastive\nDecoding (DCD) to enhance the reasoning capabilities of Large Language Models\n(LLMs) during inference. In contrast to previous approaches that relied on\nsmaller amateur models or analysis of hidden state differences, DCD employs\nContrastive Chain-of-thought Prompting and advanced distillation techniques,\nincluding Dropout and Quantization. This approach effectively addresses the\nlimitations of Contrastive Decoding (CD), which typically requires both an\nexpert and an amateur model, thus increasing computational resource demands. By\nintegrating contrastive prompts with distillation, DCD obviates the need for an\namateur model and reduces memory usage. Our evaluations demonstrate that DCD\nsignificantly enhances LLM performance across a range of reasoning benchmarks,\nsurpassing both CD and existing methods in the GSM8K and StrategyQA datasets."
                },
                "authors": [
                    {
                        "name": "Phuc Phan"
                    },
                    {
                        "name": "Hieu Tran"
                    },
                    {
                        "name": "Long Phan"
                    }
                ],
                "author_detail": {
                    "name": "Long Phan"
                },
                "author": "Long Phan",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14874v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14874v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12867v1",
                "updated": "2024-08-23T06:48:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    6,
                    48,
                    46,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T06:48:46Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    6,
                    48,
                    46,
                    4,
                    236,
                    0
                ],
                "title": "Semantic Alignment for Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Alignment for Multimodal Large Language Models"
                },
                "summary": "Research on Multi-modal Large Language Models (MLLMs) towards the multi-image\ncross-modal instruction has received increasing attention and made significant\nprogress, particularly in scenarios involving closely resembling images (e.g.,\nchange captioning). Existing MLLMs typically follow a two-step process in their\npipelines: first, extracting visual tokens independently for each input image,\nand then aligning these visual tokens from different images with the Large\nLanguage Model (LLM) in its textual feature space. However, the independent\nextraction of visual tokens for each image may result in different semantics\nbeing prioritized for different images in the first step, leading to a lack of\npreservation of linking information among images for subsequent LLM analysis.\nThis issue becomes more serious in scenarios where significant variations exist\namong the images (e.g., visual storytelling). To address this challenge, we\nintroduce Semantic Alignment for Multi-modal large language models (SAM). By\ninvolving the bidirectional semantic guidance between different images in the\nvisual-token extraction process, SAM aims to enhance the preservation of\nlinking information for coherent analysis and align the semantics of different\nimages before feeding them into LLM. As the test bed, we propose a large-scale\ndataset named MmLINK consisting of 69K samples. Different from most existing\ndatasets for MLLMs fine-tuning, our MmLINK dataset comprises multi-modal\ninstructions with significantly diverse images. Extensive experiments on the\ngroup captioning task and the storytelling task prove the effectiveness of our\nSAM model, surpassing the state-of-the-art methods by a large margin (+37% for\ngroup captioning and +22% for storytelling on CIDEr score). Project page:\nhttps://mccartney01.github.io/SAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research on Multi-modal Large Language Models (MLLMs) towards the multi-image\ncross-modal instruction has received increasing attention and made significant\nprogress, particularly in scenarios involving closely resembling images (e.g.,\nchange captioning). Existing MLLMs typically follow a two-step process in their\npipelines: first, extracting visual tokens independently for each input image,\nand then aligning these visual tokens from different images with the Large\nLanguage Model (LLM) in its textual feature space. However, the independent\nextraction of visual tokens for each image may result in different semantics\nbeing prioritized for different images in the first step, leading to a lack of\npreservation of linking information among images for subsequent LLM analysis.\nThis issue becomes more serious in scenarios where significant variations exist\namong the images (e.g., visual storytelling). To address this challenge, we\nintroduce Semantic Alignment for Multi-modal large language models (SAM). By\ninvolving the bidirectional semantic guidance between different images in the\nvisual-token extraction process, SAM aims to enhance the preservation of\nlinking information for coherent analysis and align the semantics of different\nimages before feeding them into LLM. As the test bed, we propose a large-scale\ndataset named MmLINK consisting of 69K samples. Different from most existing\ndatasets for MLLMs fine-tuning, our MmLINK dataset comprises multi-modal\ninstructions with significantly diverse images. Extensive experiments on the\ngroup captioning task and the storytelling task prove the effectiveness of our\nSAM model, surpassing the state-of-the-art methods by a large margin (+37% for\ngroup captioning and +22% for storytelling on CIDEr score). Project page:\nhttps://mccartney01.github.io/SAM."
                },
                "authors": [
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Mengze Li"
                    },
                    {
                        "name": "Jingyuan Chen"
                    },
                    {
                        "name": "Wei Ji"
                    },
                    {
                        "name": "Wang Lin"
                    },
                    {
                        "name": "Jinyang Gao"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Zhou Zhao"
                    },
                    {
                        "name": "Fei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Wu"
                },
                "author": "Fei Wu",
                "arxiv_comment": "Accepted by MM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12194v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12194v2",
                "updated": "2024-08-23T06:46:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    6,
                    46,
                    41,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-22T08:16:07Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    8,
                    16,
                    7,
                    3,
                    235,
                    0
                ],
                "title": "Large Language Models as Foundations for Next-Gen Dense Retrieval: A\n  Comprehensive Empirical Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models as Foundations for Next-Gen Dense Retrieval: A\n  Comprehensive Empirical Assessment"
                },
                "summary": "Pretrained language models like BERT and T5 serve as crucial backbone\nencoders for dense retrieval. However, these models often exhibit limited\ngeneralization capabilities and face challenges in improving in domain\naccuracy. Recent research has explored using large language models (LLMs) as\nretrievers, achieving SOTA performance across various tasks. Despite these\nadvancements, the specific benefits of LLMs over traditional retrievers and the\nimpact of different LLM configurations, such as parameter sizes, pretraining\nduration, and alignment processes on retrieval tasks remain unclear. In this\nwork, we conduct a comprehensive empirical study on a wide range of retrieval\ntasks, including in domain accuracy, data efficiency, zero shot generalization,\nlengthy retrieval, instruction based retrieval, and multi task learning. We\nevaluate over 15 different backbone LLMs and non LLMs. Our findings reveal that\nlarger models and extensive pretraining consistently enhance in domain accuracy\nand data efficiency. Additionally, larger models demonstrate significant\npotential in zero shot generalization, lengthy retrieval, instruction based\nretrieval, and multi task learning. These results underscore the advantages of\nLLMs as versatile and effective backbone encoders in dense retrieval, providing\nvaluable insights for future research and development in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pretrained language models like BERT and T5 serve as crucial backbone\nencoders for dense retrieval. However, these models often exhibit limited\ngeneralization capabilities and face challenges in improving in domain\naccuracy. Recent research has explored using large language models (LLMs) as\nretrievers, achieving SOTA performance across various tasks. Despite these\nadvancements, the specific benefits of LLMs over traditional retrievers and the\nimpact of different LLM configurations, such as parameter sizes, pretraining\nduration, and alignment processes on retrieval tasks remain unclear. In this\nwork, we conduct a comprehensive empirical study on a wide range of retrieval\ntasks, including in domain accuracy, data efficiency, zero shot generalization,\nlengthy retrieval, instruction based retrieval, and multi task learning. We\nevaluate over 15 different backbone LLMs and non LLMs. Our findings reveal that\nlarger models and extensive pretraining consistently enhance in domain accuracy\nand data efficiency. Additionally, larger models demonstrate significant\npotential in zero shot generalization, lengthy retrieval, instruction based\nretrieval, and multi task learning. These results underscore the advantages of\nLLMs as versatile and effective backbone encoders in dense retrieval, providing\nvaluable insights for future research and development in this field."
                },
                "authors": [
                    {
                        "name": "Kun Luo"
                    },
                    {
                        "name": "Minghao Qin"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "arxiv_comment": "Submitted to EMNLP24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12194v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12194v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12857v1",
                "updated": "2024-08-23T05:54:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    5,
                    54,
                    53,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T05:54:53Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    5,
                    54,
                    53,
                    4,
                    236,
                    0
                ],
                "title": "Memory-Efficient LLM Training with Online Subspace Descent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Efficient LLM Training with Online Subspace Descent"
                },
                "summary": "Recently, a wide range of memory-efficient LLM training algorithms have\ngained substantial popularity. These methods leverage the low-rank structure of\ngradients to project optimizer states into a subspace using projection matrix\nfound by singular value decomposition (SVD). However, convergence of these\nalgorithms is highly dependent on the update rules of their projection matrix.\nIn this work, we provide the \\emph{first} convergence guarantee for arbitrary\nupdate rules of projection matrix. This guarantee is generally applicable to\noptimizers that can be analyzed with Hamiltonian Descent, including most common\nones, such as LION, Adam. Inspired by our theoretical understanding, we propose\nOnline Subspace Descent, a new family of subspace descent optimizer without\nSVD. Instead of updating the projection matrix with eigenvectors, Online\nSubspace Descent updates the projection matrix with online PCA. Online Subspace\nDescent is flexible and introduces only minimum overhead to training. We show\nthat for the task of pretraining LLaMA models ranging from 60M to 7B parameters\non the C4 dataset, Online Subspace Descent achieves lower perplexity and better\ndownstream tasks performance than state-of-the-art low-rank training methods\nacross different settings and narrows the gap with full-rank baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, a wide range of memory-efficient LLM training algorithms have\ngained substantial popularity. These methods leverage the low-rank structure of\ngradients to project optimizer states into a subspace using projection matrix\nfound by singular value decomposition (SVD). However, convergence of these\nalgorithms is highly dependent on the update rules of their projection matrix.\nIn this work, we provide the \\emph{first} convergence guarantee for arbitrary\nupdate rules of projection matrix. This guarantee is generally applicable to\noptimizers that can be analyzed with Hamiltonian Descent, including most common\nones, such as LION, Adam. Inspired by our theoretical understanding, we propose\nOnline Subspace Descent, a new family of subspace descent optimizer without\nSVD. Instead of updating the projection matrix with eigenvectors, Online\nSubspace Descent updates the projection matrix with online PCA. Online Subspace\nDescent is flexible and introduces only minimum overhead to training. We show\nthat for the task of pretraining LLaMA models ranging from 60M to 7B parameters\non the C4 dataset, Online Subspace Descent achieves lower perplexity and better\ndownstream tasks performance than state-of-the-art low-rank training methods\nacross different settings and narrows the gap with full-rank baselines."
                },
                "authors": [
                    {
                        "name": "Kaizhao Liang"
                    },
                    {
                        "name": "Bo Liu"
                    },
                    {
                        "name": "Lizhang Chen"
                    },
                    {
                        "name": "Qiang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Liu"
                },
                "author": "Qiang Liu",
                "arxiv_comment": "Code is available at\n  https://github.com/kyleliang919/Online-Subspace-Descent",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12844v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12844v1",
                "updated": "2024-08-23T05:25:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    5,
                    25,
                    11,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T05:25:11Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    5,
                    25,
                    11,
                    4,
                    236,
                    0
                ],
                "title": "Predicting Affective States from Screen Text Sentiment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting Affective States from Screen Text Sentiment"
                },
                "summary": "The proliferation of mobile sensing technologies has enabled the study of\nvarious physiological and behavioural phenomena through unobtrusive data\ncollection from smartphone sensors. This approach offers real-time insights\ninto individuals' physical and mental states, creating opportunities for\npersonalised treatment and interventions. However, the potential of analysing\nthe textual content viewed on smartphones to predict affective states remains\nunderexplored. To better understand how the screen text that users are exposed\nto and interact with can influence their affects, we investigated a subset of\ndata obtained from a digital phenotyping study of Australian university\nstudents conducted in 2023. We employed linear regression, zero-shot, and\nmulti-shot prompting using a large language model (LLM) to analyse\nrelationships between screen text and affective states. Our findings indicate\nthat multi-shot prompting substantially outperforms both linear regression and\nzero-shot prompting, highlighting the importance of context in affect\nprediction. We discuss the value of incorporating textual and sentiment data\nfor improving affect prediction, providing a basis for future advancements in\nunderstanding smartphone use and wellbeing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of mobile sensing technologies has enabled the study of\nvarious physiological and behavioural phenomena through unobtrusive data\ncollection from smartphone sensors. This approach offers real-time insights\ninto individuals' physical and mental states, creating opportunities for\npersonalised treatment and interventions. However, the potential of analysing\nthe textual content viewed on smartphones to predict affective states remains\nunderexplored. To better understand how the screen text that users are exposed\nto and interact with can influence their affects, we investigated a subset of\ndata obtained from a digital phenotyping study of Australian university\nstudents conducted in 2023. We employed linear regression, zero-shot, and\nmulti-shot prompting using a large language model (LLM) to analyse\nrelationships between screen text and affective states. Our findings indicate\nthat multi-shot prompting substantially outperforms both linear regression and\nzero-shot prompting, highlighting the importance of context in affect\nprediction. We discuss the value of incorporating textual and sentiment data\nfor improving affect prediction, providing a basis for future advancements in\nunderstanding smartphone use and wellbeing."
                },
                "authors": [
                    {
                        "name": "Songyan Teng"
                    },
                    {
                        "name": "Tianyi Zhang"
                    },
                    {
                        "name": "Simon D'Alfonso"
                    },
                    {
                        "name": "Vassilis Kostakos"
                    }
                ],
                "author_detail": {
                    "name": "Vassilis Kostakos"
                },
                "author": "Vassilis Kostakos",
                "arxiv_doi": "10.1145/3675094.3678489",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3675094.3678489",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.12844v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12844v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "7 pages",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.18093v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.18093v2",
                "updated": "2024-08-23T05:03:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    5,
                    3,
                    44,
                    4,
                    236,
                    0
                ],
                "published": "2024-02-28T06:28:15Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    6,
                    28,
                    15,
                    2,
                    59,
                    0
                ],
                "title": "ChatSpamDetector: Leveraging Large Language Models for Effective\n  Phishing Email Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatSpamDetector: Leveraging Large Language Models for Effective\n  Phishing Email Detection"
                },
                "summary": "The proliferation of phishing sites and emails poses significant challenges\nto existing cybersecurity efforts. Despite advances in malicious email filters\nand email security protocols, problems with oversight and false positives\npersist. Users often struggle to understand why emails are flagged as\npotentially fraudulent, risking the possibility of missing important\ncommunications or mistakenly trusting deceptive phishing emails. This study\nintroduces ChatSpamDetector, a system that uses large language models (LLMs) to\ndetect phishing emails. By converting email data into a prompt suitable for LLM\nanalysis, the system provides a highly accurate determination of whether an\nemail is phishing or not. Importantly, it offers detailed reasoning for its\nphishing determinations, assisting users in making informed decisions about how\nto handle suspicious emails. We conducted an evaluation using a comprehensive\nphishing email dataset and compared our system to several LLMs and baseline\nsystems. We confirmed that our system using GPT-4 has superior detection\ncapabilities with an accuracy of 99.70%. Advanced contextual interpretation by\nLLMs enables the identification of various phishing tactics and impersonations,\nmaking them a potentially powerful tool in the fight against email-based\nphishing threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of phishing sites and emails poses significant challenges\nto existing cybersecurity efforts. Despite advances in malicious email filters\nand email security protocols, problems with oversight and false positives\npersist. Users often struggle to understand why emails are flagged as\npotentially fraudulent, risking the possibility of missing important\ncommunications or mistakenly trusting deceptive phishing emails. This study\nintroduces ChatSpamDetector, a system that uses large language models (LLMs) to\ndetect phishing emails. By converting email data into a prompt suitable for LLM\nanalysis, the system provides a highly accurate determination of whether an\nemail is phishing or not. Importantly, it offers detailed reasoning for its\nphishing determinations, assisting users in making informed decisions about how\nto handle suspicious emails. We conducted an evaluation using a comprehensive\nphishing email dataset and compared our system to several LLMs and baseline\nsystems. We confirmed that our system using GPT-4 has superior detection\ncapabilities with an accuracy of 99.70%. Advanced contextual interpretation by\nLLMs enables the identification of various phishing tactics and impersonations,\nmaking them a potentially powerful tool in the fight against email-based\nphishing threats."
                },
                "authors": [
                    {
                        "name": "Takashi Koide"
                    },
                    {
                        "name": "Naoki Fukushi"
                    },
                    {
                        "name": "Hiroki Nakano"
                    },
                    {
                        "name": "Daiki Chiba"
                    }
                ],
                "author_detail": {
                    "name": "Daiki Chiba"
                },
                "author": "Daiki Chiba",
                "arxiv_comment": "Accepted at SecureComm 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.18093v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.18093v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12837v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12837v1",
                "updated": "2024-08-23T04:54:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    4,
                    54,
                    18,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T04:54:18Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    4,
                    54,
                    18,
                    4,
                    236,
                    0
                ],
                "title": "Underwater SONAR Image Classification and Analysis using LIME-based\n  Explainable Artificial Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Underwater SONAR Image Classification and Analysis using LIME-based\n  Explainable Artificial Intelligence"
                },
                "summary": "Deep learning techniques have revolutionized image classification by\nmimicking human cognition and automating complex decision-making processes.\nHowever, the deployment of AI systems in the wild, especially in high-security\ndomains such as defence, is curbed by the lack of explainability of the model.\nTo this end, eXplainable AI (XAI) is an emerging area of research that is\nintended to explore the unexplained hidden black box nature of deep neural\nnetworks. This paper explores the application of the eXplainable Artificial\nIntelligence (XAI) tool to interpret the underwater image classification\nresults, one of the first works in the domain to the best of our knowledge. Our\nstudy delves into the realm of SONAR image classification using a custom\ndataset derived from diverse sources, including the Seabed Objects KLSG\ndataset, the camera SONAR dataset, the mine SONAR images dataset, and the SCTD\ndataset. An extensive analysis of transfer learning techniques for image\nclassification using benchmark Convolutional Neural Network (CNN) architectures\nsuch as VGG16, ResNet50, InceptionV3, DenseNet121, etc. is carried out. On top\nof this classification model, a post-hoc XAI technique, viz. Local\nInterpretable Model-Agnostic Explanations (LIME) are incorporated to provide\ntransparent justifications for the model's decisions by perturbing input data\nlocally to see how predictions change. Furthermore, Submodular Picks LIME\n(SP-LIME) a version of LIME particular to images, that perturbs the image based\non the submodular picks is also extensively studied. To this end, two\nsubmodular optimization algorithms i.e. Quickshift and Simple Linear Iterative\nClustering (SLIC) are leveraged towards submodular picks. The extensive\nanalysis of XAI techniques highlights interpretability of the results in a more\nhuman-compliant way, thus boosting our confidence and reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning techniques have revolutionized image classification by\nmimicking human cognition and automating complex decision-making processes.\nHowever, the deployment of AI systems in the wild, especially in high-security\ndomains such as defence, is curbed by the lack of explainability of the model.\nTo this end, eXplainable AI (XAI) is an emerging area of research that is\nintended to explore the unexplained hidden black box nature of deep neural\nnetworks. This paper explores the application of the eXplainable Artificial\nIntelligence (XAI) tool to interpret the underwater image classification\nresults, one of the first works in the domain to the best of our knowledge. Our\nstudy delves into the realm of SONAR image classification using a custom\ndataset derived from diverse sources, including the Seabed Objects KLSG\ndataset, the camera SONAR dataset, the mine SONAR images dataset, and the SCTD\ndataset. An extensive analysis of transfer learning techniques for image\nclassification using benchmark Convolutional Neural Network (CNN) architectures\nsuch as VGG16, ResNet50, InceptionV3, DenseNet121, etc. is carried out. On top\nof this classification model, a post-hoc XAI technique, viz. Local\nInterpretable Model-Agnostic Explanations (LIME) are incorporated to provide\ntransparent justifications for the model's decisions by perturbing input data\nlocally to see how predictions change. Furthermore, Submodular Picks LIME\n(SP-LIME) a version of LIME particular to images, that perturbs the image based\non the submodular picks is also extensively studied. To this end, two\nsubmodular optimization algorithms i.e. Quickshift and Simple Linear Iterative\nClustering (SLIC) are leveraged towards submodular picks. The extensive\nanalysis of XAI techniques highlights interpretability of the results in a more\nhuman-compliant way, thus boosting our confidence and reliability."
                },
                "authors": [
                    {
                        "name": "Purushothaman Natarajan"
                    },
                    {
                        "name": "Athira Nambiar"
                    }
                ],
                "author_detail": {
                    "name": "Athira Nambiar"
                },
                "author": "Athira Nambiar",
                "arxiv_comment": "55 pages, 9 tables, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12837v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12837v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07 (Primary) 68T45, 68U10 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.8; I.2.10; I.5.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12834v1",
                "updated": "2024-08-23T04:44:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    4,
                    44,
                    5,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T04:44:05Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    4,
                    44,
                    5,
                    4,
                    236,
                    0
                ],
                "title": "CLLMFS: A Contrastive Learning enhanced Large Language Model Framework\n  for Few-Shot Named Entity Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLLMFS: A Contrastive Learning enhanced Large Language Model Framework\n  for Few-Shot Named Entity Recognition"
                },
                "summary": "Few-shot Named Entity Recognition (NER), the task of identifying named\nentities with only a limited amount of labeled data, has gained increasing\nsignificance in natural language processing. While existing methodologies have\nshown some effectiveness, such as enriching label semantics through various\nprompting modes or employing metric learning techniques, their performance\nexhibits limited robustness across diverse domains due to the lack of rich\nknowledge in their pre-trained models. To address this issue, we propose\nCLLMFS, a Contrastive Learning enhanced Large Language Model (LLM) Framework\nfor Few-Shot Named Entity Recognition, achieving promising results with limited\ntraining data. Considering the impact of LLM's internal representations on\ndownstream tasks, CLLMFS integrates Low-Rank Adaptation (LoRA) and contrastive\nlearning mechanisms specifically tailored for few-shot NER. By enhancing the\nmodel's internal representations, CLLMFS effectively improves both entity\nboundary awareness ability and entity recognition accuracy. Our method has\nachieved state-of-the-art performance improvements on F1-score ranging from\n2.58\\% to 97.74\\% over existing best-performing methods across several\nrecognized benchmarks. Furthermore, through cross-domain NER experiments\nconducted on multiple datasets, we have further validated the robust\ngeneralization capability of our method. Our code will be released in the near\nfuture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot Named Entity Recognition (NER), the task of identifying named\nentities with only a limited amount of labeled data, has gained increasing\nsignificance in natural language processing. While existing methodologies have\nshown some effectiveness, such as enriching label semantics through various\nprompting modes or employing metric learning techniques, their performance\nexhibits limited robustness across diverse domains due to the lack of rich\nknowledge in their pre-trained models. To address this issue, we propose\nCLLMFS, a Contrastive Learning enhanced Large Language Model (LLM) Framework\nfor Few-Shot Named Entity Recognition, achieving promising results with limited\ntraining data. Considering the impact of LLM's internal representations on\ndownstream tasks, CLLMFS integrates Low-Rank Adaptation (LoRA) and contrastive\nlearning mechanisms specifically tailored for few-shot NER. By enhancing the\nmodel's internal representations, CLLMFS effectively improves both entity\nboundary awareness ability and entity recognition accuracy. Our method has\nachieved state-of-the-art performance improvements on F1-score ranging from\n2.58\\% to 97.74\\% over existing best-performing methods across several\nrecognized benchmarks. Furthermore, through cross-domain NER experiments\nconducted on multiple datasets, we have further validated the robust\ngeneralization capability of our method. Our code will be released in the near\nfuture."
                },
                "authors": [
                    {
                        "name": "Yafeng Zhang"
                    },
                    {
                        "name": "Zilan Yu"
                    },
                    {
                        "name": "Yuang Huang"
                    },
                    {
                        "name": "Jing Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Tang"
                },
                "author": "Jing Tang",
                "arxiv_comment": "27TH EUROPEAN CONFERENCE ON ARTIFICIAL INTELLIGENCE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12832v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12832v1",
                "updated": "2024-08-23T04:28:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    4,
                    28,
                    56,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T04:28:56Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    4,
                    28,
                    56,
                    4,
                    236,
                    0
                ],
                "title": "LIMP: Large Language Model Enhanced Intent-aware Mobility Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LIMP: Large Language Model Enhanced Intent-aware Mobility Prediction"
                },
                "summary": "Human mobility prediction is essential for applications like urban planning\nand transportation management, yet it remains challenging due to the complex,\noften implicit, intentions behind human behavior. Existing models predominantly\nfocus on spatiotemporal patterns, paying less attention to the underlying\nintentions that govern movements. Recent advancements in large language models\n(LLMs) offer a promising alternative research angle for integrating commonsense\nreasoning into mobility prediction. However, it is a non-trivial problem\nbecause LLMs are not natively built for mobility intention inference, and they\nalso face scalability issues and integration difficulties with spatiotemporal\nmodels. To address these challenges, we propose a novel LIMP (LLMs for\nIntent-ware Mobility Prediction) framework. Specifically, LIMP introduces an\n\"Analyze-Abstract-Infer\" (A2I) agentic workflow to unleash LLM's commonsense\nreasoning power for mobility intention inference. Besides, we design an\nefficient fine-tuning scheme to transfer reasoning power from commercial LLM to\nsmaller-scale, open-source language model, ensuring LIMP's scalability to\nmillions of mobility records. Moreover, we propose a transformer-based\nintention-aware mobility prediction model to effectively harness the intention\ninference ability of LLM. Evaluated on two real-world datasets, LIMP\nsignificantly outperforms baseline models, demonstrating improved accuracy in\nnext-location prediction and effective intention inference. The\ninterpretability of intention-aware mobility prediction highlights our LIMP\nframework's potential for real-world applications. Codes and data can be found\nin https://github.com/tsinghua-fib-lab/LIMP .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human mobility prediction is essential for applications like urban planning\nand transportation management, yet it remains challenging due to the complex,\noften implicit, intentions behind human behavior. Existing models predominantly\nfocus on spatiotemporal patterns, paying less attention to the underlying\nintentions that govern movements. Recent advancements in large language models\n(LLMs) offer a promising alternative research angle for integrating commonsense\nreasoning into mobility prediction. However, it is a non-trivial problem\nbecause LLMs are not natively built for mobility intention inference, and they\nalso face scalability issues and integration difficulties with spatiotemporal\nmodels. To address these challenges, we propose a novel LIMP (LLMs for\nIntent-ware Mobility Prediction) framework. Specifically, LIMP introduces an\n\"Analyze-Abstract-Infer\" (A2I) agentic workflow to unleash LLM's commonsense\nreasoning power for mobility intention inference. Besides, we design an\nefficient fine-tuning scheme to transfer reasoning power from commercial LLM to\nsmaller-scale, open-source language model, ensuring LIMP's scalability to\nmillions of mobility records. Moreover, we propose a transformer-based\nintention-aware mobility prediction model to effectively harness the intention\ninference ability of LLM. Evaluated on two real-world datasets, LIMP\nsignificantly outperforms baseline models, demonstrating improved accuracy in\nnext-location prediction and effective intention inference. The\ninterpretability of intention-aware mobility prediction highlights our LIMP\nframework's potential for real-world applications. Codes and data can be found\nin https://github.com/tsinghua-fib-lab/LIMP ."
                },
                "authors": [
                    {
                        "name": "Songwei Li"
                    },
                    {
                        "name": "Jie Feng"
                    },
                    {
                        "name": "Jiawei Chi"
                    },
                    {
                        "name": "Xinyuan Hu"
                    },
                    {
                        "name": "Xiaomeng Zhao"
                    },
                    {
                        "name": "Fengli Xu"
                    }
                ],
                "author_detail": {
                    "name": "Fengli Xu"
                },
                "author": "Fengli Xu",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12832v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12832v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11824v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11824v2",
                "updated": "2024-08-23T04:13:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    4,
                    13,
                    48,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-05T06:31:39Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    6,
                    31,
                    39,
                    0,
                    218,
                    0
                ],
                "title": "AppAgent v2: Advanced Agent for Flexible Mobile Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AppAgent v2: Advanced Agent for Flexible Mobile Interactions"
                },
                "summary": "With the advancement of Multimodal Large Language Models (MLLM), LLM-driven\nvisual agents are increasingly impacting software interfaces, particularly\nthose with graphical user interfaces. This work introduces a novel LLM-based\nmultimodal agent framework for mobile devices. This framework, capable of\nnavigating mobile devices, emulates human-like interactions. Our agent\nconstructs a flexible action space that enhances adaptability across various\napplications including parser, text and vision descriptions. The agent operates\nthrough two main phases: exploration and deployment. During the exploration\nphase, functionalities of user interface elements are documented either through\nagent-driven or manual explorations into a customized structured knowledge\nbase. In the deployment phase, RAG technology enables efficient retrieval and\nupdate from this knowledge base, thereby empowering the agent to perform tasks\neffectively and accurately. This includes performing complex, multi-step\noperations across various applications, thereby demonstrating the framework's\nadaptability and precision in handling customized task workflows. Our\nexperimental results across various benchmarks demonstrate the framework's\nsuperior performance, confirming its effectiveness in real-world scenarios. Our\ncode will be open source soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancement of Multimodal Large Language Models (MLLM), LLM-driven\nvisual agents are increasingly impacting software interfaces, particularly\nthose with graphical user interfaces. This work introduces a novel LLM-based\nmultimodal agent framework for mobile devices. This framework, capable of\nnavigating mobile devices, emulates human-like interactions. Our agent\nconstructs a flexible action space that enhances adaptability across various\napplications including parser, text and vision descriptions. The agent operates\nthrough two main phases: exploration and deployment. During the exploration\nphase, functionalities of user interface elements are documented either through\nagent-driven or manual explorations into a customized structured knowledge\nbase. In the deployment phase, RAG technology enables efficient retrieval and\nupdate from this knowledge base, thereby empowering the agent to perform tasks\neffectively and accurately. This includes performing complex, multi-step\noperations across various applications, thereby demonstrating the framework's\nadaptability and precision in handling customized task workflows. Our\nexperimental results across various benchmarks demonstrate the framework's\nsuperior performance, confirming its effectiveness in real-world scenarios. Our\ncode will be open source soon."
                },
                "authors": [
                    {
                        "name": "Yanda Li"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Wanqi Yang"
                    },
                    {
                        "name": "Bin Fu"
                    },
                    {
                        "name": "Pei Cheng"
                    },
                    {
                        "name": "Xin Chen"
                    },
                    {
                        "name": "Ling Chen"
                    },
                    {
                        "name": "Yunchao Wei"
                    }
                ],
                "author_detail": {
                    "name": "Yunchao Wei"
                },
                "author": "Yunchao Wei",
                "arxiv_comment": "Pre-print version, some content needs to be supplemented",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11824v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11824v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00276v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00276v4",
                "updated": "2024-08-23T04:06:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    4,
                    6,
                    41,
                    4,
                    236,
                    0
                ],
                "published": "2024-03-30T08:02:16Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    8,
                    2,
                    16,
                    5,
                    90,
                    0
                ],
                "title": "Instruction-Driven Game Engines on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-Driven Game Engines on Large Language Models"
                },
                "summary": "The Instruction-Driven Game Engine (IDGE) project aims to democratize game\ndevelopment by enabling a large language model (LLM) to follow free-form game\nrules and autonomously generate game-play processes. The IDGE allows users to\ncreate games by issuing simple natural language instructions, which\nsignificantly lowers the barrier for game development. We approach the learning\nprocess for IDGEs as a Next State Prediction task, wherein the model\nautoregressively predicts in-game states given player actions. It is a\nchallenging task because the computation of in-game states must be precise;\notherwise, slight errors could disrupt the game-play. To address this, we train\nthe IDGE in a curriculum manner that progressively increases the model's\nexposure to complex scenarios. Our initial progress lies in developing an IDGE\nfor Poker, a universally cherished card game. The engine we've designed not\nonly supports a wide range of poker variants but also allows for high\ncustomization of rules through natural language inputs. Furthermore, it also\nfavors rapid prototyping of new games from minimal samples, proposing an\ninnovative paradigm in game development that relies on minimal prompt and data\nengineering. This work lays the groundwork for future advancements in\ninstruction-driven game creation, potentially transforming how games are\ndesigned and played.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Instruction-Driven Game Engine (IDGE) project aims to democratize game\ndevelopment by enabling a large language model (LLM) to follow free-form game\nrules and autonomously generate game-play processes. The IDGE allows users to\ncreate games by issuing simple natural language instructions, which\nsignificantly lowers the barrier for game development. We approach the learning\nprocess for IDGEs as a Next State Prediction task, wherein the model\nautoregressively predicts in-game states given player actions. It is a\nchallenging task because the computation of in-game states must be precise;\notherwise, slight errors could disrupt the game-play. To address this, we train\nthe IDGE in a curriculum manner that progressively increases the model's\nexposure to complex scenarios. Our initial progress lies in developing an IDGE\nfor Poker, a universally cherished card game. The engine we've designed not\nonly supports a wide range of poker variants but also allows for high\ncustomization of rules through natural language inputs. Furthermore, it also\nfavors rapid prototyping of new games from minimal samples, proposing an\ninnovative paradigm in game development that relies on minimal prompt and data\nengineering. This work lays the groundwork for future advancements in\ninstruction-driven game creation, potentially transforming how games are\ndesigned and played."
                },
                "authors": [
                    {
                        "name": "Hongqiu Wu"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Xingyuan Liu"
                    },
                    {
                        "name": "Hai Zhao"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00276v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00276v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05842v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05842v2",
                "updated": "2024-08-23T03:57:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    3,
                    57,
                    24,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-11T18:32:29Z",
                "published_parsed": [
                    2024,
                    8,
                    11,
                    18,
                    32,
                    29,
                    6,
                    224,
                    0
                ],
                "title": "Scaling Virtual World with Delta-Engine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Virtual World with Delta-Engine"
                },
                "summary": "In this paper, we focus on the \\emph{virtual world}, a cyberspace where\npeople can live in. An ideal virtual world shares great similarity with our\nreal world. One of the crucial aspects is its evolving nature, reflected by\nindividuals' capability to grow and thereby influence the objective world. Such\ndynamics is unpredictable and beyond the reach of existing systems. For this,\nwe propose a special engine called \\textbf{\\emph{Delta-Engine}} to drive this\nvirtual world. $\\Delta$ associates the world's evolution to the engine's\nscalability. It consists of a base engine and a neural proxy. The base engine\nprograms the prototype of the virtual world; given a trigger, the neural proxy\ngenerates new snippets on the base engine through \\emph{incremental\nprediction}.\n  This paper presents a full-stack introduction to the delta-engine. The key\nfeature of the delta-engine is its scalability to unknown elements within the\nworld, Technically, it derives from the prefect co-work of the neural proxy and\nthe base engine, and the alignment with high-quality data. We introduce an\nengine-oriented fine-tuning method that embeds the base engine into the proxy.\nWe then discuss the human-LLM collaborative design to produce novel and\ninteresting data efficiently. Eventually, we propose three evaluation\nprinciples to comprehensively assess the performance of a delta engine: naive\nevaluation, incremental evaluation, and adversarial evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we focus on the \\emph{virtual world}, a cyberspace where\npeople can live in. An ideal virtual world shares great similarity with our\nreal world. One of the crucial aspects is its evolving nature, reflected by\nindividuals' capability to grow and thereby influence the objective world. Such\ndynamics is unpredictable and beyond the reach of existing systems. For this,\nwe propose a special engine called \\textbf{\\emph{Delta-Engine}} to drive this\nvirtual world. $\\Delta$ associates the world's evolution to the engine's\nscalability. It consists of a base engine and a neural proxy. The base engine\nprograms the prototype of the virtual world; given a trigger, the neural proxy\ngenerates new snippets on the base engine through \\emph{incremental\nprediction}.\n  This paper presents a full-stack introduction to the delta-engine. The key\nfeature of the delta-engine is its scalability to unknown elements within the\nworld, Technically, it derives from the prefect co-work of the neural proxy and\nthe base engine, and the alignment with high-quality data. We introduce an\nengine-oriented fine-tuning method that embeds the base engine into the proxy.\nWe then discuss the human-LLM collaborative design to produce novel and\ninteresting data efficiently. Eventually, we propose three evaluation\nprinciples to comprehensively assess the performance of a delta engine: naive\nevaluation, incremental evaluation, and adversarial evaluation."
                },
                "authors": [
                    {
                        "name": "Hongqiu Wu"
                    },
                    {
                        "name": "Zekai Xu"
                    },
                    {
                        "name": "Tianyang Xu"
                    },
                    {
                        "name": "Jiale Hong"
                    },
                    {
                        "name": "Weiqi Wu"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Hai Zhao"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Zhezhi He"
                    }
                ],
                "author_detail": {
                    "name": "Zhezhi He"
                },
                "author": "Zhezhi He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05842v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05842v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12821v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12821v1",
                "updated": "2024-08-23T03:45:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    3,
                    45,
                    31,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T03:45:31Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    3,
                    45,
                    31,
                    4,
                    236,
                    0
                ],
                "title": "Examining the Commitments and Difficulties Inherent in Multimodal\n  Foundation Models for Street View Imagery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Examining the Commitments and Difficulties Inherent in Multimodal\n  Foundation Models for Street View Imagery"
                },
                "summary": "The emergence of Large Language Models (LLMs) and multimodal foundation\nmodels (FMs) has generated heightened interest in their applications that\nintegrate vision and language. This paper investigates the capabilities of\nChatGPT-4V and Gemini Pro for Street View Imagery, Built Environment, and\nInterior by evaluating their performance across various tasks. The assessments\ninclude street furniture identification, pedestrian and car counts, and road\nwidth measurement in Street View Imagery; building function classification,\nbuilding age analysis, building height analysis, and building structure\nclassification in the Built Environment; and interior room classification,\ninterior design style analysis, interior furniture counts, and interior length\nmeasurement in Interior. The results reveal proficiency in length measurement,\nstyle analysis, question answering, and basic image understanding, but\nhighlight limitations in detailed recognition and counting tasks. While\nzero-shot learning shows potential, performance varies depending on the problem\ndomains and image complexities. This study provides new insights into the\nstrengths and weaknesses of multimodal foundation models for practical\nchallenges in Street View Imagery, Built Environment, and Interior. Overall,\nthe findings demonstrate foundational multimodal intelligence, emphasizing the\npotential of FMs to drive forward interdisciplinary applications at the\nintersection of computer vision and language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of Large Language Models (LLMs) and multimodal foundation\nmodels (FMs) has generated heightened interest in their applications that\nintegrate vision and language. This paper investigates the capabilities of\nChatGPT-4V and Gemini Pro for Street View Imagery, Built Environment, and\nInterior by evaluating their performance across various tasks. The assessments\ninclude street furniture identification, pedestrian and car counts, and road\nwidth measurement in Street View Imagery; building function classification,\nbuilding age analysis, building height analysis, and building structure\nclassification in the Built Environment; and interior room classification,\ninterior design style analysis, interior furniture counts, and interior length\nmeasurement in Interior. The results reveal proficiency in length measurement,\nstyle analysis, question answering, and basic image understanding, but\nhighlight limitations in detailed recognition and counting tasks. While\nzero-shot learning shows potential, performance varies depending on the problem\ndomains and image complexities. This study provides new insights into the\nstrengths and weaknesses of multimodal foundation models for practical\nchallenges in Street View Imagery, Built Environment, and Interior. Overall,\nthe findings demonstrate foundational multimodal intelligence, emphasizing the\npotential of FMs to drive forward interdisciplinary applications at the\nintersection of computer vision and language."
                },
                "authors": [
                    {
                        "name": "Zhenyuan Yang"
                    },
                    {
                        "name": "Xuhui Lin"
                    },
                    {
                        "name": "Qinyi He"
                    },
                    {
                        "name": "Ziye Huang"
                    },
                    {
                        "name": "Zhengliang Liu"
                    },
                    {
                        "name": "Hanqi Jiang"
                    },
                    {
                        "name": "Peng Shu"
                    },
                    {
                        "name": "Zihao Wu"
                    },
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Stephen Law"
                    },
                    {
                        "name": "Gengchen Mai"
                    },
                    {
                        "name": "Tianming Liu"
                    },
                    {
                        "name": "Tao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tao Yang"
                },
                "author": "Tao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12821v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12821v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11609v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11609v2",
                "updated": "2024-08-23T03:40:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    3,
                    40,
                    44,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-21T13:34:29Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    13,
                    34,
                    29,
                    2,
                    234,
                    0
                ],
                "title": "Xinyu: An Efficient LLM-based System for Commentary Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Xinyu: An Efficient LLM-based System for Commentary Generation"
                },
                "summary": "Commentary provides readers with a deep understanding of events by presenting\ndiverse arguments and evidence. However, creating commentary is a\ntime-consuming task, even for skilled commentators. Large language models\n(LLMs) have simplified the process of natural language generation, but their\ndirect application in commentary creation still faces challenges due to unique\ntask requirements. These requirements can be categorized into two levels: 1)\nfundamental requirements, which include creating well-structured and logically\nconsistent narratives, and 2) advanced requirements, which involve generating\nquality arguments and providing convincing evidence. In this paper, we\nintroduce Xinyu, an efficient LLM-based system designed to assist commentators\nin generating Chinese commentaries. To meet the fundamental requirements, we\ndeconstruct the generation process into sequential steps, proposing targeted\nstrategies and supervised fine-tuning (SFT) for each step. To address the\nadvanced requirements, we present an argument ranking model for arguments and\nestablish a comprehensive evidence database that includes up-to-date events and\nclassic books, thereby strengthening the substantiation of the evidence with\nretrieval augmented generation (RAG) technology. To evaluate the generated\ncommentaries more fairly, corresponding to the two-level requirements, we\nintroduce a comprehensive evaluation metric that considers five distinct\nperspectives in commentary generation. Our experiments confirm the\neffectiveness of our proposed system. We also observe a significant increase in\nthe efficiency of commentators in real-world scenarios, with the average time\nspent on creating a commentary dropping from 4 hours to 20 minutes.\nImportantly, such an increase in efficiency does not compromise the quality of\nthe commentaries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Commentary provides readers with a deep understanding of events by presenting\ndiverse arguments and evidence. However, creating commentary is a\ntime-consuming task, even for skilled commentators. Large language models\n(LLMs) have simplified the process of natural language generation, but their\ndirect application in commentary creation still faces challenges due to unique\ntask requirements. These requirements can be categorized into two levels: 1)\nfundamental requirements, which include creating well-structured and logically\nconsistent narratives, and 2) advanced requirements, which involve generating\nquality arguments and providing convincing evidence. In this paper, we\nintroduce Xinyu, an efficient LLM-based system designed to assist commentators\nin generating Chinese commentaries. To meet the fundamental requirements, we\ndeconstruct the generation process into sequential steps, proposing targeted\nstrategies and supervised fine-tuning (SFT) for each step. To address the\nadvanced requirements, we present an argument ranking model for arguments and\nestablish a comprehensive evidence database that includes up-to-date events and\nclassic books, thereby strengthening the substantiation of the evidence with\nretrieval augmented generation (RAG) technology. To evaluate the generated\ncommentaries more fairly, corresponding to the two-level requirements, we\nintroduce a comprehensive evaluation metric that considers five distinct\nperspectives in commentary generation. Our experiments confirm the\neffectiveness of our proposed system. We also observe a significant increase in\nthe efficiency of commentators in real-world scenarios, with the average time\nspent on creating a commentary dropping from 4 hours to 20 minutes.\nImportantly, such an increase in efficiency does not compromise the quality of\nthe commentaries."
                },
                "authors": [
                    {
                        "name": "Yiquan Wu"
                    },
                    {
                        "name": "Bo Tang"
                    },
                    {
                        "name": "Chenyang Xi"
                    },
                    {
                        "name": "Yu Yu"
                    },
                    {
                        "name": "Pengyu Wang"
                    },
                    {
                        "name": "Yifei Liu"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Haiying Deng"
                    },
                    {
                        "name": "Zhiyu Li"
                    },
                    {
                        "name": "Feiyu Xiong"
                    },
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Zhonghao Wang"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Yi Luo"
                    },
                    {
                        "name": "Mingchuan Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mingchuan Yang"
                },
                "author": "Mingchuan Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11609v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11609v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.00380v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.00380v2",
                "updated": "2024-08-23T03:39:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    3,
                    39,
                    57,
                    4,
                    236,
                    0
                ],
                "published": "2024-06-01T09:36:16Z",
                "published_parsed": [
                    2024,
                    6,
                    1,
                    9,
                    36,
                    16,
                    5,
                    153,
                    0
                ],
                "title": "The Best of Both Worlds: Toward an Honest and Helpful Large Language\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Best of Both Worlds: Toward an Honest and Helpful Large Language\n  Model"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success across various\nindustries due to their exceptional generative capabilities. However, for safe\nand effective real-world deployments, ensuring honesty and helpfulness is\ncritical. This paper addresses the question: Can we prioritize the helpfulness\nof LLMs while preserving their honesty? To begin with, we establish exhaustive\nprinciples aimed at guaranteeing the honesty of LLM. Additionally, we introduce\na novel dataset, referred to as HoneSet, comprising 930 queries spanning six\ncategories meticulously crafted to assess an LLM's capacity for maintaining\nhonesty. Subsequently, we present two approaches to augmenting honesty and\nhelpfulness in LLMs: a training-free enhancement and a fine-tuning-based\nimprovement. The training-free approach, which is based on curiosity-driven\nprompting, empowers LLMs to articulate internal confusion and uncertainty\nregarding queries, thereby optimizing their responses. Conversely, the\nfine-tuning-based method employs a two-stage process inspired by curriculum\nlearning: initially instructing LLMs to discern between honest and dishonest\nresponses, then refining their training to enhance helpfulness. Experiments\nconducted on nine prominent LLMs demonstrate a significant improvement in\nalignment with honesty across all models through the implementation of our\nproposed enhancements. Particularly noteworthy is the 65.3% enhancement\nobserved in Llama3-8b and the remarkable 124.7% improvement in Mistral-7b, as\nmeasured by the H$^{2}$ (honest and helpful) assessment. We believe that our\nwork can pave the way for developing more trustworthy LLMs for real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success across various\nindustries due to their exceptional generative capabilities. However, for safe\nand effective real-world deployments, ensuring honesty and helpfulness is\ncritical. This paper addresses the question: Can we prioritize the helpfulness\nof LLMs while preserving their honesty? To begin with, we establish exhaustive\nprinciples aimed at guaranteeing the honesty of LLM. Additionally, we introduce\na novel dataset, referred to as HoneSet, comprising 930 queries spanning six\ncategories meticulously crafted to assess an LLM's capacity for maintaining\nhonesty. Subsequently, we present two approaches to augmenting honesty and\nhelpfulness in LLMs: a training-free enhancement and a fine-tuning-based\nimprovement. The training-free approach, which is based on curiosity-driven\nprompting, empowers LLMs to articulate internal confusion and uncertainty\nregarding queries, thereby optimizing their responses. Conversely, the\nfine-tuning-based method employs a two-stage process inspired by curriculum\nlearning: initially instructing LLMs to discern between honest and dishonest\nresponses, then refining their training to enhance helpfulness. Experiments\nconducted on nine prominent LLMs demonstrate a significant improvement in\nalignment with honesty across all models through the implementation of our\nproposed enhancements. Particularly noteworthy is the 65.3% enhancement\nobserved in Llama3-8b and the remarkable 124.7% improvement in Mistral-7b, as\nmeasured by the H$^{2}$ (honest and helpful) assessment. We believe that our\nwork can pave the way for developing more trustworthy LLMs for real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Chujie Gao"
                    },
                    {
                        "name": "Qihui Zhang"
                    },
                    {
                        "name": "Dongping Chen"
                    },
                    {
                        "name": "Yue Huang"
                    },
                    {
                        "name": "Siyuan Wu"
                    },
                    {
                        "name": "Zhengyan Fu"
                    },
                    {
                        "name": "Yao Wan"
                    },
                    {
                        "name": "Xiangliang Zhang"
                    },
                    {
                        "name": "Lichao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Lichao Sun"
                },
                "author": "Lichao Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.00380v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.00380v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12815v1",
                "updated": "2024-08-23T03:21:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    3,
                    21,
                    51,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T03:21:51Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    3,
                    21,
                    51,
                    4,
                    236,
                    0
                ],
                "title": "Staircase Cascaded Fusion of Lightweight Local Pattern Recognition and\n  Long-Range Dependencies for Structural Crack Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Staircase Cascaded Fusion of Lightweight Local Pattern Recognition and\n  Long-Range Dependencies for Structural Crack Segmentation"
                },
                "summary": "Detecting cracks with pixel-level precision for key structures is a\nsignificant challenge, as existing methods struggle to effectively integrate\nlocal textures and pixel dependencies of cracks. Furthermore, these methods\noften possess numerous parameters and substantial computational requirements,\ncomplicating deployment on edge devices. In this paper, we propose a staircase\ncascaded fusion crack segmentation network (CrackSCF) that generates\nhigh-quality crack segmentation maps using minimal computational resources. We\nconstructed a staircase cascaded fusion module that effectively captures local\npatterns of cracks and long-range dependencies of pixels, and it can suppress\nbackground noise well. To reduce the computational resources required by the\nmodel, we introduced a lightweight convolution block, which replaces all\nconvolution operations in the network, significantly reducing the required\ncomputation and parameters without affecting the network's performance. To\nevaluate our method, we created a challenging benchmark dataset called TUT and\nconducted experiments on this dataset and five other public datasets. The\nexperimental results indicate that our method offers significant advantages\nover existing methods, especially in handling background noise interference and\ndetailed crack segmentation. The F1 and mIoU scores on the TUT dataset are\n0.8382 and 0.8473, respectively, achieving state-of-the-art (SOTA) performance\nwhile requiring the least computational resources. The code and dataset is\navailable at https://github.com/Karl1109/CrackSCF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting cracks with pixel-level precision for key structures is a\nsignificant challenge, as existing methods struggle to effectively integrate\nlocal textures and pixel dependencies of cracks. Furthermore, these methods\noften possess numerous parameters and substantial computational requirements,\ncomplicating deployment on edge devices. In this paper, we propose a staircase\ncascaded fusion crack segmentation network (CrackSCF) that generates\nhigh-quality crack segmentation maps using minimal computational resources. We\nconstructed a staircase cascaded fusion module that effectively captures local\npatterns of cracks and long-range dependencies of pixels, and it can suppress\nbackground noise well. To reduce the computational resources required by the\nmodel, we introduced a lightweight convolution block, which replaces all\nconvolution operations in the network, significantly reducing the required\ncomputation and parameters without affecting the network's performance. To\nevaluate our method, we created a challenging benchmark dataset called TUT and\nconducted experiments on this dataset and five other public datasets. The\nexperimental results indicate that our method offers significant advantages\nover existing methods, especially in handling background noise interference and\ndetailed crack segmentation. The F1 and mIoU scores on the TUT dataset are\n0.8382 and 0.8473, respectively, achieving state-of-the-art (SOTA) performance\nwhile requiring the least computational resources. The code and dataset is\navailable at https://github.com/Karl1109/CrackSCF."
                },
                "authors": [
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Chen Jia"
                    },
                    {
                        "name": "Fan Shi"
                    },
                    {
                        "name": "Xu Cheng"
                    },
                    {
                        "name": "Mianzhao Wang"
                    },
                    {
                        "name": "Shengyong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Shengyong Chen"
                },
                "author": "Shengyong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12812v1",
                "updated": "2024-08-23T03:16:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    3,
                    16,
                    26,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T03:16:26Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    3,
                    16,
                    26,
                    4,
                    236,
                    0
                ],
                "title": "Grounding Fallacies Misrepresenting Scientific Publications in Evidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounding Fallacies Misrepresenting Scientific Publications in Evidence"
                },
                "summary": "Health-related misinformation claims often falsely cite a credible biomedical\npublication as evidence, which superficially appears to support the false\nclaim. The publication does not really support the claim, but a reader could\nbelieve it thanks to the use of logical fallacies. Here, we aim to detect and\nto highlight such fallacies, which requires carefully assessing the exact\ncontent of the misrepresented publications. To achieve this, we introduce\nMissciPlus, an extension of the fallacy detection dataset Missci. MissciPlus\nbuilds on Missci by grounding the applied fallacies in real-world passages from\nmisrepresented studies. This creates a realistic test-bed for detecting and\nverbalizing these fallacies under real-world input conditions, and enables\nnovel passage-retrieval tasks. MissciPlus is the first logical fallacy dataset\nwhich pairs the real-world misrepresented evidence with incorrect claims,\nidentical to the input to evidence-based fact-checking models. With MissciPlus,\nwe i) benchmark retrieval models in identifying passages that support claims\nonly when fallacies are applied, ii) evaluate how well LLMs articulate\nfallacious reasoning from misrepresented scientific passages, and iii) assess\nthe effectiveness of fact-checking models in refuting claims that misrepresent\nbiomedical research. Our findings show that current fact-checking models\nstruggle to use relevant passages from misrepresented publications to refute\nmisinformation. Moreover, these passages can mislead LLMs into accepting false\nclaims as true.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Health-related misinformation claims often falsely cite a credible biomedical\npublication as evidence, which superficially appears to support the false\nclaim. The publication does not really support the claim, but a reader could\nbelieve it thanks to the use of logical fallacies. Here, we aim to detect and\nto highlight such fallacies, which requires carefully assessing the exact\ncontent of the misrepresented publications. To achieve this, we introduce\nMissciPlus, an extension of the fallacy detection dataset Missci. MissciPlus\nbuilds on Missci by grounding the applied fallacies in real-world passages from\nmisrepresented studies. This creates a realistic test-bed for detecting and\nverbalizing these fallacies under real-world input conditions, and enables\nnovel passage-retrieval tasks. MissciPlus is the first logical fallacy dataset\nwhich pairs the real-world misrepresented evidence with incorrect claims,\nidentical to the input to evidence-based fact-checking models. With MissciPlus,\nwe i) benchmark retrieval models in identifying passages that support claims\nonly when fallacies are applied, ii) evaluate how well LLMs articulate\nfallacious reasoning from misrepresented scientific passages, and iii) assess\nthe effectiveness of fact-checking models in refuting claims that misrepresent\nbiomedical research. Our findings show that current fact-checking models\nstruggle to use relevant passages from misrepresented publications to refute\nmisinformation. Moreover, these passages can mislead LLMs into accepting false\nclaims as true."
                },
                "authors": [
                    {
                        "name": "Max Glockner"
                    },
                    {
                        "name": "Yufang Hou"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Iryna Gurevych"
                    }
                ],
                "author_detail": {
                    "name": "Iryna Gurevych"
                },
                "author": "Iryna Gurevych",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12806v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12806v1",
                "updated": "2024-08-23T02:56:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    2,
                    56,
                    13,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T02:56:13Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    2,
                    56,
                    13,
                    4,
                    236,
                    0
                ],
                "title": "Is Generative AI the Next Tactical Cyber Weapon For Threat Actors?\n  Unforeseen Implications of AI Generated Cyber Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Generative AI the Next Tactical Cyber Weapon For Threat Actors?\n  Unforeseen Implications of AI Generated Cyber Attacks"
                },
                "summary": "In an era where digital threats are increasingly sophisticated, the\nintersection of Artificial Intelligence and cybersecurity presents both\npromising defenses and potent dangers. This paper delves into the escalating\nthreat posed by the misuse of AI, specifically through the use of Large\nLanguage Models (LLMs). This study details various techniques like the switch\nmethod and character play method, which can be exploited by cybercriminals to\ngenerate and automate cyber attacks. Through a series of controlled\nexperiments, the paper demonstrates how these models can be manipulated to\nbypass ethical and privacy safeguards to effectively generate cyber attacks\nsuch as social engineering, malicious code, payload generation, and spyware. By\ntesting these AI generated attacks on live systems, the study assesses their\neffectiveness and the vulnerabilities they exploit, offering a practical\nperspective on the risks AI poses to critical infrastructure. We also introduce\nOccupy AI, a customized, finetuned LLM specifically engineered to automate and\nexecute cyberattacks. This specialized AI driven tool is adept at crafting\nsteps and generating executable code for a variety of cyber threats, including\nphishing, malware injection, and system exploitation. The results underscore\nthe urgency for ethical AI practices, robust cybersecurity measures, and\nregulatory oversight to mitigate AI related threats. This paper aims to elevate\nawareness within the cybersecurity community about the evolving digital threat\nlandscape, advocating for proactive defense strategies and responsible AI\ndevelopment to protect against emerging cyber threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In an era where digital threats are increasingly sophisticated, the\nintersection of Artificial Intelligence and cybersecurity presents both\npromising defenses and potent dangers. This paper delves into the escalating\nthreat posed by the misuse of AI, specifically through the use of Large\nLanguage Models (LLMs). This study details various techniques like the switch\nmethod and character play method, which can be exploited by cybercriminals to\ngenerate and automate cyber attacks. Through a series of controlled\nexperiments, the paper demonstrates how these models can be manipulated to\nbypass ethical and privacy safeguards to effectively generate cyber attacks\nsuch as social engineering, malicious code, payload generation, and spyware. By\ntesting these AI generated attacks on live systems, the study assesses their\neffectiveness and the vulnerabilities they exploit, offering a practical\nperspective on the risks AI poses to critical infrastructure. We also introduce\nOccupy AI, a customized, finetuned LLM specifically engineered to automate and\nexecute cyberattacks. This specialized AI driven tool is adept at crafting\nsteps and generating executable code for a variety of cyber threats, including\nphishing, malware injection, and system exploitation. The results underscore\nthe urgency for ethical AI practices, robust cybersecurity measures, and\nregulatory oversight to mitigate AI related threats. This paper aims to elevate\nawareness within the cybersecurity community about the evolving digital threat\nlandscape, advocating for proactive defense strategies and responsible AI\ndevelopment to protect against emerging cyber threats."
                },
                "authors": [
                    {
                        "name": "Yusuf Usman"
                    },
                    {
                        "name": "Aadesh Upadhyay"
                    },
                    {
                        "name": "Prashnna Gyawali"
                    },
                    {
                        "name": "Robin Chataut"
                    }
                ],
                "author_detail": {
                    "name": "Robin Chataut"
                },
                "author": "Robin Chataut",
                "arxiv_comment": "Journal Paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12806v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12806v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "Primary 03C90, Secondary 03-02,",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12798v1",
                "updated": "2024-08-23T02:21:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    2,
                    21,
                    21,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T02:21:21Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    2,
                    21,
                    21,
                    4,
                    236,
                    0
                ],
                "title": "BackdoorLLM: A Comprehensive Benchmark for Backdoor Attacks on Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BackdoorLLM: A Comprehensive Benchmark for Backdoor Attacks on Large\n  Language Models"
                },
                "summary": "Generative Large Language Models (LLMs) have made significant strides across\nvarious tasks, but they remain vulnerable to backdoor attacks, where specific\ntriggers in the prompt cause the LLM to generate adversary-desired responses.\nWhile most backdoor research has focused on vision or text classification\ntasks, backdoor attacks in text generation have been largely overlooked. In\nthis work, we introduce \\textit{BackdoorLLM}, the first comprehensive benchmark\nfor studying backdoor attacks on LLMs. \\textit{BackdoorLLM} features: 1) a\nrepository of backdoor benchmarks with a standardized training pipeline, 2)\ndiverse attack strategies, including data poisoning, weight poisoning, hidden\nstate attacks, and chain-of-thought attacks, 3) extensive evaluations with over\n200 experiments on 8 attacks across 7 scenarios and 6 model architectures, and\n4) key insights into the effectiveness and limitations of backdoors in LLMs. We\nhope \\textit{BackdoorLLM} will raise awareness of backdoor threats and\ncontribute to advancing AI safety. The code is available at\n\\url{https://github.com/bboylyg/BackdoorLLM}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Large Language Models (LLMs) have made significant strides across\nvarious tasks, but they remain vulnerable to backdoor attacks, where specific\ntriggers in the prompt cause the LLM to generate adversary-desired responses.\nWhile most backdoor research has focused on vision or text classification\ntasks, backdoor attacks in text generation have been largely overlooked. In\nthis work, we introduce \\textit{BackdoorLLM}, the first comprehensive benchmark\nfor studying backdoor attacks on LLMs. \\textit{BackdoorLLM} features: 1) a\nrepository of backdoor benchmarks with a standardized training pipeline, 2)\ndiverse attack strategies, including data poisoning, weight poisoning, hidden\nstate attacks, and chain-of-thought attacks, 3) extensive evaluations with over\n200 experiments on 8 attacks across 7 scenarios and 6 model architectures, and\n4) key insights into the effectiveness and limitations of backdoors in LLMs. We\nhope \\textit{BackdoorLLM} will raise awareness of backdoor threats and\ncontribute to advancing AI safety. The code is available at\n\\url{https://github.com/bboylyg/BackdoorLLM}."
                },
                "authors": [
                    {
                        "name": "Yige Li"
                    },
                    {
                        "name": "Hanxun Huang"
                    },
                    {
                        "name": "Yunhan Zhao"
                    },
                    {
                        "name": "Xingjun Ma"
                    },
                    {
                        "name": "Jun Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jun Sun"
                },
                "author": "Jun Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18558v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18558v3",
                "updated": "2024-08-23T02:00:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    2,
                    0,
                    27,
                    4,
                    236,
                    0
                ],
                "published": "2024-05-28T20:05:20Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    20,
                    5,
                    20,
                    1,
                    149,
                    0
                ],
                "title": "\"Golden Ratio Yoshimura\" for Meta-Stable and Massively Reconfigurable\n  Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Golden Ratio Yoshimura\" for Meta-Stable and Massively Reconfigurable\n  Deployment"
                },
                "summary": "Yoshimura origami is a classical folding pattern that has inspired many\ndeployable structure designs. Its applications span from space exploration,\nkinetic architectures, and soft robots to even everyday household items.\nHowever, despite its wide usage, Yoshimura has been fixated on a set of design\nconstraints to ensure its flat-foldability. Through extensive kinematic\nanalysis and prototype tests, this study presents a new Yoshimura that\nintentionally defies these constraints. Remarkably, one can impart a unique\nmeta-stability by using the Golden Ratio angle to define the triangular facets\nof a generalized Yoshimura. As a result, when its facets are strategically\npopped out, a ``Golden Ratio Yoshimura'' boom with $m$ modules can be\ntheoretically reconfigured into $8^m$ geometrically unique and load-bearing\nshapes. This result not only challenges the existing design norms but also\nopens up a new avenue to create deployable and versatile structural systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yoshimura origami is a classical folding pattern that has inspired many\ndeployable structure designs. Its applications span from space exploration,\nkinetic architectures, and soft robots to even everyday household items.\nHowever, despite its wide usage, Yoshimura has been fixated on a set of design\nconstraints to ensure its flat-foldability. Through extensive kinematic\nanalysis and prototype tests, this study presents a new Yoshimura that\nintentionally defies these constraints. Remarkably, one can impart a unique\nmeta-stability by using the Golden Ratio angle to define the triangular facets\nof a generalized Yoshimura. As a result, when its facets are strategically\npopped out, a ``Golden Ratio Yoshimura'' boom with $m$ modules can be\ntheoretically reconfigured into $8^m$ geometrically unique and load-bearing\nshapes. This result not only challenges the existing design norms but also\nopens up a new avenue to create deployable and versatile structural systems."
                },
                "authors": [
                    {
                        "name": "Vishrut Deshpande"
                    },
                    {
                        "name": "Yogesh Phalak"
                    },
                    {
                        "name": "Ziyang Zhou"
                    },
                    {
                        "name": "Ian Walker"
                    },
                    {
                        "name": "Suyi Li"
                    }
                ],
                "author_detail": {
                    "name": "Suyi Li"
                },
                "author": "Suyi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18558v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18558v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16686v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16686v2",
                "updated": "2024-08-23T01:52:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    1,
                    52,
                    10,
                    4,
                    236,
                    0
                ],
                "published": "2024-07-23T17:50:45Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    50,
                    45,
                    1,
                    205,
                    0
                ],
                "title": "Can Large Language Models Automatically Jailbreak GPT-4V?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Automatically Jailbreak GPT-4V?"
                },
                "summary": "GPT-4V has attracted considerable attention due to its extraordinary capacity\nfor integrating and processing multimodal information. At the same time, its\nability of face recognition raises new safety concerns of privacy leakage.\nDespite researchers' efforts in safety alignment through RLHF or preprocessing\nfilters, vulnerabilities might still be exploited. In our study, we introduce\nAutoJailbreak, an innovative automatic jailbreak technique inspired by prompt\noptimization. We leverage Large Language Models (LLMs) for red-teaming to\nrefine the jailbreak prompt and employ weak-to-strong in-context learning\nprompts to boost efficiency. Furthermore, we present an effective search method\nthat incorporates early stopping to minimize optimization time and token\nexpenditure. Our experiments demonstrate that AutoJailbreak significantly\nsurpasses conventional methods, achieving an Attack Success Rate (ASR)\nexceeding 95.3\\%. This research sheds light on strengthening GPT-4V security,\nunderscoring the potential for LLMs to be exploited in compromising GPT-4V\nintegrity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT-4V has attracted considerable attention due to its extraordinary capacity\nfor integrating and processing multimodal information. At the same time, its\nability of face recognition raises new safety concerns of privacy leakage.\nDespite researchers' efforts in safety alignment through RLHF or preprocessing\nfilters, vulnerabilities might still be exploited. In our study, we introduce\nAutoJailbreak, an innovative automatic jailbreak technique inspired by prompt\noptimization. We leverage Large Language Models (LLMs) for red-teaming to\nrefine the jailbreak prompt and employ weak-to-strong in-context learning\nprompts to boost efficiency. Furthermore, we present an effective search method\nthat incorporates early stopping to minimize optimization time and token\nexpenditure. Our experiments demonstrate that AutoJailbreak significantly\nsurpasses conventional methods, achieving an Attack Success Rate (ASR)\nexceeding 95.3\\%. This research sheds light on strengthening GPT-4V security,\nunderscoring the potential for LLMs to be exploited in compromising GPT-4V\nintegrity."
                },
                "authors": [
                    {
                        "name": "Yuanwei Wu"
                    },
                    {
                        "name": "Yue Huang"
                    },
                    {
                        "name": "Yixin Liu"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Pan Zhou"
                    },
                    {
                        "name": "Lichao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Lichao Sun"
                },
                "author": "Lichao Sun",
                "arxiv_comment": "TrustNLP@NAACL2024 (Fourth Workshop on Trustworthy Natural Language\n  Processing)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16686v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16686v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12789v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12789v1",
                "updated": "2024-08-23T01:44:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    1,
                    44,
                    10,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T01:44:10Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    1,
                    44,
                    10,
                    4,
                    236,
                    0
                ],
                "title": "Context-Aware Temporal Embedding of Objects in Video Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-Aware Temporal Embedding of Objects in Video Data"
                },
                "summary": "In video analysis, understanding the temporal context is crucial for\nrecognizing object interactions, event patterns, and contextual changes over\ntime. The proposed model leverages adjacency and semantic similarities between\nobjects from neighboring video frames to construct context-aware temporal\nobject embeddings. Unlike traditional methods that rely solely on visual\nappearance, our temporal embedding model considers the contextual relationships\nbetween objects, creating a meaningful embedding space where temporally\nconnected object's vectors are positioned in proximity. Empirical studies\ndemonstrate that our context-aware temporal embeddings can be used in\nconjunction with conventional visual embeddings to enhance the effectiveness of\ndownstream applications. Moreover, the embeddings can be used to narrate a\nvideo using a Large Language Model (LLM). This paper describes the intricate\ndetails of the proposed objective function to generate context-aware temporal\nobject embeddings for video data and showcases the potential applications of\nthe generated embeddings in video analysis and object classification tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In video analysis, understanding the temporal context is crucial for\nrecognizing object interactions, event patterns, and contextual changes over\ntime. The proposed model leverages adjacency and semantic similarities between\nobjects from neighboring video frames to construct context-aware temporal\nobject embeddings. Unlike traditional methods that rely solely on visual\nappearance, our temporal embedding model considers the contextual relationships\nbetween objects, creating a meaningful embedding space where temporally\nconnected object's vectors are positioned in proximity. Empirical studies\ndemonstrate that our context-aware temporal embeddings can be used in\nconjunction with conventional visual embeddings to enhance the effectiveness of\ndownstream applications. Moreover, the embeddings can be used to narrate a\nvideo using a Large Language Model (LLM). This paper describes the intricate\ndetails of the proposed objective function to generate context-aware temporal\nobject embeddings for video data and showcases the potential applications of\nthe generated embeddings in video analysis and object classification tasks."
                },
                "authors": [
                    {
                        "name": "Ahnaf Farhan"
                    },
                    {
                        "name": "M. Shahriar Hossain"
                    }
                ],
                "author_detail": {
                    "name": "M. Shahriar Hossain"
                },
                "author": "M. Shahriar Hossain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12789v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12789v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12787v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12787v1",
                "updated": "2024-08-23T01:37:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    1,
                    37,
                    29,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T01:37:29Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    1,
                    37,
                    29,
                    4,
                    236,
                    0
                ],
                "title": "LLM-PBE: Assessing Data Privacy in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-PBE: Assessing Data Privacy in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have become integral to numerous domains,\nsignificantly advancing applications in data management, mining, and analysis.\nTheir profound capabilities in processing and interpreting complex language\ndata, however, bring to light pressing concerns regarding data privacy,\nespecially the risk of unintentional training data leakage. Despite the\ncritical nature of this issue, there has been no existing literature to offer a\ncomprehensive assessment of data privacy risks in LLMs. Addressing this gap,\nour paper introduces LLM-PBE, a toolkit crafted specifically for the systematic\nevaluation of data privacy risks in LLMs. LLM-PBE is designed to analyze\nprivacy across the entire lifecycle of LLMs, incorporating diverse attack and\ndefense strategies, and handling various data types and metrics. Through\ndetailed experimentation with multiple LLMs, LLM-PBE facilitates an in-depth\nexploration of data privacy concerns, shedding light on influential factors\nsuch as model size, data characteristics, and evolving temporal dimensions.\nThis study not only enriches the understanding of privacy issues in LLMs but\nalso serves as a vital resource for future research in the field. Aimed at\nenhancing the breadth of knowledge in this area, the findings, resources, and\nour full technical report are made available at https://llm-pbe.github.io/,\nproviding an open platform for academic and practical advancements in LLM\nprivacy assessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become integral to numerous domains,\nsignificantly advancing applications in data management, mining, and analysis.\nTheir profound capabilities in processing and interpreting complex language\ndata, however, bring to light pressing concerns regarding data privacy,\nespecially the risk of unintentional training data leakage. Despite the\ncritical nature of this issue, there has been no existing literature to offer a\ncomprehensive assessment of data privacy risks in LLMs. Addressing this gap,\nour paper introduces LLM-PBE, a toolkit crafted specifically for the systematic\nevaluation of data privacy risks in LLMs. LLM-PBE is designed to analyze\nprivacy across the entire lifecycle of LLMs, incorporating diverse attack and\ndefense strategies, and handling various data types and metrics. Through\ndetailed experimentation with multiple LLMs, LLM-PBE facilitates an in-depth\nexploration of data privacy concerns, shedding light on influential factors\nsuch as model size, data characteristics, and evolving temporal dimensions.\nThis study not only enriches the understanding of privacy issues in LLMs but\nalso serves as a vital resource for future research in the field. Aimed at\nenhancing the breadth of knowledge in this area, the findings, resources, and\nour full technical report are made available at https://llm-pbe.github.io/,\nproviding an open platform for academic and practical advancements in LLM\nprivacy assessment."
                },
                "authors": [
                    {
                        "name": "Qinbin Li"
                    },
                    {
                        "name": "Junyuan Hong"
                    },
                    {
                        "name": "Chulin Xie"
                    },
                    {
                        "name": "Jeffrey Tan"
                    },
                    {
                        "name": "Rachel Xin"
                    },
                    {
                        "name": "Junyi Hou"
                    },
                    {
                        "name": "Xavier Yin"
                    },
                    {
                        "name": "Zhun Wang"
                    },
                    {
                        "name": "Dan Hendrycks"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Bingsheng He"
                    },
                    {
                        "name": "Dawn Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawn Song"
                },
                "author": "Dawn Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12787v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12787v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12247v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12247v2",
                "updated": "2024-08-23T01:25:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    1,
                    25,
                    26,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-22T09:36:15Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    9,
                    36,
                    15,
                    3,
                    235,
                    0
                ],
                "title": "Enhanced Fine-Tuning of Lightweight Domain-Specific Q&A Model Based on\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Fine-Tuning of Lightweight Domain-Specific Q&A Model Based on\n  Large Language Models"
                },
                "summary": "Large language models (LLMs) excel at general question-answering (Q&A) but\noften fall short in specialized domains due to a lack of domain-specific\nknowledge. Commercial companies face the dual challenges of privacy protection\nand resource constraints when involving LLMs for fine-tuning. This paper\npropose a novel framework, Self-Evolution, designed to address these issues by\nleveraging lightweight open-source LLMs through multiple iterative fine-tuning\nrounds. To enhance the efficiency of iterative fine-tuning, Self-Evolution\nemploy a strategy that filters and reinforces the knowledge with higher value\nduring the iterative process. We employed Self-Evolution on Qwen1.5-7B-Chat\nusing 4,000 documents containing rich domain knowledge from China Mobile,\nachieving a performance score 174% higher on domain-specific question-answering\nevaluations than Qwen1.5-7B-Chat and even 22% higher than Qwen1.5-72B-Chat.\nSelf-Evolution has been deployed in China Mobile's daily operation and\nmaintenance for 117 days, and it improves the efficiency of locating alarms,\nfixing problems, and finding related reports, with an average efficiency\nimprovement of over 18.6%. In addition, we release Self-Evolution framework\ncode in https://github.com/Zero-Pointer/Self-Evolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at general question-answering (Q&A) but\noften fall short in specialized domains due to a lack of domain-specific\nknowledge. Commercial companies face the dual challenges of privacy protection\nand resource constraints when involving LLMs for fine-tuning. This paper\npropose a novel framework, Self-Evolution, designed to address these issues by\nleveraging lightweight open-source LLMs through multiple iterative fine-tuning\nrounds. To enhance the efficiency of iterative fine-tuning, Self-Evolution\nemploy a strategy that filters and reinforces the knowledge with higher value\nduring the iterative process. We employed Self-Evolution on Qwen1.5-7B-Chat\nusing 4,000 documents containing rich domain knowledge from China Mobile,\nachieving a performance score 174% higher on domain-specific question-answering\nevaluations than Qwen1.5-7B-Chat and even 22% higher than Qwen1.5-72B-Chat.\nSelf-Evolution has been deployed in China Mobile's daily operation and\nmaintenance for 117 days, and it improves the efficiency of locating alarms,\nfixing problems, and finding related reports, with an average efficiency\nimprovement of over 18.6%. In addition, we release Self-Evolution framework\ncode in https://github.com/Zero-Pointer/Self-Evolution."
                },
                "authors": [
                    {
                        "name": "Shenglin Zhang"
                    },
                    {
                        "name": "Pengtian Zhu"
                    },
                    {
                        "name": "Minghua Ma"
                    },
                    {
                        "name": "Jiagang Wang"
                    },
                    {
                        "name": "Yongqian Sun"
                    },
                    {
                        "name": "Dongwen Li"
                    },
                    {
                        "name": "Jingyu Wang"
                    },
                    {
                        "name": "Qianying Guo"
                    },
                    {
                        "name": "Xiaolei Hua"
                    },
                    {
                        "name": "Lin Zhu"
                    },
                    {
                        "name": "Dan Pei"
                    }
                ],
                "author_detail": {
                    "name": "Dan Pei"
                },
                "author": "Dan Pei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12247v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12247v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08554v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08554v2",
                "updated": "2024-08-23T01:09:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    1,
                    9,
                    8,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-16T06:39:08Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    39,
                    8,
                    4,
                    229,
                    0
                ],
                "title": "ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration for Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have revolutionized natural language processing\ntasks. However, their practical application is constrained by substantial\nmemory and computational demands. Post-training quantization (PTQ) is\nconsidered an effective method to accelerate LLM inference. Despite its growing\npopularity in LLM model compression, PTQ deployment faces two major challenges.\nFirst, low-bit quantization leads to performance degradation. Second,\nrestricted by the limited integer computing unit type on GPUs, quantized matrix\noperations with different precisions cannot be effectively accelerated. To\naddress these issues, we introduce a novel arbitrary-bit quantization algorithm\nand inference framework, ABQ-LLM. It achieves superior performance across\nvarious quantization settings and enables efficient arbitrary-precision\nquantized inference on the GPU. ABQ-LLM introduces several key innovations: (1)\na distribution correction method for transformer blocks to mitigate\ndistribution differences caused by full quantization of weights and\nactivations, improving performance at low bit-widths. (2) the bit balance\nstrategy to counteract performance degradation from asymmetric distribution\nissues at very low bit-widths (e.g., 2-bit). (3) an innovative quantization\nacceleration framework that reconstructs the quantization matrix multiplication\nof arbitrary precision combinations based on BTC (Binary TensorCore)\nequivalents, gets rid of the limitations of INT4/INT8 computing units. ABQ-LLM\ncan convert each component bit width gain into actual acceleration gain,\nmaximizing performance under mixed precision(e.g., W6A6, W2A8). Based on W2*A8\nquantization configuration on LLaMA-7B model, it achieved a WikiText2\nperplexity of 7.59 (2.17$\\downarrow $ vs 9.76 in AffineQuant). Compared to\nSmoothQuant, we realized 1.6$\\times$ acceleration improvement and 2.7$\\times$\nmemory compression gain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized natural language processing\ntasks. However, their practical application is constrained by substantial\nmemory and computational demands. Post-training quantization (PTQ) is\nconsidered an effective method to accelerate LLM inference. Despite its growing\npopularity in LLM model compression, PTQ deployment faces two major challenges.\nFirst, low-bit quantization leads to performance degradation. Second,\nrestricted by the limited integer computing unit type on GPUs, quantized matrix\noperations with different precisions cannot be effectively accelerated. To\naddress these issues, we introduce a novel arbitrary-bit quantization algorithm\nand inference framework, ABQ-LLM. It achieves superior performance across\nvarious quantization settings and enables efficient arbitrary-precision\nquantized inference on the GPU. ABQ-LLM introduces several key innovations: (1)\na distribution correction method for transformer blocks to mitigate\ndistribution differences caused by full quantization of weights and\nactivations, improving performance at low bit-widths. (2) the bit balance\nstrategy to counteract performance degradation from asymmetric distribution\nissues at very low bit-widths (e.g., 2-bit). (3) an innovative quantization\nacceleration framework that reconstructs the quantization matrix multiplication\nof arbitrary precision combinations based on BTC (Binary TensorCore)\nequivalents, gets rid of the limitations of INT4/INT8 computing units. ABQ-LLM\ncan convert each component bit width gain into actual acceleration gain,\nmaximizing performance under mixed precision(e.g., W6A6, W2A8). Based on W2*A8\nquantization configuration on LLaMA-7B model, it achieved a WikiText2\nperplexity of 7.59 (2.17$\\downarrow $ vs 9.76 in AffineQuant). Compared to\nSmoothQuant, we realized 1.6$\\times$ acceleration improvement and 2.7$\\times$\nmemory compression gain."
                },
                "authors": [
                    {
                        "name": "Chao Zeng"
                    },
                    {
                        "name": "Songwei Liu"
                    },
                    {
                        "name": "Yusheng Xie"
                    },
                    {
                        "name": "Hong Liu"
                    },
                    {
                        "name": "Xiaojian Wang"
                    },
                    {
                        "name": "Miao Wei"
                    },
                    {
                        "name": "Shu Yang"
                    },
                    {
                        "name": "Fangmin Chen"
                    },
                    {
                        "name": "Xing Mei"
                    }
                ],
                "author_detail": {
                    "name": "Xing Mei"
                },
                "author": "Xing Mei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08554v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08554v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.08702v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.08702v4",
                "updated": "2024-08-23T01:03:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    1,
                    3,
                    13,
                    4,
                    236,
                    0
                ],
                "published": "2023-12-14T07:38:12Z",
                "published_parsed": [
                    2023,
                    12,
                    14,
                    7,
                    38,
                    12,
                    3,
                    348,
                    0
                ],
                "title": "Rational Sensibility: LLM Enhanced Empathetic Response Generation Guided\n  by Self-presentation Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rational Sensibility: LLM Enhanced Empathetic Response Generation Guided\n  by Self-presentation Theory"
                },
                "summary": "The development of Large Language Models (LLMs) provides human-centered\nArtificial General Intelligence (AGI) with a glimmer of hope. Empathy serves as\na key emotional attribute of humanity, playing an irreplaceable role in\nhuman-centered AGI. Despite numerous researches aim to improve the cognitive\nempathy of models by incorporating external knowledge, there has been limited\nattention on the sensibility and rationality of the conversation itself, which\nare vital components of the empathy. However, the rationality information\nwithin the conversation is restricted, and previous methods of extending\nknowledge are subject to semantic conflict and single-role view. In this paper,\nwe design an innovative encoder module inspired by self-presentation theory in\nsociology, which specifically processes sensibility and rationality sentences\nin dialogues. And we employ a LLM as a rational brain to decipher profound\nlogical information preserved within the conversation, which assists our model\nin assessing the balance between sensibility and rationality to produce\nhigh-quality empathetic response. Experimental results demonstrate that our\nmodel outperforms other methods in both automatic and human evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of Large Language Models (LLMs) provides human-centered\nArtificial General Intelligence (AGI) with a glimmer of hope. Empathy serves as\na key emotional attribute of humanity, playing an irreplaceable role in\nhuman-centered AGI. Despite numerous researches aim to improve the cognitive\nempathy of models by incorporating external knowledge, there has been limited\nattention on the sensibility and rationality of the conversation itself, which\nare vital components of the empathy. However, the rationality information\nwithin the conversation is restricted, and previous methods of extending\nknowledge are subject to semantic conflict and single-role view. In this paper,\nwe design an innovative encoder module inspired by self-presentation theory in\nsociology, which specifically processes sensibility and rationality sentences\nin dialogues. And we employ a LLM as a rational brain to decipher profound\nlogical information preserved within the conversation, which assists our model\nin assessing the balance between sensibility and rationality to produce\nhigh-quality empathetic response. Experimental results demonstrate that our\nmodel outperforms other methods in both automatic and human evaluations."
                },
                "authors": [
                    {
                        "name": "Linzhuang Sun"
                    },
                    {
                        "name": "Yao Dong"
                    },
                    {
                        "name": "Nan Xu"
                    },
                    {
                        "name": "Jingxuan Wei"
                    },
                    {
                        "name": "Bihui Yu"
                    },
                    {
                        "name": "Yin Luo"
                    }
                ],
                "author_detail": {
                    "name": "Yin Luo"
                },
                "author": "Yin Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.08702v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.08702v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12780v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12780v1",
                "updated": "2024-08-23T00:59:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    0,
                    59,
                    38,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T00:59:38Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    0,
                    59,
                    38,
                    4,
                    236,
                    0
                ],
                "title": "Quality or Quantity? On Data Scale and Diversity in Adapting Large\n  Language Models for Low-Resource Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quality or Quantity? On Data Scale and Diversity in Adapting Large\n  Language Models for Low-Resource Translation"
                },
                "summary": "Despite the recent popularity of Large Language Models (LLMs) in Machine\nTranslation (MT), their performance in low-resource translation still lags\nsignificantly behind Neural Machine Translation (NMT) models. In this paper, we\nexplore what it would take to adapt LLMs for low-resource settings. In\nparticular, we re-examine the role of two factors: a) the importance and\napplication of parallel data, and b) diversity in Supervised Fine-Tuning (SFT).\nRecently, parallel data has been shown to be less important for MT using LLMs\nthan in previous MT research. Similarly, diversity during SFT has been shown to\npromote significant transfer in LLMs across languages and tasks. However, for\nlow-resource LLM-MT, we show that the opposite is true for both of these\nconsiderations: a) parallel data is critical during both pretraining and SFT,\nand b) diversity tends to cause interference, not transfer. Our experiments,\nconducted with 3 LLMs across 2 low-resourced language groups - indigenous\nAmerican and North-East Indian - reveal consistent patterns in both cases,\nunderscoring the generalizability of our findings. We believe these insights\nwill be valuable for scaling to massively multilingual LLM-MT models that can\neffectively serve lower-resource languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the recent popularity of Large Language Models (LLMs) in Machine\nTranslation (MT), their performance in low-resource translation still lags\nsignificantly behind Neural Machine Translation (NMT) models. In this paper, we\nexplore what it would take to adapt LLMs for low-resource settings. In\nparticular, we re-examine the role of two factors: a) the importance and\napplication of parallel data, and b) diversity in Supervised Fine-Tuning (SFT).\nRecently, parallel data has been shown to be less important for MT using LLMs\nthan in previous MT research. Similarly, diversity during SFT has been shown to\npromote significant transfer in LLMs across languages and tasks. However, for\nlow-resource LLM-MT, we show that the opposite is true for both of these\nconsiderations: a) parallel data is critical during both pretraining and SFT,\nand b) diversity tends to cause interference, not transfer. Our experiments,\nconducted with 3 LLMs across 2 low-resourced language groups - indigenous\nAmerican and North-East Indian - reveal consistent patterns in both cases,\nunderscoring the generalizability of our findings. We believe these insights\nwill be valuable for scaling to massively multilingual LLM-MT models that can\neffectively serve lower-resource languages."
                },
                "authors": [
                    {
                        "name": "Vivek Iyer"
                    },
                    {
                        "name": "Bhavitvya Malik"
                    },
                    {
                        "name": "Pavel Stepachev"
                    },
                    {
                        "name": "Pinzhen Chen"
                    },
                    {
                        "name": "Barry Haddow"
                    },
                    {
                        "name": "Alexandra Birch"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Birch"
                },
                "author": "Alexandra Birch",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12780v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12780v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12779v1",
                "updated": "2024-08-23T00:57:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    0,
                    57,
                    37,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T00:57:37Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    0,
                    57,
                    37,
                    4,
                    236,
                    0
                ],
                "title": "Investigating LLM Applications in E-Commerce",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating LLM Applications in E-Commerce"
                },
                "summary": "The emergence of Large Language Models (LLMs) has revolutionized natural\nlanguage processing in various applications especially in e-commerce. One\ncrucial step before the application of such LLMs in these fields is to\nunderstand and compare the performance in different use cases in such tasks.\nThis paper explored the efficacy of LLMs in the e-commerce domain, focusing on\ninstruction-tuning an open source LLM model with public e-commerce datasets of\nvarying sizes and comparing the performance with the conventional models\nprevalent in industrial applications. We conducted a comprehensive comparison\nbetween LLMs and traditional pre-trained language models across specific tasks\nintrinsic to the e-commerce domain, namely classification, generation,\nsummarization, and named entity recognition (NER). Furthermore, we examined the\neffectiveness of the current niche industrial application of very large LLM,\nusing in-context learning, in e-commerce specific tasks. Our findings indicate\nthat few-shot inference with very large LLMs often does not outperform\nfine-tuning smaller pre-trained models, underscoring the importance of\ntask-specific model optimization.Additionally, we investigated different\ntraining methodologies such as single-task training, mixed-task training, and\nLoRA merging both within domain/tasks and between different tasks. Through\nrigorous experimentation and analysis, this paper offers valuable insights into\nthe potential effectiveness of LLMs to advance natural language processing\ncapabilities within the e-commerce industry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of Large Language Models (LLMs) has revolutionized natural\nlanguage processing in various applications especially in e-commerce. One\ncrucial step before the application of such LLMs in these fields is to\nunderstand and compare the performance in different use cases in such tasks.\nThis paper explored the efficacy of LLMs in the e-commerce domain, focusing on\ninstruction-tuning an open source LLM model with public e-commerce datasets of\nvarying sizes and comparing the performance with the conventional models\nprevalent in industrial applications. We conducted a comprehensive comparison\nbetween LLMs and traditional pre-trained language models across specific tasks\nintrinsic to the e-commerce domain, namely classification, generation,\nsummarization, and named entity recognition (NER). Furthermore, we examined the\neffectiveness of the current niche industrial application of very large LLM,\nusing in-context learning, in e-commerce specific tasks. Our findings indicate\nthat few-shot inference with very large LLMs often does not outperform\nfine-tuning smaller pre-trained models, underscoring the importance of\ntask-specific model optimization.Additionally, we investigated different\ntraining methodologies such as single-task training, mixed-task training, and\nLoRA merging both within domain/tasks and between different tasks. Through\nrigorous experimentation and analysis, this paper offers valuable insights into\nthe potential effectiveness of LLMs to advance natural language processing\ncapabilities within the e-commerce industry."
                },
                "authors": [
                    {
                        "name": "Chester Palen-Michel"
                    },
                    {
                        "name": "Ruixiang Wang"
                    },
                    {
                        "name": "Yipeng Zhang"
                    },
                    {
                        "name": "David Yu"
                    },
                    {
                        "name": "Canran Xu"
                    },
                    {
                        "name": "Zhe Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Wu"
                },
                "author": "Zhe Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12775v1",
                "updated": "2024-08-23T00:49:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    0,
                    49,
                    36,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T00:49:36Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    0,
                    49,
                    36,
                    4,
                    236,
                    0
                ],
                "title": "Intelligent OPC Engineer Assistant for Semiconductor Manufacturing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent OPC Engineer Assistant for Semiconductor Manufacturing"
                },
                "summary": "Advancements in chip design and manufacturing have enabled the processing of\ncomplex tasks such as deep learning and natural language processing, paving the\nway for the development of artificial general intelligence (AGI). AI, on the\nother hand, can be leveraged to innovate and streamline semiconductor\ntechnology from planning and implementation to manufacturing. In this paper, we\npresent \\textit{Intelligent OPC Engineer Assistant}, an AI/LLM-powered\nmethodology designed to solve the core manufacturing-aware optimization problem\nknown as optical proximity correction (OPC). The methodology involves a\nreinforcement learning-based OPC recipe search and a customized multi-modal\nagent system for recipe summarization. Experiments demonstrate that our\nmethodology can efficiently build OPC recipes on various chip designs with\nspecially handled design topologies, a task that typically requires the\nfull-time effort of OPC engineers with years of experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in chip design and manufacturing have enabled the processing of\ncomplex tasks such as deep learning and natural language processing, paving the\nway for the development of artificial general intelligence (AGI). AI, on the\nother hand, can be leveraged to innovate and streamline semiconductor\ntechnology from planning and implementation to manufacturing. In this paper, we\npresent \\textit{Intelligent OPC Engineer Assistant}, an AI/LLM-powered\nmethodology designed to solve the core manufacturing-aware optimization problem\nknown as optical proximity correction (OPC). The methodology involves a\nreinforcement learning-based OPC recipe search and a customized multi-modal\nagent system for recipe summarization. Experiments demonstrate that our\nmethodology can efficiently build OPC recipes on various chip designs with\nspecially handled design topologies, a task that typically requires the\nfull-time effort of OPC engineers with years of experience."
                },
                "authors": [
                    {
                        "name": "Guojin Chen"
                    },
                    {
                        "name": "Haoyu Yang"
                    },
                    {
                        "name": "Haoxing Ren"
                    },
                    {
                        "name": "Bei Yu"
                    }
                ],
                "author_detail": {
                    "name": "Bei Yu"
                },
                "author": "Bei Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07892v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07892v2",
                "updated": "2024-08-23T00:38:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    0,
                    38,
                    34,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-15T02:41:25Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    2,
                    41,
                    25,
                    3,
                    228,
                    0
                ],
                "title": "Personhood credentials: Artificial intelligence and the value of\n  privacy-preserving tools to distinguish who is real online",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personhood credentials: Artificial intelligence and the value of\n  privacy-preserving tools to distinguish who is real online"
                },
                "summary": "Anonymity is an important principle online. However, malicious actors have\nlong used misleading identities to conduct fraud, spread disinformation, and\ncarry out other deceptive schemes. With the advent of increasingly capable AI,\nbad actors can amplify the potential scale and effectiveness of their\noperations, intensifying the challenge of balancing anonymity and\ntrustworthiness online. In this paper, we analyze the value of a new tool to\naddress this challenge: \"personhood credentials\" (PHCs), digital credentials\nthat empower users to demonstrate that they are real people -- not AIs -- to\nonline services, without disclosing any personal information. Such credentials\ncan be issued by a range of trusted institutions -- governments or otherwise. A\nPHC system, according to our definition, could be local or global, and does not\nneed to be biometrics-based. Two trends in AI contribute to the urgency of the\nchallenge: AI's increasing indistinguishability from people online (i.e.,\nlifelike content and avatars, agentic activity), and AI's increasing\nscalability (i.e., cost-effectiveness, accessibility). Drawing on a long\nhistory of research into anonymous credentials and \"proof-of-personhood\"\nsystems, personhood credentials give people a way to signal their\ntrustworthiness on online platforms, and offer service providers new tools for\nreducing misuse by bad actors. In contrast, existing countermeasures to\nautomated deception -- such as CAPTCHAs -- are inadequate against sophisticated\nAI, while stringent identity verification solutions are insufficiently private\nfor many use-cases. After surveying the benefits of personhood credentials, we\nalso examine deployment risks and design challenges. We conclude with\nactionable next steps for policymakers, technologists, and standards bodies to\nconsider in consultation with the public.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anonymity is an important principle online. However, malicious actors have\nlong used misleading identities to conduct fraud, spread disinformation, and\ncarry out other deceptive schemes. With the advent of increasingly capable AI,\nbad actors can amplify the potential scale and effectiveness of their\noperations, intensifying the challenge of balancing anonymity and\ntrustworthiness online. In this paper, we analyze the value of a new tool to\naddress this challenge: \"personhood credentials\" (PHCs), digital credentials\nthat empower users to demonstrate that they are real people -- not AIs -- to\nonline services, without disclosing any personal information. Such credentials\ncan be issued by a range of trusted institutions -- governments or otherwise. A\nPHC system, according to our definition, could be local or global, and does not\nneed to be biometrics-based. Two trends in AI contribute to the urgency of the\nchallenge: AI's increasing indistinguishability from people online (i.e.,\nlifelike content and avatars, agentic activity), and AI's increasing\nscalability (i.e., cost-effectiveness, accessibility). Drawing on a long\nhistory of research into anonymous credentials and \"proof-of-personhood\"\nsystems, personhood credentials give people a way to signal their\ntrustworthiness on online platforms, and offer service providers new tools for\nreducing misuse by bad actors. In contrast, existing countermeasures to\nautomated deception -- such as CAPTCHAs -- are inadequate against sophisticated\nAI, while stringent identity verification solutions are insufficiently private\nfor many use-cases. After surveying the benefits of personhood credentials, we\nalso examine deployment risks and design challenges. We conclude with\nactionable next steps for policymakers, technologists, and standards bodies to\nconsider in consultation with the public."
                },
                "authors": [
                    {
                        "name": "Steven Adler"
                    },
                    {
                        "name": "Zo√´ Hitzig"
                    },
                    {
                        "name": "Shrey Jain"
                    },
                    {
                        "name": "Catherine Brewer"
                    },
                    {
                        "name": "Wayne Chang"
                    },
                    {
                        "name": "Ren√©e DiResta"
                    },
                    {
                        "name": "Eddy Lazzarin"
                    },
                    {
                        "name": "Sean McGregor"
                    },
                    {
                        "name": "Wendy Seltzer"
                    },
                    {
                        "name": "Divya Siddarth"
                    },
                    {
                        "name": "Nouran Soliman"
                    },
                    {
                        "name": "Tobin South"
                    },
                    {
                        "name": "Connor Spelliscy"
                    },
                    {
                        "name": "Manu Sporny"
                    },
                    {
                        "name": "Varya Srivastava"
                    },
                    {
                        "name": "John Bailey"
                    },
                    {
                        "name": "Brian Christian"
                    },
                    {
                        "name": "Andrew Critch"
                    },
                    {
                        "name": "Ronnie Falcon"
                    },
                    {
                        "name": "Heather Flanagan"
                    },
                    {
                        "name": "Kim Hamilton Duffy"
                    },
                    {
                        "name": "Eric Ho"
                    },
                    {
                        "name": "Claire R. Leibowicz"
                    },
                    {
                        "name": "Srikanth Nadhamuni"
                    },
                    {
                        "name": "Alan Z. Rozenshtein"
                    },
                    {
                        "name": "David Schnurr"
                    },
                    {
                        "name": "Evan Shapiro"
                    },
                    {
                        "name": "Lacey Strahm"
                    },
                    {
                        "name": "Andrew Trask"
                    },
                    {
                        "name": "Zoe Weinberg"
                    },
                    {
                        "name": "Cedric Whitney"
                    },
                    {
                        "name": "Tom Zick"
                    }
                ],
                "author_detail": {
                    "name": "Tom Zick"
                },
                "author": "Tom Zick",
                "arxiv_comment": "63 pages, 7 figures, 5 tables; minor additions to acknowledgments and\n  wording changes for clarity",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07892v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07892v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10981v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10981v2",
                "updated": "2024-08-23T00:17:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    0,
                    17,
                    2,
                    4,
                    236,
                    0
                ],
                "published": "2024-04-17T01:27:42Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    1,
                    27,
                    42,
                    2,
                    108,
                    0
                ],
                "title": "A Survey on Retrieval-Augmented Text Generation for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Retrieval-Augmented Text Generation for Large Language\n  Models"
                },
                "summary": "Retrieval-Augmented Generation (RAG) merges retrieval methods with deep\nlearning advancements to address the static limitations of large language\nmodels (LLMs) by enabling the dynamic integration of up-to-date external\ninformation. This methodology, focusing primarily on the text domain, provides\na cost-effective solution to the generation of plausible but possibly incorrect\nresponses by LLMs, thereby enhancing the accuracy and reliability of their\noutputs through the use of real-world data. As RAG grows in complexity and\nincorporates multiple concepts that can influence its performance, this paper\norganizes the RAG paradigm into four categories: pre-retrieval, retrieval,\npost-retrieval, and generation, offering a detailed perspective from the\nretrieval viewpoint. It outlines RAG's evolution and discusses the field's\nprogression through the analysis of significant studies. Additionally, the\npaper introduces evaluation methods for RAG, addressing the challenges faced\nand proposing future research directions. By offering an organized framework\nand categorization, the study aims to consolidate existing research on RAG,\nclarify its technological underpinnings, and highlight its potential to broaden\nthe adaptability and applications of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) merges retrieval methods with deep\nlearning advancements to address the static limitations of large language\nmodels (LLMs) by enabling the dynamic integration of up-to-date external\ninformation. This methodology, focusing primarily on the text domain, provides\na cost-effective solution to the generation of plausible but possibly incorrect\nresponses by LLMs, thereby enhancing the accuracy and reliability of their\noutputs through the use of real-world data. As RAG grows in complexity and\nincorporates multiple concepts that can influence its performance, this paper\norganizes the RAG paradigm into four categories: pre-retrieval, retrieval,\npost-retrieval, and generation, offering a detailed perspective from the\nretrieval viewpoint. It outlines RAG's evolution and discusses the field's\nprogression through the analysis of significant studies. Additionally, the\npaper introduces evaluation methods for RAG, addressing the challenges faced\nand proposing future research directions. By offering an organized framework\nand categorization, the study aims to consolidate existing research on RAG,\nclarify its technological underpinnings, and highlight its potential to broaden\nthe adaptability and applications of LLMs."
                },
                "authors": [
                    {
                        "name": "Yizheng Huang"
                    },
                    {
                        "name": "Jimmy Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jimmy Huang"
                },
                "author": "Jimmy Huang",
                "arxiv_comment": "Ongoing Work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10981v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10981v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18966v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18966v3",
                "updated": "2024-08-23T00:14:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    0,
                    14,
                    51,
                    4,
                    236,
                    0
                ],
                "published": "2024-06-27T07:56:44Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    7,
                    56,
                    44,
                    3,
                    179,
                    0
                ],
                "title": "UniGen: A Unified Framework for Textual Dataset Generation Using Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniGen: A Unified Framework for Textual Dataset Generation Using Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) such as GPT-4 and Llama3 have significantly\nimpacted various fields by enabling high-quality synthetic data generation and\nreducing dependence on expensive human-generated datasets. Despite this,\nchallenges remain in the areas of generalization, controllability, diversity,\nand truthfulness within the existing generative frameworks. To address these\nchallenges, this paper presents UniGen, a comprehensive LLM-powered framework\ndesigned to produce diverse, accurate, and highly controllable datasets. UniGen\nis adaptable, supporting all types of text datasets and enhancing the\ngenerative process through innovative mechanisms. To augment data diversity,\nUniGen incorporates an attribute-guided generation module and a group checking\nfeature. For accuracy, it employs a code-based mathematical assessment for\nlabel verification alongside a retrieval-augmented generation technique for\nfactual validation. The framework also allows for user-specified constraints,\nenabling customization of the data generation process to suit particular\nrequirements. Extensive experiments demonstrate the superior quality of data\ngenerated by UniGen, and each module within UniGen plays a critical role in\nthis enhancement. Additionally, UniGen is applied in two practical scenarios:\nbenchmarking LLMs and data augmentation. The results indicate that UniGen\neffectively supports dynamic and evolving benchmarking, and that data\naugmentation improves LLM capabilities in various domains, including\nagent-oriented abilities and reasoning skills.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) such as GPT-4 and Llama3 have significantly\nimpacted various fields by enabling high-quality synthetic data generation and\nreducing dependence on expensive human-generated datasets. Despite this,\nchallenges remain in the areas of generalization, controllability, diversity,\nand truthfulness within the existing generative frameworks. To address these\nchallenges, this paper presents UniGen, a comprehensive LLM-powered framework\ndesigned to produce diverse, accurate, and highly controllable datasets. UniGen\nis adaptable, supporting all types of text datasets and enhancing the\ngenerative process through innovative mechanisms. To augment data diversity,\nUniGen incorporates an attribute-guided generation module and a group checking\nfeature. For accuracy, it employs a code-based mathematical assessment for\nlabel verification alongside a retrieval-augmented generation technique for\nfactual validation. The framework also allows for user-specified constraints,\nenabling customization of the data generation process to suit particular\nrequirements. Extensive experiments demonstrate the superior quality of data\ngenerated by UniGen, and each module within UniGen plays a critical role in\nthis enhancement. Additionally, UniGen is applied in two practical scenarios:\nbenchmarking LLMs and data augmentation. The results indicate that UniGen\neffectively supports dynamic and evolving benchmarking, and that data\naugmentation improves LLM capabilities in various domains, including\nagent-oriented abilities and reasoning skills."
                },
                "authors": [
                    {
                        "name": "Siyuan Wu"
                    },
                    {
                        "name": "Yue Huang"
                    },
                    {
                        "name": "Chujie Gao"
                    },
                    {
                        "name": "Dongping Chen"
                    },
                    {
                        "name": "Qihui Zhang"
                    },
                    {
                        "name": "Yao Wan"
                    },
                    {
                        "name": "Tianyi Zhou"
                    },
                    {
                        "name": "Xiangliang Zhang"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Chaowei Xiao"
                    },
                    {
                        "name": "Lichao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Lichao Sun"
                },
                "author": "Lichao Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18966v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18966v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12757v1",
                "updated": "2024-08-22T23:00:40Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    23,
                    0,
                    40,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T23:00:40Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    23,
                    0,
                    40,
                    3,
                    235,
                    0
                ],
                "title": "NanoFlow: Towards Optimal Large Language Model Serving Throughput",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NanoFlow: Towards Optimal Large Language Model Serving Throughput"
                },
                "summary": "The increasing usage of Large Language Models (LLMs) has resulted in a\nsurging demand for planet-scale serving systems, where tens of thousands of\nGPUs continuously serve hundreds of millions of users. Consequently, throughput\n(under reasonable latency constraints) has emerged as a key metric that\ndetermines serving systems' performance. To boost throughput, various methods\nof inter-device parallelism (e.g., data, tensor, pipeline) have been explored.\nHowever, existing methods do not consider overlapping the utilization of\ndifferent resources within a single device, leading to underutilization and\nsub-optimal performance.\n  We propose NanoFlow, a novel serving framework that exploits intra-device\nparallelism, which overlaps the usage of resources including compute, memory,\nand network within a single device through operation co-scheduling. To exploit\nintra-device parallelism, NanoFlow introduces two key innovations: First,\nNanoFlow splits requests into nano-batches at the granularity of operations,\nwhich breaks the dependency of sequential operations in LLM inference and\nenables overlapping; then, to get benefit from overlapping, NanoFlow uses an\noperation-level pipeline with execution unit scheduling, which partitions the\ndevice's functional units and simultaneously executes different operations in\neach unit. NanoFlow automates the pipeline setup using a parameter search\nalgorithm, which enables easily porting NanoFlow to different models. We\nimplement NanoFlow on NVIDIA GPUs and evaluate end-to-end serving throughput on\nseveral popular models such as LLaMA-2-70B, Mixtral 8x7B, LLaMA-3-8B, etc..\nWith practical workloads, NanoFlow provides 1.91x throughput boost compared to\nstate-of-the-art serving systems achieving 59% to 72% of optimal throughput\nacross ported models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing usage of Large Language Models (LLMs) has resulted in a\nsurging demand for planet-scale serving systems, where tens of thousands of\nGPUs continuously serve hundreds of millions of users. Consequently, throughput\n(under reasonable latency constraints) has emerged as a key metric that\ndetermines serving systems' performance. To boost throughput, various methods\nof inter-device parallelism (e.g., data, tensor, pipeline) have been explored.\nHowever, existing methods do not consider overlapping the utilization of\ndifferent resources within a single device, leading to underutilization and\nsub-optimal performance.\n  We propose NanoFlow, a novel serving framework that exploits intra-device\nparallelism, which overlaps the usage of resources including compute, memory,\nand network within a single device through operation co-scheduling. To exploit\nintra-device parallelism, NanoFlow introduces two key innovations: First,\nNanoFlow splits requests into nano-batches at the granularity of operations,\nwhich breaks the dependency of sequential operations in LLM inference and\nenables overlapping; then, to get benefit from overlapping, NanoFlow uses an\noperation-level pipeline with execution unit scheduling, which partitions the\ndevice's functional units and simultaneously executes different operations in\neach unit. NanoFlow automates the pipeline setup using a parameter search\nalgorithm, which enables easily porting NanoFlow to different models. We\nimplement NanoFlow on NVIDIA GPUs and evaluate end-to-end serving throughput on\nseveral popular models such as LLaMA-2-70B, Mixtral 8x7B, LLaMA-3-8B, etc..\nWith practical workloads, NanoFlow provides 1.91x throughput boost compared to\nstate-of-the-art serving systems achieving 59% to 72% of optimal throughput\nacross ported models."
                },
                "authors": [
                    {
                        "name": "Kan Zhu"
                    },
                    {
                        "name": "Yilong Zhao"
                    },
                    {
                        "name": "Liangyu Zhao"
                    },
                    {
                        "name": "Gefei Zuo"
                    },
                    {
                        "name": "Yile Gu"
                    },
                    {
                        "name": "Dedong Xie"
                    },
                    {
                        "name": "Yufei Gao"
                    },
                    {
                        "name": "Qinyu Xu"
                    },
                    {
                        "name": "Tian Tang"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Keisuke Kamahori"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Stephanie Wang"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Baris Kasikci"
                    }
                ],
                "author_detail": {
                    "name": "Baris Kasikci"
                },
                "author": "Baris Kasikci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.03154v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.03154v2",
                "updated": "2024-08-22T22:42:40Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    22,
                    42,
                    40,
                    3,
                    235,
                    0
                ],
                "published": "2024-03-05T17:47:22Z",
                "published_parsed": [
                    2024,
                    3,
                    5,
                    17,
                    47,
                    22,
                    1,
                    65,
                    0
                ],
                "title": "Quantum Many-Body Physics Calculations with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Many-Body Physics Calculations with Large Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated an unprecedented ability to\nperform complex tasks in multiple domains, including mathematical and\nscientific reasoning. We demonstrate that with carefully designed prompts, LLMs\ncan accurately carry out key calculations in research papers in theoretical\nphysics. We focus on a broadly used approximation method in quantum physics:\nthe Hartree-Fock method, requiring an analytic multi-step calculation deriving\napproximate Hamiltonian and corresponding self-consistency equations. To carry\nout the calculations using LLMs, we design multi-step prompt templates that\nbreak down the analytic calculation into standardized steps with placeholders\nfor problem-specific information. We evaluate GPT-4's performance in executing\nthe calculation for 15 research papers from the past decade, demonstrating\nthat, with correction of intermediate steps, it can correctly derive the final\nHartree-Fock Hamiltonian in 13 cases and makes minor errors in 2 cases.\nAggregating across all research papers, we find an average score of 87.5 (out\nof 100) on the execution of individual calculation steps. Overall, the\nrequisite skill for doing these calculations is at the graduate level in\nquantum condensed matter theory. We further use LLMs to mitigate the two\nprimary bottlenecks in this evaluation process: (i) extracting information from\npapers to fill in templates and (ii) automatic scoring of the calculation\nsteps, demonstrating good results in both cases. The strong performance is the\nfirst step for developing algorithms that automatically explore theoretical\nhypotheses at an unprecedented scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated an unprecedented ability to\nperform complex tasks in multiple domains, including mathematical and\nscientific reasoning. We demonstrate that with carefully designed prompts, LLMs\ncan accurately carry out key calculations in research papers in theoretical\nphysics. We focus on a broadly used approximation method in quantum physics:\nthe Hartree-Fock method, requiring an analytic multi-step calculation deriving\napproximate Hamiltonian and corresponding self-consistency equations. To carry\nout the calculations using LLMs, we design multi-step prompt templates that\nbreak down the analytic calculation into standardized steps with placeholders\nfor problem-specific information. We evaluate GPT-4's performance in executing\nthe calculation for 15 research papers from the past decade, demonstrating\nthat, with correction of intermediate steps, it can correctly derive the final\nHartree-Fock Hamiltonian in 13 cases and makes minor errors in 2 cases.\nAggregating across all research papers, we find an average score of 87.5 (out\nof 100) on the execution of individual calculation steps. Overall, the\nrequisite skill for doing these calculations is at the graduate level in\nquantum condensed matter theory. We further use LLMs to mitigate the two\nprimary bottlenecks in this evaluation process: (i) extracting information from\npapers to fill in templates and (ii) automatic scoring of the calculation\nsteps, demonstrating good results in both cases. The strong performance is the\nfirst step for developing algorithms that automatically explore theoretical\nhypotheses at an unprecedented scale."
                },
                "authors": [
                    {
                        "name": "Haining Pan"
                    },
                    {
                        "name": "Nayantara Mudur"
                    },
                    {
                        "name": "Will Taranto"
                    },
                    {
                        "name": "Maria Tikhanovskaya"
                    },
                    {
                        "name": "Subhashini Venugopalan"
                    },
                    {
                        "name": "Yasaman Bahri"
                    },
                    {
                        "name": "Michael P. Brenner"
                    },
                    {
                        "name": "Eun-Ah Kim"
                    }
                ],
                "author_detail": {
                    "name": "Eun-Ah Kim"
                },
                "author": "Eun-Ah Kim",
                "arxiv_comment": "9 pages, 4 figures. Supplemental material in the source file",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.03154v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.03154v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.other",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12748v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12748v1",
                "updated": "2024-08-22T22:13:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    22,
                    13,
                    13,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T22:13:13Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    22,
                    13,
                    13,
                    3,
                    235,
                    0
                ],
                "title": "SLM Meets LLM: Balancing Latency, Interpretability and Consistency in\n  Hallucination Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLM Meets LLM: Balancing Latency, Interpretability and Consistency in\n  Hallucination Detection"
                },
                "summary": "Large language models (LLMs) are highly capable but face latency challenges\nin real-time applications, such as conducting online hallucination detection.\nTo overcome this issue, we propose a novel framework that leverages a small\nlanguage model (SLM) classifier for initial detection, followed by a LLM as\nconstrained reasoner to generate detailed explanations for detected\nhallucinated content. This study optimizes the real-time interpretable\nhallucination detection by introducing effective prompting techniques that\nalign LLM-generated explanations with SLM decisions. Empirical experiment\nresults demonstrate its effectiveness, thereby enhancing the overall user\nexperience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are highly capable but face latency challenges\nin real-time applications, such as conducting online hallucination detection.\nTo overcome this issue, we propose a novel framework that leverages a small\nlanguage model (SLM) classifier for initial detection, followed by a LLM as\nconstrained reasoner to generate detailed explanations for detected\nhallucinated content. This study optimizes the real-time interpretable\nhallucination detection by introducing effective prompting techniques that\nalign LLM-generated explanations with SLM decisions. Empirical experiment\nresults demonstrate its effectiveness, thereby enhancing the overall user\nexperience."
                },
                "authors": [
                    {
                        "name": "Mengya Hu"
                    },
                    {
                        "name": "Rui Xu"
                    },
                    {
                        "name": "Deren Lei"
                    },
                    {
                        "name": "Yaxi Li"
                    },
                    {
                        "name": "Mingyu Wang"
                    },
                    {
                        "name": "Emily Ching"
                    },
                    {
                        "name": "Eslam Kamal"
                    },
                    {
                        "name": "Alex Deng"
                    }
                ],
                "author_detail": {
                    "name": "Alex Deng"
                },
                "author": "Alex Deng",
                "arxiv_comment": "preprint under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12748v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12748v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12742v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12742v1",
                "updated": "2024-08-22T21:51:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    21,
                    51,
                    38,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T21:51:38Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    21,
                    51,
                    38,
                    3,
                    235,
                    0
                ],
                "title": "TReX- Reusing Vision Transformer's Attention for Efficient Xbar-based\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TReX- Reusing Vision Transformer's Attention for Efficient Xbar-based\n  Computing"
                },
                "summary": "Due to the high computation overhead of Vision Transformers (ViTs), In-memory\nComputing architectures are being researched towards energy-efficient\ndeployment in edge-computing scenarios. Prior works have proposed efficient\nalgorithm-hardware co-design and IMC-architectural improvements to improve the\nenergy-efficiency of IMC-implemented ViTs. However, all prior works have\nneglected the overhead and co-depencence of attention blocks on the\naccuracy-energy-delay-area of IMC-implemented ViTs. To this end, we propose\nTReX- an attention-reuse-driven ViT optimization framework that effectively\nperforms attention reuse in ViT models to achieve optimal\naccuracy-energy-delay-area tradeoffs. TReX optimally chooses the transformer\nencoders for attention reuse to achieve near iso-accuracy performance while\nmeeting the user-specified delay requirement. Based on our analysis on the\nImagenet-1k dataset, we find that TReX achieves 2.3x (2.19x) EDAP reduction and\n1.86x (1.79x) TOPS/mm2 improvement with ~1% accuracy drop in case of DeiT-S\n(LV-ViT-S) ViT models. Additionally, TReX achieves high accuracy at high EDAP\nreduction compared to state-of-the-art token pruning and weight sharing\napproaches. On NLP tasks such as CoLA, TReX leads to 2% higher non-ideal\naccuracy compared to baseline at 1.6x lower EDAP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the high computation overhead of Vision Transformers (ViTs), In-memory\nComputing architectures are being researched towards energy-efficient\ndeployment in edge-computing scenarios. Prior works have proposed efficient\nalgorithm-hardware co-design and IMC-architectural improvements to improve the\nenergy-efficiency of IMC-implemented ViTs. However, all prior works have\nneglected the overhead and co-depencence of attention blocks on the\naccuracy-energy-delay-area of IMC-implemented ViTs. To this end, we propose\nTReX- an attention-reuse-driven ViT optimization framework that effectively\nperforms attention reuse in ViT models to achieve optimal\naccuracy-energy-delay-area tradeoffs. TReX optimally chooses the transformer\nencoders for attention reuse to achieve near iso-accuracy performance while\nmeeting the user-specified delay requirement. Based on our analysis on the\nImagenet-1k dataset, we find that TReX achieves 2.3x (2.19x) EDAP reduction and\n1.86x (1.79x) TOPS/mm2 improvement with ~1% accuracy drop in case of DeiT-S\n(LV-ViT-S) ViT models. Additionally, TReX achieves high accuracy at high EDAP\nreduction compared to state-of-the-art token pruning and weight sharing\napproaches. On NLP tasks such as CoLA, TReX leads to 2% higher non-ideal\naccuracy compared to baseline at 1.6x lower EDAP."
                },
                "authors": [
                    {
                        "name": "Abhishek Moitra"
                    },
                    {
                        "name": "Abhiroop Bhattacharjee"
                    },
                    {
                        "name": "Youngeun Kim"
                    },
                    {
                        "name": "Priyadarshini Panda"
                    }
                ],
                "author_detail": {
                    "name": "Priyadarshini Panda"
                },
                "author": "Priyadarshini Panda",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12742v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12742v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12726v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12726v1",
                "updated": "2024-08-22T20:35:42Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    20,
                    35,
                    42,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T20:35:42Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    20,
                    35,
                    42,
                    3,
                    235,
                    0
                ],
                "title": "Macro-Queries: An Exploration into Guided Chart Generation from High\n  Level Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Macro-Queries: An Exploration into Guided Chart Generation from High\n  Level Prompts"
                },
                "summary": "This paper explores the intersection of data visualization and Large Language\nModels (LLMs). Driven by the need to make a broader range of data visualization\ntypes accessible for novice users, we present a guided LLM-based pipeline\ndesigned to transform data, guided by high-level user questions (referred to as\nmacro-queries), into a diverse set of useful visualizations. This approach\nleverages various prompting techniques, fine-tuning inspired by Abela's Chart\nTaxonomy, and integrated SQL tool usage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the intersection of data visualization and Large Language\nModels (LLMs). Driven by the need to make a broader range of data visualization\ntypes accessible for novice users, we present a guided LLM-based pipeline\ndesigned to transform data, guided by high-level user questions (referred to as\nmacro-queries), into a diverse set of useful visualizations. This approach\nleverages various prompting techniques, fine-tuning inspired by Abela's Chart\nTaxonomy, and integrated SQL tool usage."
                },
                "authors": [
                    {
                        "name": "Christopher J. Lee"
                    },
                    {
                        "name": "Giorgio Tran"
                    },
                    {
                        "name": "Roderick Tabalba"
                    },
                    {
                        "name": "Jason Leigh"
                    },
                    {
                        "name": "Ryan Longman"
                    }
                ],
                "author_detail": {
                    "name": "Ryan Longman"
                },
                "author": "Ryan Longman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12726v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.16694v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.16694v5",
                "updated": "2024-08-22T19:46:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    19,
                    46,
                    37,
                    3,
                    235,
                    0
                ],
                "published": "2024-01-30T02:41:05Z",
                "published_parsed": [
                    2024,
                    1,
                    30,
                    2,
                    41,
                    5,
                    1,
                    30,
                    0
                ],
                "title": "etuner: A Redundancy-Aware Framework for Efficient Continual Learning\n  Application on Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "etuner: A Redundancy-Aware Framework for Efficient Continual Learning\n  Application on Edge Devices"
                },
                "summary": "Many emerging applications, such as robot-assisted eldercare and object\nrecognition, generally employ deep learning neural networks (DNNs) and require\nthe deployment of DNN models on edge devices. These applications naturally\nrequire i) handling streaming-in inference requests and ii) fine-tuning the\ndeployed models to adapt to possible deployment scenario changes. Continual\nlearning (CL) is widely adopted to satisfy these needs. CL is a popular deep\nlearning paradigm that handles both continuous model fine-tuning and overtime\ninference requests. However, an inappropriate model fine-tuning scheme could\ninvolve significant redundancy and consume considerable time and energy, making\nit challenging to apply CL on edge devices. In this paper, we propose ETuner,\nan efficient edge continual learning framework that optimizes inference\naccuracy, fine-tuning execution time, and energy efficiency through both\ninter-tuning and intra-tuning optimizations. Experimental results show that, on\naverage, ETuner reduces overall fine-tuning execution time by 64%, energy\nconsumption by 56%, and improves average inference accuracy by 1.75% over the\nimmediate model fine-tuning approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many emerging applications, such as robot-assisted eldercare and object\nrecognition, generally employ deep learning neural networks (DNNs) and require\nthe deployment of DNN models on edge devices. These applications naturally\nrequire i) handling streaming-in inference requests and ii) fine-tuning the\ndeployed models to adapt to possible deployment scenario changes. Continual\nlearning (CL) is widely adopted to satisfy these needs. CL is a popular deep\nlearning paradigm that handles both continuous model fine-tuning and overtime\ninference requests. However, an inappropriate model fine-tuning scheme could\ninvolve significant redundancy and consume considerable time and energy, making\nit challenging to apply CL on edge devices. In this paper, we propose ETuner,\nan efficient edge continual learning framework that optimizes inference\naccuracy, fine-tuning execution time, and energy efficiency through both\ninter-tuning and intra-tuning optimizations. Experimental results show that, on\naverage, ETuner reduces overall fine-tuning execution time by 64%, energy\nconsumption by 56%, and improves average inference accuracy by 1.75% over the\nimmediate model fine-tuning approach."
                },
                "authors": [
                    {
                        "name": "Sheng Li"
                    },
                    {
                        "name": "Geng Yuan"
                    },
                    {
                        "name": "Yawen Wu"
                    },
                    {
                        "name": "Yue Dai"
                    },
                    {
                        "name": "Tianyu Wang"
                    },
                    {
                        "name": "Chao Wu"
                    },
                    {
                        "name": "Alex K. Jones"
                    },
                    {
                        "name": "Jingtong Hu"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Xulong Tang"
                    }
                ],
                "author_detail": {
                    "name": "Xulong Tang"
                },
                "author": "Xulong Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.16694v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.16694v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15809v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15809v2",
                "updated": "2024-08-22T19:25:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    19,
                    25,
                    51,
                    3,
                    235,
                    0
                ],
                "published": "2024-06-22T10:25:55Z",
                "published_parsed": [
                    2024,
                    6,
                    22,
                    10,
                    25,
                    55,
                    5,
                    174,
                    0
                ],
                "title": "LaMSUM: Creating Extractive Summaries of User Generated Content using\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaMSUM: Creating Extractive Summaries of User Generated Content using\n  LLMs"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\na wide range of NLP tasks, including summarization. LLMs inherently produce\nabstractive summaries by paraphrasing the original text, while the generation\nof extractive summaries - selecting specific subsets from the original text -\nremains largely unexplored. LLMs have a limited context window size,\nrestricting the amount of data that can be processed at once. We tackle this\nchallenge by introducing LaMSUM, a novel multi-level framework designed to\ngenerate extractive summaries from large collections of user-generated text\nusing LLMs. LaMSUM integrates summarization with different voting methods to\nachieve robust summaries. Extensive evaluation using four popular LLMs (Llama\n3, Mixtral, Gemini, GPT-4o) demonstrates that LaMSUM outperforms\nstate-of-the-art extractive summarization methods. Overall, this work\nrepresents one of the first attempts to achieve extractive summarization by\nleveraging the power of LLMs, and is likely to spark further interest within\nthe research community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive performance across\na wide range of NLP tasks, including summarization. LLMs inherently produce\nabstractive summaries by paraphrasing the original text, while the generation\nof extractive summaries - selecting specific subsets from the original text -\nremains largely unexplored. LLMs have a limited context window size,\nrestricting the amount of data that can be processed at once. We tackle this\nchallenge by introducing LaMSUM, a novel multi-level framework designed to\ngenerate extractive summaries from large collections of user-generated text\nusing LLMs. LaMSUM integrates summarization with different voting methods to\nachieve robust summaries. Extensive evaluation using four popular LLMs (Llama\n3, Mixtral, Gemini, GPT-4o) demonstrates that LaMSUM outperforms\nstate-of-the-art extractive summarization methods. Overall, this work\nrepresents one of the first attempts to achieve extractive summarization by\nleveraging the power of LLMs, and is likely to spark further interest within\nthe research community."
                },
                "authors": [
                    {
                        "name": "Garima Chhikara"
                    },
                    {
                        "name": "Anurag Sharma"
                    },
                    {
                        "name": "V. Gurucharan"
                    },
                    {
                        "name": "Kripabandhu Ghosh"
                    },
                    {
                        "name": "Abhijnan Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Abhijnan Chakraborty"
                },
                "author": "Abhijnan Chakraborty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15809v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15809v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12687v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12687v1",
                "updated": "2024-08-22T19:00:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    19,
                    0,
                    50,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T19:00:50Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    19,
                    0,
                    50,
                    3,
                    235,
                    0
                ],
                "title": "Bridging the gap between natural user expression with complex automation\n  programming in smart homes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the gap between natural user expression with complex automation\n  programming in smart homes"
                },
                "summary": "A long-standing challenge in end-user programming (EUP) is to trade off\nbetween natural user expression and the complexity of programming tasks. As\nlarge language models (LLMs) are empowered to handle semantic inference and\nnatural language understanding, it remains under-explored how such capabilities\ncan facilitate end-users to configure complex automation more naturally and\neasily. We propose AwareAuto, an EUP system that standardizes user expression\nand finishes two-step inference with the LLMs to achieve automation generation.\nAwareAuto allows contextual, multi-modality, and flexible user expression to\nconfigure complex automation tasks (e.g., dynamic parameters, multiple\nconditional branches, and temporal constraints), which are non-manageable in\ntraditional EUP solutions. By studying realistic, complex rules data, AwareAuto\ngains 91.7% accuracy in matching user intentions and feasibility. We introduced\nuser interaction to ensure system controllability and usability. We discuss the\nopportunities and challenges of incorporating LLMs in end-user programming\ntechniques and grounding complex smart home contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A long-standing challenge in end-user programming (EUP) is to trade off\nbetween natural user expression and the complexity of programming tasks. As\nlarge language models (LLMs) are empowered to handle semantic inference and\nnatural language understanding, it remains under-explored how such capabilities\ncan facilitate end-users to configure complex automation more naturally and\neasily. We propose AwareAuto, an EUP system that standardizes user expression\nand finishes two-step inference with the LLMs to achieve automation generation.\nAwareAuto allows contextual, multi-modality, and flexible user expression to\nconfigure complex automation tasks (e.g., dynamic parameters, multiple\nconditional branches, and temporal constraints), which are non-manageable in\ntraditional EUP solutions. By studying realistic, complex rules data, AwareAuto\ngains 91.7% accuracy in matching user intentions and feasibility. We introduced\nuser interaction to ensure system controllability and usability. We discuss the\nopportunities and challenges of incorporating LLMs in end-user programming\ntechniques and grounding complex smart home contexts."
                },
                "authors": [
                    {
                        "name": "Yingtian Shi"
                    },
                    {
                        "name": "Xiaoyi Liu"
                    },
                    {
                        "name": "Chun Yu"
                    },
                    {
                        "name": "Tianao Yang"
                    },
                    {
                        "name": "Cheng Gao"
                    },
                    {
                        "name": "Chen Liang"
                    },
                    {
                        "name": "Yuanchun Shi"
                    }
                ],
                "author_detail": {
                    "name": "Yuanchun Shi"
                },
                "author": "Yuanchun Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12687v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12687v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12680v1",
                "updated": "2024-08-22T18:39:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    18,
                    39,
                    0,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T18:39:00Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    18,
                    39,
                    0,
                    3,
                    235,
                    0
                ],
                "title": "Can LLMs Understand Social Norms in Autonomous Driving Games?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Understand Social Norms in Autonomous Driving Games?"
                },
                "summary": "Social norm is defined as a shared standard of acceptable behavior in a\nsociety. The emergence of social norms fosters coordination among agents\nwithout any hard-coded rules, which is crucial for the large-scale deployment\nof AVs in an intelligent transportation system. This paper explores the\napplication of LLMs in understanding and modeling social norms in autonomous\ndriving games. We introduce LLMs into autonomous driving games as intelligent\nagents who make decisions according to text prompts. These agents are referred\nto as LLM-based agents. Our framework involves LLM-based agents playing Markov\ngames in a multi-agent system (MAS), allowing us to investigate the emergence\nof social norms among individual agents. We aim to identify social norms by\ndesigning prompts and utilizing LLMs on textual information related to the\nenvironment setup and the observations of LLM-based agents. Using the OpenAI\nChat API powered by GPT-4.0, we conduct experiments to simulate interactions\nand evaluate the performance of LLM-based agents in two driving scenarios:\nunsignalized intersection and highway platoon. The results show that LLM-based\nagents can handle dynamically changing environments in Markov games, and social\nnorms evolve among LLM-based agents in both scenarios. In the intersection\ngame, LLM-based agents tend to adopt a conservative driving policy when facing\na potential car crash. The advantage of LLM-based agents in games lies in their\nstrong operability and analyzability, which facilitate experimental design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social norm is defined as a shared standard of acceptable behavior in a\nsociety. The emergence of social norms fosters coordination among agents\nwithout any hard-coded rules, which is crucial for the large-scale deployment\nof AVs in an intelligent transportation system. This paper explores the\napplication of LLMs in understanding and modeling social norms in autonomous\ndriving games. We introduce LLMs into autonomous driving games as intelligent\nagents who make decisions according to text prompts. These agents are referred\nto as LLM-based agents. Our framework involves LLM-based agents playing Markov\ngames in a multi-agent system (MAS), allowing us to investigate the emergence\nof social norms among individual agents. We aim to identify social norms by\ndesigning prompts and utilizing LLMs on textual information related to the\nenvironment setup and the observations of LLM-based agents. Using the OpenAI\nChat API powered by GPT-4.0, we conduct experiments to simulate interactions\nand evaluate the performance of LLM-based agents in two driving scenarios:\nunsignalized intersection and highway platoon. The results show that LLM-based\nagents can handle dynamically changing environments in Markov games, and social\nnorms evolve among LLM-based agents in both scenarios. In the intersection\ngame, LLM-based agents tend to adopt a conservative driving policy when facing\na potential car crash. The advantage of LLM-based agents in games lies in their\nstrong operability and analyzability, which facilitate experimental design."
                },
                "authors": [
                    {
                        "name": "Boxuan Wang"
                    },
                    {
                        "name": "Haonan Duan"
                    },
                    {
                        "name": "Yanhao Feng"
                    },
                    {
                        "name": "Xu Chen"
                    },
                    {
                        "name": "Yongjie Fu"
                    },
                    {
                        "name": "Zhaobin Mo"
                    },
                    {
                        "name": "Xuan Di"
                    }
                ],
                "author_detail": {
                    "name": "Xuan Di"
                },
                "author": "Xuan Di",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12673v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12673v1",
                "updated": "2024-08-22T18:26:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    18,
                    26,
                    31,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T18:26:31Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    18,
                    26,
                    31,
                    3,
                    235,
                    0
                ],
                "title": "Enhancing Transferability of Adversarial Attacks with GE-AdvGAN+: A\n  Comprehensive Framework for Gradient Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Transferability of Adversarial Attacks with GE-AdvGAN+: A\n  Comprehensive Framework for Gradient Editing"
                },
                "summary": "Transferable adversarial attacks pose significant threats to deep neural\nnetworks, particularly in black-box scenarios where internal model information\nis inaccessible. Studying adversarial attack methods helps advance the\nperformance of defense mechanisms and explore model vulnerabilities. These\nmethods can uncover and exploit weaknesses in models, promoting the development\nof more robust architectures. However, current methods for transferable attacks\noften come with substantial computational costs, limiting their deployment and\napplication, especially in edge computing scenarios. Adversarial generative\nmodels, such as Generative Adversarial Networks (GANs), are characterized by\ntheir ability to generate samples without the need for retraining after an\ninitial training phase. GE-AdvGAN, a recent method for transferable adversarial\nattacks, is based on this principle. In this paper, we propose a novel general\nframework for gradient editing-based transferable attacks, named GE-AdvGAN+,\nwhich integrates nearly all mainstream attack methods to enhance\ntransferability while significantly reducing computational resource\nconsumption. Our experiments demonstrate the compatibility and effectiveness of\nour framework. Compared to the baseline AdvGAN, our best-performing method,\nGE-AdvGAN++, achieves an average ASR improvement of 47.8. Additionally, it\nsurpasses the latest competing algorithm, GE-AdvGAN, with an average ASR\nincrease of 5.9. The framework also exhibits enhanced computational efficiency,\nachieving 2217.7 FPS, outperforming traditional methods such as BIM and\nMI-FGSM. The implementation code for our GE-AdvGAN+ framework is available at\nhttps://github.com/GEAdvGANP",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transferable adversarial attacks pose significant threats to deep neural\nnetworks, particularly in black-box scenarios where internal model information\nis inaccessible. Studying adversarial attack methods helps advance the\nperformance of defense mechanisms and explore model vulnerabilities. These\nmethods can uncover and exploit weaknesses in models, promoting the development\nof more robust architectures. However, current methods for transferable attacks\noften come with substantial computational costs, limiting their deployment and\napplication, especially in edge computing scenarios. Adversarial generative\nmodels, such as Generative Adversarial Networks (GANs), are characterized by\ntheir ability to generate samples without the need for retraining after an\ninitial training phase. GE-AdvGAN, a recent method for transferable adversarial\nattacks, is based on this principle. In this paper, we propose a novel general\nframework for gradient editing-based transferable attacks, named GE-AdvGAN+,\nwhich integrates nearly all mainstream attack methods to enhance\ntransferability while significantly reducing computational resource\nconsumption. Our experiments demonstrate the compatibility and effectiveness of\nour framework. Compared to the baseline AdvGAN, our best-performing method,\nGE-AdvGAN++, achieves an average ASR improvement of 47.8. Additionally, it\nsurpasses the latest competing algorithm, GE-AdvGAN, with an average ASR\nincrease of 5.9. The framework also exhibits enhanced computational efficiency,\nachieving 2217.7 FPS, outperforming traditional methods such as BIM and\nMI-FGSM. The implementation code for our GE-AdvGAN+ framework is available at\nhttps://github.com/GEAdvGANP"
                },
                "authors": [
                    {
                        "name": "Zhibo Jin"
                    },
                    {
                        "name": "Jiayu Zhang"
                    },
                    {
                        "name": "Zhiyu Zhu"
                    },
                    {
                        "name": "Yuchen Zhang"
                    },
                    {
                        "name": "Jiahao Huang"
                    },
                    {
                        "name": "Jianlong Zhou"
                    },
                    {
                        "name": "Fang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Fang Chen"
                },
                "author": "Fang Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12673v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12673v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20234v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20234v2",
                "updated": "2024-08-22T18:17:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    18,
                    17,
                    38,
                    3,
                    235,
                    0
                ],
                "published": "2024-05-30T16:36:47Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    16,
                    36,
                    47,
                    3,
                    151,
                    0
                ],
                "title": "Unmasking Context Injection on Interactive Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmasking Context Injection on Interactive Large Language Models"
                },
                "summary": "Large Language Models (LLMs) such as ChatGPT and Llama have become prevalent\nin real-world applications, exhibiting impressive text generation performance.\nLLMs are fundamentally developed from a scenario where the input data remains\nstatic and unstructured. To behave interactively, LLM-based chat systems must\nintegrate prior chat history as context into their inputs, following a\npre-defined structure. However, LLMs cannot separate user inputs from context,\nenabling context injection. This paper introduces a methodology to\nsystematically inject misleading context (i.e., chat history) into LLM\nconversations without prior knowledge of the target model. We formalize context\ninjection through a response retrieval task and propose an LLM-Guided Genetic\nAlgorithm (LLMGA) to create effective injection prompts, leading the target LLM\nto recognize injected context as genuine. We explore how context injection can\nbe used to elicit disallowed content, posing risks of illegal actions and\ninappropriate responses. Our elicitation strategies, including acceptance\nelicitation and word anonymization, can effectively elicit disallowed responses\nwith success rates reaching 97% on ChatGPT. Comprehensive evaluations on\nreal-world LLMs including ChatGPT and Llama-2/3 demonstrate the efficacy of\nLLMGA and our injection strategies. We also discuss potential countermeasures\nthat can be adopted for detecting injection and developing more secure models.\nOur findings provide insights into the challenges associated with the\nreal-world deployment of LLMs for interactive and structured data scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) such as ChatGPT and Llama have become prevalent\nin real-world applications, exhibiting impressive text generation performance.\nLLMs are fundamentally developed from a scenario where the input data remains\nstatic and unstructured. To behave interactively, LLM-based chat systems must\nintegrate prior chat history as context into their inputs, following a\npre-defined structure. However, LLMs cannot separate user inputs from context,\nenabling context injection. This paper introduces a methodology to\nsystematically inject misleading context (i.e., chat history) into LLM\nconversations without prior knowledge of the target model. We formalize context\ninjection through a response retrieval task and propose an LLM-Guided Genetic\nAlgorithm (LLMGA) to create effective injection prompts, leading the target LLM\nto recognize injected context as genuine. We explore how context injection can\nbe used to elicit disallowed content, posing risks of illegal actions and\ninappropriate responses. Our elicitation strategies, including acceptance\nelicitation and word anonymization, can effectively elicit disallowed responses\nwith success rates reaching 97% on ChatGPT. Comprehensive evaluations on\nreal-world LLMs including ChatGPT and Llama-2/3 demonstrate the efficacy of\nLLMGA and our injection strategies. We also discuss potential countermeasures\nthat can be adopted for detecting injection and developing more secure models.\nOur findings provide insights into the challenges associated with the\nreal-world deployment of LLMs for interactive and structured data scenarios."
                },
                "authors": [
                    {
                        "name": "Cheng'an Wei"
                    },
                    {
                        "name": "Yue Zhao"
                    },
                    {
                        "name": "Yujia Gong"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Lu Xiang"
                    },
                    {
                        "name": "Shenchen Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Shenchen Zhu"
                },
                "author": "Shenchen Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20234v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20234v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12599v1",
                "updated": "2024-08-22T17:59:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    59,
                    4,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T17:59:04Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    59,
                    4,
                    3,
                    235,
                    0
                ],
                "title": "Controllable Text Generation for Large Language Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controllable Text Generation for Large Language Models: A Survey"
                },
                "summary": "In Natural Language Processing (NLP), Large Language Models (LLMs) have\ndemonstrated high text generation quality. However, in real-world applications,\nLLMs must meet increasingly complex requirements. Beyond avoiding misleading or\ninappropriate content, LLMs are also expected to cater to specific user needs,\nsuch as imitating particular writing styles or generating text with poetic\nrichness. These varied demands have driven the development of Controllable Text\nGeneration (CTG) techniques, which ensure that outputs adhere to predefined\ncontrol conditions--such as safety, sentiment, thematic consistency, and\nlinguistic style--while maintaining high standards of helpfulness, fluency, and\ndiversity.\n  This paper systematically reviews the latest advancements in CTG for LLMs,\noffering a comprehensive definition of its core concepts and clarifying the\nrequirements for control conditions and text quality. We categorize CTG tasks\ninto two primary types: content control and attribute control. The key methods\nare discussed, including model retraining, fine-tuning, reinforcement learning,\nprompt engineering, latent space manipulation, and decoding-time intervention.\nWe analyze each method's characteristics, advantages, and limitations,\nproviding nuanced insights for achieving generation control. Additionally, we\nreview CTG evaluation methods, summarize its applications across domains, and\naddress key challenges in current research, including reduced fluency and\npracticality. We also propose several appeals, such as placing greater emphasis\non real-world applications in future research. This paper aims to offer\nvaluable guidance to researchers and developers in the field. Our reference\nlist and Chinese version are open-sourced at\nhttps://github.com/IAAR-Shanghai/CTGSurvey.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Natural Language Processing (NLP), Large Language Models (LLMs) have\ndemonstrated high text generation quality. However, in real-world applications,\nLLMs must meet increasingly complex requirements. Beyond avoiding misleading or\ninappropriate content, LLMs are also expected to cater to specific user needs,\nsuch as imitating particular writing styles or generating text with poetic\nrichness. These varied demands have driven the development of Controllable Text\nGeneration (CTG) techniques, which ensure that outputs adhere to predefined\ncontrol conditions--such as safety, sentiment, thematic consistency, and\nlinguistic style--while maintaining high standards of helpfulness, fluency, and\ndiversity.\n  This paper systematically reviews the latest advancements in CTG for LLMs,\noffering a comprehensive definition of its core concepts and clarifying the\nrequirements for control conditions and text quality. We categorize CTG tasks\ninto two primary types: content control and attribute control. The key methods\nare discussed, including model retraining, fine-tuning, reinforcement learning,\nprompt engineering, latent space manipulation, and decoding-time intervention.\nWe analyze each method's characteristics, advantages, and limitations,\nproviding nuanced insights for achieving generation control. Additionally, we\nreview CTG evaluation methods, summarize its applications across domains, and\naddress key challenges in current research, including reduced fluency and\npracticality. We also propose several appeals, such as placing greater emphasis\non real-world applications in future research. This paper aims to offer\nvaluable guidance to researchers and developers in the field. Our reference\nlist and Chinese version are open-sourced at\nhttps://github.com/IAAR-Shanghai/CTGSurvey."
                },
                "authors": [
                    {
                        "name": "Xun Liang"
                    },
                    {
                        "name": "Hanyu Wang"
                    },
                    {
                        "name": "Yezhaohui Wang"
                    },
                    {
                        "name": "Shichao Song"
                    },
                    {
                        "name": "Jiawei Yang"
                    },
                    {
                        "name": "Simin Niu"
                    },
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Dan Liu"
                    },
                    {
                        "name": "Shunyu Yao"
                    },
                    {
                        "name": "Feiyu Xiong"
                    },
                    {
                        "name": "Zhiyu Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyu Li"
                },
                "author": "Zhiyu Li",
                "arxiv_comment": "52 pages, 11 figures, 7 tables, 11 equations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "A.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13709v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13709v2",
                "updated": "2024-08-22T17:56:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    15,
                    3,
                    235,
                    0
                ],
                "published": "2024-07-18T17:08:10Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    17,
                    8,
                    10,
                    3,
                    200,
                    0
                ],
                "title": "Understanding Reference Policies in Direct Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Reference Policies in Direct Preference Optimization"
                },
                "summary": "Direct Preference Optimization (DPO) has become a widely used training method\nfor the instruction fine-tuning of large language models (LLMs). In this work,\nwe explore an under-investigated aspect of DPO - its dependency on the\nreference model or policy. Such reference policies, typically instantiated as\nthe model to be further fine-tuned, are important since they can impose an\nupper limit on DPO's effectiveness. Therefore, we address three related\nresearch questions in this work. First, we explore the optimal strength of the\nKL divergence constraint in DPO, which penalizes deviations from the reference\npolicy, and find that DPO is sensitive to this strength. Next, we examine the\nnecessity of the KL-constraint from the reference policies in DPO by providing\nboth theoretical and empirical comparisons between DPO and related learning\nobjectives, demonstrating DPO's superiority in this controlled setting.\nAdditionally, we investigate whether DPO benefits from stronger reference\npolicies, finding that a stronger reference policy can lead to improved\nperformance, but only when it is similar to the model being fine-tuned. Our\nfindings highlight the confounding role of reference policies in DPO and offer\ninsights for best practices, while also identifying open research questions for\nfuture studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Preference Optimization (DPO) has become a widely used training method\nfor the instruction fine-tuning of large language models (LLMs). In this work,\nwe explore an under-investigated aspect of DPO - its dependency on the\nreference model or policy. Such reference policies, typically instantiated as\nthe model to be further fine-tuned, are important since they can impose an\nupper limit on DPO's effectiveness. Therefore, we address three related\nresearch questions in this work. First, we explore the optimal strength of the\nKL divergence constraint in DPO, which penalizes deviations from the reference\npolicy, and find that DPO is sensitive to this strength. Next, we examine the\nnecessity of the KL-constraint from the reference policies in DPO by providing\nboth theoretical and empirical comparisons between DPO and related learning\nobjectives, demonstrating DPO's superiority in this controlled setting.\nAdditionally, we investigate whether DPO benefits from stronger reference\npolicies, finding that a stronger reference policy can lead to improved\nperformance, but only when it is similar to the model being fine-tuned. Our\nfindings highlight the confounding role of reference policies in DPO and offer\ninsights for best practices, while also identifying open research questions for\nfuture studies."
                },
                "authors": [
                    {
                        "name": "Yixin Liu"
                    },
                    {
                        "name": "Pengfei Liu"
                    },
                    {
                        "name": "Arman Cohan"
                    }
                ],
                "author_detail": {
                    "name": "Arman Cohan"
                },
                "author": "Arman Cohan",
                "arxiv_comment": "GitHub Repo: https://github.com/yale-nlp/refdpo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13709v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13709v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12590v1",
                "updated": "2024-08-22T17:55:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    55,
                    22,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T17:55:22Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    55,
                    22,
                    3,
                    235,
                    0
                ],
                "title": "xGen-VideoSyn-1: High-fidelity Text-to-Video Synthesis with Compressed\n  Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xGen-VideoSyn-1: High-fidelity Text-to-Video Synthesis with Compressed\n  Representations"
                },
                "summary": "We present xGen-VideoSyn-1, a text-to-video (T2V) generation model capable of\nproducing realistic scenes from textual descriptions. Building on recent\nadvancements, such as OpenAI's Sora, we explore the latent diffusion model\n(LDM) architecture and introduce a video variational autoencoder (VidVAE).\nVidVAE compresses video data both spatially and temporally, significantly\nreducing the length of visual tokens and the computational demands associated\nwith generating long-sequence videos. To further address the computational\ncosts, we propose a divide-and-merge strategy that maintains temporal\nconsistency across video segments. Our Diffusion Transformer (DiT) model\nincorporates spatial and temporal self-attention layers, enabling robust\ngeneralization across different timeframes and aspect ratios. We have devised a\ndata processing pipeline from the very beginning and collected over 13M\nhigh-quality video-text pairs. The pipeline includes multiple steps such as\nclipping, text detection, motion estimation, aesthetics scoring, and dense\ncaptioning based on our in-house video-LLM model. Training the VidVAE and DiT\nmodels required approximately 40 and 642 H100 days, respectively. Our model\nsupports over 14-second 720p video generation in an end-to-end way and\ndemonstrates competitive performance against state-of-the-art T2V models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present xGen-VideoSyn-1, a text-to-video (T2V) generation model capable of\nproducing realistic scenes from textual descriptions. Building on recent\nadvancements, such as OpenAI's Sora, we explore the latent diffusion model\n(LDM) architecture and introduce a video variational autoencoder (VidVAE).\nVidVAE compresses video data both spatially and temporally, significantly\nreducing the length of visual tokens and the computational demands associated\nwith generating long-sequence videos. To further address the computational\ncosts, we propose a divide-and-merge strategy that maintains temporal\nconsistency across video segments. Our Diffusion Transformer (DiT) model\nincorporates spatial and temporal self-attention layers, enabling robust\ngeneralization across different timeframes and aspect ratios. We have devised a\ndata processing pipeline from the very beginning and collected over 13M\nhigh-quality video-text pairs. The pipeline includes multiple steps such as\nclipping, text detection, motion estimation, aesthetics scoring, and dense\ncaptioning based on our in-house video-LLM model. Training the VidVAE and DiT\nmodels required approximately 40 and 642 H100 days, respectively. Our model\nsupports over 14-second 720p video generation in an end-to-end way and\ndemonstrates competitive performance against state-of-the-art T2V models."
                },
                "authors": [
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Congying Xia"
                    },
                    {
                        "name": "Krithika Ramakrishnan"
                    },
                    {
                        "name": "Michael Ryoo"
                    },
                    {
                        "name": "Lifu Tu"
                    },
                    {
                        "name": "Yihao Feng"
                    },
                    {
                        "name": "Manli Shu"
                    },
                    {
                        "name": "Honglu Zhou"
                    },
                    {
                        "name": "Anas Awadalla"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Senthil Purushwalkam"
                    },
                    {
                        "name": "Le Xue"
                    },
                    {
                        "name": "Yingbo Zhou"
                    },
                    {
                        "name": "Huan Wang"
                    },
                    {
                        "name": "Silvio Savarese"
                    },
                    {
                        "name": "Juan Carlos Niebles"
                    },
                    {
                        "name": "Zeyuan Chen"
                    },
                    {
                        "name": "Ran Xu"
                    },
                    {
                        "name": "Caiming Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Caiming Xiong"
                },
                "author": "Caiming Xiong",
                "arxiv_comment": "Accepted by ECCV24 AI4VA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12579v1",
                "updated": "2024-08-22T17:44:40Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    44,
                    40,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T17:44:40Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    44,
                    40,
                    3,
                    235,
                    0
                ],
                "title": "RuleAlign: Making Large Language Models Better Physicians with\n  Diagnostic Rule Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RuleAlign: Making Large Language Models Better Physicians with\n  Diagnostic Rule Alignment"
                },
                "summary": "Large Language Models (LLMs) like GPT-4, MedPaLM-2, and Med-Gemini achieve\nperformance competitively with human experts across various medical benchmarks.\nHowever, they still face challenges in making professional diagnoses akin to\nphysicians, particularly in efficiently gathering patient information and\nreasoning the final diagnosis. To this end, we introduce the RuleAlign\nframework, designed to align LLMs with specific diagnostic rules. We develop a\nmedical dialogue dataset comprising rule-based communications between patients\nand physicians and design an alignment learning approach through preference\nlearning. Experimental results demonstrate the effectiveness of the proposed\napproach. We hope that our work can serve as an inspiration for exploring the\npotential of LLMs as AI physicians.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) like GPT-4, MedPaLM-2, and Med-Gemini achieve\nperformance competitively with human experts across various medical benchmarks.\nHowever, they still face challenges in making professional diagnoses akin to\nphysicians, particularly in efficiently gathering patient information and\nreasoning the final diagnosis. To this end, we introduce the RuleAlign\nframework, designed to align LLMs with specific diagnostic rules. We develop a\nmedical dialogue dataset comprising rule-based communications between patients\nand physicians and design an alignment learning approach through preference\nlearning. Experimental results demonstrate the effectiveness of the proposed\napproach. We hope that our work can serve as an inspiration for exploring the\npotential of LLMs as AI physicians."
                },
                "authors": [
                    {
                        "name": "Xiaohan Wang"
                    },
                    {
                        "name": "Xiaoyan Yang"
                    },
                    {
                        "name": "Yuqi Zhu"
                    },
                    {
                        "name": "Yue Shen"
                    },
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Peng Wei"
                    },
                    {
                        "name": "Lei Liang"
                    },
                    {
                        "name": "Jinjie Gu"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Ningyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ningyu Zhang"
                },
                "author": "Ningyu Zhang",
                "arxiv_comment": "Ongoing work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.21051v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.21051v2",
                "updated": "2024-08-22T17:27:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    27,
                    45,
                    3,
                    235,
                    0
                ],
                "published": "2024-05-31T17:43:59Z",
                "published_parsed": [
                    2024,
                    5,
                    31,
                    17,
                    43,
                    59,
                    4,
                    152,
                    0
                ],
                "title": "Good Modelling Software Practices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Good Modelling Software Practices"
                },
                "summary": "Frequently in socio-environmental sciences, models are used as tools to\nrepresent, understand, project and predict the behaviour of these complex\nsystems. Along the modelling chain, Good Modelling Practices have been evolving\nthat ensure -- amongst others -- that models are transparent and their results\nreplicable. Whenever such models are represented in software, Good Modelling\nmeet Good Software Practices, such as a tractable development workflow, good\ncode, collaborative development and governance, continuous integration and\ndeployment; and they meet Good Scientific Practices, such as attribution of\ncopyrights and acknowledgement of intellectual property, publication of a\nsoftware paper and archiving. Too often in existing socio-environmental model\nsoftware, these practices have been regarded as an add-on to be considered at a\nlater stage only; modellers have shied away from publishing their model as open\nsource out of fear that having to add good practices is too demanding. We here\nargue for making a habit of following a list of simple and not so simple\npractices early on in the implementation of the model life cycle. We\ncontextualise cherry-picked and hands-on practices for supporting Good\nModelling Practice, and we demonstrate their application in the example context\nof the Viable North Sea fisheries socio-ecological systems model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frequently in socio-environmental sciences, models are used as tools to\nrepresent, understand, project and predict the behaviour of these complex\nsystems. Along the modelling chain, Good Modelling Practices have been evolving\nthat ensure -- amongst others -- that models are transparent and their results\nreplicable. Whenever such models are represented in software, Good Modelling\nmeet Good Software Practices, such as a tractable development workflow, good\ncode, collaborative development and governance, continuous integration and\ndeployment; and they meet Good Scientific Practices, such as attribution of\ncopyrights and acknowledgement of intellectual property, publication of a\nsoftware paper and archiving. Too often in existing socio-environmental model\nsoftware, these practices have been regarded as an add-on to be considered at a\nlater stage only; modellers have shied away from publishing their model as open\nsource out of fear that having to add good practices is too demanding. We here\nargue for making a habit of following a list of simple and not so simple\npractices early on in the implementation of the model life cycle. We\ncontextualise cherry-picked and hands-on practices for supporting Good\nModelling Practice, and we demonstrate their application in the example context\nof the Viable North Sea fisheries socio-ecological systems model."
                },
                "authors": [
                    {
                        "name": "Carsten Lemmen"
                    },
                    {
                        "name": "Philipp Sebastian Sommer"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Sebastian Sommer"
                },
                "author": "Philipp Sebastian Sommer",
                "arxiv_comment": "2 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.21051v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.21051v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.1.0; D.2.4; D.2.5; D.2.11; D.2.12; G.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08924v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08924v2",
                "updated": "2024-08-22T17:21:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    21,
                    34,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-15T14:51:32Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    14,
                    51,
                    32,
                    3,
                    228,
                    0
                ],
                "title": "Prefix Guidance: A Steering Wheel for Large Language Models to Defend\n  Against Jailbreak Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefix Guidance: A Steering Wheel for Large Language Models to Defend\n  Against Jailbreak Attacks"
                },
                "summary": "In recent years, the rapid development of large language models (LLMs) has\nachieved remarkable performance across various tasks. However, research\nindicates that LLMs are vulnerable to jailbreak attacks, where adversaries can\ninduce the generation of harmful content through meticulously crafted prompts.\nThis vulnerability poses significant challenges to the secure use and promotion\nof LLMs. Existing defense methods offer protection from different perspectives\nbut often suffer from insufficient effectiveness or a significant impact on the\nmodel's capabilities. In this paper, we propose a plug-and-play and\neasy-to-deploy jailbreak defense framework, namely Prefix Guidance (PG), which\nguides the model to identify harmful prompts by directly setting the first few\ntokens of the model's output. This approach combines the model's inherent\nsecurity capabilities with an external classifier to defend against jailbreak\nattacks. We demonstrate the effectiveness of PG across three models and five\nattack methods. Compared to baselines, our approach is generally more effective\non average. Additionally, results on the Just-Eval benchmark further confirm\nPG's superiority to preserve the model's performance. our code is available at\nhttps://github.com/weiyezhimeng/Prefix-Guidance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the rapid development of large language models (LLMs) has\nachieved remarkable performance across various tasks. However, research\nindicates that LLMs are vulnerable to jailbreak attacks, where adversaries can\ninduce the generation of harmful content through meticulously crafted prompts.\nThis vulnerability poses significant challenges to the secure use and promotion\nof LLMs. Existing defense methods offer protection from different perspectives\nbut often suffer from insufficient effectiveness or a significant impact on the\nmodel's capabilities. In this paper, we propose a plug-and-play and\neasy-to-deploy jailbreak defense framework, namely Prefix Guidance (PG), which\nguides the model to identify harmful prompts by directly setting the first few\ntokens of the model's output. This approach combines the model's inherent\nsecurity capabilities with an external classifier to defend against jailbreak\nattacks. We demonstrate the effectiveness of PG across three models and five\nattack methods. Compared to baselines, our approach is generally more effective\non average. Additionally, results on the Just-Eval benchmark further confirm\nPG's superiority to preserve the model's performance. our code is available at\nhttps://github.com/weiyezhimeng/Prefix-Guidance."
                },
                "authors": [
                    {
                        "name": "Jiawei Zhao"
                    },
                    {
                        "name": "Kejiang Chen"
                    },
                    {
                        "name": "Xiaojian Yuan"
                    },
                    {
                        "name": "Weiming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weiming Zhang"
                },
                "author": "Weiming Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08924v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08924v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12547v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12547v1",
                "updated": "2024-08-22T17:01:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    1,
                    34,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T17:01:34Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    1,
                    34,
                    3,
                    235,
                    0
                ],
                "title": "Towards Evaluating and Building Versatile Large Language Models for\n  Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Evaluating and Building Versatile Large Language Models for\n  Medicine"
                },
                "summary": "In this study, we present MedS-Bench, a comprehensive benchmark designed to\nevaluate the performance of large language models (LLMs) in clinical contexts.\nUnlike existing benchmarks that focus on multiple-choice question answering,\nMedS-Bench spans 11 high-level clinical tasks, including clinical report\nsummarization, treatment recommendations, diagnosis, named entity recognition,\nand medical concept explanation, among others. We evaluated six leading LLMs,\ne.g., MEDITRON, Mistral, InternLM 2, Llama 3, GPT-4, and Claude-3.5 using\nfew-shot prompting, and found that even the most sophisticated models struggle\nwith these complex tasks. To address these limitations, we developed MedS-Ins,\na large-scale instruction tuning dataset for medicine. MedS-Ins comprises 58\nmedically oriented language corpora, totaling 13.5 million samples across 122\ntasks. To demonstrate the dataset's utility, we conducted a proof-of-concept\nexperiment by performing instruction tuning on a lightweight, open-source\nmedical language model. The resulting model, MMedIns-Llama 3, significantly\noutperformed existing models across nearly all clinical tasks. To promote\nfurther advancements in the application of LLMs to clinical challenges, we have\nmade the MedS-Ins dataset fully accessible and invite the research community to\ncontribute to its expansion.Additionally, we have launched a dynamic\nleaderboard for MedS-Bench, which we plan to regularly update the test set to\ntrack progress and enhance the adaptation of general LLMs to the medical\ndomain. Leaderboard: https://henrychur.github.io/MedS-Bench/. Github:\nhttps://github.com/MAGIC-AI4Med/MedS-Ins.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we present MedS-Bench, a comprehensive benchmark designed to\nevaluate the performance of large language models (LLMs) in clinical contexts.\nUnlike existing benchmarks that focus on multiple-choice question answering,\nMedS-Bench spans 11 high-level clinical tasks, including clinical report\nsummarization, treatment recommendations, diagnosis, named entity recognition,\nand medical concept explanation, among others. We evaluated six leading LLMs,\ne.g., MEDITRON, Mistral, InternLM 2, Llama 3, GPT-4, and Claude-3.5 using\nfew-shot prompting, and found that even the most sophisticated models struggle\nwith these complex tasks. To address these limitations, we developed MedS-Ins,\na large-scale instruction tuning dataset for medicine. MedS-Ins comprises 58\nmedically oriented language corpora, totaling 13.5 million samples across 122\ntasks. To demonstrate the dataset's utility, we conducted a proof-of-concept\nexperiment by performing instruction tuning on a lightweight, open-source\nmedical language model. The resulting model, MMedIns-Llama 3, significantly\noutperformed existing models across nearly all clinical tasks. To promote\nfurther advancements in the application of LLMs to clinical challenges, we have\nmade the MedS-Ins dataset fully accessible and invite the research community to\ncontribute to its expansion.Additionally, we have launched a dynamic\nleaderboard for MedS-Bench, which we plan to regularly update the test set to\ntrack progress and enhance the adaptation of general LLMs to the medical\ndomain. Leaderboard: https://henrychur.github.io/MedS-Bench/. Github:\nhttps://github.com/MAGIC-AI4Med/MedS-Ins."
                },
                "authors": [
                    {
                        "name": "Chaoyi Wu"
                    },
                    {
                        "name": "Pengcheng Qiu"
                    },
                    {
                        "name": "Jinxin Liu"
                    },
                    {
                        "name": "Hongfei Gu"
                    },
                    {
                        "name": "Na Li"
                    },
                    {
                        "name": "Ya Zhang"
                    },
                    {
                        "name": "Yanfeng Wang"
                    },
                    {
                        "name": "Weidi Xie"
                    }
                ],
                "author_detail": {
                    "name": "Weidi Xie"
                },
                "author": "Weidi Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12547v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12547v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12513v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12513v1",
                "updated": "2024-08-22T16:08:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    16,
                    8,
                    45,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T16:08:45Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    16,
                    8,
                    45,
                    3,
                    235,
                    0
                ],
                "title": "Beyond Shortsighted Navigation: Merging Best View Trajectory Planning\n  with Robot Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Shortsighted Navigation: Merging Best View Trajectory Planning\n  with Robot Navigation"
                },
                "summary": "Gathering visual information effectively to monitor known environments is a\nkey challenge in robotics. To be as efficient as human surveyors, robotic\nsystems must continuously collect observational data required to complete their\nsurvey task. Inspection personnel instinctively know to look at relevant\nequipment that happens to be ``along the way.'' In this paper, we introduce a\nnovel framework for continuous long-horizon viewpoint planning, for ground\nrobots, applied to tasks involving patrolling, monitoring or visual data\ngathering in known environments. Our approach to Long Horizon Viewpoint\nPlanning (LHVP), enables the robot to autonomously navigate and collect\nenvironmental data optimizing for coverage over the horizon of the patrol.\nLeveraging a quadruped's mobility and sensory capabilities, our LHVP framework\nplans patrol paths that account for coupling the viewpoint planner for the arm\ncamera with the mobile base's navigation planner. The viewpath optimization\nalgorithm seeks a balance between comprehensive environmental coverage and\ndynamically feasible movements, thus ensuring prolonged and effective operation\nin scenarios including monitoring, security surveillance, and disaster\nresponse. We validate our approach through simulations and in the real world\nand show that our LHVP significantly outperforms naive patrolling methods in\nterms of area coverage generating information-gathering trajectories for the\nrobot arm. Our results indicate a promising direction for the deployment of\nmobile robots in long-term, autonomous surveying, and environmental data\ncollection tasks, highlighting the potential of intelligent robotic systems in\nchallenging real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gathering visual information effectively to monitor known environments is a\nkey challenge in robotics. To be as efficient as human surveyors, robotic\nsystems must continuously collect observational data required to complete their\nsurvey task. Inspection personnel instinctively know to look at relevant\nequipment that happens to be ``along the way.'' In this paper, we introduce a\nnovel framework for continuous long-horizon viewpoint planning, for ground\nrobots, applied to tasks involving patrolling, monitoring or visual data\ngathering in known environments. Our approach to Long Horizon Viewpoint\nPlanning (LHVP), enables the robot to autonomously navigate and collect\nenvironmental data optimizing for coverage over the horizon of the patrol.\nLeveraging a quadruped's mobility and sensory capabilities, our LHVP framework\nplans patrol paths that account for coupling the viewpoint planner for the arm\ncamera with the mobile base's navigation planner. The viewpath optimization\nalgorithm seeks a balance between comprehensive environmental coverage and\ndynamically feasible movements, thus ensuring prolonged and effective operation\nin scenarios including monitoring, security surveillance, and disaster\nresponse. We validate our approach through simulations and in the real world\nand show that our LHVP significantly outperforms naive patrolling methods in\nterms of area coverage generating information-gathering trajectories for the\nrobot arm. Our results indicate a promising direction for the deployment of\nmobile robots in long-term, autonomous surveying, and environmental data\ncollection tasks, highlighting the potential of intelligent robotic systems in\nchallenging real-world applications."
                },
                "authors": [
                    {
                        "name": "Srinath Tankasala"
                    },
                    {
                        "name": "Roberto Mart√≠n-Mart√≠n"
                    },
                    {
                        "name": "Mitch Pryor"
                    }
                ],
                "author_detail": {
                    "name": "Mitch Pryor"
                },
                "author": "Mitch Pryor",
                "arxiv_comment": "7 pages, 8 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12513v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12513v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10259v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10259v3",
                "updated": "2024-08-22T15:52:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    15,
                    52,
                    13,
                    3,
                    235,
                    0
                ],
                "published": "2024-04-16T03:26:43Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    3,
                    26,
                    43,
                    1,
                    107,
                    0
                ],
                "title": "Uncovering Latent Arguments in Social Media Messaging by Employing\n  LLMs-in-the-Loop Strategy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering Latent Arguments in Social Media Messaging by Employing\n  LLMs-in-the-Loop Strategy"
                },
                "summary": "The widespread use of social media has led to a surge in popularity for\nautomated methods of analyzing public opinion. Supervised methods are adept at\ntext categorization, yet the dynamic nature of social media discussions poses a\ncontinual challenge for these techniques due to the constant shifting of the\nfocus. On the other hand, traditional unsupervised methods for extracting\nthemes from public discourse, such as topic modeling, often reveal overarching\npatterns that might not capture specific nuances. Consequently, a significant\nportion of research into social media discourse still depends on\nlabor-intensive manual coding techniques and a human-in-the-loop approach,\nwhich are both time-consuming and costly. In this work, we study the problem of\ndiscovering arguments associated with a specific theme. We propose a generic\nLLMs-in-the-Loop strategy that leverages the advanced capabilities of Large\nLanguage Models (LLMs) to extract latent arguments from social media messaging.\nTo demonstrate our approach, we apply our framework to contentious topics. We\nuse two publicly available datasets: (1) the climate campaigns dataset of 14k\nFacebook ads with 25 themes and (2) the COVID-19 vaccine campaigns dataset of\n9k Facebook ads with 14 themes. Additionally, we design a downstream task as\nstance prediction by leveraging talking points in climate debates. Furthermore,\nwe analyze demographic targeting and the adaptation of messaging based on\nreal-world events.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread use of social media has led to a surge in popularity for\nautomated methods of analyzing public opinion. Supervised methods are adept at\ntext categorization, yet the dynamic nature of social media discussions poses a\ncontinual challenge for these techniques due to the constant shifting of the\nfocus. On the other hand, traditional unsupervised methods for extracting\nthemes from public discourse, such as topic modeling, often reveal overarching\npatterns that might not capture specific nuances. Consequently, a significant\nportion of research into social media discourse still depends on\nlabor-intensive manual coding techniques and a human-in-the-loop approach,\nwhich are both time-consuming and costly. In this work, we study the problem of\ndiscovering arguments associated with a specific theme. We propose a generic\nLLMs-in-the-Loop strategy that leverages the advanced capabilities of Large\nLanguage Models (LLMs) to extract latent arguments from social media messaging.\nTo demonstrate our approach, we apply our framework to contentious topics. We\nuse two publicly available datasets: (1) the climate campaigns dataset of 14k\nFacebook ads with 25 themes and (2) the COVID-19 vaccine campaigns dataset of\n9k Facebook ads with 14 themes. Additionally, we design a downstream task as\nstance prediction by leveraging talking points in climate debates. Furthermore,\nwe analyze demographic targeting and the adaptation of messaging based on\nreal-world events."
                },
                "authors": [
                    {
                        "name": "Tunazzina Islam"
                    },
                    {
                        "name": "Dan Goldwasser"
                    }
                ],
                "author_detail": {
                    "name": "Dan Goldwasser"
                },
                "author": "Dan Goldwasser",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10259v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10259v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11484v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11484v5",
                "updated": "2024-08-22T15:44:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    15,
                    44,
                    27,
                    3,
                    235,
                    0
                ],
                "published": "2024-07-16T08:20:39Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    8,
                    20,
                    39,
                    1,
                    198,
                    0
                ],
                "title": "The Oscars of AI Theater: A Survey on Role-Playing with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Oscars of AI Theater: A Survey on Role-Playing with Language Models"
                },
                "summary": "This survey explores the burgeoning field of role-playing with language\nmodels, focusing on their development from early persona-based models to\nadvanced character-driven simulations facilitated by Large Language Models\n(LLMs). Initially confined to simple persona consistency due to limited model\ncapabilities, role-playing tasks have now expanded to embrace complex character\nportrayals involving character consistency, behavioral alignment, and overall\nattractiveness. We provide a comprehensive taxonomy of the critical components\nin designing these systems, including data, models and alignment, agent\narchitecture and evaluation. This survey not only outlines the current\nmethodologies and challenges, such as managing dynamic personal profiles and\nachieving high-level persona consistency but also suggests avenues for future\nresearch in improving the depth and realism of role-playing applications. The\ngoal is to guide future research by offering a structured overview of current\nmethodologies and identifying potential areas for improvement. Related\nresources and papers are available at\nhttps://github.com/nuochenpku/Awesome-Role-Play-Papers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This survey explores the burgeoning field of role-playing with language\nmodels, focusing on their development from early persona-based models to\nadvanced character-driven simulations facilitated by Large Language Models\n(LLMs). Initially confined to simple persona consistency due to limited model\ncapabilities, role-playing tasks have now expanded to embrace complex character\nportrayals involving character consistency, behavioral alignment, and overall\nattractiveness. We provide a comprehensive taxonomy of the critical components\nin designing these systems, including data, models and alignment, agent\narchitecture and evaluation. This survey not only outlines the current\nmethodologies and challenges, such as managing dynamic personal profiles and\nachieving high-level persona consistency but also suggests avenues for future\nresearch in improving the depth and realism of role-playing applications. The\ngoal is to guide future research by offering a structured overview of current\nmethodologies and identifying potential areas for improvement. Related\nresources and papers are available at\nhttps://github.com/nuochenpku/Awesome-Role-Play-Papers."
                },
                "authors": [
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Yang Deng"
                    },
                    {
                        "name": "Jia Li"
                    }
                ],
                "author_detail": {
                    "name": "Jia Li"
                },
                "author": "Jia Li",
                "arxiv_comment": "28 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11484v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11484v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12496v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12496v1",
                "updated": "2024-08-22T15:41:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    15,
                    41,
                    58,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T15:41:58Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    15,
                    41,
                    58,
                    3,
                    235,
                    0
                ],
                "title": "MEDCO: Medical Education Copilots Based on A Multi-Agent Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEDCO: Medical Education Copilots Based on A Multi-Agent Framework"
                },
                "summary": "Large language models (LLMs) have had a significant impact on diverse\nresearch domains, including medicine and healthcare. However, the potential of\nLLMs as copilots in medical education remains underexplored. Current\nAI-assisted educational tools are limited by their solitary learning approach\nand inability to simulate the multi-disciplinary and interactive nature of\nactual medical training. To address these limitations, we propose MEDCO\n(Medical EDucation COpilots), a novel multi-agent-based copilot system\nspecially developed to emulate real-world medical training environments. MEDCO\nincorporates three primary agents: an agentic patient, an expert doctor, and a\nradiologist, facilitating a multi-modal and interactive learning environment.\nOur framework emphasizes the learning of proficient question-asking skills,\nmulti-disciplinary collaboration, and peer discussions between students. Our\nexperiments show that simulated virtual students who underwent training with\nMEDCO not only achieved substantial performance enhancements comparable to\nthose of advanced models, but also demonstrated human-like learning behaviors\nand improvements, coupled with an increase in the number of learning samples.\nThis work contributes to medical education by introducing a copilot that\nimplements an interactive and collaborative learning approach. It also provides\nvaluable insights into the effectiveness of AI-integrated training paradigms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have had a significant impact on diverse\nresearch domains, including medicine and healthcare. However, the potential of\nLLMs as copilots in medical education remains underexplored. Current\nAI-assisted educational tools are limited by their solitary learning approach\nand inability to simulate the multi-disciplinary and interactive nature of\nactual medical training. To address these limitations, we propose MEDCO\n(Medical EDucation COpilots), a novel multi-agent-based copilot system\nspecially developed to emulate real-world medical training environments. MEDCO\nincorporates three primary agents: an agentic patient, an expert doctor, and a\nradiologist, facilitating a multi-modal and interactive learning environment.\nOur framework emphasizes the learning of proficient question-asking skills,\nmulti-disciplinary collaboration, and peer discussions between students. Our\nexperiments show that simulated virtual students who underwent training with\nMEDCO not only achieved substantial performance enhancements comparable to\nthose of advanced models, but also demonstrated human-like learning behaviors\nand improvements, coupled with an increase in the number of learning samples.\nThis work contributes to medical education by introducing a copilot that\nimplements an interactive and collaborative learning approach. It also provides\nvaluable insights into the effectiveness of AI-integrated training paradigms."
                },
                "authors": [
                    {
                        "name": "Hao Wei"
                    },
                    {
                        "name": "Jianing Qiu"
                    },
                    {
                        "name": "Haibao Yu"
                    },
                    {
                        "name": "Wu Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Wu Yuan"
                },
                "author": "Wu Yuan",
                "arxiv_journal_ref": "ECCV 2024 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12496v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12496v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12494v1",
                "updated": "2024-08-22T15:35:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    15,
                    35,
                    46,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T15:35:46Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    15,
                    35,
                    46,
                    3,
                    235,
                    0
                ],
                "title": "GenderCARE: A Comprehensive Framework for Assessing and Reducing Gender\n  Bias in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenderCARE: A Comprehensive Framework for Assessing and Reducing Gender\n  Bias in Large Language Models"
                },
                "summary": "Large language models (LLMs) have exhibited remarkable capabilities in\nnatural language generation, but they have also been observed to magnify\nsocietal biases, particularly those related to gender. In response to this\nissue, several benchmarks have been proposed to assess gender bias in LLMs.\nHowever, these benchmarks often lack practical flexibility or inadvertently\nintroduce biases. To address these shortcomings, we introduce GenderCARE, a\ncomprehensive framework that encompasses innovative Criteria, bias Assessment,\nReduction techniques, and Evaluation metrics for quantifying and mitigating\ngender bias in LLMs. To begin, we establish pioneering criteria for gender\nequality benchmarks, spanning dimensions such as inclusivity, diversity,\nexplainability, objectivity, robustness, and realisticity. Guided by these\ncriteria, we construct GenderPair, a novel pair-based benchmark designed to\nassess gender bias in LLMs comprehensively. Our benchmark provides standardized\nand realistic evaluations, including previously overlooked gender groups such\nas transgender and non-binary individuals. Furthermore, we develop effective\ndebiasing techniques that incorporate counterfactual data augmentation and\nspecialized fine-tuning strategies to reduce gender bias in LLMs without\ncompromising their overall performance. Extensive experiments demonstrate a\nsignificant reduction in various gender bias benchmarks, with reductions\npeaking at over 90% and averaging above 35% across 17 different LLMs.\nImportantly, these reductions come with minimal variability in mainstream\nlanguage tasks, remaining below 2%. By offering a realistic assessment and\ntailored reduction of gender biases, we hope that our GenderCARE can represent\na significant step towards achieving fairness and equity in LLMs. More details\nare available at https://github.com/kstanghere/GenderCARE-ccs24.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have exhibited remarkable capabilities in\nnatural language generation, but they have also been observed to magnify\nsocietal biases, particularly those related to gender. In response to this\nissue, several benchmarks have been proposed to assess gender bias in LLMs.\nHowever, these benchmarks often lack practical flexibility or inadvertently\nintroduce biases. To address these shortcomings, we introduce GenderCARE, a\ncomprehensive framework that encompasses innovative Criteria, bias Assessment,\nReduction techniques, and Evaluation metrics for quantifying and mitigating\ngender bias in LLMs. To begin, we establish pioneering criteria for gender\nequality benchmarks, spanning dimensions such as inclusivity, diversity,\nexplainability, objectivity, robustness, and realisticity. Guided by these\ncriteria, we construct GenderPair, a novel pair-based benchmark designed to\nassess gender bias in LLMs comprehensively. Our benchmark provides standardized\nand realistic evaluations, including previously overlooked gender groups such\nas transgender and non-binary individuals. Furthermore, we develop effective\ndebiasing techniques that incorporate counterfactual data augmentation and\nspecialized fine-tuning strategies to reduce gender bias in LLMs without\ncompromising their overall performance. Extensive experiments demonstrate a\nsignificant reduction in various gender bias benchmarks, with reductions\npeaking at over 90% and averaging above 35% across 17 different LLMs.\nImportantly, these reductions come with minimal variability in mainstream\nlanguage tasks, remaining below 2%. By offering a realistic assessment and\ntailored reduction of gender biases, we hope that our GenderCARE can represent\na significant step towards achieving fairness and equity in LLMs. More details\nare available at https://github.com/kstanghere/GenderCARE-ccs24."
                },
                "authors": [
                    {
                        "name": "Kunsheng Tang"
                    },
                    {
                        "name": "Wenbo Zhou"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Aishan Liu"
                    },
                    {
                        "name": "Gelei Deng"
                    },
                    {
                        "name": "Shuai Li"
                    },
                    {
                        "name": "Peigui Qi"
                    },
                    {
                        "name": "Weiming Zhang"
                    },
                    {
                        "name": "Tianwei Zhang"
                    },
                    {
                        "name": "Nenghai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Nenghai Yu"
                },
                "author": "Nenghai Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]