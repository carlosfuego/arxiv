[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2403.06892v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.06892v2",
                "updated": "2024-12-02T11:24:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    24,
                    20,
                    0,
                    337,
                    0
                ],
                "published": "2024-03-11T16:48:25Z",
                "published_parsed": [
                    2024,
                    3,
                    11,
                    16,
                    48,
                    25,
                    0,
                    71,
                    0
                ],
                "title": "Real-time Transformer-based Open-Vocabulary Detection with Efficient\n  Fusion Head",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time Transformer-based Open-Vocabulary Detection with Efficient\n  Fusion Head"
                },
                "summary": "End-to-end transformer-based detectors (DETRs) have shown exceptional\nperformance in both closed-set and open-vocabulary object detection (OVD) tasks\nthrough the integration of language modalities. However, their demanding\ncomputational requirements have hindered their practical application in\nreal-time object detection (OD) scenarios. In this paper, we scrutinize the\nlimitations of two leading models in the OVDEval benchmark, OmDet and\nGrounding-DINO, and introduce OmDet-Turbo. This novel transformer-based\nreal-time OVD model features an innovative Efficient Fusion Head (EFH) module\ndesigned to alleviate the bottlenecks observed in OmDet and Grounding-DINO.\nNotably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with\nTensorRT and language cache techniques applied. Notably, in zero-shot scenarios\non COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on\npar with current state-of-the-art supervised models. Furthermore, it\nestablishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting an\nAP of 30.1 and an NMS-AP of 26.86, respectively. The practicality of\nOmDet-Turbo in industrial applications is underscored by its exceptional\nperformance on benchmark datasets and superior inference speed, positioning it\nas a compelling choice for real-time object detection tasks. Code:\n\\url{https://github.com/om-ai-lab/OmDet}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-end transformer-based detectors (DETRs) have shown exceptional\nperformance in both closed-set and open-vocabulary object detection (OVD) tasks\nthrough the integration of language modalities. However, their demanding\ncomputational requirements have hindered their practical application in\nreal-time object detection (OD) scenarios. In this paper, we scrutinize the\nlimitations of two leading models in the OVDEval benchmark, OmDet and\nGrounding-DINO, and introduce OmDet-Turbo. This novel transformer-based\nreal-time OVD model features an innovative Efficient Fusion Head (EFH) module\ndesigned to alleviate the bottlenecks observed in OmDet and Grounding-DINO.\nNotably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with\nTensorRT and language cache techniques applied. Notably, in zero-shot scenarios\non COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on\npar with current state-of-the-art supervised models. Furthermore, it\nestablishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting an\nAP of 30.1 and an NMS-AP of 26.86, respectively. The practicality of\nOmDet-Turbo in industrial applications is underscored by its exceptional\nperformance on benchmark datasets and superior inference speed, positioning it\nas a compelling choice for real-time object detection tasks. Code:\n\\url{https://github.com/om-ai-lab/OmDet}"
                },
                "authors": [
                    {
                        "name": "Tiancheng Zhao"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Xuan He"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Kyusong Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kyusong Lee"
                },
                "author": "Kyusong Lee",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.06892v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.06892v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04762v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04762v2",
                "updated": "2024-12-02T06:30:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    6,
                    30,
                    4,
                    0,
                    337,
                    0
                ],
                "published": "2024-11-07T14:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    59,
                    44,
                    3,
                    312,
                    0
                ],
                "title": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems"
                },
                "summary": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms."
                },
                "authors": [
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Jiaxu Wu"
                    },
                    {
                        "name": "Zemin Sun"
                    },
                    {
                        "name": "Long He"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Abbas Jamalipour"
                    },
                    {
                        "name": "Shiwen Mao"
                    }
                ],
                "author_detail": {
                    "name": "Shiwen Mao"
                },
                "author": "Shiwen Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04762v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04762v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02532v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02532v3",
                "updated": "2024-11-30T21:33:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    30,
                    21,
                    33,
                    59,
                    5,
                    335,
                    0
                ],
                "published": "2024-06-04T17:53:36Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    53,
                    36,
                    1,
                    156,
                    0
                ],
                "title": "SpecExec: Massively Parallel Speculative Decoding for Interactive LLM\n  Inference on Consumer Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecExec: Massively Parallel Speculative Decoding for Interactive LLM\n  Inference on Consumer Devices"
                },
                "summary": "As large language models gain widespread adoption, running them efficiently\nbecomes crucial. Recent works on LLM inference use speculative decoding to\nachieve extreme speedups. However, most of these works implicitly design their\nalgorithms for high-end datacenter hardware. In this work, we ask the opposite\nquestion: how fast can we run LLMs on consumer machines? Consumer GPUs can no\nlonger fit the largest available models (50B+ parameters) and must offload them\nto RAM or SSD. When running with offloaded parameters, the inference engine can\nprocess batches of hundreds or thousands of tokens at the same time as just one\ntoken, making it a natural fit for speculative decoding. We propose SpecExec\n(Speculative Execution), a simple parallel decoding method that can generate up\nto 20 tokens per target model iteration for popular LLM families. It utilizes\nthe high spikiness of the token probabilities distribution in modern LLMs and a\nhigh degree of alignment between model output probabilities. SpecExec takes the\nmost probable tokens continuation from the draft model to build a \"cache\" tree\nfor the target model, which then gets validated in a single pass. Using\nSpecExec, we demonstrate inference of 50B+ parameter LLMs on consumer GPUs with\nRAM offloading at 4-6 tokens per second with 4-bit quantization or 2-3 tokens\nper second with 16-bit weights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models gain widespread adoption, running them efficiently\nbecomes crucial. Recent works on LLM inference use speculative decoding to\nachieve extreme speedups. However, most of these works implicitly design their\nalgorithms for high-end datacenter hardware. In this work, we ask the opposite\nquestion: how fast can we run LLMs on consumer machines? Consumer GPUs can no\nlonger fit the largest available models (50B+ parameters) and must offload them\nto RAM or SSD. When running with offloaded parameters, the inference engine can\nprocess batches of hundreds or thousands of tokens at the same time as just one\ntoken, making it a natural fit for speculative decoding. We propose SpecExec\n(Speculative Execution), a simple parallel decoding method that can generate up\nto 20 tokens per target model iteration for popular LLM families. It utilizes\nthe high spikiness of the token probabilities distribution in modern LLMs and a\nhigh degree of alignment between model output probabilities. SpecExec takes the\nmost probable tokens continuation from the draft model to build a \"cache\" tree\nfor the target model, which then gets validated in a single pass. Using\nSpecExec, we demonstrate inference of 50B+ parameter LLMs on consumer GPUs with\nRAM offloading at 4-6 tokens per second with 4-bit quantization or 2-3 tokens\nper second with 16-bit weights."
                },
                "authors": [
                    {
                        "name": "Ruslan Svirschevski"
                    },
                    {
                        "name": "Avner May"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Max Ryabinin"
                    }
                ],
                "author_detail": {
                    "name": "Max Ryabinin"
                },
                "author": "Max Ryabinin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02532v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02532v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19730v1",
                "updated": "2024-11-29T14:23:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    14,
                    23,
                    25,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T14:23:25Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    14,
                    23,
                    25,
                    4,
                    334,
                    0
                ],
                "title": "Ten Ways in which Virtual Reality Differs from Video Streaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ten Ways in which Virtual Reality Differs from Video Streaming"
                },
                "summary": "Virtual Reality (VR) applications have a number of unique characteristics\nthat set them apart from traditional video streaming. These characteristics\nhave major implications on the design of VR rendering, adaptation, prefetching,\ncaching, and transport mechanisms. This paper contrasts VR to video streaming,\nstored 2D video streaming in particular, and discusses how to rethink system\nand network support for VR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual Reality (VR) applications have a number of unique characteristics\nthat set them apart from traditional video streaming. These characteristics\nhave major implications on the design of VR rendering, adaptation, prefetching,\ncaching, and transport mechanisms. This paper contrasts VR to video streaming,\nstored 2D video streaming in particular, and discusses how to rethink system\nand network support for VR."
                },
                "authors": [
                    {
                        "name": "Gustavo de Veciana"
                    },
                    {
                        "name": "Sonia Fahmy"
                    },
                    {
                        "name": "George Kesidis"
                    },
                    {
                        "name": "Voicu Popescu"
                    }
                ],
                "author_detail": {
                    "name": "Voicu Popescu"
                },
                "author": "Voicu Popescu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19574v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19574v1",
                "updated": "2024-11-29T09:42:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    9,
                    42,
                    38,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T09:42:38Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    9,
                    42,
                    38,
                    4,
                    334,
                    0
                ],
                "title": "KV Shifting Attention Enhances Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Shifting Attention Enhances Language Modeling"
                },
                "summary": "The current large language models are mainly based on decode-only structure\ntransformers, which have great in-context learning (ICL) capabilities. It is\ngenerally believed that the important foundation of its ICL capability is the\ninduction heads mechanism, which requires at least two layers attention. In\norder to more efficiently implement the ability of the model's induction, we\nrevisit the induction heads mechanism and proposed a KV shifting attention. We\ntheoretically prove that the KV shifting attention reducing the model's\nrequirements for the depth and width of the induction heads mechanism. Our\nexperimental results demonstrate that KV shifting attention is beneficial to\nlearning induction heads and language modeling, which lead to better\nperformance or faster convergence from toy models to the pre-training models\nwith more than 10 B parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current large language models are mainly based on decode-only structure\ntransformers, which have great in-context learning (ICL) capabilities. It is\ngenerally believed that the important foundation of its ICL capability is the\ninduction heads mechanism, which requires at least two layers attention. In\norder to more efficiently implement the ability of the model's induction, we\nrevisit the induction heads mechanism and proposed a KV shifting attention. We\ntheoretically prove that the KV shifting attention reducing the model's\nrequirements for the depth and width of the induction heads mechanism. Our\nexperimental results demonstrate that KV shifting attention is beneficial to\nlearning induction heads and language modeling, which lead to better\nperformance or faster convergence from toy models to the pre-training models\nwith more than 10 B parameters."
                },
                "authors": [
                    {
                        "name": "Mingyu Xu"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Bingning Wang"
                    },
                    {
                        "name": "Weipeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Weipeng Chen"
                },
                "author": "Weipeng Chen",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19574v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19574v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07533v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07533v3",
                "updated": "2024-11-29T08:48:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    8,
                    48,
                    1,
                    4,
                    334,
                    0
                ],
                "published": "2024-05-13T08:03:32Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    8,
                    3,
                    32,
                    0,
                    134,
                    0
                ],
                "title": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials"
                },
                "summary": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities."
                },
                "authors": [
                    {
                        "name": "Sandro Rodriguez Garzon"
                    },
                    {
                        "name": "Dennis Natusch"
                    },
                    {
                        "name": "Artur Philipp"
                    },
                    {
                        "name": "Axel KÃ¼pper"
                    },
                    {
                        "name": "Hans Joachim Einsiedler"
                    },
                    {
                        "name": "Daniela Schneider"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Schneider"
                },
                "author": "Daniela Schneider",
                "arxiv_comment": "Accepted by and presented at 21st Annual International Conference on\n  Privacy, Security, and Trust (PST2024). Publication by IEEE still pending",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.07533v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07533v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18191v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18191v2",
                "updated": "2024-11-29T08:33:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    8,
                    33,
                    49,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-27T10:14:38Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    10,
                    14,
                    38,
                    2,
                    332,
                    0
                ],
                "title": "InputSnatch: Stealing Input in LLM Services via Timing Side-Channel\n  Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InputSnatch: Stealing Input in LLM Services via Timing Side-Channel\n  Attacks"
                },
                "summary": "Large language models (LLMs) possess extensive knowledge and\nquestion-answering capabilities, having been widely deployed in\nprivacy-sensitive domains like finance and medical consultation. During LLM\ninferences, cache-sharing methods are commonly employed to enhance efficiency\nby reusing cached states or responses for the same or similar inference\nrequests. However, we identify that these cache mechanisms pose a risk of\nprivate input leakage, as the caching can result in observable variations in\nresponse times, making them a strong candidate for a timing-based attack hint.\n  In this study, we propose a novel timing-based side-channel attack to execute\ninput theft in LLMs inference. The cache-based attack faces the challenge of\nconstructing candidate inputs in a large search space to hit and steal cached\nuser queries. To address these challenges, we propose two primary components.\nThe input constructor employs machine learning techniques and LLM-based\napproaches for vocabulary correlation learning while implementing optimized\nsearch mechanisms for generalized input construction. The time analyzer\nimplements statistical time fitting with outlier elimination to identify cache\nhit patterns, continuously providing feedback to refine the constructor's\nsearch strategy. We conduct experiments across two cache mechanisms and the\nresults demonstrate that our approach consistently attains high attack success\nrates in various applications. Our work highlights the security vulnerabilities\nassociated with performance optimizations, underscoring the necessity of\nprioritizing privacy and security alongside enhancements in LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) possess extensive knowledge and\nquestion-answering capabilities, having been widely deployed in\nprivacy-sensitive domains like finance and medical consultation. During LLM\ninferences, cache-sharing methods are commonly employed to enhance efficiency\nby reusing cached states or responses for the same or similar inference\nrequests. However, we identify that these cache mechanisms pose a risk of\nprivate input leakage, as the caching can result in observable variations in\nresponse times, making them a strong candidate for a timing-based attack hint.\n  In this study, we propose a novel timing-based side-channel attack to execute\ninput theft in LLMs inference. The cache-based attack faces the challenge of\nconstructing candidate inputs in a large search space to hit and steal cached\nuser queries. To address these challenges, we propose two primary components.\nThe input constructor employs machine learning techniques and LLM-based\napproaches for vocabulary correlation learning while implementing optimized\nsearch mechanisms for generalized input construction. The time analyzer\nimplements statistical time fitting with outlier elimination to identify cache\nhit patterns, continuously providing feedback to refine the constructor's\nsearch strategy. We conduct experiments across two cache mechanisms and the\nresults demonstrate that our approach consistently attains high attack success\nrates in various applications. Our work highlights the security vulnerabilities\nassociated with performance optimizations, underscoring the necessity of\nprioritizing privacy and security alongside enhancements in LLM inference."
                },
                "authors": [
                    {
                        "name": "Xinyao Zheng"
                    },
                    {
                        "name": "Husheng Han"
                    },
                    {
                        "name": "Shangyi Shi"
                    },
                    {
                        "name": "Qiyan Fang"
                    },
                    {
                        "name": "Zidong Du"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Qi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Qi Guo"
                },
                "author": "Qi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18191v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18191v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19379v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19379v1",
                "updated": "2024-11-28T21:10:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    21,
                    10,
                    20,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-28T21:10:20Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    21,
                    10,
                    20,
                    3,
                    333,
                    0
                ],
                "title": "Marconi: Prefix Caching for the Era of Hybrid LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Marconi: Prefix Caching for the Era of Hybrid LLMs"
                },
                "summary": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems."
                },
                "authors": [
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Zhuang Wang"
                    },
                    {
                        "name": "Zhen Jia"
                    },
                    {
                        "name": "Can Karakus"
                    },
                    {
                        "name": "Luca Zancato"
                    },
                    {
                        "name": "Tri Dao"
                    },
                    {
                        "name": "Ravi Netravali"
                    },
                    {
                        "name": "Yida Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yida Wang"
                },
                "author": "Yida Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19379v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19379v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19248v1",
                "updated": "2024-11-28T16:35:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    16,
                    35,
                    22,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-28T16:35:22Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    16,
                    35,
                    22,
                    3,
                    333,
                    0
                ],
                "title": "Reflecting Intelligent Surfaces-Assisted Multiple-Antenna Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reflecting Intelligent Surfaces-Assisted Multiple-Antenna Coded Caching"
                },
                "summary": "Reconfigurable intelligent surface (RIS) has been treated as a core technique\nin improving wireless propagation environments for the next generation wireless\ncommunication systems. This paper proposes a new coded caching problem,\nreferred to as Reconfigurable Intelligent Surface (RIS)-assisted\nmultiple-antenna coded caching, which is composed of a server with multiple\nantennas and some single-antenna cache-aided users. Different from the existing\nmulti-antenna coded caching problems, we introduce a passive RIS (with limited\nnumber of units) into the systems to further increase the multicast gain (i.e.,\ndegrees of freedom (DoF)) in the transmission, which is done by using\nRIS-assisted interference nulling. That is, by using RIS, we can `erase' any\npath between one transmission antenna and one receive antenna. We first propose\na new RIS-assisted interference nulling approach to search for the phase-shift\ncoefficients of RIS for the sake of interference nulling, which converges\nfaster than the state-of-the-art algorithm. After erasing some paths in each\ntime slot, the delivery can be divided into several non-overlapping groups\nincluding transmission antennas and users, where in each group the transmission\nantennas serve the contained users without suffering interference from the\ntransmissions by other groups. The division of groups for the sake of\nmaximizing the DoF could be formulated into a combinatorial optimization\nproblem. We propose a grouping algorithm which can find the optimal solution\nwith low complexity, and the corresponding coded caching scheme achieving this\nDoF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconfigurable intelligent surface (RIS) has been treated as a core technique\nin improving wireless propagation environments for the next generation wireless\ncommunication systems. This paper proposes a new coded caching problem,\nreferred to as Reconfigurable Intelligent Surface (RIS)-assisted\nmultiple-antenna coded caching, which is composed of a server with multiple\nantennas and some single-antenna cache-aided users. Different from the existing\nmulti-antenna coded caching problems, we introduce a passive RIS (with limited\nnumber of units) into the systems to further increase the multicast gain (i.e.,\ndegrees of freedom (DoF)) in the transmission, which is done by using\nRIS-assisted interference nulling. That is, by using RIS, we can `erase' any\npath between one transmission antenna and one receive antenna. We first propose\na new RIS-assisted interference nulling approach to search for the phase-shift\ncoefficients of RIS for the sake of interference nulling, which converges\nfaster than the state-of-the-art algorithm. After erasing some paths in each\ntime slot, the delivery can be divided into several non-overlapping groups\nincluding transmission antennas and users, where in each group the transmission\nantennas serve the contained users without suffering interference from the\ntransmissions by other groups. The division of groups for the sake of\nmaximizing the DoF could be formulated into a combinatorial optimization\nproblem. We propose a grouping algorithm which can find the optimal solution\nwith low complexity, and the corresponding coded caching scheme achieving this\nDoF."
                },
                "authors": [
                    {
                        "name": "Xiaofan Niu"
                    },
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Robert Caiming Qiu"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "arxiv_comment": "The short version of this paper was presented in 2024 IEEE\n  Information Theory Workshop, Nov. 24-28, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12468v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12468v2",
                "updated": "2024-11-28T14:42:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    14,
                    42,
                    54,
                    3,
                    333,
                    0
                ],
                "published": "2024-04-18T19:04:33Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    19,
                    4,
                    33,
                    3,
                    109,
                    0
                ],
                "title": "Fresh Caching of Dynamic Contents using Restless Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fresh Caching of Dynamic Contents using Restless Multi-armed Bandits"
                },
                "summary": "We consider a dynamic content caching problem wherein the contents get\nupdated at a central server, and local copies of a subset of contents are\ncached at a local cache associated with a Base station (BS). When a content\nrequest arrives, based on whether the content is in the local cache, the BS can\ndecide whether to fetch the content from the central server or serve the cached\nversion from the local cache. Fetching a content incurs a fixed fetching cost,\nand serving the cached version incurs an ageing cost proportional to the\nage-of-version (AoV) of the content. The BS has only partial information\nregarding AoVs of the contents. We formulate an optimal content fetching and\ncaching problem to minimize the average cost subject to cache capacity\nconstraints. The problem suffers from the curse of dimensionality and is\nprovably hard to solve. We formulate this problem as a continuous time restless\nmulti-armed bandit process (RMAB), where a single content problem of the\ncorresponding RMAB is a partially observable Markov decision process. We\nreformulate the single content problem as a semi-Markov decision process, prove\nindexability, and provide a Whittle index based solution to this problem.\nFinally, we compare the performance with recent work and show that our proposed\npolicy is optimal via simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a dynamic content caching problem wherein the contents get\nupdated at a central server, and local copies of a subset of contents are\ncached at a local cache associated with a Base station (BS). When a content\nrequest arrives, based on whether the content is in the local cache, the BS can\ndecide whether to fetch the content from the central server or serve the cached\nversion from the local cache. Fetching a content incurs a fixed fetching cost,\nand serving the cached version incurs an ageing cost proportional to the\nage-of-version (AoV) of the content. The BS has only partial information\nregarding AoVs of the contents. We formulate an optimal content fetching and\ncaching problem to minimize the average cost subject to cache capacity\nconstraints. The problem suffers from the curse of dimensionality and is\nprovably hard to solve. We formulate this problem as a continuous time restless\nmulti-armed bandit process (RMAB), where a single content problem of the\ncorresponding RMAB is a partially observable Markov decision process. We\nreformulate the single content problem as a semi-Markov decision process, prove\nindexability, and provide a Whittle index based solution to this problem.\nFinally, we compare the performance with recent work and show that our proposed\npolicy is optimal via simulations."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "arxiv_comment": "14 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12468v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12468v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19108v1",
                "updated": "2024-11-28T12:50:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    12,
                    50,
                    5,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-28T12:50:05Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    12,
                    50,
                    5,
                    3,
                    333,
                    0
                ],
                "title": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model"
                },
                "summary": "As a fundamental backbone for video generation, diffusion models are\nchallenged by low inference speed due to the sequential nature of denoising.\nPrevious methods speed up the models by caching and reusing model outputs at\nuniformly selected timesteps. However, such a strategy neglects the fact that\ndifferences among model outputs are not uniform across timesteps, which hinders\nselecting the appropriate model outputs to cache, leading to a poor balance\nbetween inference efficiency and visual quality. In this study, we introduce\nTimestep Embedding Aware Cache (TeaCache), a training-free caching approach\nthat estimates and leverages the fluctuating differences among model outputs\nacross timesteps. Rather than directly using the time-consuming model outputs,\nTeaCache focuses on model inputs, which have a strong correlation with the\nmodeloutputs while incurring negligible computational cost. TeaCache first\nmodulates the noisy inputs using the timestep embeddings to ensure their\ndifferences better approximating those of model outputs. TeaCache then\nintroduces a rescaling strategy to refine the estimated differences and\nutilizes them to indicate output caching. Experiments show that TeaCache\nachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%\nVbench score) degradation of visual quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a fundamental backbone for video generation, diffusion models are\nchallenged by low inference speed due to the sequential nature of denoising.\nPrevious methods speed up the models by caching and reusing model outputs at\nuniformly selected timesteps. However, such a strategy neglects the fact that\ndifferences among model outputs are not uniform across timesteps, which hinders\nselecting the appropriate model outputs to cache, leading to a poor balance\nbetween inference efficiency and visual quality. In this study, we introduce\nTimestep Embedding Aware Cache (TeaCache), a training-free caching approach\nthat estimates and leverages the fluctuating differences among model outputs\nacross timesteps. Rather than directly using the time-consuming model outputs,\nTeaCache focuses on model inputs, which have a strong correlation with the\nmodeloutputs while incurring negligible computational cost. TeaCache first\nmodulates the noisy inputs using the timestep embeddings to ensure their\ndifferences better approximating those of model outputs. TeaCache then\nintroduces a rescaling strategy to refine the estimated differences and\nutilizes them to indicate output caching. Experiments show that TeaCache\nachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%\nVbench score) degradation of visual quality."
                },
                "authors": [
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Shiwei Zhang"
                    },
                    {
                        "name": "Xiaofeng Wang"
                    },
                    {
                        "name": "Yujie Wei"
                    },
                    {
                        "name": "Haonan Qiu"
                    },
                    {
                        "name": "Yuzhong Zhao"
                    },
                    {
                        "name": "Yingya Zhang"
                    },
                    {
                        "name": "Qixiang Ye"
                    },
                    {
                        "name": "Fang Wan"
                    }
                ],
                "author_detail": {
                    "name": "Fang Wan"
                },
                "author": "Fang Wan",
                "arxiv_comment": "Project: https://liewfeng.github.io/TeaCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18077v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18077v2",
                "updated": "2024-11-28T02:01:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    2,
                    1,
                    50,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-27T06:10:49Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    6,
                    10,
                    49,
                    2,
                    332,
                    0
                ],
                "title": "MiniKV: Pushing the Limits of LLM Inference via 2-Bit\n  Layer-Discriminative KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniKV: Pushing the Limits of LLM Inference via 2-Bit\n  Layer-Discriminative KV Cache"
                },
                "summary": "How to efficiently serve LLMs in practice has become exceptionally\nchallenging due to their prohibitive memory and computation requirements. In\nthis study, we investigate optimizing the KV cache, whose memory footprint\nposes a critical bottleneck in LLM inference, especially when dealing with long\ncontext tasks. To tackle the challenge, we introduce MiniKV, a KV cache\noptimization method that simultaneously preserves long context task accuracy\nwhile significantly reducing KV cache size via a novel 2-bit\nlayer-discriminative KV cache. More importantly, we develop specialized CUDA\nkernels to make MiniKV compatible with FlashAttention. Experiments on a wide\nrange of long context tasks show that MiniKV effectively achieves 86% KV cache\ncompression ratio while recovering over 98.5% of accuracy, outperforming\nstate-of-the-art methods while achieving excellent measured system performance\nimprovements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to efficiently serve LLMs in practice has become exceptionally\nchallenging due to their prohibitive memory and computation requirements. In\nthis study, we investigate optimizing the KV cache, whose memory footprint\nposes a critical bottleneck in LLM inference, especially when dealing with long\ncontext tasks. To tackle the challenge, we introduce MiniKV, a KV cache\noptimization method that simultaneously preserves long context task accuracy\nwhile significantly reducing KV cache size via a novel 2-bit\nlayer-discriminative KV cache. More importantly, we develop specialized CUDA\nkernels to make MiniKV compatible with FlashAttention. Experiments on a wide\nrange of long context tasks show that MiniKV effectively achieves 86% KV cache\ncompression ratio while recovering over 98.5% of accuracy, outperforming\nstate-of-the-art methods while achieving excellent measured system performance\nimprovements."
                },
                "authors": [
                    {
                        "name": "Akshat Sharma"
                    },
                    {
                        "name": "Hangliang Ding"
                    },
                    {
                        "name": "Jianping Li"
                    },
                    {
                        "name": "Neel Dani"
                    },
                    {
                        "name": "Minjia Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Minjia Zhang"
                },
                "author": "Minjia Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18077v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18077v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18566v1",
                "updated": "2024-11-27T18:09:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    9,
                    29,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T18:09:29Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    9,
                    29,
                    2,
                    332,
                    0
                ],
                "title": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software"
                },
                "summary": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software."
                },
                "authors": [
                    {
                        "name": "Oliver Maximilian Zobel"
                    },
                    {
                        "name": "Johannes Maierhofer"
                    },
                    {
                        "name": "Andreas KÃ¶stler"
                    },
                    {
                        "name": "Daniel J. Rixen"
                    }
                ],
                "author_detail": {
                    "name": "Daniel J. Rixen"
                },
                "author": "Daniel J. Rixen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.08895v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.08895v4",
                "updated": "2024-11-27T18:05:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    5,
                    57,
                    2,
                    332,
                    0
                ],
                "published": "2024-01-17T00:36:58Z",
                "published_parsed": [
                    2024,
                    1,
                    17,
                    0,
                    36,
                    58,
                    2,
                    17,
                    0
                ],
                "title": "cedar: Optimized and Unified Machine Learning Input Data Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cedar: Optimized and Unified Machine Learning Input Data Pipelines"
                },
                "summary": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems."
                },
                "authors": [
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Emanuel Adamiak"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    }
                ],
                "author_detail": {
                    "name": "Christos Kozyrakis"
                },
                "author": "Christos Kozyrakis",
                "arxiv_comment": "Published in PVLDB Volume 18, Issue 2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.08895v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.08895v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18424v1",
                "updated": "2024-11-27T15:07:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    7,
                    28,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T15:07:28Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    7,
                    28,
                    2,
                    332,
                    0
                ],
                "title": "FastSwitch: Optimizing Context Switching Efficiency in Fairness-aware\n  Large Language Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastSwitch: Optimizing Context Switching Efficiency in Fairness-aware\n  Large Language Model Serving"
                },
                "summary": "Serving numerous users and requests concurrently requires good fairness in\nLarge Language Models (LLMs) serving system. This ensures that, at the same\ncost, the system can meet the Service Level Objectives (SLOs) of more users ,\nsuch as time to first token (TTFT) and time between tokens (TBT), rather than\nallowing a few users to experience performance far exceeding the SLOs. To\nachieve better fairness, the preemption-based scheduling policy dynamically\nadjusts the priority of each request to maintain balance during runtime.\nHowever, existing systems tend to overly prioritize throughput, overlooking the\noverhead caused by preemption-induced context switching, which is crucial for\nmaintaining fairness through priority adjustments. In this work, we identify\nthree main challenges that result in this overhead. 1) Inadequate I/O\nutilization. 2) GPU idleness. 3) Unnecessary I/O transmission during multi-turn\nconversations. Our key insight is that the block-based KV cache memory policy\nin existing systems, while achieving near-zero memory waste, leads to\ndiscontinuity and insufficient granularity in the KV cache memory. To respond,\nwe introduce FastSwitch, a fairness-aware serving system that not only aligns\nwith existing KV cache memory allocation policy but also mitigates context\nswitching overhead. Our evaluation shows that FastSwitch outperforms the\nstate-of-the-art LLM serving system vLLM with speedups of 1.4-11.2x across\ndifferent tail TTFT and TBT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving numerous users and requests concurrently requires good fairness in\nLarge Language Models (LLMs) serving system. This ensures that, at the same\ncost, the system can meet the Service Level Objectives (SLOs) of more users ,\nsuch as time to first token (TTFT) and time between tokens (TBT), rather than\nallowing a few users to experience performance far exceeding the SLOs. To\nachieve better fairness, the preemption-based scheduling policy dynamically\nadjusts the priority of each request to maintain balance during runtime.\nHowever, existing systems tend to overly prioritize throughput, overlooking the\noverhead caused by preemption-induced context switching, which is crucial for\nmaintaining fairness through priority adjustments. In this work, we identify\nthree main challenges that result in this overhead. 1) Inadequate I/O\nutilization. 2) GPU idleness. 3) Unnecessary I/O transmission during multi-turn\nconversations. Our key insight is that the block-based KV cache memory policy\nin existing systems, while achieving near-zero memory waste, leads to\ndiscontinuity and insufficient granularity in the KV cache memory. To respond,\nwe introduce FastSwitch, a fairness-aware serving system that not only aligns\nwith existing KV cache memory allocation policy but also mitigates context\nswitching overhead. Our evaluation shows that FastSwitch outperforms the\nstate-of-the-art LLM serving system vLLM with speedups of 1.4-11.2x across\ndifferent tail TTFT and TBT."
                },
                "authors": [
                    {
                        "name": "Ao Shen"
                    },
                    {
                        "name": "Zhiyao Li"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17616v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17616v2",
                "updated": "2024-11-27T14:43:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    14,
                    43,
                    46,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-26T17:28:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Accelerating Vision Diffusion Transformers with Skip Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Vision Diffusion Transformers with Skip Branches"
                },
                "summary": "Diffusion Transformers (DiT), an emerging image and video generation model\narchitecture, has demonstrated great potential because of its high generation\nquality and scalability properties. Despite the impressive performance, its\npractical deployment is constrained by computational complexity and redundancy\nin the sequential denoising process. While feature caching across timesteps has\nproven effective in accelerating diffusion models, its application to DiT is\nlimited by fundamental architectural differences from U-Net-based approaches.\nThrough empirical analysis of DiT feature dynamics, we identify that\nsignificant feature variation between DiT blocks presents a key challenge for\nfeature reusability. To address this, we convert standard DiT into Skip-DiT\nwith skip branches to enhance feature smoothness. Further, we introduce\nSkip-Cache which utilizes the skip branches to cache DiT features across\ntimesteps at the inference time. We validated effectiveness of our proposal on\ndifferent DiT backbones for video and image generation, showcasing skip\nbranches to help preserve generation quality and achieve higher speedup.\nExperimental results indicate that Skip-DiT achieves a 1.5x speedup almost for\nfree and a 2.2x speedup with only a minor reduction in quantitative metrics.\nCode is available at https://github.com/OpenSparseLLMs/Skip-DiT.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT), an emerging image and video generation model\narchitecture, has demonstrated great potential because of its high generation\nquality and scalability properties. Despite the impressive performance, its\npractical deployment is constrained by computational complexity and redundancy\nin the sequential denoising process. While feature caching across timesteps has\nproven effective in accelerating diffusion models, its application to DiT is\nlimited by fundamental architectural differences from U-Net-based approaches.\nThrough empirical analysis of DiT feature dynamics, we identify that\nsignificant feature variation between DiT blocks presents a key challenge for\nfeature reusability. To address this, we convert standard DiT into Skip-DiT\nwith skip branches to enhance feature smoothness. Further, we introduce\nSkip-Cache which utilizes the skip branches to cache DiT features across\ntimesteps at the inference time. We validated effectiveness of our proposal on\ndifferent DiT backbones for video and image generation, showcasing skip\nbranches to help preserve generation quality and achieve higher speedup.\nExperimental results indicate that Skip-DiT achieves a 1.5x speedup almost for\nfree and a 2.2x speedup with only a minor reduction in quantitative metrics.\nCode is available at https://github.com/OpenSparseLLMs/Skip-DiT.git."
                },
                "authors": [
                    {
                        "name": "Guanjie Chen"
                    },
                    {
                        "name": "Xinyu Zhao"
                    },
                    {
                        "name": "Yucheng Zhou"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17616v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17616v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17459v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17459v2",
                "updated": "2024-11-27T08:21:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    8,
                    21,
                    47,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-26T14:23:53Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    23,
                    53,
                    1,
                    331,
                    0
                ],
                "title": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model"
                },
                "summary": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE."
                },
                "authors": [
                    {
                        "name": "Zongjian Li"
                    },
                    {
                        "name": "Bin Lin"
                    },
                    {
                        "name": "Yang Ye"
                    },
                    {
                        "name": "Liuhan Chen"
                    },
                    {
                        "name": "Xinhua Cheng"
                    },
                    {
                        "name": "Shenghai Yuan"
                    },
                    {
                        "name": "Li Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Li Yuan"
                },
                "author": "Li Yuan",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17459v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17459v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15785v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15785v2",
                "updated": "2024-11-27T03:07:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    3,
                    7,
                    20,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-24T11:30:00Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    11,
                    30,
                    0,
                    6,
                    329,
                    0
                ],
                "title": "A Method for Building Large Language Models with Predefined KV Cache\n  Capacity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Method for Building Large Language Models with Predefined KV Cache\n  Capacity"
                },
                "summary": "This paper introduces a novel approach, the Bounded-Cache Transformer (BCT),\nfor building large language models with a predefined Key-Value (KV) cache\ncapacity. The BCT addresses the excessive memory consumption issue in\ntraditional KV caches by implementing a bounded-length KV cache, which is\nparticularly suitable for the attention layers in Transformer decode-only\narchitectures. By dynamically updating the key-value vector sequences, the BCT\nachieves efficient inference within limited cache capacity, significantly\nreducing memory usage while maintaining model performance and system\nthroughput. Experimental results demonstrate that the BCT significantly reduces\nmemory usage while maintaining the model's inference quality, offering a new\nsolution for efficient inference in large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel approach, the Bounded-Cache Transformer (BCT),\nfor building large language models with a predefined Key-Value (KV) cache\ncapacity. The BCT addresses the excessive memory consumption issue in\ntraditional KV caches by implementing a bounded-length KV cache, which is\nparticularly suitable for the attention layers in Transformer decode-only\narchitectures. By dynamically updating the key-value vector sequences, the BCT\nachieves efficient inference within limited cache capacity, significantly\nreducing memory usage while maintaining model performance and system\nthroughput. Experimental results demonstrate that the BCT significantly reduces\nmemory usage while maintaining the model's inference quality, offering a new\nsolution for efficient inference in large language models."
                },
                "authors": [
                    {
                        "name": "Zhonghua Yi"
                    },
                    {
                        "name": "Ge Niu"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Wei Tang"
                    },
                    {
                        "name": "Liqiu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Liqiu Zhang"
                },
                "author": "Liqiu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15785v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15785v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17685v1",
                "updated": "2024-11-26T18:52:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    52,
                    6,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T18:52:06Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    52,
                    6,
                    1,
                    331,
                    0
                ],
                "title": "Attamba: Attending To Multi-Token States",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attamba: Attending To Multi-Token States"
                },
                "summary": "When predicting the next token in a sequence, vanilla transformers compute\nattention over all previous tokens, resulting in quadratic scaling of compute\nwith sequence length. State-space models compress the entire sequence of tokens\ninto a fixed-dimensional representation to improve efficiency, while other\narchitectures achieve sub-quadratic complexity via low-rank projections or\nsparse attention patterns over the sequence. In this paper, we introduce\nAttamba, a novel architecture that uses state-space models to compress chunks\nof tokens and applies attention on these compressed key-value representations.\nWe find that replacing key and value projections in a transformer with SSMs can\nimprove model quality and enable flexible token chunking, resulting in 24%\nimproved perplexity with transformer of similar KV-Cache and attention\nfootprint, and ~4 times smaller KV-Cache and Attention FLOPs for 5% perplexity\ntrade-off. Attamba can perform attention on chunked-sequences of variable\nlength, enabling a smooth transition between quadratic and linear scaling,\noffering adaptable efficiency gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When predicting the next token in a sequence, vanilla transformers compute\nattention over all previous tokens, resulting in quadratic scaling of compute\nwith sequence length. State-space models compress the entire sequence of tokens\ninto a fixed-dimensional representation to improve efficiency, while other\narchitectures achieve sub-quadratic complexity via low-rank projections or\nsparse attention patterns over the sequence. In this paper, we introduce\nAttamba, a novel architecture that uses state-space models to compress chunks\nof tokens and applies attention on these compressed key-value representations.\nWe find that replacing key and value projections in a transformer with SSMs can\nimprove model quality and enable flexible token chunking, resulting in 24%\nimproved perplexity with transformer of similar KV-Cache and attention\nfootprint, and ~4 times smaller KV-Cache and Attention FLOPs for 5% perplexity\ntrade-off. Attamba can perform attention on chunked-sequences of variable\nlength, enabling a smooth transition between quadratic and linear scaling,\noffering adaptable efficiency gains."
                },
                "authors": [
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Safeen Huda"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17800v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17800v1",
                "updated": "2024-11-26T18:42:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    42,
                    42,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T18:42:42Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    42,
                    42,
                    1,
                    331,
                    0
                ],
                "title": "STAR: Synthesis of Tailored Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STAR: Synthesis of Tailored Architectures"
                },
                "summary": "Iterative improvement of model architectures is fundamental to deep learning:\nTransformers first enabled scaling, and recent advances in model hybridization\nhave pushed the quality-efficiency frontier. However, optimizing architectures\nremains challenging and expensive. Current automated or manual approaches fall\nshort, largely due to limited progress in the design of search spaces and due\nto the simplicity of resulting patterns and heuristics. In this work, we\npropose a new approach for the synthesis of tailored architectures (STAR). Our\napproach combines a novel search space based on the theory of linear\ninput-varying systems, supporting a hierarchical numerical encoding into\narchitecture genomes. STAR genomes are automatically refined and recombined\nwith gradient-free, evolutionary algorithms to optimize for multiple model\nquality and efficiency metrics. Using STAR, we optimize large populations of\nnew architectures, leveraging diverse computational units and interconnection\npatterns, improving over highly-optimized Transformers and striped hybrid\nmodels on the frontier of quality, parameter size, and inference cache for\nautoregressive language modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative improvement of model architectures is fundamental to deep learning:\nTransformers first enabled scaling, and recent advances in model hybridization\nhave pushed the quality-efficiency frontier. However, optimizing architectures\nremains challenging and expensive. Current automated or manual approaches fall\nshort, largely due to limited progress in the design of search spaces and due\nto the simplicity of resulting patterns and heuristics. In this work, we\npropose a new approach for the synthesis of tailored architectures (STAR). Our\napproach combines a novel search space based on the theory of linear\ninput-varying systems, supporting a hierarchical numerical encoding into\narchitecture genomes. STAR genomes are automatically refined and recombined\nwith gradient-free, evolutionary algorithms to optimize for multiple model\nquality and efficiency metrics. Using STAR, we optimize large populations of\nnew architectures, leveraging diverse computational units and interconnection\npatterns, improving over highly-optimized Transformers and striped hybrid\nmodels on the frontier of quality, parameter size, and inference cache for\nautoregressive language modeling."
                },
                "authors": [
                    {
                        "name": "Armin W. Thomas"
                    },
                    {
                        "name": "Rom Parnichkun"
                    },
                    {
                        "name": "Alexander Amini"
                    },
                    {
                        "name": "Stefano Massaroli"
                    },
                    {
                        "name": "Michael Poli"
                    }
                ],
                "author_detail": {
                    "name": "Michael Poli"
                },
                "author": "Michael Poli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17800v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17800v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15651v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15651v3",
                "updated": "2024-11-26T17:28:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    6,
                    1,
                    331,
                    0
                ],
                "published": "2024-03-22T23:47:19Z",
                "published_parsed": [
                    2024,
                    3,
                    22,
                    23,
                    47,
                    19,
                    4,
                    82,
                    0
                ],
                "title": "GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering"
                },
                "summary": "In this paper, we present GaNI, a Global and Near-field Illumination-aware\nneural inverse rendering technique that can reconstruct geometry, albedo, and\nroughness parameters from images of a scene captured with co-located light and\ncamera. Existing inverse rendering techniques with co-located light-camera\nfocus on single objects only, without modeling global illumination and\nnear-field lighting more prominent in scenes with multiple objects. We\nintroduce a system that solves this problem in two stages; we first reconstruct\nthe geometry powered by neural volumetric rendering NeuS, followed by inverse\nneural radiosity that uses the previously predicted geometry to estimate albedo\nand roughness. However, such a naive combination fails and we propose multiple\ntechnical contributions that enable this two-stage approach. We observe that\nNeuS fails to handle near-field illumination and strong specular reflections\nfrom the flashlight in a scene. We propose to implicitly model the effects of\nnear-field illumination and introduce a surface angle loss function to handle\nspecular reflections. Similarly, we observe that invNeRad assumes constant\nillumination throughout the capture and cannot handle moving flashlights during\ncapture. We propose a light position-aware radiance cache network and\nadditional smoothness priors on roughness to reconstruct reflectance.\nExperimental evaluation on synthetic and real data shows that our method\noutperforms the existing co-located light-camera-based inverse rendering\ntechniques. Our approach produces significantly better reflectance and slightly\nbetter geometry than capture strategies that do not require a dark room.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present GaNI, a Global and Near-field Illumination-aware\nneural inverse rendering technique that can reconstruct geometry, albedo, and\nroughness parameters from images of a scene captured with co-located light and\ncamera. Existing inverse rendering techniques with co-located light-camera\nfocus on single objects only, without modeling global illumination and\nnear-field lighting more prominent in scenes with multiple objects. We\nintroduce a system that solves this problem in two stages; we first reconstruct\nthe geometry powered by neural volumetric rendering NeuS, followed by inverse\nneural radiosity that uses the previously predicted geometry to estimate albedo\nand roughness. However, such a naive combination fails and we propose multiple\ntechnical contributions that enable this two-stage approach. We observe that\nNeuS fails to handle near-field illumination and strong specular reflections\nfrom the flashlight in a scene. We propose to implicitly model the effects of\nnear-field illumination and introduce a surface angle loss function to handle\nspecular reflections. Similarly, we observe that invNeRad assumes constant\nillumination throughout the capture and cannot handle moving flashlights during\ncapture. We propose a light position-aware radiance cache network and\nadditional smoothness priors on roughness to reconstruct reflectance.\nExperimental evaluation on synthetic and real data shows that our method\noutperforms the existing co-located light-camera-based inverse rendering\ntechniques. Our approach produces significantly better reflectance and slightly\nbetter geometry than capture strategies that do not require a dark room."
                },
                "authors": [
                    {
                        "name": "Jiaye Wu"
                    },
                    {
                        "name": "Saeed Hadadan"
                    },
                    {
                        "name": "Geng Lin"
                    },
                    {
                        "name": "Matthias Zwicker"
                    },
                    {
                        "name": "David Jacobs"
                    },
                    {
                        "name": "Roni Sengupta"
                    }
                ],
                "author_detail": {
                    "name": "Roni Sengupta"
                },
                "author": "Roni Sengupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15651v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15651v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17559v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17559v1",
                "updated": "2024-11-26T16:21:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    16,
                    21,
                    10,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T16:21:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    16,
                    21,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Degrees of Freedom of Cache-Aided Interference Channels Assisted by\n  Active Intelligent Reflecting Surfaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Degrees of Freedom of Cache-Aided Interference Channels Assisted by\n  Active Intelligent Reflecting Surfaces"
                },
                "summary": "This paper studies cache-aided wireless networks in the presence of active\nintelligent reflecting surfaces (IRS) from an information-theoretic\nperspective. Specifically, we explore interference management in a cache-aided\nwireless network assisted by an active IRS, to enhance the achievable degrees\nof freedom (DoF). To this end, we jointly design the content placement,\ndelivery phase, and phase shifts of the IRS and propose a one-shot achievable\nscheme. Our scheme exploits transmitters' cooperation, cache contents (as side\ninformation), interference alignment, and IRS capabilities, adapting to the\nnetwork's parameters. We derive the achievable one-shot sum-DoF for different\nsizes of cache memories, network configurations, and numbers of IRS elements.\nOur results highlight the potential of deploying an IRS in cache-aided wireless\ncommunication systems, underscoring the enhancement of achievable DoF for\nvarious parameter regimes, particularly when the sizes of the caches\n(especially at the transmitters) are inadequate. Notably, we show that access\nto an IRS with a sufficient number of elements enables the achievement of the\nmaximum possible DoF for various parameter regimes of interest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies cache-aided wireless networks in the presence of active\nintelligent reflecting surfaces (IRS) from an information-theoretic\nperspective. Specifically, we explore interference management in a cache-aided\nwireless network assisted by an active IRS, to enhance the achievable degrees\nof freedom (DoF). To this end, we jointly design the content placement,\ndelivery phase, and phase shifts of the IRS and propose a one-shot achievable\nscheme. Our scheme exploits transmitters' cooperation, cache contents (as side\ninformation), interference alignment, and IRS capabilities, adapting to the\nnetwork's parameters. We derive the achievable one-shot sum-DoF for different\nsizes of cache memories, network configurations, and numbers of IRS elements.\nOur results highlight the potential of deploying an IRS in cache-aided wireless\ncommunication systems, underscoring the enhancement of achievable DoF for\nvarious parameter regimes, particularly when the sizes of the caches\n(especially at the transmitters) are inadequate. Notably, we show that access\nto an IRS with a sufficient number of elements enables the achievement of the\nmaximum possible DoF for various parameter regimes of interest."
                },
                "authors": [
                    {
                        "name": "Abolfazl Changizi"
                    },
                    {
                        "name": "Ali H. Abdollahi Bafghi"
                    },
                    {
                        "name": "Masoumeh Nasiri-Kenari"
                    }
                ],
                "author_detail": {
                    "name": "Masoumeh Nasiri-Kenari"
                },
                "author": "Masoumeh Nasiri-Kenari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17559v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17559v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17786v1",
                "updated": "2024-11-26T15:03:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T15:03:14Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "title": "DreamCache: Finetuning-Free Lightweight Personalized Image Generation\n  via Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DreamCache: Finetuning-Free Lightweight Personalized Image Generation\n  via Feature Caching"
                },
                "summary": "Personalized image generation requires text-to-image generative models that\ncapture the core features of a reference subject to allow for controlled\ngeneration across different contexts. Existing methods face challenges due to\ncomplex training requirements, high inference costs, limited flexibility, or a\ncombination of these issues. In this paper, we introduce DreamCache, a scalable\napproach for efficient and high-quality personalized image generation. By\ncaching a small number of reference image features from a subset of layers and\na single timestep of the pretrained diffusion denoiser, DreamCache enables\ndynamic modulation of the generated image features through lightweight, trained\nconditioning adapters. DreamCache achieves state-of-the-art image and text\nalignment, utilizing an order of magnitude fewer extra parameters, and is both\nmore computationally effective and versatile than existing models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized image generation requires text-to-image generative models that\ncapture the core features of a reference subject to allow for controlled\ngeneration across different contexts. Existing methods face challenges due to\ncomplex training requirements, high inference costs, limited flexibility, or a\ncombination of these issues. In this paper, we introduce DreamCache, a scalable\napproach for efficient and high-quality personalized image generation. By\ncaching a small number of reference image features from a subset of layers and\na single timestep of the pretrained diffusion denoiser, DreamCache enables\ndynamic modulation of the generated image features through lightweight, trained\nconditioning adapters. DreamCache achieves state-of-the-art image and text\nalignment, utilizing an order of magnitude fewer extra parameters, and is both\nmore computationally effective and versatile than existing models."
                },
                "authors": [
                    {
                        "name": "Emanuele Aiello"
                    },
                    {
                        "name": "Umberto Michieli"
                    },
                    {
                        "name": "Diego Valsesia"
                    },
                    {
                        "name": "Mete Ozay"
                    },
                    {
                        "name": "Enrico Magli"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Magli"
                },
                "author": "Enrico Magli",
                "arxiv_comment": "16 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17116v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17116v1",
                "updated": "2024-11-26T05:10:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    10,
                    4,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T05:10:04Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    10,
                    4,
                    1,
                    331,
                    0
                ],
                "title": "Star Attention: Efficient LLM Inference over Long Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Star Attention: Efficient LLM Inference over Long Sequences"
                },
                "summary": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n95-100% of accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n95-100% of accuracy."
                },
                "authors": [
                    {
                        "name": "Shantanu Acharya"
                    },
                    {
                        "name": "Fei Jia"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Code: https://github.com/NVIDIA/Star-Attention",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17116v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17089v1",
                "updated": "2024-11-26T04:03:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    4,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T04:03:14Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    4,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "title": "Efficient LLM Inference with I/O-Aware Partial KV Cache Recomputation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference with I/O-Aware Partial KV Cache Recomputation"
                },
                "summary": "Inference for Large Language Models (LLMs) is computationally demanding. To\nreduce the cost of auto-regressive decoding, Key-Value (KV) caching is used to\nstore intermediate activations, enabling GPUs to perform only the incremental\ncomputation required for each new token. This approach significantly lowers the\ncomputational overhead for token generation. However, the memory required for\nKV caching grows rapidly, often exceeding the capacity of GPU memory. A\ncost-effective alternative is to offload KV cache to CPU memory, which\nalleviates GPU memory pressure but shifts the bottleneck to the limited\nbandwidth of the PCIe connection between the CPU and GPU. Existing methods\nattempt to address these issues by overlapping GPU computation with I/O or\nemploying CPU-GPU heterogeneous execution, but they are hindered by excessive\ndata movement and dependence on CPU capabilities. In this paper, we introduce\nan efficient CPU-GPU I/O-aware LLM inference method that avoids transferring\nthe entire KV cache from CPU to GPU by recomputing partial KV cache from\nactivations while concurrently transferring the remaining KV cache via PCIe\nbus. This approach overlaps GPU recomputation with data transfer to minimize\nidle GPU time and maximize inference performance. Our method is fully automated\nby integrating a profiler module that utilizes input characteristics and system\nhardware information, a scheduler module to optimize the distribution of\ncomputation and communication workloads, and a runtime module to efficiently\nexecute the derived execution plan. Experimental results show that our method\nachieves up to 35.8% lower latency and 46.2% higher throughput during decoding\ncompared to state-of-the-art approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference for Large Language Models (LLMs) is computationally demanding. To\nreduce the cost of auto-regressive decoding, Key-Value (KV) caching is used to\nstore intermediate activations, enabling GPUs to perform only the incremental\ncomputation required for each new token. This approach significantly lowers the\ncomputational overhead for token generation. However, the memory required for\nKV caching grows rapidly, often exceeding the capacity of GPU memory. A\ncost-effective alternative is to offload KV cache to CPU memory, which\nalleviates GPU memory pressure but shifts the bottleneck to the limited\nbandwidth of the PCIe connection between the CPU and GPU. Existing methods\nattempt to address these issues by overlapping GPU computation with I/O or\nemploying CPU-GPU heterogeneous execution, but they are hindered by excessive\ndata movement and dependence on CPU capabilities. In this paper, we introduce\nan efficient CPU-GPU I/O-aware LLM inference method that avoids transferring\nthe entire KV cache from CPU to GPU by recomputing partial KV cache from\nactivations while concurrently transferring the remaining KV cache via PCIe\nbus. This approach overlaps GPU recomputation with data transfer to minimize\nidle GPU time and maximize inference performance. Our method is fully automated\nby integrating a profiler module that utilizes input characteristics and system\nhardware information, a scheduler module to optimize the distribution of\ncomputation and communication workloads, and a runtime module to efficiently\nexecute the derived execution plan. Experimental results show that our method\nachieves up to 35.8% lower latency and 46.2% higher throughput during decoding\ncompared to state-of-the-art approaches."
                },
                "authors": [
                    {
                        "name": "Chaoyi Jiang"
                    },
                    {
                        "name": "Lei Gao"
                    },
                    {
                        "name": "Hossein Entezari Zarch"
                    },
                    {
                        "name": "Murali Annavaram"
                    }
                ],
                "author_detail": {
                    "name": "Murali Annavaram"
                },
                "author": "Murali Annavaram",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16375v1",
                "updated": "2024-11-25T13:33:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    33,
                    41,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T13:33:41Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    33,
                    41,
                    0,
                    330,
                    0
                ],
                "title": "Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal\n  Generation and Cache Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal\n  Generation and Cache Sharing"
                },
                "summary": "With the advance of diffusion models, today's video generation has achieved\nimpressive quality. To extend the generation length and facilitate real-world\napplications, a majority of video diffusion models (VDMs) generate videos in an\nautoregressive manner, i.e., generating subsequent clips conditioned on the\nlast frame(s) of the previous clip. However, existing autoregressive VDMs are\nhighly inefficient and redundant: The model must re-compute all the conditional\nframes that are overlapped between adjacent clips. This issue is exacerbated\nwhen the conditional frames are extended autoregressively to provide the model\nwith long-term context. In such cases, the computational demands increase\nsignificantly (i.e., with a quadratic complexity w.r.t. the autoregression\nstep). In this paper, we propose Ca2-VDM, an efficient autoregressive VDM with\nCausal generation and Cache sharing. For causal generation, it introduces\nunidirectional feature computation, which ensures that the cache of conditional\nframes can be precomputed in previous autoregression steps and reused in every\nsubsequent step, eliminating redundant computations. For cache sharing, it\nshares the cache across all denoising steps to avoid the huge cache storage\ncost. Extensive experiments demonstrated that our Ca2-VDM achieves\nstate-of-the-art quantitative and qualitative video generation results and\nsignificantly improves the generation speed. Code is available at\nhttps://github.com/Dawn-LX/CausalCache-VDM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advance of diffusion models, today's video generation has achieved\nimpressive quality. To extend the generation length and facilitate real-world\napplications, a majority of video diffusion models (VDMs) generate videos in an\nautoregressive manner, i.e., generating subsequent clips conditioned on the\nlast frame(s) of the previous clip. However, existing autoregressive VDMs are\nhighly inefficient and redundant: The model must re-compute all the conditional\nframes that are overlapped between adjacent clips. This issue is exacerbated\nwhen the conditional frames are extended autoregressively to provide the model\nwith long-term context. In such cases, the computational demands increase\nsignificantly (i.e., with a quadratic complexity w.r.t. the autoregression\nstep). In this paper, we propose Ca2-VDM, an efficient autoregressive VDM with\nCausal generation and Cache sharing. For causal generation, it introduces\nunidirectional feature computation, which ensures that the cache of conditional\nframes can be precomputed in previous autoregression steps and reused in every\nsubsequent step, eliminating redundant computations. For cache sharing, it\nshares the cache across all denoising steps to avoid the huge cache storage\ncost. Extensive experiments demonstrated that our Ca2-VDM achieves\nstate-of-the-art quantitative and qualitative video generation results and\nsignificantly improves the generation speed. Code is available at\nhttps://github.com/Dawn-LX/CausalCache-VDM"
                },
                "authors": [
                    {
                        "name": "Kaifeng Gao"
                    },
                    {
                        "name": "Jiaxin Shi"
                    },
                    {
                        "name": "Hanwang Zhang"
                    },
                    {
                        "name": "Chunping Wang"
                    },
                    {
                        "name": "Jun Xiao"
                    },
                    {
                        "name": "Long Chen"
                    }
                ],
                "author_detail": {
                    "name": "Long Chen"
                },
                "author": "Long Chen",
                "arxiv_comment": "Technical Report. Code is available at\n  https://github.com/Dawn-LX/CausalCache-VDM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19315v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19315v2",
                "updated": "2024-11-25T12:14:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    14,
                    33,
                    0,
                    330,
                    0
                ],
                "published": "2024-09-28T11:00:11Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    11,
                    0,
                    11,
                    5,
                    272,
                    0
                ],
                "title": "Analog In-Memory Computing Attention Mechanism for Fast and\n  Energy-Efficient Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analog In-Memory Computing Attention Mechanism for Fast and\n  Energy-Efficient Large Language Models"
                },
                "summary": "Transformer networks, driven by self-attention, are central to Large Language\nModels. In generative Transformers, self-attention uses cache memory to store\ntoken projections, avoiding recomputation at each time step. However,\nGPU-stored projections must be loaded into SRAM for each new generation step,\ncausing latency and energy bottlenecks.\n  We present a custom self-attention in-memory computing architecture based on\nemerging charge-based memories called gain cells, which can be efficiently\nwritten to store new tokens during sequence generation and enable parallel\nanalog dot-product computation required for self-attention. However, the analog\ngain cell circuits introduce non-idealities and constraints preventing the\ndirect mapping of pre-trained models. To circumvent this problem, we design an\ninitialization algorithm achieving text processing performance comparable to\nGPT-2 without training from scratch. Our architecture respectively reduces\nattention latency and energy consumption by up to two and five orders of\nmagnitude compared to GPUs, marking a significant step toward ultra-fast,\nlow-power generative Transformers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer networks, driven by self-attention, are central to Large Language\nModels. In generative Transformers, self-attention uses cache memory to store\ntoken projections, avoiding recomputation at each time step. However,\nGPU-stored projections must be loaded into SRAM for each new generation step,\ncausing latency and energy bottlenecks.\n  We present a custom self-attention in-memory computing architecture based on\nemerging charge-based memories called gain cells, which can be efficiently\nwritten to store new tokens during sequence generation and enable parallel\nanalog dot-product computation required for self-attention. However, the analog\ngain cell circuits introduce non-idealities and constraints preventing the\ndirect mapping of pre-trained models. To circumvent this problem, we design an\ninitialization algorithm achieving text processing performance comparable to\nGPT-2 without training from scratch. Our architecture respectively reduces\nattention latency and energy consumption by up to two and five orders of\nmagnitude compared to GPUs, marking a significant step toward ultra-fast,\nlow-power generative Transformers."
                },
                "authors": [
                    {
                        "name": "Nathan Leroux"
                    },
                    {
                        "name": "Paul-Philipp Manea"
                    },
                    {
                        "name": "Chirag Sudarshan"
                    },
                    {
                        "name": "Jan Finkbeiner"
                    },
                    {
                        "name": "Sebastian Siegel"
                    },
                    {
                        "name": "John Paul Strachan"
                    },
                    {
                        "name": "Emre Neftci"
                    }
                ],
                "author_detail": {
                    "name": "Emre Neftci"
                },
                "author": "Emre Neftci",
                "arxiv_comment": "25 pages, 6 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19315v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19315v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11469v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11469v2",
                "updated": "2024-11-24T21:57:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    21,
                    57,
                    29,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-18T11:12:57Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    11,
                    12,
                    57,
                    0,
                    323,
                    0
                ],
                "title": "Deegen: A JIT-Capable VM Generator for Dynamic Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deegen: A JIT-Capable VM Generator for Dynamic Languages"
                },
                "summary": "Building a high-performance JIT-capable VM for a dynamic language has\ntraditionally required a tremendous amount of time, money, and expertise. We\npresent Deegen, a meta-compiler that allows users to generate a\nhigh-performance JIT-capable VM for their own language at an engineering cost\nsimilar to writing a simple interpreter. Deegen takes in the execution\nsemantics of the bytecodes implemented as C++ functions, and automatically\ngenerates a two-tier VM execution engine with a state-of-the-art interpreter, a\nstate-of-the-art baseline JIT, and the tier-switching logic that connects them\ninto a self-adaptive system.\n  We are the first to demonstrate the automatic generation of a JIT compiler,\nand the automatic generation of an interpreter that outperforms the state of\nthe art. Our performance comes from a long list of optimizations supported by\nDeegen, including bytecode specialization and quickening, register pinning, tag\nregister optimization, call inline caching, generic inline caching, JIT\npolymorphic IC, JIT IC inline slab, type-check removal and strength reduction,\ntype-based slow-path extraction and outlining, JIT hot-cold code splitting, and\nJIT OSR-entry. These optimizations are either employed automatically, or guided\nby the language implementer through intuitive APIs. As a result, the\ndisassembly of the Deegen-generated interpreter, baseline JIT, and the\ngenerated JIT code rivals the assembly code hand-written by experts in\nstate-of-the-art VMs.\n  We implement LuaJIT Remake (LJR), a standard-compliant Lua 5.1 VM, using\nDeegen. Across 44 benchmarks, LJR's interpreter is on average 179% faster than\nthe official PUC Lua interpreter, and 31% faster than LuaJIT's interpreter.\nLJR's baseline JIT has negligible startup delay, and its execution performance\nis on average 360% faster than PUC Lua and only 33% slower (but faster on 13/44\nbenchmarks) than LuaJIT's optimizing JIT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building a high-performance JIT-capable VM for a dynamic language has\ntraditionally required a tremendous amount of time, money, and expertise. We\npresent Deegen, a meta-compiler that allows users to generate a\nhigh-performance JIT-capable VM for their own language at an engineering cost\nsimilar to writing a simple interpreter. Deegen takes in the execution\nsemantics of the bytecodes implemented as C++ functions, and automatically\ngenerates a two-tier VM execution engine with a state-of-the-art interpreter, a\nstate-of-the-art baseline JIT, and the tier-switching logic that connects them\ninto a self-adaptive system.\n  We are the first to demonstrate the automatic generation of a JIT compiler,\nand the automatic generation of an interpreter that outperforms the state of\nthe art. Our performance comes from a long list of optimizations supported by\nDeegen, including bytecode specialization and quickening, register pinning, tag\nregister optimization, call inline caching, generic inline caching, JIT\npolymorphic IC, JIT IC inline slab, type-check removal and strength reduction,\ntype-based slow-path extraction and outlining, JIT hot-cold code splitting, and\nJIT OSR-entry. These optimizations are either employed automatically, or guided\nby the language implementer through intuitive APIs. As a result, the\ndisassembly of the Deegen-generated interpreter, baseline JIT, and the\ngenerated JIT code rivals the assembly code hand-written by experts in\nstate-of-the-art VMs.\n  We implement LuaJIT Remake (LJR), a standard-compliant Lua 5.1 VM, using\nDeegen. Across 44 benchmarks, LJR's interpreter is on average 179% faster than\nthe official PUC Lua interpreter, and 31% faster than LuaJIT's interpreter.\nLJR's baseline JIT has negligible startup delay, and its execution performance\nis on average 360% faster than PUC Lua and only 33% slower (but faster on 13/44\nbenchmarks) than LuaJIT's optimizing JIT."
                },
                "authors": [
                    {
                        "name": "Haoran Xu"
                    },
                    {
                        "name": "Fredrik Kjolstad"
                    }
                ],
                "author_detail": {
                    "name": "Fredrik Kjolstad"
                },
                "author": "Fredrik Kjolstad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11469v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11469v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17741v1",
                "updated": "2024-11-24T16:20:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    16,
                    20,
                    57,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-24T16:20:57Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    16,
                    20,
                    57,
                    6,
                    329,
                    0
                ],
                "title": "Chameleon: Adaptive Caching and Scheduling for Many-Adapter LLM\n  Inference Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chameleon: Adaptive Caching and Scheduling for Many-Adapter LLM\n  Inference Environments"
                },
                "summary": "The widespread adoption of LLMs has driven an exponential rise in their\ndeployment, imposing substantial demands on inference clusters. These clusters\nmust handle numerous concurrent queries for different LLM downstream tasks. To\nhandle multi-task settings with vast LLM parameter counts, methods like\nLow-Rank Adaptation (LoRA) enable task-specific fine-tuning while sharing most\nof the base LLM model across tasks. Hence, they allow concurrent task serving\nwith minimal memory requirements. However, existing LLM serving systems face\ninefficiencies: they overlook workload heterogeneity, impose high link\nbandwidth from frequent adapter loading, and suffer from head-of-line blocking\nin their schedulers. To address these challenges, we present Chameleon, a novel\nLLM serving system optimized for many adapter environments, that relies on two\ncore ideas: adapter caching and adapter-aware scheduling. First, Chameleon\ncaches popular adapters in GPU memory, minimizing the adapter loading times.\nImportantly, it uses the otherwise idle GPU memory, avoiding extra memory\ncosts. Second, Chameleon uses a non-preemptive multi-queue scheduling to\nefficiently account for workload heterogeneity. In this way, Chameleon\nsimultaneously prevents head of line blocking and starvation. We implement\nChameleon on top of a state-of-the-art LLM serving platform and evaluate it\nwith real-world production traces and open-source LLMs. Under high loads,\nChameleon reduces P99 and P50 TTFT latency by 80.7% and 48.1%, respectively,\nwhile improving throughput by 1.5x compared to state-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption of LLMs has driven an exponential rise in their\ndeployment, imposing substantial demands on inference clusters. These clusters\nmust handle numerous concurrent queries for different LLM downstream tasks. To\nhandle multi-task settings with vast LLM parameter counts, methods like\nLow-Rank Adaptation (LoRA) enable task-specific fine-tuning while sharing most\nof the base LLM model across tasks. Hence, they allow concurrent task serving\nwith minimal memory requirements. However, existing LLM serving systems face\ninefficiencies: they overlook workload heterogeneity, impose high link\nbandwidth from frequent adapter loading, and suffer from head-of-line blocking\nin their schedulers. To address these challenges, we present Chameleon, a novel\nLLM serving system optimized for many adapter environments, that relies on two\ncore ideas: adapter caching and adapter-aware scheduling. First, Chameleon\ncaches popular adapters in GPU memory, minimizing the adapter loading times.\nImportantly, it uses the otherwise idle GPU memory, avoiding extra memory\ncosts. Second, Chameleon uses a non-preemptive multi-queue scheduling to\nefficiently account for workload heterogeneity. In this way, Chameleon\nsimultaneously prevents head of line blocking and starvation. We implement\nChameleon on top of a state-of-the-art LLM serving platform and evaluate it\nwith real-world production traces and open-source LLMs. Under high loads,\nChameleon reduces P99 and P50 TTFT latency by 80.7% and 48.1%, respectively,\nwhile improving throughput by 1.5x compared to state-of-the-art baselines."
                },
                "authors": [
                    {
                        "name": "Nikoleta Iliakopoulou"
                    },
                    {
                        "name": "Jovan Stojkovic"
                    },
                    {
                        "name": "Chloe Alverti"
                    },
                    {
                        "name": "Tianyin Xu"
                    },
                    {
                        "name": "Hubertus Franke"
                    },
                    {
                        "name": "Josep Torrellas"
                    }
                ],
                "author_detail": {
                    "name": "Josep Torrellas"
                },
                "author": "Josep Torrellas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.0; D.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15735v1",
                "updated": "2024-11-24T06:43:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    6,
                    43,
                    38,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-24T06:43:38Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    6,
                    43,
                    38,
                    6,
                    329,
                    0
                ],
                "title": "Test-time Alignment-Enhanced Adapter for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time Alignment-Enhanced Adapter for Vision-Language Models"
                },
                "summary": "Test-time adaptation with pre-trained vision-language models (VLMs) has\nattracted increasing attention for tackling the issue of distribution shift\nduring the test phase. While prior methods have shown effectiveness in\naddressing distribution shift by adjusting classification logits, they are not\noptimal due to keeping text features unchanged. To address this issue, we\nintroduce a new approach called Test-time Alignment-Enhanced Adapter (TAEA),\nwhich trains an adapter with test samples to adjust text features during the\ntest phase. We can enhance the text-to-image alignment prediction by utilizing\nan adapter to adapt text features. Furthermore, we also propose to adopt the\nnegative cache from TDA as enhancement module, which further improves the\nperformance of TAEA. Our approach outperforms the state-of-the-art TTA method\nof pre-trained VLMs by an average of 0.75% on the out-of-distribution benchmark\nand 2.5% on the cross-domain benchmark, with an acceptable training time. Code\nwill be available at https://github.com/BaoshunWq/clip-TAEA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time adaptation with pre-trained vision-language models (VLMs) has\nattracted increasing attention for tackling the issue of distribution shift\nduring the test phase. While prior methods have shown effectiveness in\naddressing distribution shift by adjusting classification logits, they are not\noptimal due to keeping text features unchanged. To address this issue, we\nintroduce a new approach called Test-time Alignment-Enhanced Adapter (TAEA),\nwhich trains an adapter with test samples to adjust text features during the\ntest phase. We can enhance the text-to-image alignment prediction by utilizing\nan adapter to adapt text features. Furthermore, we also propose to adopt the\nnegative cache from TDA as enhancement module, which further improves the\nperformance of TAEA. Our approach outperforms the state-of-the-art TTA method\nof pre-trained VLMs by an average of 0.75% on the out-of-distribution benchmark\nand 2.5% on the cross-domain benchmark, with an acceptable training time. Code\nwill be available at https://github.com/BaoshunWq/clip-TAEA."
                },
                "authors": [
                    {
                        "name": "Baoshun Tong"
                    },
                    {
                        "name": "Kaiyu Song"
                    },
                    {
                        "name": "Hanjiang Lai"
                    }
                ],
                "author_detail": {
                    "name": "Hanjiang Lai"
                },
                "author": "Hanjiang Lai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09688v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09688v2",
                "updated": "2024-11-23T22:11:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    23,
                    22,
                    11,
                    42,
                    5,
                    328,
                    0
                ],
                "published": "2024-11-14T18:54:19Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    54,
                    19,
                    3,
                    319,
                    0
                ],
                "title": "Squeezed Attention: Accelerating Long Context Length LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Squeezed Attention: Accelerating Long Context Length LLM Inference"
                },
                "summary": "Emerging Large Language Model (LLM) applications require long input prompts\nto perform complex downstream tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nto process user inputs quickly, as they are received. In this work, we propose\nSqueezed Attention as a mechanism to accelerate LLM applications where a large\nportion of the input prompt is fixed. We first leverage K-means clustering\noffline to group the keys for the fixed context based on semantic similarity\nand represent each cluster with a single centroid value. During inference, we\ncompare query tokens from the user input with the centroids to predict which of\nthe keys from the fixed context are semantically relevant and need to be loaded\nduring inference. We then compute exact attention using only these important\nkeys from the fixed context, thereby reducing bandwidth and computational\ncosts. We also extend our method to use a hierarchical centroid lookup to\nidentify important keys, which can reduce the complexity of attention from\nlinear to logarithmic with respect to the context length. We implement\noptimized Triton kernels for centroid comparison and sparse FlashAttention with\nimportant keys, achieving more than 4x speedups during both the prefill and\ngeneration phases for long-context inference. Furthermore, we have extensively\nevaluated our method on various long-context benchmarks including LongBench,\nwhere it achieves a 3x reduction in KV cache budget without accuracy loss and\nup to an 8x reduction with <0.5 point accuracy gap for various models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging Large Language Model (LLM) applications require long input prompts\nto perform complex downstream tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nto process user inputs quickly, as they are received. In this work, we propose\nSqueezed Attention as a mechanism to accelerate LLM applications where a large\nportion of the input prompt is fixed. We first leverage K-means clustering\noffline to group the keys for the fixed context based on semantic similarity\nand represent each cluster with a single centroid value. During inference, we\ncompare query tokens from the user input with the centroids to predict which of\nthe keys from the fixed context are semantically relevant and need to be loaded\nduring inference. We then compute exact attention using only these important\nkeys from the fixed context, thereby reducing bandwidth and computational\ncosts. We also extend our method to use a hierarchical centroid lookup to\nidentify important keys, which can reduce the complexity of attention from\nlinear to logarithmic with respect to the context length. We implement\noptimized Triton kernels for centroid comparison and sparse FlashAttention with\nimportant keys, achieving more than 4x speedups during both the prefill and\ngeneration phases for long-context inference. Furthermore, we have extensively\nevaluated our method on various long-context benchmarks including LongBench,\nwhere it achieves a 3x reduction in KV cache budget without accuracy loss and\nup to an 8x reduction with <0.5 point accuracy gap for various models."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Hiva Mohammadzadeh"
                    },
                    {
                        "name": "Monishwaran Maheswaran"
                    },
                    {
                        "name": "June Paik"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09688v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09688v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.05396v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.05396v3",
                "updated": "2024-11-23T10:42:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    23,
                    10,
                    42,
                    11,
                    5,
                    328,
                    0
                ],
                "published": "2024-02-08T04:16:35Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    4,
                    16,
                    35,
                    3,
                    39,
                    0
                ],
                "title": "TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph\n  Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph\n  Representation Learning"
                },
                "summary": "Recently, Temporal Graph Neural Networks (TGNNs) have demonstrated\nstate-of-the-art performance in various high-impact applications, including\nfraud detection and content recommendation. Despite the success of TGNNs, they\nare prone to the prevalent noise found in real-world dynamic graphs like\ntime-deprecated links and skewed interaction distribution. The noise causes two\ncritical issues that significantly compromise the accuracy of TGNNs: (1) models\nare supervised by inferior interactions, and (2) noisy input induces high\nvariance in the aggregated messages. However, current TGNN denoising techniques\ndo not consider the diverse and dynamic noise pattern of each node. In\naddition, they also suffer from the excessive mini-batch generation overheads\ncaused by traversing more neighbors. We believe the remedy for fast and\naccurate TGNNs lies in temporal adaptive sampling. In this work, we propose\nTASER, the first adaptive sampling method for TGNNs optimized for accuracy,\nefficiency, and scalability. TASER adapts its mini-batch selection based on\ntraining dynamics and temporal neighbor selection based on the contextual,\nstructural, and temporal properties of past interactions. To alleviate the\nbottleneck in mini-batch generation, TASER implements a pure GPU-based temporal\nneighbor finder and a dedicated GPU feature cache. We evaluate the performance\nof TASER using two state-of-the-art backbone TGNNs. On five popular datasets,\nTASER outperforms the corresponding baselines by an average of 2.3% in Mean\nReciprocal Rank (MRR) while achieving an average of 5.1x speedup in training\ntime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Temporal Graph Neural Networks (TGNNs) have demonstrated\nstate-of-the-art performance in various high-impact applications, including\nfraud detection and content recommendation. Despite the success of TGNNs, they\nare prone to the prevalent noise found in real-world dynamic graphs like\ntime-deprecated links and skewed interaction distribution. The noise causes two\ncritical issues that significantly compromise the accuracy of TGNNs: (1) models\nare supervised by inferior interactions, and (2) noisy input induces high\nvariance in the aggregated messages. However, current TGNN denoising techniques\ndo not consider the diverse and dynamic noise pattern of each node. In\naddition, they also suffer from the excessive mini-batch generation overheads\ncaused by traversing more neighbors. We believe the remedy for fast and\naccurate TGNNs lies in temporal adaptive sampling. In this work, we propose\nTASER, the first adaptive sampling method for TGNNs optimized for accuracy,\nefficiency, and scalability. TASER adapts its mini-batch selection based on\ntraining dynamics and temporal neighbor selection based on the contextual,\nstructural, and temporal properties of past interactions. To alleviate the\nbottleneck in mini-batch generation, TASER implements a pure GPU-based temporal\nneighbor finder and a dedicated GPU feature cache. We evaluate the performance\nof TASER using two state-of-the-art backbone TGNNs. On five popular datasets,\nTASER outperforms the corresponding baselines by an average of 2.3% in Mean\nReciprocal Rank (MRR) while achieving an average of 5.1x speedup in training\ntime."
                },
                "authors": [
                    {
                        "name": "Gangda Deng"
                    },
                    {
                        "name": "Hongkuan Zhou"
                    },
                    {
                        "name": "Hanqing Zeng"
                    },
                    {
                        "name": "Yinglong Xia"
                    },
                    {
                        "name": "Christopher Leung"
                    },
                    {
                        "name": "Jianbo Li"
                    },
                    {
                        "name": "Rajgopal Kannan"
                    },
                    {
                        "name": "Viktor Prasanna"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Prasanna"
                },
                "author": "Viktor Prasanna",
                "arxiv_comment": "IPDPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.05396v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.05396v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02109v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02109v2",
                "updated": "2024-11-23T01:44:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    23,
                    1,
                    44,
                    0,
                    5,
                    328,
                    0
                ],
                "published": "2024-07-02T09:51:56Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    9,
                    51,
                    56,
                    1,
                    184,
                    0
                ],
                "title": "HRSAM: Efficient Interactive Segmentation in High-Resolution Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HRSAM: Efficient Interactive Segmentation in High-Resolution Images"
                },
                "summary": "The Segment Anything Model (SAM) has advanced interactive segmentation but is\nlimited by the high computational cost on high-resolution images. This requires\ndownsampling to meet GPU constraints, sacrificing the fine-grained details\nneeded for high-precision interactive segmentation. To address SAM's\nlimitations, we focus on visual length extrapolation and propose a lightweight\nmodel named HRSAM. The extrapolation enables HRSAM trained on low resolutions\nto generalize to high resolutions. We begin by finding the link between the\nextrapolation and attention scores, which leads us to base HRSAM on Swin\nattention. We then introduce the Flexible Local Attention (FLA) framework,\nusing CUDA-optimized Efficient Memory Attention to accelerate HRSAM. Within\nFLA, we implement Flash Swin attention, achieving over a 35% speedup compared\nto traditional Swin attention, and propose a KV-only padding mechanism to\nenhance extrapolation. We also develop the Cycle-scan module that uses State\nSpace models to efficiently expand HRSAM's receptive field. We further develop\nthe HRSAM++ within FLA by adding an anchor map, providing multi-scale data\naugmentation for the extrapolation and a larger receptive field at slight\ncomputational cost. Experiments show that, under standard training, HRSAMs\nsurpass the previous SOTA with only 38% of the latency. With SAM-distillation,\nthe extrapolation enables HRSAMs to outperform the teacher model at lower\nlatency. Further finetuning achieves performance significantly exceeding the\nprevious SOTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Segment Anything Model (SAM) has advanced interactive segmentation but is\nlimited by the high computational cost on high-resolution images. This requires\ndownsampling to meet GPU constraints, sacrificing the fine-grained details\nneeded for high-precision interactive segmentation. To address SAM's\nlimitations, we focus on visual length extrapolation and propose a lightweight\nmodel named HRSAM. The extrapolation enables HRSAM trained on low resolutions\nto generalize to high resolutions. We begin by finding the link between the\nextrapolation and attention scores, which leads us to base HRSAM on Swin\nattention. We then introduce the Flexible Local Attention (FLA) framework,\nusing CUDA-optimized Efficient Memory Attention to accelerate HRSAM. Within\nFLA, we implement Flash Swin attention, achieving over a 35% speedup compared\nto traditional Swin attention, and propose a KV-only padding mechanism to\nenhance extrapolation. We also develop the Cycle-scan module that uses State\nSpace models to efficiently expand HRSAM's receptive field. We further develop\nthe HRSAM++ within FLA by adding an anchor map, providing multi-scale data\naugmentation for the extrapolation and a larger receptive field at slight\ncomputational cost. Experiments show that, under standard training, HRSAMs\nsurpass the previous SOTA with only 38% of the latency. With SAM-distillation,\nthe extrapolation enables HRSAMs to outperform the teacher model at lower\nlatency. Further finetuning achieves performance significantly exceeding the\nprevious SOTA."
                },
                "authors": [
                    {
                        "name": "You Huang"
                    },
                    {
                        "name": "Wenbin Lai"
                    },
                    {
                        "name": "Jiayi Ji"
                    },
                    {
                        "name": "Liujuan Cao"
                    },
                    {
                        "name": "Shengchuan Zhang"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02109v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02109v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15322v1",
                "updated": "2024-11-22T19:30:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    19,
                    30,
                    40,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T19:30:40Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    19,
                    30,
                    40,
                    4,
                    327,
                    0
                ],
                "title": "Deep Learning-Based Automatic Delineation of Liver Domes in kV Triggered\n  Images for Online Breath-hold Reproducibility Verification of Liver\n  Stereotactic Body Radiation Therapy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning-Based Automatic Delineation of Liver Domes in kV Triggered\n  Images for Online Breath-hold Reproducibility Verification of Liver\n  Stereotactic Body Radiation Therapy"
                },
                "summary": "Stereotactic Body Radiation Therapy (SBRT) can be a precise, minimally\ninvasive treatment method for liver cancer and liver metastases. However, the\neffectiveness of SBRT relies on the accurate delivery of the dose to the tumor\nwhile sparing healthy tissue. Challenges persist in ensuring breath-hold\nreproducibility, with current methods often requiring manual verification of\nliver dome positions from kV-triggered images. To address this, we propose a\nproof-of-principle study of a deep learning-based pipeline to automatically\ndelineate the liver dome from kV-planar images. From 24 patients who received\nSBRT for liver cancer or metastasis inside liver, 711 KV-triggered images\nacquired for online breath-hold verification were included in the current\nstudy. We developed a pipeline comprising a trained U-Net for automatic liver\ndome region segmentation from the triggered images followed by extraction of\nthe liver dome via thresholding, edge detection, and morphological operations.\nThe performance and generalizability of the pipeline was evaluated using 2-fold\ncross validation. The training of the U-Net model for liver region segmentation\ntook under 30 minutes and the automatic delineation of a liver dome for any\ntriggered image took less than one second. The RMSE and rate of detection for\nFold1 with 366 images was (6.4 +/- 1.6) mm and 91.7%, respectively. For Fold2\nwith 345 images, the RMSE and rate of detection was (7.7 +/- 2.3) mm and 76.3%\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stereotactic Body Radiation Therapy (SBRT) can be a precise, minimally\ninvasive treatment method for liver cancer and liver metastases. However, the\neffectiveness of SBRT relies on the accurate delivery of the dose to the tumor\nwhile sparing healthy tissue. Challenges persist in ensuring breath-hold\nreproducibility, with current methods often requiring manual verification of\nliver dome positions from kV-triggered images. To address this, we propose a\nproof-of-principle study of a deep learning-based pipeline to automatically\ndelineate the liver dome from kV-planar images. From 24 patients who received\nSBRT for liver cancer or metastasis inside liver, 711 KV-triggered images\nacquired for online breath-hold verification were included in the current\nstudy. We developed a pipeline comprising a trained U-Net for automatic liver\ndome region segmentation from the triggered images followed by extraction of\nthe liver dome via thresholding, edge detection, and morphological operations.\nThe performance and generalizability of the pipeline was evaluated using 2-fold\ncross validation. The training of the U-Net model for liver region segmentation\ntook under 30 minutes and the automatic delineation of a liver dome for any\ntriggered image took less than one second. The RMSE and rate of detection for\nFold1 with 366 images was (6.4 +/- 1.6) mm and 91.7%, respectively. For Fold2\nwith 345 images, the RMSE and rate of detection was (7.7 +/- 2.3) mm and 76.3%\nrespectively."
                },
                "authors": [
                    {
                        "name": "Sugandima Weragoda"
                    },
                    {
                        "name": "Ping Xia"
                    },
                    {
                        "name": "Kevin Stephans"
                    },
                    {
                        "name": "Neil Woody"
                    },
                    {
                        "name": "Michael Martens"
                    },
                    {
                        "name": "Robert Brown"
                    },
                    {
                        "name": "Bingqi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Bingqi Guo"
                },
                "author": "Bingqi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15102v1",
                "updated": "2024-11-22T18:06:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T18:06:14Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "title": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution"
                },
                "summary": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods."
                },
                "authors": [
                    {
                        "name": "Fengyuan Liu"
                    },
                    {
                        "name": "Nikhil Kandpal"
                    },
                    {
                        "name": "Colin Raffel"
                    }
                ],
                "author_detail": {
                    "name": "Colin Raffel"
                },
                "author": "Colin Raffel",
                "arxiv_comment": "29 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15024v1",
                "updated": "2024-11-22T15:55:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T15:55:19Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "title": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models"
                },
                "summary": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04032v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04032v5",
                "updated": "2024-11-21T05:55:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    5,
                    55,
                    43,
                    3,
                    326,
                    0
                ],
                "published": "2024-02-06T14:26:22Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    14,
                    26,
                    22,
                    1,
                    37,
                    0
                ],
                "title": "ProactivePIM: Accelerating Weight-Sharing Embedding Layer with PIM for\n  Scalable Recommendation System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProactivePIM: Accelerating Weight-Sharing Embedding Layer with PIM for\n  Scalable Recommendation System"
                },
                "summary": "The model size growth of personalized recommendation systems poses new\nchallenges for inference. Weight-sharing algorithms have been proposed for size\nreduction, but they increase memory access. Recent advancements in\nprocessing-in-memory (PIM) enhanced the model throughput by exploiting memory\nparallelism, but such algorithms introduce massive CPU-PIM communication into\nprior PIM systems. We propose ProactivePIM, a PIM system for weight-sharing\nrecommendation system acceleration. ProactivePIM integrates a cache within the\nPIM with a prefetching scheme to leverage a unique locality of the algorithm\nand eliminate communication overhead through a subtable mapping strategy.\nProactivePIM achieves a 4.8x speedup compared to prior works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The model size growth of personalized recommendation systems poses new\nchallenges for inference. Weight-sharing algorithms have been proposed for size\nreduction, but they increase memory access. Recent advancements in\nprocessing-in-memory (PIM) enhanced the model throughput by exploiting memory\nparallelism, but such algorithms introduce massive CPU-PIM communication into\nprior PIM systems. We propose ProactivePIM, a PIM system for weight-sharing\nrecommendation system acceleration. ProactivePIM integrates a cache within the\nPIM with a prefetching scheme to leverage a unique locality of the algorithm\nand eliminate communication overhead through a subtable mapping strategy.\nProactivePIM achieves a 4.8x speedup compared to prior works."
                },
                "authors": [
                    {
                        "name": "Youngsuk Kim"
                    },
                    {
                        "name": "Junghwan Lim"
                    },
                    {
                        "name": "Hyuk-Jae Lee"
                    },
                    {
                        "name": "Chae Eun Rhee"
                    }
                ],
                "author_detail": {
                    "name": "Chae Eun Rhee"
                },
                "author": "Chae Eun Rhee",
                "arxiv_comment": "8 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04032v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04032v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13854v1",
                "updated": "2024-11-21T05:26:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    5,
                    26,
                    57,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T05:26:57Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    5,
                    26,
                    57,
                    3,
                    326,
                    0
                ],
                "title": "Static Reuse Profile Estimation for Array Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static Reuse Profile Estimation for Array Applications"
                },
                "summary": "Reuse distance analysis is a widely recognized method for application\ncharacterization that illustrates cache locality. Although there are various\ntechniques to calculate the reuse profile from dynamic memory traces, it is\nboth time and space-consuming due to the requirement to collect dynamic memory\ntraces at runtime. In contrast, static analysis reuse profile estimation is a\npromisingly faster approach since it is calculated at compile time without\nrunning the program or collecting memory traces. This work presents a static\nanalysis technique to estimate the reuse profile of loop-based programs. For an\ninput program, we generate a basic block-level control flow graph and the\nexecution count by analyzing the LLVM IR of the program. We present the memory\naccesses of the application kernel in a compact bracketed format and use a\nrecursive algorithm to predict the reuse distance histogram. We deploy a\nseparate predictor that unrolls the loop(s) for smaller bounds and generates a\ntemporary reuse distance profile for those small cases. Using these smaller\nprofiles, the reuse profile is extrapolated for the actual loop bound(s). We\nuse this reuse profile to predict the cache hit rate. Results show that our\nmodel can predict cache hit rates with an average accuracy of 95% relative to\nthe dynamic reuse profile methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reuse distance analysis is a widely recognized method for application\ncharacterization that illustrates cache locality. Although there are various\ntechniques to calculate the reuse profile from dynamic memory traces, it is\nboth time and space-consuming due to the requirement to collect dynamic memory\ntraces at runtime. In contrast, static analysis reuse profile estimation is a\npromisingly faster approach since it is calculated at compile time without\nrunning the program or collecting memory traces. This work presents a static\nanalysis technique to estimate the reuse profile of loop-based programs. For an\ninput program, we generate a basic block-level control flow graph and the\nexecution count by analyzing the LLVM IR of the program. We present the memory\naccesses of the application kernel in a compact bracketed format and use a\nrecursive algorithm to predict the reuse distance histogram. We deploy a\nseparate predictor that unrolls the loop(s) for smaller bounds and generates a\ntemporary reuse distance profile for those small cases. Using these smaller\nprofiles, the reuse profile is extrapolated for the actual loop bound(s). We\nuse this reuse profile to predict the cache hit rate. Results show that our\nmodel can predict cache hit rates with an average accuracy of 95% relative to\nthe dynamic reuse profile methods."
                },
                "authors": [
                    {
                        "name": "Abdur Razzak"
                    },
                    {
                        "name": "Atanu Barai"
                    },
                    {
                        "name": "Nandakishore Santhi"
                    },
                    {
                        "name": "Abdel-Hameed A. Badawy"
                    }
                ],
                "author_detail": {
                    "name": "Abdel-Hameed A. Badawy"
                },
                "author": "Abdel-Hameed A. Badawy",
                "arxiv_comment": "Accepted in The International Symposium on Memory Systems (MEMSYS\n  24), September 30 to October 03, 2024, Washington, DC, USA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.02243v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.02243v3",
                "updated": "2024-11-21T04:12:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    4,
                    12,
                    53,
                    3,
                    326,
                    0
                ],
                "published": "2023-06-04T03:06:37Z",
                "published_parsed": [
                    2023,
                    6,
                    4,
                    3,
                    6,
                    37,
                    6,
                    155,
                    0
                ],
                "title": "Retrieval-Enhanced Visual Prompt Learning for Few-shot Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Enhanced Visual Prompt Learning for Few-shot Classification"
                },
                "summary": "The Contrastive Language-Image Pretraining (CLIP) model has been widely used\nin various downstream vision tasks. The few-shot learning paradigm has been\nwidely adopted to augment its capacity for these tasks. However, current\nparadigms may struggle with fine-grained classification, such as satellite\nimage recognition, due to widening domain gaps. To address this limitation, we\npropose retrieval-enhanced visual prompt learning (RePrompt), which introduces\nretrieval mechanisms to cache and reuse the knowledge of downstream tasks.\nRePrompt constructs a retrieval database from either training examples or\nexternal data if available, and uses a retrieval mechanism to enhance multiple\nstages of a simple prompt learning baseline, thus narrowing the domain gap.\nDuring inference, our enhanced model can reference similar samples brought by\nretrieval to make more accurate predictions. A detailed analysis reveals that\nretrieval helps to improve the distribution of late features, thus, improving\ngeneralization for downstream tasks. Reprompt attains state-of-the-art\nperformance on a wide range of vision datasets, including 11 image datasets, 3\nvideo datasets, 1 multi-view dataset, and 4 domain generalization benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Contrastive Language-Image Pretraining (CLIP) model has been widely used\nin various downstream vision tasks. The few-shot learning paradigm has been\nwidely adopted to augment its capacity for these tasks. However, current\nparadigms may struggle with fine-grained classification, such as satellite\nimage recognition, due to widening domain gaps. To address this limitation, we\npropose retrieval-enhanced visual prompt learning (RePrompt), which introduces\nretrieval mechanisms to cache and reuse the knowledge of downstream tasks.\nRePrompt constructs a retrieval database from either training examples or\nexternal data if available, and uses a retrieval mechanism to enhance multiple\nstages of a simple prompt learning baseline, thus narrowing the domain gap.\nDuring inference, our enhanced model can reference similar samples brought by\nretrieval to make more accurate predictions. A detailed analysis reveals that\nretrieval helps to improve the distribution of late features, thus, improving\ngeneralization for downstream tasks. Reprompt attains state-of-the-art\nperformance on a wide range of vision datasets, including 11 image datasets, 3\nvideo datasets, 1 multi-view dataset, and 4 domain generalization benchmarks."
                },
                "authors": [
                    {
                        "name": "Jintao Rong"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Linlin Ou"
                    },
                    {
                        "name": "Tianxiao Chen"
                    },
                    {
                        "name": "Xinyi Yu"
                    },
                    {
                        "name": "Yifan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yifan Liu"
                },
                "author": "Yifan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.02243v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.02243v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13820v1",
                "updated": "2024-11-21T03:52:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    52,
                    41,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T03:52:41Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    52,
                    41,
                    3,
                    326,
                    0
                ],
                "title": "InstCache: A Predictive Cache for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstCache: A Predictive Cache for LLM Serving"
                },
                "summary": "Large language models are revolutionizing every aspect of human life.\nHowever, the unprecedented power comes at the cost of significant computing\nintensity, suggesting long latency and large energy footprint. Key-Value Cache\nand Semantic Cache have been proposed as a solution to the above problem, but\nboth suffer from limited scalability due to significant memory cost for each\ntoken or instruction embeddings. Motivated by the observations that most\ninstructions are short, repetitive and predictable by LLMs, we propose to\npredict user-instructions by an instruction-aligned LLM and store them in a\npredictive cache, so-called InstCache. We introduce an instruction\npre-population algorithm based on the negative log likelihood of instructions,\ndetermining the cache size with regard to the hit rate. The proposed InstCache\nis efficiently implemented as a hash table with minimal lookup latency for\ndeployment. Experimental results show that InstCache can achieve up to 51.34%\nhit rate on LMSys dataset, which corresponds to a 2x speedup, at a memory cost\nof only 4.5GB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are revolutionizing every aspect of human life.\nHowever, the unprecedented power comes at the cost of significant computing\nintensity, suggesting long latency and large energy footprint. Key-Value Cache\nand Semantic Cache have been proposed as a solution to the above problem, but\nboth suffer from limited scalability due to significant memory cost for each\ntoken or instruction embeddings. Motivated by the observations that most\ninstructions are short, repetitive and predictable by LLMs, we propose to\npredict user-instructions by an instruction-aligned LLM and store them in a\npredictive cache, so-called InstCache. We introduce an instruction\npre-population algorithm based on the negative log likelihood of instructions,\ndetermining the cache size with regard to the hit rate. The proposed InstCache\nis efficiently implemented as a hash table with minimal lookup latency for\ndeployment. Experimental results show that InstCache can achieve up to 51.34%\nhit rate on LMSys dataset, which corresponds to a 2x speedup, at a memory cost\nof only 4.5GB."
                },
                "authors": [
                    {
                        "name": "Longwei Zou"
                    },
                    {
                        "name": "Tingfeng Liu"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Jiangang Kong"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22649v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22649v2",
                "updated": "2024-11-21T03:34:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    34,
                    44,
                    3,
                    326,
                    0
                ],
                "published": "2024-10-30T02:36:55Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    36,
                    55,
                    2,
                    304,
                    0
                ],
                "title": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting"
                },
                "summary": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs. Our code is available at\nhttps://github.com/Leopold2333/WaveRoRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs. Our code is available at\nhttps://github.com/Leopold2333/WaveRoRA."
                },
                "authors": [
                    {
                        "name": "Aobo Liang"
                    },
                    {
                        "name": "Yan Sun"
                    },
                    {
                        "name": "Nadra Guizani"
                    }
                ],
                "author_detail": {
                    "name": "Nadra Guizani"
                },
                "author": "Nadra Guizani",
                "arxiv_comment": "Model architecture changed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22649v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22649v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13786v1",
                "updated": "2024-11-21T02:15:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    2,
                    15,
                    52,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T02:15:52Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    2,
                    15,
                    52,
                    3,
                    326,
                    0
                ],
                "title": "Adaptable Embeddings Network (AEN)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptable Embeddings Network (AEN)"
                },
                "summary": "Modern day Language Models see extensive use in text classification, yet this\ncomes at significant computational cost. Compute-effective classification\nmodels are needed for low-resource environments, most notably on edge devices.\nWe introduce Adaptable Embeddings Networks (AEN), a novel dual-encoder\narchitecture using Kernel Density Estimation (KDE). This architecture allows\nfor runtime adaptation of classification criteria without retraining and is\nnon-autoregressive. Through thorough synthetic data experimentation, we\ndemonstrate our model outputs comparable and in certain cases superior results\nto that of autoregressive models an order of magnitude larger than AEN's size.\nThe architecture's ability to preprocess and cache condition embeddings makes\nit ideal for edge computing applications and real-time monitoring systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern day Language Models see extensive use in text classification, yet this\ncomes at significant computational cost. Compute-effective classification\nmodels are needed for low-resource environments, most notably on edge devices.\nWe introduce Adaptable Embeddings Networks (AEN), a novel dual-encoder\narchitecture using Kernel Density Estimation (KDE). This architecture allows\nfor runtime adaptation of classification criteria without retraining and is\nnon-autoregressive. Through thorough synthetic data experimentation, we\ndemonstrate our model outputs comparable and in certain cases superior results\nto that of autoregressive models an order of magnitude larger than AEN's size.\nThe architecture's ability to preprocess and cache condition embeddings makes\nit ideal for edge computing applications and real-time monitoring systems."
                },
                "authors": [
                    {
                        "name": "Stan Loosmore"
                    },
                    {
                        "name": "Alexander Titus"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Titus"
                },
                "author": "Alexander Titus",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13676v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13676v1",
                "updated": "2024-11-20T19:51:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    51,
                    25,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T19:51:25Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    51,
                    25,
                    2,
                    325,
                    0
                ],
                "title": "Hymba: A Hybrid-head Architecture for Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hymba: A Hybrid-head Architecture for Small Language Models"
                },
                "summary": "We propose Hymba, a family of small language models featuring a hybrid-head\nparallel architecture that integrates transformer attention mechanisms with\nstate space models (SSMs) for enhanced efficiency. Attention heads provide\nhigh-resolution recall, while SSM heads enable efficient context summarization.\nAdditionally, we introduce learnable meta tokens that are prepended to prompts,\nstoring critical information and alleviating the \"forced-to-attend\" burden\nassociated with attention mechanisms. This model is further optimized by\nincorporating cross-layer key-value (KV) sharing and partial sliding window\nattention, resulting in a compact cache size. During development, we conducted\na controlled study comparing various architectures under identical settings and\nobserved significant advantages of our proposed architecture. Notably, Hymba\nachieves state-of-the-art results for small LMs: Our Hymba-1.5B-Base model\nsurpasses all sub-2B public models in performance and even outperforms\nLlama-3.2-3B with 1.32% higher average accuracy, an 11.67x cache size\nreduction, and 3.49x throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Hymba, a family of small language models featuring a hybrid-head\nparallel architecture that integrates transformer attention mechanisms with\nstate space models (SSMs) for enhanced efficiency. Attention heads provide\nhigh-resolution recall, while SSM heads enable efficient context summarization.\nAdditionally, we introduce learnable meta tokens that are prepended to prompts,\nstoring critical information and alleviating the \"forced-to-attend\" burden\nassociated with attention mechanisms. This model is further optimized by\nincorporating cross-layer key-value (KV) sharing and partial sliding window\nattention, resulting in a compact cache size. During development, we conducted\na controlled study comparing various architectures under identical settings and\nobserved significant advantages of our proposed architecture. Notably, Hymba\nachieves state-of-the-art results for small LMs: Our Hymba-1.5B-Base model\nsurpasses all sub-2B public models in performance and even outperforms\nLlama-3.2-3B with 1.32% higher average accuracy, an 11.67x cache size\nreduction, and 3.49x throughput."
                },
                "authors": [
                    {
                        "name": "Xin Dong"
                    },
                    {
                        "name": "Yonggan Fu"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Wonmin Byeon"
                    },
                    {
                        "name": "Zijia Chen"
                    },
                    {
                        "name": "Ameya Sunil Mahabaleshwarkar"
                    },
                    {
                        "name": "Shih-Yang Liu"
                    },
                    {
                        "name": "Matthijs Van Keirsbilck"
                    },
                    {
                        "name": "Min-Hung Chen"
                    },
                    {
                        "name": "Yoshi Suhara"
                    },
                    {
                        "name": "Yingyan Lin"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    }
                ],
                "author_detail": {
                    "name": "Pavlo Molchanov"
                },
                "author": "Pavlo Molchanov",
                "arxiv_comment": "20 pages, models are available on huggingface",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13676v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13676v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17720v1",
                "updated": "2024-11-20T19:44:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    44,
                    26,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T19:44:26Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    44,
                    26,
                    2,
                    325,
                    0
                ],
                "title": "MAS-Attention: Memory-Aware Stream Processing for Attention Acceleration\n  on Resource-Constrained Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAS-Attention: Memory-Aware Stream Processing for Attention Acceleration\n  on Resource-Constrained Edge Devices"
                },
                "summary": "The advent of foundation models have revolutionized various fields, enabling\nunprecedented task accuracy and flexibility in computational linguistics,\ncomputer vision and other domains. Attention mechanism has become an essential\ncomponent of foundation models, due to their superb capability of capturing\ncorrelations in a sequence. However, attention results in quadratic complexity\nin memory and compute as the context length grows. Although many fusion-based\nexact attention acceleration algorithms have been developed for\ndatacenter-grade GPUs and accelerators leveraging multi-core parallelism and\ndata locality, yet it remains a significant challenge to accelerate attention\non resource-constrained edge neural accelerators with limited compute units and\nstringent on-chip caches. In this paper, we propose a scheme for exact\nattention inference acceleration on memory-constrained edge accelerators, by\nparallelizing the utilization of heterogeneous compute units, i.e., vector\nprocessing units and matrix processing units. Our method involves scheduling\nworkloads onto these different compute units in a multi-tiered tiling scheme to\nprocess tiled vector workloads and matrix workloads in attention as two\nstreams, respecting the workload dependencies. We search for tiling factors to\nmaximize the parallelization of both compute units while considering I/O\noverhead, and propose a proactive cache overwrite strategy to avoid undesirable\ncache spills in reality. Extensive results based on open-sourced simulation\nframeworks show up to 2.75x speedup and 54% reduction in energy consumption as\ncompared to the state-of-the-art attention fusion method (FLAT) in the edge\ncomputing scenario. Further experiments on a real-world edge neural processing\nunit demonstrate speedup of up to 1.76x for attention as compared to FLAT,\nwithout affecting model output accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of foundation models have revolutionized various fields, enabling\nunprecedented task accuracy and flexibility in computational linguistics,\ncomputer vision and other domains. Attention mechanism has become an essential\ncomponent of foundation models, due to their superb capability of capturing\ncorrelations in a sequence. However, attention results in quadratic complexity\nin memory and compute as the context length grows. Although many fusion-based\nexact attention acceleration algorithms have been developed for\ndatacenter-grade GPUs and accelerators leveraging multi-core parallelism and\ndata locality, yet it remains a significant challenge to accelerate attention\non resource-constrained edge neural accelerators with limited compute units and\nstringent on-chip caches. In this paper, we propose a scheme for exact\nattention inference acceleration on memory-constrained edge accelerators, by\nparallelizing the utilization of heterogeneous compute units, i.e., vector\nprocessing units and matrix processing units. Our method involves scheduling\nworkloads onto these different compute units in a multi-tiered tiling scheme to\nprocess tiled vector workloads and matrix workloads in attention as two\nstreams, respecting the workload dependencies. We search for tiling factors to\nmaximize the parallelization of both compute units while considering I/O\noverhead, and propose a proactive cache overwrite strategy to avoid undesirable\ncache spills in reality. Extensive results based on open-sourced simulation\nframeworks show up to 2.75x speedup and 54% reduction in energy consumption as\ncompared to the state-of-the-art attention fusion method (FLAT) in the edge\ncomputing scenario. Further experiments on a real-world edge neural processing\nunit demonstrate speedup of up to 1.76x for attention as compared to FLAT,\nwithout affecting model output accuracy."
                },
                "authors": [
                    {
                        "name": "Mohammadali Shakerdargah"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Chao Gao"
                    },
                    {
                        "name": "Di Niu"
                    }
                ],
                "author_detail": {
                    "name": "Di Niu"
                },
                "author": "Di Niu",
                "arxiv_comment": "10 pages, 6 figures, under review for MLSys 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.4; I.2.7; I.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13532v1",
                "updated": "2024-11-20T18:31:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    31,
                    39,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T18:31:39Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    31,
                    39,
                    2,
                    325,
                    0
                ],
                "title": "A Distributed-memory Tridiagonal Solver Based on a Specialised Data\n  Structure Optimised for CPU and GPU Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Distributed-memory Tridiagonal Solver Based on a Specialised Data\n  Structure Optimised for CPU and GPU Architectures"
                },
                "summary": "Various numerical methods used for solving partial differential equations\n(PDE) result in tridiagonal systems. Solving tridiagonal systems on\ndistributed-memory environments is not straightforward, and often requires\nsignificant amount of communication. In this article, we present a novel\ndistributed-memory tridiagonal solver algorithm, DistD2-TDS, based on a\nspecialised data structure. DistD2-TDS algorithm takes advantage of the\ndiagonal dominance in tridiagonal systems to reduce the communications in\ndistributed-memory environments. The underlying data structure plays a crucial\nrole for the performance of the algorithm. First, the data structure improves\ndata localities and makes it possible to minimise data movements via cache\nblocking and kernel fusion strategies. Second, data continuity enables a\ncontiguous data access pattern and results in efficient utilisation of the\navailable memory bandwidth. Finally, the data layout supports vectorisation on\nCPUs and thread level parallelisation on GPUs for improved performance. In\norder to demonstrate the robustness of the algorithm, we implemented and\nbenchmarked the algorithm on CPUs and GPUs. We investigated the single rank\nperformance and compared against existing algorithms. Furthermore, we analysed\nthe strong scaling of the implementation up to 384 NVIDIA H100 GPUs and up to\n8192 AMD EPYC 7742 CPUs. Finally, we demonstrated a practical use case of the\nalgorithm by using compact finite difference schemes to solve a 3D non-linear\nPDE. The results demonstrate that DistD2 algorithm can sustain around 66% of\nthe theoretical peak bandwidth at scale on CPU and GPU based supercomputers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Various numerical methods used for solving partial differential equations\n(PDE) result in tridiagonal systems. Solving tridiagonal systems on\ndistributed-memory environments is not straightforward, and often requires\nsignificant amount of communication. In this article, we present a novel\ndistributed-memory tridiagonal solver algorithm, DistD2-TDS, based on a\nspecialised data structure. DistD2-TDS algorithm takes advantage of the\ndiagonal dominance in tridiagonal systems to reduce the communications in\ndistributed-memory environments. The underlying data structure plays a crucial\nrole for the performance of the algorithm. First, the data structure improves\ndata localities and makes it possible to minimise data movements via cache\nblocking and kernel fusion strategies. Second, data continuity enables a\ncontiguous data access pattern and results in efficient utilisation of the\navailable memory bandwidth. Finally, the data layout supports vectorisation on\nCPUs and thread level parallelisation on GPUs for improved performance. In\norder to demonstrate the robustness of the algorithm, we implemented and\nbenchmarked the algorithm on CPUs and GPUs. We investigated the single rank\nperformance and compared against existing algorithms. Furthermore, we analysed\nthe strong scaling of the implementation up to 384 NVIDIA H100 GPUs and up to\n8192 AMD EPYC 7742 CPUs. Finally, we demonstrated a practical use case of the\nalgorithm by using compact finite difference schemes to solve a 3D non-linear\nPDE. The results demonstrate that DistD2 algorithm can sustain around 66% of\nthe theoretical peak bandwidth at scale on CPU and GPU based supercomputers."
                },
                "authors": [
                    {
                        "name": "Semih Akkurt"
                    },
                    {
                        "name": "SÃ©bastien Lemaire"
                    },
                    {
                        "name": "Paul Bartholomew"
                    },
                    {
                        "name": "Sylvain Laizet"
                    }
                ],
                "author_detail": {
                    "name": "Sylvain Laizet"
                },
                "author": "Sylvain Laizet",
                "arxiv_comment": "42 pages, 13 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13373v1",
                "updated": "2024-11-20T14:52:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    52,
                    36,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T14:52:36Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    52,
                    36,
                    2,
                    325,
                    0
                ],
                "title": "Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2\n  experiment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2\n  experiment"
                },
                "summary": "Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge\nExchange Recombination Spectroscopy (CHERS) and Motional Stark effect\ndiagnostics (MSE), are a well-known tool to access important information about\nmagnetically confined plasmas, such as radial profiles of ion temperature, ion\nflow, impurity content and intensity and direction of the magnetic field. For\nthis purpose, a DNBI was installed and operated in the RFX-mod experiment,\nwhich was designed to confine plasma mainly through the Reversed Field Pinch\nconfiguration. The DNBI, designed and built by the Budker Institute of Plasma\nPhysics, was based on a source of positive hydrogen ions, accelerated to 50 keV\nand for an equivalent neutral beam current of about 5 A at the source. The beam\ncould be modulated and the maximum overall duration was 50 ms. With the upgrade\nof RFX-mod to the present RFX-mod2 machine, the DNBI is being renovated to\nsolve several plant faults and improve the overall reliability of the system.\nThe 50 kV power supply is being improved, as well as the power supplies in the\nhigh voltage deck and its insulation transformer. The control system,\noriginally based on CAMAC technology, was redesigned to be fully replaced. This\ncontribution reviews the technical criticalities emerged in the DNBI check-up\nand the new solutions adopted to make the DNBI operative and more reliable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge\nExchange Recombination Spectroscopy (CHERS) and Motional Stark effect\ndiagnostics (MSE), are a well-known tool to access important information about\nmagnetically confined plasmas, such as radial profiles of ion temperature, ion\nflow, impurity content and intensity and direction of the magnetic field. For\nthis purpose, a DNBI was installed and operated in the RFX-mod experiment,\nwhich was designed to confine plasma mainly through the Reversed Field Pinch\nconfiguration. The DNBI, designed and built by the Budker Institute of Plasma\nPhysics, was based on a source of positive hydrogen ions, accelerated to 50 keV\nand for an equivalent neutral beam current of about 5 A at the source. The beam\ncould be modulated and the maximum overall duration was 50 ms. With the upgrade\nof RFX-mod to the present RFX-mod2 machine, the DNBI is being renovated to\nsolve several plant faults and improve the overall reliability of the system.\nThe 50 kV power supply is being improved, as well as the power supplies in the\nhigh voltage deck and its insulation transformer. The control system,\noriginally based on CAMAC technology, was redesigned to be fully replaced. This\ncontribution reviews the technical criticalities emerged in the DNBI check-up\nand the new solutions adopted to make the DNBI operative and more reliable."
                },
                "authors": [
                    {
                        "name": "Marco Barbisan"
                    },
                    {
                        "name": "Marco Boldrin"
                    },
                    {
                        "name": "Luca Cinnirella"
                    },
                    {
                        "name": "Bruno Laterza"
                    },
                    {
                        "name": "Alberto Maistrello"
                    },
                    {
                        "name": "Lionello Marrelli"
                    },
                    {
                        "name": "Federico Molon"
                    },
                    {
                        "name": "Simone Peruzzo"
                    },
                    {
                        "name": "Cesare Taliercio"
                    },
                    {
                        "name": "Marco Valisa"
                    },
                    {
                        "name": "Enrico Zampiva"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Zampiva"
                },
                "author": "Enrico Zampiva",
                "arxiv_comment": "6 pages (excl. highlights), 8 figures. Contribution to the 33rd\n  Symposium on Fusion Technology (SOFT), 22-27 September 2024. This is a\n  preprint for the \"Fusion Engineering and Design\" journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18003v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18003v4",
                "updated": "2024-11-20T02:04:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    2,
                    4,
                    10,
                    2,
                    325,
                    0
                ],
                "published": "2024-07-25T12:56:22Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    12,
                    56,
                    22,
                    3,
                    207,
                    0
                ],
                "title": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption"
                },
                "summary": "Large Language Models (LLMs), epitomized by ChatGPT's release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture's struggle with handling long texts. KV Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV Cache and elaborate on various\nmethods currently used to optimize the KV Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field. Links to the papers\nmentioned in this review can be found in our Github Repo\nhttps://github.com/zcli-charlie/Awesome-KV-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), epitomized by ChatGPT's release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture's struggle with handling long texts. KV Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV Cache and elaborate on various\nmethods currently used to optimize the KV Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field. Links to the papers\nmentioned in this review can be found in our Github Repo\nhttps://github.com/zcli-charlie/Awesome-KV-Cache."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Hongyi Zhang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "Published on the First Conference on Language Modeling (COLM 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18003v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18003v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17918v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17918v3",
                "updated": "2024-11-19T18:24:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    24,
                    3,
                    1,
                    324,
                    0
                ],
                "published": "2024-06-25T20:00:32Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    20,
                    0,
                    32,
                    1,
                    177,
                    0
                ],
                "title": "GraphSnapShot: Graph Machine Learning Acceleration with Fast Storage and\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphSnapShot: Graph Machine Learning Acceleration with Fast Storage and\n  Retrieval"
                },
                "summary": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Roger Waleffe"
                    },
                    {
                        "name": "Meng Jiang"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17918v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17918v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12430v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12430v1",
                "updated": "2024-11-19T11:40:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    11,
                    40,
                    56,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T11:40:56Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    11,
                    40,
                    56,
                    1,
                    324,
                    0
                ],
                "title": "An Eulerian approach to regularized JKO scheme with low-rank tensor\n  decompositions for Bayesian inversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Eulerian approach to regularized JKO scheme with low-rank tensor\n  decompositions for Bayesian inversion"
                },
                "summary": "The possibility of using the Eulerian discretization for the problem of\nmodelling high-dimensional distributions and sampling, is studied. The problem\nis posed as a minimization problem over the space of probability measures with\nrespect to the Wasserstein distance and solved with entropy-regularized JKO\nscheme. Each proximal step can be formulated as a fixed-point equation and\nsolved with accelerated methods, such as Anderson's. The usage of low-rank\nTensor Train format allows to overcome the \\emph{curse of dimensionality}, i.e.\nthe exponential growth of degrees of freedom with dimension, inherent to\nEulerian approaches. The resulting method requires only pointwise computations\nof the unnormalized posterior and is, in particular, gradient-free. Fixed\nEulerian grid allows to employ a caching strategy, significally reducing the\nexpensive evaluations of the posterior. When the Eulerian model of the target\ndistribution is fitted, the passage back to the Lagrangian perspective can also\nbe made, allowing to approximately sample from it. We test our method both for\nsynthetic target distributions and particular Bayesian inverse problems and\nreport comparable or better performance than the baseline Metropolis-Hastings\nMCMC with same amount of resources. Finally, the fitted model can be modified\nto facilitate the solution of certain associated problems, which we demonstrate\nby fitting an importance distribution for a particular quantity of interest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The possibility of using the Eulerian discretization for the problem of\nmodelling high-dimensional distributions and sampling, is studied. The problem\nis posed as a minimization problem over the space of probability measures with\nrespect to the Wasserstein distance and solved with entropy-regularized JKO\nscheme. Each proximal step can be formulated as a fixed-point equation and\nsolved with accelerated methods, such as Anderson's. The usage of low-rank\nTensor Train format allows to overcome the \\emph{curse of dimensionality}, i.e.\nthe exponential growth of degrees of freedom with dimension, inherent to\nEulerian approaches. The resulting method requires only pointwise computations\nof the unnormalized posterior and is, in particular, gradient-free. Fixed\nEulerian grid allows to employ a caching strategy, significally reducing the\nexpensive evaluations of the posterior. When the Eulerian model of the target\ndistribution is fitted, the passage back to the Lagrangian perspective can also\nbe made, allowing to approximately sample from it. We test our method both for\nsynthetic target distributions and particular Bayesian inverse problems and\nreport comparable or better performance than the baseline Metropolis-Hastings\nMCMC with same amount of resources. Finally, the fitted model can be modified\nto facilitate the solution of certain associated problems, which we demonstrate\nby fitting an importance distribution for a particular quantity of interest."
                },
                "authors": [
                    {
                        "name": "Vitalii Aksenov"
                    },
                    {
                        "name": "Martin Eigel"
                    }
                ],
                "author_detail": {
                    "name": "Martin Eigel"
                },
                "author": "Martin Eigel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12430v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "46E27, 49Q22, 62F15, 68W25",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12161v1",
                "updated": "2024-11-19T01:55:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    1,
                    55,
                    26,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T01:55:26Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    1,
                    55,
                    26,
                    1,
                    324,
                    0
                ],
                "title": "Adaptive Cache Management for Complex Storage Systems Using\n  CNN-LSTM-Based Spatiotemporal Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Cache Management for Complex Storage Systems Using\n  CNN-LSTM-Based Spatiotemporal Prediction"
                },
                "summary": "This paper proposes an intelligent cache management strategy based on\nCNN-LSTM to improve the performance and cache hit rate of storage systems.\nThrough comparative experiments with traditional algorithms (such as LRU and\nLFU) and other deep learning models (such as RNN, GRU-RNN and LSTM), the\nresults show that the CNN-LSTM model has significant advantages in cache demand\nprediction. The MSE and MAE values of this model are significantly reduced,\nproving its effectiveness under complex data access patterns. This study not\nonly verifies the potential of deep learning technology in storage system\noptimization, but also provides direction and reference for further optimizing\nand improving cache management strategies. This intelligent cache management\nstrategy performs well in complex storage environments. By combining the\nspatial feature extraction capabilities of convolutional neural networks and\nthe time series modeling capabilities of long short-term memory networks, the\nCNN-LSTM model can more accurately predict cache needs, thereby Dynamically\noptimize cache allocation to improve system response speed and resource\nutilization. This research provides theoretical support and practical reference\nfor cache optimization under large-scale data access modes, and is of great\nsignificance to improving the performance of future storage systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes an intelligent cache management strategy based on\nCNN-LSTM to improve the performance and cache hit rate of storage systems.\nThrough comparative experiments with traditional algorithms (such as LRU and\nLFU) and other deep learning models (such as RNN, GRU-RNN and LSTM), the\nresults show that the CNN-LSTM model has significant advantages in cache demand\nprediction. The MSE and MAE values of this model are significantly reduced,\nproving its effectiveness under complex data access patterns. This study not\nonly verifies the potential of deep learning technology in storage system\noptimization, but also provides direction and reference for further optimizing\nand improving cache management strategies. This intelligent cache management\nstrategy performs well in complex storage environments. By combining the\nspatial feature extraction capabilities of convolutional neural networks and\nthe time series modeling capabilities of long short-term memory networks, the\nCNN-LSTM model can more accurately predict cache needs, thereby Dynamically\noptimize cache allocation to improve system response speed and resource\nutilization. This research provides theoretical support and practical reference\nfor cache optimization under large-scale data access modes, and is of great\nsignificance to improving the performance of future storage systems."
                },
                "authors": [
                    {
                        "name": "Xiaoye Wang"
                    },
                    {
                        "name": "Xuan Li"
                    },
                    {
                        "name": "Linji Wang"
                    },
                    {
                        "name": "Tingyi Ruan"
                    },
                    {
                        "name": "Pochun Li"
                    }
                ],
                "author_detail": {
                    "name": "Pochun Li"
                },
                "author": "Pochun Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11843v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11843v1",
                "updated": "2024-11-18T18:59:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    15,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T18:59:15Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    15,
                    0,
                    323,
                    0
                ],
                "title": "Bi-Mamba: Towards Accurate 1-Bit State Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bi-Mamba: Towards Accurate 1-Bit State Space Models"
                },
                "summary": "The typical selective state-space model (SSM) of Mamba addresses several\nlimitations of Transformers, such as quadratic computational complexity with\nsequence length and significant inference-time memory requirements due to the\nkey-value cache. However, the growing size of Mamba models continues to pose\ntraining and deployment challenges and raises environmental concerns due to\nconsiderable energy consumption. In this work, we introduce Bi-Mamba, a\nscalable and powerful 1-bit Mamba architecture designed for more efficient\nlarge language models with multiple sizes across 780M, 1.3B, and 2.7B. Bi-Mamba\nmodels are trained from scratch on data volume as regular LLM pertaining using\nan autoregressive distillation loss. Extensive experimental results on language\nmodeling demonstrate that Bi-Mamba achieves performance comparable to its\nfull-precision counterparts (e.g., FP16 or BF16) and much better accuracy than\npost-training-binarization (PTB) Mamba baselines, while significantly reducing\nmemory footprint and energy consumption compared to the original Mamba model.\nOur study pioneers a new linear computational complexity LLM framework under\nlow-bit representation and facilitates the future design of specialized\nhardware tailored for efficient 1-bit Mamba-based LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The typical selective state-space model (SSM) of Mamba addresses several\nlimitations of Transformers, such as quadratic computational complexity with\nsequence length and significant inference-time memory requirements due to the\nkey-value cache. However, the growing size of Mamba models continues to pose\ntraining and deployment challenges and raises environmental concerns due to\nconsiderable energy consumption. In this work, we introduce Bi-Mamba, a\nscalable and powerful 1-bit Mamba architecture designed for more efficient\nlarge language models with multiple sizes across 780M, 1.3B, and 2.7B. Bi-Mamba\nmodels are trained from scratch on data volume as regular LLM pertaining using\nan autoregressive distillation loss. Extensive experimental results on language\nmodeling demonstrate that Bi-Mamba achieves performance comparable to its\nfull-precision counterparts (e.g., FP16 or BF16) and much better accuracy than\npost-training-binarization (PTB) Mamba baselines, while significantly reducing\nmemory footprint and energy consumption compared to the original Mamba model.\nOur study pioneers a new linear computational complexity LLM framework under\nlow-bit representation and facilitates the future design of specialized\nhardware tailored for efficient 1-bit Mamba-based LLMs."
                },
                "authors": [
                    {
                        "name": "Shengkun Tang"
                    },
                    {
                        "name": "Liqun Ma"
                    },
                    {
                        "name": "Haonan Li"
                    },
                    {
                        "name": "Mingjie Sun"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11843v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11843v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11739v1",
                "updated": "2024-11-18T17:08:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    8,
                    35,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T17:08:35Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    8,
                    35,
                    0,
                    323,
                    0
                ],
                "title": "QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou"
                },
                "summary": "In recent years, with the significant evolution of multi-modal large models,\nmany recommender researchers realized the potential of multi-modal information\nfor user interest modeling. In industry, a wide-used modeling architecture is a\ncascading paradigm: (1) first pre-training a multi-modal model to provide\nomnipotent representations for downstream services; (2) The downstream\nrecommendation model takes the multi-modal representation as additional input\nto fit real user-item behaviours. Although such paradigm achieves remarkable\nimprovements, however, there still exist two problems that limit model\nperformance: (1) Representation Unmatching: The pre-trained multi-modal model\nis always supervised by the classic NLP/CV tasks, while the recommendation\nmodels are supervised by real user-item interaction. As a result, the two\nfundamentally different tasks' goals were relatively separate, and there was a\nlack of consistent objective on their representations; (2) Representation\nUnlearning: The generated multi-modal representations are always stored in\ncache store and serve as extra fixed input of recommendation model, thus could\nnot be updated by recommendation model gradient, further unfriendly for\ndownstream training. Inspired by the two difficulties challenges in downstream\ntasks usage, we introduce a quantitative multi-modal framework to customize the\nspecialized and trainable multi-modal information for different downstream\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, with the significant evolution of multi-modal large models,\nmany recommender researchers realized the potential of multi-modal information\nfor user interest modeling. In industry, a wide-used modeling architecture is a\ncascading paradigm: (1) first pre-training a multi-modal model to provide\nomnipotent representations for downstream services; (2) The downstream\nrecommendation model takes the multi-modal representation as additional input\nto fit real user-item behaviours. Although such paradigm achieves remarkable\nimprovements, however, there still exist two problems that limit model\nperformance: (1) Representation Unmatching: The pre-trained multi-modal model\nis always supervised by the classic NLP/CV tasks, while the recommendation\nmodels are supervised by real user-item interaction. As a result, the two\nfundamentally different tasks' goals were relatively separate, and there was a\nlack of consistent objective on their representations; (2) Representation\nUnlearning: The generated multi-modal representations are always stored in\ncache store and serve as extra fixed input of recommendation model, thus could\nnot be updated by recommendation model gradient, further unfriendly for\ndownstream training. Inspired by the two difficulties challenges in downstream\ntasks usage, we introduce a quantitative multi-modal framework to customize the\nspecialized and trainable multi-modal information for different downstream\nmodels."
                },
                "authors": [
                    {
                        "name": "Xinchen Luo"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Tianyu Sun"
                    },
                    {
                        "name": "Jinkai Yu"
                    },
                    {
                        "name": "Rui Huang"
                    },
                    {
                        "name": "Wei Yuan"
                    },
                    {
                        "name": "Hezheng Lin"
                    },
                    {
                        "name": "Yichen Zheng"
                    },
                    {
                        "name": "Shiyao Wang"
                    },
                    {
                        "name": "Qigen Hu"
                    },
                    {
                        "name": "Changqing Qiu"
                    },
                    {
                        "name": "Jiaqi Zhang"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Zhiheng Yan"
                    },
                    {
                        "name": "Jingming Zhang"
                    },
                    {
                        "name": "Simin Zhang"
                    },
                    {
                        "name": "Mingxing Wen"
                    },
                    {
                        "name": "Zhaojie Liu"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11300v1",
                "updated": "2024-11-18T05:50:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    5,
                    50,
                    58,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T05:50:58Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    5,
                    50,
                    58,
                    0,
                    323,
                    0
                ],
                "title": "Accelerating spherical K-means clustering for large-scale sparse\n  document data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating spherical K-means clustering for large-scale sparse\n  document data"
                },
                "summary": "This paper presents an accelerated spherical K-means clustering algorithm for\nlarge-scale and high-dimensional sparse document data sets. We design an\nalgorithm working in an architecture-friendly manner (AFM), which is a\nprocedure of suppressing performance-degradation factors such as the numbers of\ninstructions, branch mispredictions, and cache misses in CPUs of a modern\ncomputer system. For the AFM operation, we leverage unique universal\ncharacteristics (UCs) of a data-object and a cluster's mean set, which are\nskewed distributions on data relationships such as Zipf's law and a\nfeature-value concentration phenomenon. The UCs indicate that the most part of\nthe number of multiplications for similarity calculations is executed regarding\nterms with high document frequencies (df) and the most part of a similarity\nbetween an object- and a mean-feature vector is obtained by the multiplications\nregarding a few high mean-feature values. Our proposed algorithm applies an\ninverted-index data structure to a mean set, extracts the specific region with\nhigh-df terms and high mean-feature values in the mean-inverted index by newly\nintroduced two structural parameters, and exploits the index divided into three\nparts for efficient pruning. The algorithm determines the two structural\nparameters by minimizing the approximate number of multiplications related to\nthat of instructions, reduces the branch mispredictions by sharing the index\nstructure including the two parameters with all the objects, and suppressing\nthe cache misses by keeping in the caches the frequently used data in the\nforegoing specific region, resulting in working in the AFM. We experimentally\ndemonstrate that our algorithm efficiently achieves superior speed performance\nin large-scale documents compared with algorithms using the state-of-the-art\ntechniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an accelerated spherical K-means clustering algorithm for\nlarge-scale and high-dimensional sparse document data sets. We design an\nalgorithm working in an architecture-friendly manner (AFM), which is a\nprocedure of suppressing performance-degradation factors such as the numbers of\ninstructions, branch mispredictions, and cache misses in CPUs of a modern\ncomputer system. For the AFM operation, we leverage unique universal\ncharacteristics (UCs) of a data-object and a cluster's mean set, which are\nskewed distributions on data relationships such as Zipf's law and a\nfeature-value concentration phenomenon. The UCs indicate that the most part of\nthe number of multiplications for similarity calculations is executed regarding\nterms with high document frequencies (df) and the most part of a similarity\nbetween an object- and a mean-feature vector is obtained by the multiplications\nregarding a few high mean-feature values. Our proposed algorithm applies an\ninverted-index data structure to a mean set, extracts the specific region with\nhigh-df terms and high mean-feature values in the mean-inverted index by newly\nintroduced two structural parameters, and exploits the index divided into three\nparts for efficient pruning. The algorithm determines the two structural\nparameters by minimizing the approximate number of multiplications related to\nthat of instructions, reduces the branch mispredictions by sharing the index\nstructure including the two parameters with all the objects, and suppressing\nthe cache misses by keeping in the caches the frequently used data in the\nforegoing specific region, resulting in working in the AFM. We experimentally\ndemonstrate that our algorithm efficiently achieves superior speed performance\nin large-scale documents compared with algorithms using the state-of-the-art\ntechniques."
                },
                "authors": [
                    {
                        "name": "Kazuo Aoyama"
                    },
                    {
                        "name": "Kazumi Saito"
                    }
                ],
                "author_detail": {
                    "name": "Kazumi Saito"
                },
                "author": "Kazumi Saito",
                "arxiv_comment": "28 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13588v1",
                "updated": "2024-11-18T02:49:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    2,
                    49,
                    23,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T02:49:23Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    2,
                    49,
                    23,
                    0,
                    323,
                    0
                ],
                "title": "Unveiling Redundancy in Diffusion Transformers (DiTs): A Systematic\n  Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Redundancy in Diffusion Transformers (DiTs): A Systematic\n  Study"
                },
                "summary": "The increased model capacity of Diffusion Transformers (DiTs) and the demand\nfor generating higher resolutions of images and videos have led to a\nsignificant rise in inference latency, impacting real-time performance\nadversely. While prior research has highlighted the presence of high similarity\nin activation values between adjacent diffusion steps (referred to as\nredundancy) and proposed various caching mechanisms to mitigate computational\noverhead, the exploration of redundancy in existing literature remains limited,\nwith findings often not generalizable across different DiT models. This study\naims to address this gap by conducting a comprehensive investigation into\nredundancy across a broad spectrum of mainstream DiT models. Our experimental\nanalysis reveals substantial variations in the distribution of redundancy\nacross diffusion steps among different DiT models. Interestingly, within a\nsingle model, the redundancy distribution remains stable regardless of\nvariations in input prompts, step counts, or scheduling strategies. Given the\nlack of a consistent pattern across diverse models, caching strategies designed\nfor a specific group of models may not easily transfer to others. To overcome\nthis challenge, we introduce a tool for analyzing the redundancy of individual\nmodels, enabling subsequent research to develop tailored caching strategies for\nspecific model architectures. The project is publicly available at\nhttps://github.com/xdit-project/DiTCacheAnalysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increased model capacity of Diffusion Transformers (DiTs) and the demand\nfor generating higher resolutions of images and videos have led to a\nsignificant rise in inference latency, impacting real-time performance\nadversely. While prior research has highlighted the presence of high similarity\nin activation values between adjacent diffusion steps (referred to as\nredundancy) and proposed various caching mechanisms to mitigate computational\noverhead, the exploration of redundancy in existing literature remains limited,\nwith findings often not generalizable across different DiT models. This study\naims to address this gap by conducting a comprehensive investigation into\nredundancy across a broad spectrum of mainstream DiT models. Our experimental\nanalysis reveals substantial variations in the distribution of redundancy\nacross diffusion steps among different DiT models. Interestingly, within a\nsingle model, the redundancy distribution remains stable regardless of\nvariations in input prompts, step counts, or scheduling strategies. Given the\nlack of a consistent pattern across diverse models, caching strategies designed\nfor a specific group of models may not easily transfer to others. To overcome\nthis challenge, we introduce a tool for analyzing the redundancy of individual\nmodels, enabling subsequent research to develop tailored caching strategies for\nspecific model architectures. The project is publicly available at\nhttps://github.com/xdit-project/DiTCacheAnalysis."
                },
                "authors": [
                    {
                        "name": "Xibo Sun"
                    },
                    {
                        "name": "Jiarui Fang"
                    },
                    {
                        "name": "Aoyu Li"
                    },
                    {
                        "name": "Jinzhe Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jinzhe Pan"
                },
                "author": "Jinzhe Pan",
                "arxiv_comment": "9 pages including reference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06392v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06392v2",
                "updated": "2024-11-18T02:10:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    2,
                    10,
                    28,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-10T08:31:18Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    8,
                    31,
                    18,
                    6,
                    315,
                    0
                ],
                "title": "LSMGraph: A High-Performance Dynamic Graph Storage System with\n  Multi-Level CSR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSMGraph: A High-Performance Dynamic Graph Storage System with\n  Multi-Level CSR"
                },
                "summary": "The growing volume of graph data may exhaust the main memory. It is crucial\nto design a disk-based graph storage system to ingest updates and analyze\ngraphs efficiently. However, existing dynamic graph storage systems suffer from\nread or write amplification and face the challenge of optimizing both read and\nwrite performance simultaneously. To address this challenge, we propose\nLSMGraph, a novel dynamic graph storage system that combines the write-friendly\nLSM-tree and the read-friendly CSR. It leverages the multi-level structure of\nLSM-trees to optimize write performance while utilizing the compact CSR\nstructures embedded in the LSM-trees to boost read performance. LSMGraph uses a\nnew memory structure, MemGraph, to efficiently cache graph updates and uses a\nmulti-level index to speed up reads within the multi-level structure.\nFurthermore, LSMGraph incorporates a vertex-grained version control mechanism\nto mitigate the impact of LSM-tree compaction on read performance and ensure\nthe correctness of concurrent read and write operations. Our evaluation shows\nthat LSMGraph significantly outperforms state-of-the-art (graph) storage\nsystems on both graph update and graph analytical workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing volume of graph data may exhaust the main memory. It is crucial\nto design a disk-based graph storage system to ingest updates and analyze\ngraphs efficiently. However, existing dynamic graph storage systems suffer from\nread or write amplification and face the challenge of optimizing both read and\nwrite performance simultaneously. To address this challenge, we propose\nLSMGraph, a novel dynamic graph storage system that combines the write-friendly\nLSM-tree and the read-friendly CSR. It leverages the multi-level structure of\nLSM-trees to optimize write performance while utilizing the compact CSR\nstructures embedded in the LSM-trees to boost read performance. LSMGraph uses a\nnew memory structure, MemGraph, to efficiently cache graph updates and uses a\nmulti-level index to speed up reads within the multi-level structure.\nFurthermore, LSMGraph incorporates a vertex-grained version control mechanism\nto mitigate the impact of LSM-tree compaction on read performance and ensure\nthe correctness of concurrent read and write operations. Our evaluation shows\nthat LSMGraph significantly outperforms state-of-the-art (graph) storage\nsystems on both graph update and graph analytical workloads."
                },
                "authors": [
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Shufeng Gong"
                    },
                    {
                        "name": "Qian Tao"
                    },
                    {
                        "name": "Sijie Shen"
                    },
                    {
                        "name": "Yanfeng Zhang"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Pengxi Liu"
                    },
                    {
                        "name": "Zhixin Zhang"
                    },
                    {
                        "name": "Hongfu Li"
                    },
                    {
                        "name": "Xiaojian Luo"
                    },
                    {
                        "name": "Ge Yu"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06392v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06392v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11091v1",
                "updated": "2024-11-17T14:47:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    17,
                    14,
                    47,
                    15,
                    6,
                    322,
                    0
                ],
                "published": "2024-11-17T14:47:15Z",
                "published_parsed": [
                    2024,
                    11,
                    17,
                    14,
                    47,
                    15,
                    6,
                    322,
                    0
                ],
                "title": "KV-Tandem -- a Modular Approach to Building High-Speed LSM Storage\n  Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Tandem -- a Modular Approach to Building High-Speed LSM Storage\n  Engines"
                },
                "summary": "We present~\\emph{KV-Tandem}, a modular architecture for building LSM-based\nstorage engines on top of simple, non-ordered persistent key-value stores\n(KVSs). KV-Tandem enables advanced functionalities such as range queries and\nsnapshot reads, while maintaining the native KVS performance for random reads\nand writes. Its modular design offers better performance trade-offs compared to\nprevious KV-separation solutions, which struggle to decompose the monolithic\nLSM structure. Central to KV-Tandem is~\\emph{LSM bypass} -- a novel algorithm\nthat offers a fast path to basic operations while ensuring the correctness of\nadvanced APIs.\n  We implement KV-Tandem in \\emph{XDP-Rocks}, a RocksDB-compatible storage\nengine that leverages the XDP KVS and incorporates practical design\noptimizations for real-world deployment. Through extensive microbenchmark and\nsystem-level comparisons, we demonstrate that XDP-Rocks achieves 3x to 4x\nperformance improvements over RocksDB across various workloads. XDP-Rocks is\nalready deployed in production, delivering significant operator cost savings\nconsistent with these performance gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present~\\emph{KV-Tandem}, a modular architecture for building LSM-based\nstorage engines on top of simple, non-ordered persistent key-value stores\n(KVSs). KV-Tandem enables advanced functionalities such as range queries and\nsnapshot reads, while maintaining the native KVS performance for random reads\nand writes. Its modular design offers better performance trade-offs compared to\nprevious KV-separation solutions, which struggle to decompose the monolithic\nLSM structure. Central to KV-Tandem is~\\emph{LSM bypass} -- a novel algorithm\nthat offers a fast path to basic operations while ensuring the correctness of\nadvanced APIs.\n  We implement KV-Tandem in \\emph{XDP-Rocks}, a RocksDB-compatible storage\nengine that leverages the XDP KVS and incorporates practical design\noptimizations for real-world deployment. Through extensive microbenchmark and\nsystem-level comparisons, we demonstrate that XDP-Rocks achieves 3x to 4x\nperformance improvements over RocksDB across various workloads. XDP-Rocks is\nalready deployed in production, delivering significant operator cost savings\nconsistent with these performance gains."
                },
                "authors": [
                    {
                        "name": "Edward Bortnikov"
                    },
                    {
                        "name": "Michael Azran"
                    },
                    {
                        "name": "Asa Bornstein"
                    },
                    {
                        "name": "Shmuel Dashevsky"
                    },
                    {
                        "name": "Dennis Huang"
                    },
                    {
                        "name": "Omer Kepten"
                    },
                    {
                        "name": "Michael Pan"
                    },
                    {
                        "name": "Gali Sheffi"
                    },
                    {
                        "name": "Moshe Twitto"
                    },
                    {
                        "name": "Tamar Weiss Orzech"
                    },
                    {
                        "name": "Idit Keidar"
                    },
                    {
                        "name": "Guy Gueta"
                    },
                    {
                        "name": "Roey Maor"
                    },
                    {
                        "name": "Niv Dayan"
                    }
                ],
                "author_detail": {
                    "name": "Niv Dayan"
                },
                "author": "Niv Dayan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07635v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07635v3",
                "updated": "2024-11-17T12:56:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    17,
                    12,
                    56,
                    16,
                    6,
                    322,
                    0
                ],
                "published": "2024-11-12T08:30:59Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    30,
                    59,
                    1,
                    317,
                    0
                ],
                "title": "Breaking the Low-Rank Dilemma of Linear Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Low-Rank Dilemma of Linear Attention"
                },
                "summary": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA."
                },
                "authors": [
                    {
                        "name": "Qihang Fan"
                    },
                    {
                        "name": "Huaibo Huang"
                    },
                    {
                        "name": "Ran He"
                    }
                ],
                "author_detail": {
                    "name": "Ran He"
                },
                "author": "Ran He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07635v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07635v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10883v1",
                "updated": "2024-11-16T20:40:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    40,
                    8,
                    5,
                    321,
                    0
                ],
                "published": "2024-11-16T20:40:08Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    40,
                    8,
                    5,
                    321,
                    0
                ],
                "title": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs"
                },
                "summary": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack."
                },
                "authors": [
                    {
                        "name": "Cheng Gu"
                    },
                    {
                        "name": "Yicheng Zhang"
                    },
                    {
                        "name": "Nael Abu-Ghazaleh"
                    }
                ],
                "author_detail": {
                    "name": "Nael Abu-Ghazaleh"
                },
                "author": "Nael Abu-Ghazaleh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.13112v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.13112v3",
                "updated": "2024-11-16T20:39:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    39,
                    46,
                    5,
                    321,
                    0
                ],
                "published": "2024-03-19T19:27:23Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    19,
                    27,
                    23,
                    1,
                    79,
                    0
                ],
                "title": "Efficient Encoder-Decoder Transformer Decoding for Decomposable Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Encoder-Decoder Transformer Decoding for Decomposable Tasks"
                },
                "summary": "Transformer-based NLP models are powerful but have high computational costs\nthat limit deployment. Finetuned encoder-decoder models are popular in\nspecialized domains and can outperform larger more generalized decoder-only\nmodels, such as GPT-4. We introduce a new configuration for encoder-decoder\nmodels that improves efficiency on structured output and decomposable tasks\nwhere multiple outputs are required for a single shared input. Our method,\nprompt-in-decoder (PiD), encodes the input once and decodes the output in\nparallel, boosting both training and inference efficiency by avoiding duplicate\ninput encoding and increasing the operational intensity (ratio of numbers of\narithmetic operation to memory access) of decoding process by sharing the input\nkey-value cache. We achieve computation reduction that roughly scales with the\nnumber of subtasks, gaining up to 4.6x speed-up over state-of-the-art models\nfor dialogue state tracking, summarization, and question-answering tasks, with\ncomparable or better performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based NLP models are powerful but have high computational costs\nthat limit deployment. Finetuned encoder-decoder models are popular in\nspecialized domains and can outperform larger more generalized decoder-only\nmodels, such as GPT-4. We introduce a new configuration for encoder-decoder\nmodels that improves efficiency on structured output and decomposable tasks\nwhere multiple outputs are required for a single shared input. Our method,\nprompt-in-decoder (PiD), encodes the input once and decodes the output in\nparallel, boosting both training and inference efficiency by avoiding duplicate\ninput encoding and increasing the operational intensity (ratio of numbers of\narithmetic operation to memory access) of decoding process by sharing the input\nkey-value cache. We achieve computation reduction that roughly scales with the\nnumber of subtasks, gaining up to 4.6x speed-up over state-of-the-art models\nfor dialogue state tracking, summarization, and question-answering tasks, with\ncomparable or better performance."
                },
                "authors": [
                    {
                        "name": "Bo-Ru Lu"
                    },
                    {
                        "name": "Nikita Haduong"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Noah A. Smith"
                    },
                    {
                        "name": "Mari Ostendorf"
                    }
                ],
                "author_detail": {
                    "name": "Mari Ostendorf"
                },
                "author": "Mari Ostendorf",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.13112v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.13112v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10803v1",
                "updated": "2024-11-16T13:45:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    13,
                    45,
                    33,
                    5,
                    321,
                    0
                ],
                "published": "2024-11-16T13:45:33Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    13,
                    45,
                    33,
                    5,
                    321,
                    0
                ],
                "title": "Multi-Stage Vision Token Dropping: Towards Efficient Multimodal Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Stage Vision Token Dropping: Towards Efficient Multimodal Large\n  Language Model"
                },
                "summary": "The vision tokens in multimodal large language models usually exhibit\nsignificant spatial and temporal redundancy and take up most of the input\ntokens, which harms their inference efficiency. To solve this problem, some\nrecent works were introduced to drop the unimportant tokens during inference\nwhere the importance of each token is decided only by the information in either\nthe vision encoding stage or the prefilling stage. In this paper, we propose\nMulti-stage Token Dropping (MustDrop) to measure the importance of each token\nfrom the whole lifecycle, including the vision encoding stage, prefilling\nstage, and decoding stage. Concretely, in the visual encoding stage, MustDrop\nmerges spatially adjacent tokens with high similarity, and establishes a key\ntoken set to retain the most vision-critical tokens, preventing them from being\ndiscarded in later stages. In the prefilling stage, MustDrop further compresses\nvision tokens by the guidance of text semantics, with a dual-attention\nfiltering strategy. In the decoding stage, an output-aware cache policy is\nproposed to further reduce the size of the KV cache. By leveraging tailored\nstrategies in the multi-stage process, MustDrop can more precisely recognize\nthe important and redundant tokens, thus achieving an optimal balance between\nperformance and efficiency. For instance, MustDrop reduces about 88.5\\% FLOPs\non LLaVA with a compression ratio of 92.2\\% while maintaining comparable\naccuracy. Our codes are available at\n\\url{https://github.com/liuting20/MustDrop}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The vision tokens in multimodal large language models usually exhibit\nsignificant spatial and temporal redundancy and take up most of the input\ntokens, which harms their inference efficiency. To solve this problem, some\nrecent works were introduced to drop the unimportant tokens during inference\nwhere the importance of each token is decided only by the information in either\nthe vision encoding stage or the prefilling stage. In this paper, we propose\nMulti-stage Token Dropping (MustDrop) to measure the importance of each token\nfrom the whole lifecycle, including the vision encoding stage, prefilling\nstage, and decoding stage. Concretely, in the visual encoding stage, MustDrop\nmerges spatially adjacent tokens with high similarity, and establishes a key\ntoken set to retain the most vision-critical tokens, preventing them from being\ndiscarded in later stages. In the prefilling stage, MustDrop further compresses\nvision tokens by the guidance of text semantics, with a dual-attention\nfiltering strategy. In the decoding stage, an output-aware cache policy is\nproposed to further reduce the size of the KV cache. By leveraging tailored\nstrategies in the multi-stage process, MustDrop can more precisely recognize\nthe important and redundant tokens, thus achieving an optimal balance between\nperformance and efficiency. For instance, MustDrop reduces about 88.5\\% FLOPs\non LLaVA with a compression ratio of 92.2\\% while maintaining comparable\naccuracy. Our codes are available at\n\\url{https://github.com/liuting20/MustDrop}."
                },
                "authors": [
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Liangtao Shi"
                    },
                    {
                        "name": "Richang Hong"
                    },
                    {
                        "name": "Yue Hu"
                    },
                    {
                        "name": "Quanjun Yin"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "8 pages, 4figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01733v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01733v2",
                "updated": "2024-11-16T07:43:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    7,
                    43,
                    28,
                    5,
                    321,
                    0
                ],
                "published": "2024-06-03T18:49:57Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    18,
                    49,
                    57,
                    0,
                    155,
                    0
                ],
                "title": "Learning-to-Cache: Accelerating Diffusion Transformer via Layer Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-to-Cache: Accelerating Diffusion Transformer via Layer Caching"
                },
                "summary": "Diffusion Transformers have recently demonstrated unprecedented generative\ncapabilities for various tasks. The encouraging results, however, come with the\ncost of slow inference, since each denoising step requires inference on a\ntransformer model with a large scale of parameters. In this study, we make an\ninteresting and somehow surprising observation: the computation of a large\nproportion of layers in the diffusion transformer, through introducing a\ncaching mechanism, can be readily removed even without updating the model\nparameters. In the case of U-ViT-H/2, for example, we may remove up to 93.68%\nof the computation in the cache steps (46.84% for all steps), with less than\n0.01 drop in FID. To achieve this, we introduce a novel scheme, named\nLearning-to-Cache (L2C), that learns to conduct caching in a dynamic manner for\ndiffusion transformers. Specifically, by leveraging the identical structure of\nlayers in transformers and the sequential nature of diffusion, we explore\nredundant computations between timesteps by treating each layer as the\nfundamental unit for caching. To address the challenge of the exponential\nsearch space in deep models for identifying layers to cache and remove, we\npropose a novel differentiable optimization objective. An input-invariant yet\ntimestep-variant router is then optimized, which can finally produce a static\ncomputation graph. Experimental results show that L2C largely outperforms\nsamplers such as DDIM and DPM-Solver, alongside prior cache-based methods at\nthe same inference speed. Code is available at\nhttps://github.com/horseee/learning-to-cache",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers have recently demonstrated unprecedented generative\ncapabilities for various tasks. The encouraging results, however, come with the\ncost of slow inference, since each denoising step requires inference on a\ntransformer model with a large scale of parameters. In this study, we make an\ninteresting and somehow surprising observation: the computation of a large\nproportion of layers in the diffusion transformer, through introducing a\ncaching mechanism, can be readily removed even without updating the model\nparameters. In the case of U-ViT-H/2, for example, we may remove up to 93.68%\nof the computation in the cache steps (46.84% for all steps), with less than\n0.01 drop in FID. To achieve this, we introduce a novel scheme, named\nLearning-to-Cache (L2C), that learns to conduct caching in a dynamic manner for\ndiffusion transformers. Specifically, by leveraging the identical structure of\nlayers in transformers and the sequential nature of diffusion, we explore\nredundant computations between timesteps by treating each layer as the\nfundamental unit for caching. To address the challenge of the exponential\nsearch space in deep models for identifying layers to cache and remove, we\npropose a novel differentiable optimization objective. An input-invariant yet\ntimestep-variant router is then optimized, which can finally produce a static\ncomputation graph. Experimental results show that L2C largely outperforms\nsamplers such as DDIM and DPM-Solver, alongside prior cache-based methods at\nthe same inference speed. Code is available at\nhttps://github.com/horseee/learning-to-cache"
                },
                "authors": [
                    {
                        "name": "Xinyin Ma"
                    },
                    {
                        "name": "Gongfan Fang"
                    },
                    {
                        "name": "Michael Bi Mi"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "arxiv_comment": "Accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01733v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01733v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v1",
                "updated": "2024-11-16T01:39:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16219v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16219v4",
                "updated": "2024-11-15T22:37:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    22,
                    37,
                    48,
                    4,
                    320,
                    0
                ],
                "published": "2024-04-24T21:35:12Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    21,
                    35,
                    12,
                    2,
                    115,
                    0
                ],
                "title": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)"
                },
                "summary": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU."
                },
                "authors": [
                    {
                        "name": "Ziyue Qiu"
                    },
                    {
                        "name": "Juncheng Yang"
                    },
                    {
                        "name": "Mor Harchol-Balter"
                    }
                ],
                "author_detail": {
                    "name": "Mor Harchol-Balter"
                },
                "author": "Mor Harchol-Balter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16219v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16219v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13853v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13853v2",
                "updated": "2024-11-15T22:30:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    22,
                    30,
                    38,
                    4,
                    320,
                    0
                ],
                "published": "2024-07-18T18:47:52Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    18,
                    47,
                    52,
                    3,
                    200,
                    0
                ],
                "title": "Forecasting GPU Performance for Deep Learning Training and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting GPU Performance for Deep Learning Training and Inference"
                },
                "summary": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 198% and 19.7% to 3.8% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior works, where both GPT3 and H100 were not\nused to train the framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 198% and 19.7% to 3.8% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior works, where both GPT3 and H100 were not\nused to train the framework."
                },
                "authors": [
                    {
                        "name": "Seonho Lee"
                    },
                    {
                        "name": "Amar Phanishayee"
                    },
                    {
                        "name": "Divya Mahajan"
                    }
                ],
                "author_detail": {
                    "name": "Divya Mahajan"
                },
                "author": "Divya Mahajan",
                "arxiv_comment": "Accepted at the 30th ACM International Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13853v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13853v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10510v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10510v1",
                "updated": "2024-11-15T16:24:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    24,
                    2,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T16:24:02Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    24,
                    2,
                    4,
                    320,
                    0
                ],
                "title": "SmoothCache: A Universal Inference Acceleration Technique for Diffusion\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmoothCache: A Universal Inference Acceleration Technique for Diffusion\n  Transformers"
                },
                "summary": "Diffusion Transformers (DiT) have emerged as powerful generative models for\nvarious tasks, including image, video, and speech synthesis. However, their\ninference process remains computationally expensive due to the repeated\nevaluation of resource-intensive attention and feed-forward modules. To address\nthis, we introduce SmoothCache, a model-agnostic inference acceleration\ntechnique for DiT architectures. SmoothCache leverages the observed high\nsimilarity between layer outputs across adjacent diffusion timesteps. By\nanalyzing layer-wise representation errors from a small calibration set,\nSmoothCache adaptively caches and reuses key features during inference. Our\nexperiments demonstrate that SmoothCache achieves 8% to 71% speed up while\nmaintaining or even improving generation quality across diverse modalities. We\nshowcase its effectiveness on DiT-XL for image generation, Open-Sora for\ntext-to-video, and Stable Audio Open for text-to-audio, highlighting its\npotential to enable real-time applications and broaden the accessibility of\npowerful DiT models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have emerged as powerful generative models for\nvarious tasks, including image, video, and speech synthesis. However, their\ninference process remains computationally expensive due to the repeated\nevaluation of resource-intensive attention and feed-forward modules. To address\nthis, we introduce SmoothCache, a model-agnostic inference acceleration\ntechnique for DiT architectures. SmoothCache leverages the observed high\nsimilarity between layer outputs across adjacent diffusion timesteps. By\nanalyzing layer-wise representation errors from a small calibration set,\nSmoothCache adaptively caches and reuses key features during inference. Our\nexperiments demonstrate that SmoothCache achieves 8% to 71% speed up while\nmaintaining or even improving generation quality across diverse modalities. We\nshowcase its effectiveness on DiT-XL for image generation, Open-Sora for\ntext-to-video, and Stable Audio Open for text-to-audio, highlighting its\npotential to enable real-time applications and broaden the accessibility of\npowerful DiT models."
                },
                "authors": [
                    {
                        "name": "Joseph Liu"
                    },
                    {
                        "name": "Joshua Geddes"
                    },
                    {
                        "name": "Ziyu Guo"
                    },
                    {
                        "name": "Haomiao Jiang"
                    },
                    {
                        "name": "Mahesh Kumar Nandwana"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Kumar Nandwana"
                },
                "author": "Mahesh Kumar Nandwana",
                "arxiv_comment": "Code can be found at https://github.com/Roblox/SmoothCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10510v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10510v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03174v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03174v2",
                "updated": "2024-11-15T07:25:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    7,
                    25,
                    54,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-05T15:22:11Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    22,
                    11,
                    1,
                    310,
                    0
                ],
                "title": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression"
                },
                "summary": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Alex Zhong"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03174v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03174v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09859v1",
                "updated": "2024-11-15T00:37:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    0,
                    37,
                    31,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T00:37:31Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    0,
                    37,
                    31,
                    4,
                    320,
                    0
                ],
                "title": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures"
                },
                "summary": "The factorization of skew-symmetric matrices is a critically understudied\narea of dense linear algebra (DLA), particularly in comparison to that of\nsymmetric matrices. While some algorithms can be adapted from the symmetric\ncase, the cost of algorithms can be reduced by exploiting skew-symmetry. A\nmotivating example is the factorization $X=LTL^T$ of a skew-symmetric matrix\n$X$, which is used in practical applications as a means of determining the\ndeterminant of $X$ as the square of the (cheaply-computed) Pfaffian of the\nskew-symmetric tridiagonal matrix $T$, for example in fields such as quantum\nelectronic structure and machine learning. Such applications also often require\npivoting in order to improve numerical stability. In this work we explore a\ncombination of known literature algorithms and new algorithms recently derived\nusing formal methods. High-performance parallel CPU implementations are\ncreated, leveraging the concept of fusion at multiple levels in order to reduce\nmemory traffic overhead, as well as the BLIS framework which provides\nhigh-performance GEMM kernels, hierarchical parallelism, and cache blocking. We\nfind that operation fusion and improved use of available bandwidth via\nparallelization of bandwidth-bound (level-2 BLAS) operations are essential for\nobtaining high performance, while a concise C++ implementation provides a clear\nand close connection to the formal derivation process without sacrificing\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The factorization of skew-symmetric matrices is a critically understudied\narea of dense linear algebra (DLA), particularly in comparison to that of\nsymmetric matrices. While some algorithms can be adapted from the symmetric\ncase, the cost of algorithms can be reduced by exploiting skew-symmetry. A\nmotivating example is the factorization $X=LTL^T$ of a skew-symmetric matrix\n$X$, which is used in practical applications as a means of determining the\ndeterminant of $X$ as the square of the (cheaply-computed) Pfaffian of the\nskew-symmetric tridiagonal matrix $T$, for example in fields such as quantum\nelectronic structure and machine learning. Such applications also often require\npivoting in order to improve numerical stability. In this work we explore a\ncombination of known literature algorithms and new algorithms recently derived\nusing formal methods. High-performance parallel CPU implementations are\ncreated, leveraging the concept of fusion at multiple levels in order to reduce\nmemory traffic overhead, as well as the BLIS framework which provides\nhigh-performance GEMM kernels, hierarchical parallelism, and cache blocking. We\nfind that operation fusion and improved use of available bandwidth via\nparallelization of bandwidth-bound (level-2 BLAS) operations are essential for\nobtaining high performance, while a concise C++ implementation provides a clear\nand close connection to the formal derivation process without sacrificing\nperformance."
                },
                "authors": [
                    {
                        "name": "Ishna Satyarth"
                    },
                    {
                        "name": "Chao Yin"
                    },
                    {
                        "name": "RuQing G. Xu"
                    },
                    {
                        "name": "Devin A. Matthews"
                    }
                ],
                "author_detail": {
                    "name": "Devin A. Matthews"
                },
                "author": "Devin A. Matthews",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09812v1",
                "updated": "2024-11-14T21:01:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    21,
                    1,
                    29,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T21:01:29Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    21,
                    1,
                    29,
                    3,
                    319,
                    0
                ],
                "title": "Edge Caching Optimization with PPO and Transfer Learning for Dynamic\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Caching Optimization with PPO and Transfer Learning for Dynamic\n  Environments"
                },
                "summary": "This paper addresses the challenge of edge caching in dynamic environments,\nwhere rising traffic loads strain backhaul links and core networks. We propose\na Proximal Policy Optimization (PPO)-based caching strategy that fully\nincorporates key file attributes such as size, lifetime, importance, and\npopularity, while also considering random file request arrivals, reflecting\nmore realistic edge caching scenarios. In dynamic environments, changes such as\nshifts in content popularity and variations in request rates frequently occur,\nmaking previously learned policies less effective as they were optimized for\nearlier conditions. Without adaptation, caching efficiency and response times\ncan degrade. While learning a new policy from scratch in a new environment is\nan option, it is highly inefficient and computationally expensive. Thus,\nadapting an existing policy to these changes is critical. To address this, we\ndevelop a mechanism that detects changes in content popularity and request\nrates, ensuring timely adjustments to the caching strategy. We also propose a\ntransfer learning-based PPO algorithm that accelerates convergence in new\nenvironments by leveraging prior knowledge. Simulation results demonstrate the\nsignificant effectiveness of our approach, outperforming a recent Deep\nReinforcement Learning (DRL)-based method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the challenge of edge caching in dynamic environments,\nwhere rising traffic loads strain backhaul links and core networks. We propose\na Proximal Policy Optimization (PPO)-based caching strategy that fully\nincorporates key file attributes such as size, lifetime, importance, and\npopularity, while also considering random file request arrivals, reflecting\nmore realistic edge caching scenarios. In dynamic environments, changes such as\nshifts in content popularity and variations in request rates frequently occur,\nmaking previously learned policies less effective as they were optimized for\nearlier conditions. Without adaptation, caching efficiency and response times\ncan degrade. While learning a new policy from scratch in a new environment is\nan option, it is highly inefficient and computationally expensive. Thus,\nadapting an existing policy to these changes is critical. To address this, we\ndevelop a mechanism that detects changes in content popularity and request\nrates, ensuring timely adjustments to the caching strategy. We also propose a\ntransfer learning-based PPO algorithm that accelerates convergence in new\nenvironments by leveraging prior knowledge. Simulation results demonstrate the\nsignificant effectiveness of our approach, outperforming a recent Deep\nReinforcement Learning (DRL)-based method."
                },
                "authors": [
                    {
                        "name": "Farnaz Niknia"
                    },
                    {
                        "name": "Ping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Wang"
                },
                "author": "Ping Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v2",
                "updated": "2024-11-14T17:46:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    17,
                    46,
                    4,
                    3,
                    319,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers"
                },
                "summary": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the $KV$ cache by nearly 50\\%. Comprehensive\nempirical evidence demonstrates that ResFormer mitigates attention\nconcentration problem in deeper layers and enhances representation across most\nlayers, outperforming the vanilla Transformer, DenseFormer, and NeuTRENO in\ntraining error as well as downstream tasks. Further visualization results\nsuggest that Resformer alleviates attention sinks through avoiding value-state\ndrains. SVFormer trains significantly faster than the vanilla Transformer and\nperforms better than other methods like GQA and CLA, with performance\ninfluenced by sequence length and cumulative learning rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the $KV$ cache by nearly 50\\%. Comprehensive\nempirical evidence demonstrates that ResFormer mitigates attention\nconcentration problem in deeper layers and enhances representation across most\nlayers, outperforming the vanilla Transformer, DenseFormer, and NeuTRENO in\ntraining error as well as downstream tasks. Further visualization results\nsuggest that Resformer alleviates attention sinks through avoiding value-state\ndrains. SVFormer trains significantly faster than the vanilla Transformer and\nperforms better than other methods like GQA and CLA, with performance\ninfluenced by sequence length and cumulative learning rate."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09546v1",
                "updated": "2024-11-14T16:01:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    1,
                    5,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T16:01:05Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    1,
                    5,
                    3,
                    319,
                    0
                ],
                "title": "Architectural Exploration of Application-Specific Resonant SRAM\n  Compute-in-Memory (rCiM)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Architectural Exploration of Application-Specific Resonant SRAM\n  Compute-in-Memory (rCiM)"
                },
                "summary": "While general-purpose computing follows Von Neumann's architecture, the data\nmovement between memory and processor elements dictates the processor's\nperformance. The evolving compute-in-memory (CiM) paradigm tackles this issue\nby facilitating simultaneous processing and storage within static random-access\nmemory (SRAM) elements. Numerous design decisions taken at different levels of\nhierarchy affect the figure of merits (FoMs) of SRAM, such as power,\nperformance, area, and yield. The absence of a rapid assessment mechanism for\nthe impact of changes at different hierarchy levels on global FoMs poses a\nchallenge to accurately evaluating innovative SRAM designs. This paper presents\nan automation tool designed to optimize the energy and latency of SRAM designs\nincorporating diverse implementation strategies for executing logic operations\nwithin the SRAM. The tool structure allows easy comparison across different\narray topologies and various design strategies to result in energy-efficient\nimplementations. Our study involves a comprehensive comparison of over 6900+\ndistinct design implementation strategies for EPFL combinational benchmark\ncircuits on the energy-recycling resonant compute-in-memory (rCiM) architecture\ndesigned using TSMC 28 nm technology. When provided with a combinational\ncircuit, the tool aims to generate an energy-efficient implementation strategy\ntailored to the specified input memory and latency constraints. The tool\nreduces 80.9% of energy consumption on average across all benchmarks while\nusing the six-topology implementation compared to baseline implementation of\nsingle-macro topology by considering the parallel processing capability of rCiM\ncache size ranging from 4KB to 192KB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While general-purpose computing follows Von Neumann's architecture, the data\nmovement between memory and processor elements dictates the processor's\nperformance. The evolving compute-in-memory (CiM) paradigm tackles this issue\nby facilitating simultaneous processing and storage within static random-access\nmemory (SRAM) elements. Numerous design decisions taken at different levels of\nhierarchy affect the figure of merits (FoMs) of SRAM, such as power,\nperformance, area, and yield. The absence of a rapid assessment mechanism for\nthe impact of changes at different hierarchy levels on global FoMs poses a\nchallenge to accurately evaluating innovative SRAM designs. This paper presents\nan automation tool designed to optimize the energy and latency of SRAM designs\nincorporating diverse implementation strategies for executing logic operations\nwithin the SRAM. The tool structure allows easy comparison across different\narray topologies and various design strategies to result in energy-efficient\nimplementations. Our study involves a comprehensive comparison of over 6900+\ndistinct design implementation strategies for EPFL combinational benchmark\ncircuits on the energy-recycling resonant compute-in-memory (rCiM) architecture\ndesigned using TSMC 28 nm technology. When provided with a combinational\ncircuit, the tool aims to generate an energy-efficient implementation strategy\ntailored to the specified input memory and latency constraints. The tool\nreduces 80.9% of energy consumption on average across all benchmarks while\nusing the six-topology implementation compared to baseline implementation of\nsingle-macro topology by considering the parallel processing capability of rCiM\ncache size ranging from 4KB to 192KB."
                },
                "authors": [
                    {
                        "name": "Dhandeep Challagundla"
                    },
                    {
                        "name": "Ignatius Bezzam"
                    },
                    {
                        "name": "Riadul Islam"
                    }
                ],
                "author_detail": {
                    "name": "Riadul Islam"
                },
                "author": "Riadul Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09473v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09473v1",
                "updated": "2024-11-14T14:28:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    14,
                    28,
                    31,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T14:28:31Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    14,
                    28,
                    31,
                    3,
                    319,
                    0
                ],
                "title": "Enhancing Scalability and Performance in Influence Maximization with\n  Optimized Parallel Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Scalability and Performance in Influence Maximization with\n  Optimized Parallel Processing"
                },
                "summary": "Influence Maximization (IM) is vital in viral marketing and biological\nnetwork analysis for identifying key influencers. Given its NP-hard nature,\napproximate solutions are employed. This paper addresses scalability challenges\nin scale-out shared memory system by focusing on the state-of-the-art Influence\nMaximization via Martingales (IMM) benchmark. To enhance the work efficiency of\nthe current IMM implementation, we propose EFFICIENTIMM with key strategies,\nincluding new parallelization scheme, NUMA-aware memory usage, dynamic load\nbalancing and fine-grained adaptive data structures. Benchmarking on a 128-core\nCPU system with 8 NUMA nodes, EFFICIENTIMM demonstrated significant performance\nimprovements, achieving an average 5.9x speedup over Ripples across 8 diverse\nSNAP datasets, when compared to the best execution times of the original\nRipples framework. Additionally, on the Youtube graph, EFFICIENTIMM\ndemonstrates a better memory access pattern with 357.4x reduction in L1+L2\ncache misses as compared to Ripples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Influence Maximization (IM) is vital in viral marketing and biological\nnetwork analysis for identifying key influencers. Given its NP-hard nature,\napproximate solutions are employed. This paper addresses scalability challenges\nin scale-out shared memory system by focusing on the state-of-the-art Influence\nMaximization via Martingales (IMM) benchmark. To enhance the work efficiency of\nthe current IMM implementation, we propose EFFICIENTIMM with key strategies,\nincluding new parallelization scheme, NUMA-aware memory usage, dynamic load\nbalancing and fine-grained adaptive data structures. Benchmarking on a 128-core\nCPU system with 8 NUMA nodes, EFFICIENTIMM demonstrated significant performance\nimprovements, achieving an average 5.9x speedup over Ripples across 8 diverse\nSNAP datasets, when compared to the best execution times of the original\nRipples framework. Additionally, on the Youtube graph, EFFICIENTIMM\ndemonstrates a better memory access pattern with 357.4x reduction in L1+L2\ncache misses as compared to Ripples."
                },
                "authors": [
                    {
                        "name": "Hanjiang Wu"
                    },
                    {
                        "name": "Huan Xu"
                    },
                    {
                        "name": "Joongun Park"
                    },
                    {
                        "name": "Jesmin Jahan Tithi"
                    },
                    {
                        "name": "Fabio Checconi"
                    },
                    {
                        "name": "Jordi Wolfson-Pou"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    },
                    {
                        "name": "Tushar Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Krishna"
                },
                "author": "Tushar Krishna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09473v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09425v1",
                "updated": "2024-11-14T13:22:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T13:22:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity"
                },
                "summary": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully."
                },
                "authors": [
                    {
                        "name": "Xiao Lv"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Shijie Guan"
                    },
                    {
                        "name": "Xiaoyou Zhou"
                    },
                    {
                        "name": "Zhiguang Qi"
                    },
                    {
                        "name": "Yaqiang Zang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09317v1",
                "updated": "2024-11-14T09:50:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    9,
                    50,
                    41,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T09:50:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    9,
                    50,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "Pie: Pooling CPU Memory for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pie: Pooling CPU Memory for LLM Inference"
                },
                "summary": "The rapid growth of LLMs has revolutionized natural language processing and\nAI analysis, but their increasing size and memory demands present significant\nchallenges. A common solution is to spill over to CPU memory; however,\ntraditional GPU-CPU memory swapping often results in higher latency and lower\nthroughput.\n  This paper introduces Pie, an LLM inference framework that addresses these\nchallenges with performance-transparent swapping and adaptive expansion. By\nleveraging predictable memory access patterns and the high bandwidth of modern\nhardware like the NVIDIA GH200 Grace Hopper Superchip, Pie enables concurrent\ndata swapping without affecting foreground computation, expanding effective\nmemory without added latency. Adaptive expansion dynamically adjusts CPU memory\nallocation based on real-time information, optimizing memory usage and\nperformance under varying conditions.\n  Pie maintains low computation latency, high throughput, and high elasticity.\nOur experimental evaluation demonstrates that Pie achieves optimal swapping\npolicy during cache warmup and effectively balances increased memory capacity\nwith negligible impact on computation. With its extended capacity, Pie\noutperforms vLLM by up to 1.9X in throughput and 2X in latency. Additionally,\nPie can reduce GPU memory usage by up to 1.67X while maintaining the same\nperformance. Compared to FlexGen, an offline profiling-based swapping solution,\nPie achieves magnitudes lower latency and 9.4X higher throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of LLMs has revolutionized natural language processing and\nAI analysis, but their increasing size and memory demands present significant\nchallenges. A common solution is to spill over to CPU memory; however,\ntraditional GPU-CPU memory swapping often results in higher latency and lower\nthroughput.\n  This paper introduces Pie, an LLM inference framework that addresses these\nchallenges with performance-transparent swapping and adaptive expansion. By\nleveraging predictable memory access patterns and the high bandwidth of modern\nhardware like the NVIDIA GH200 Grace Hopper Superchip, Pie enables concurrent\ndata swapping without affecting foreground computation, expanding effective\nmemory without added latency. Adaptive expansion dynamically adjusts CPU memory\nallocation based on real-time information, optimizing memory usage and\nperformance under varying conditions.\n  Pie maintains low computation latency, high throughput, and high elasticity.\nOur experimental evaluation demonstrates that Pie achieves optimal swapping\npolicy during cache warmup and effectively balances increased memory capacity\nwith negligible impact on computation. With its extended capacity, Pie\noutperforms vLLM by up to 1.9X in throughput and 2X in latency. Additionally,\nPie can reduce GPU memory usage by up to 1.67X while maintaining the same\nperformance. Compared to FlexGen, an offline profiling-based swapping solution,\nPie achieves magnitudes lower latency and 9.4X higher throughput."
                },
                "authors": [
                    {
                        "name": "Yi Xu"
                    },
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Xiangxi Mo"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09275v1",
                "updated": "2024-11-14T08:25:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    25,
                    31,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T08:25:31Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    25,
                    31,
                    3,
                    319,
                    0
                ],
                "title": "Pkd-tree: Parallel $k$d-tree with Batch Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pkd-tree: Parallel $k$d-tree with Batch Updates"
                },
                "summary": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code."
                },
                "authors": [
                    {
                        "name": "Ziyang Men"
                    },
                    {
                        "name": "Zheqi Shen"
                    },
                    {
                        "name": "Yan Gu"
                    },
                    {
                        "name": "Yihan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yihan Sun"
                },
                "author": "Yihan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19258v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19258v3",
                "updated": "2024-11-14T01:56:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    1,
                    56,
                    11,
                    3,
                    319,
                    0
                ],
                "published": "2024-10-25T02:22:00Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    2,
                    22,
                    0,
                    4,
                    299,
                    0
                ],
                "title": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning"
                },
                "summary": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark.Codes are\navailable at https://github.com/FYYFU/HeadKV",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark.Codes are\navailable at https://github.com/FYYFU/HeadKV"
                },
                "authors": [
                    {
                        "name": "Yu Fu"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "arxiv_comment": "18pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19258v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19258v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08672v1",
                "updated": "2024-11-13T15:07:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    15,
                    7,
                    15,
                    2,
                    318,
                    0
                ],
                "published": "2024-11-13T15:07:15Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    15,
                    7,
                    15,
                    2,
                    318,
                    0
                ],
                "title": "Joint Model Caching and Resource Allocation in Generative AI-Enabled\n  Wireless Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Model Caching and Resource Allocation in Generative AI-Enabled\n  Wireless Edge Networks"
                },
                "summary": "With the rapid advancement of artificial intelligence (AI), generative AI\n(GenAI) has emerged as a transformative tool, enabling customized and\npersonalized AI-generated content (AIGC) services. However, GenAI models with\nbillions of parameters require substantial memory capacity and computational\npower for deployment and execution, presenting significant challenges to\nresource-limited edge networks. In this paper, we address the joint model\ncaching and resource allocation problem in GenAI-enabled wireless edge\nnetworks. Our objective is to balance the trade-off between delivering\nhigh-quality AIGC and minimizing the delay in AIGC service provisioning. To\ntackle this problem, we employ a deep deterministic policy gradient\n(DDPG)-based reinforcement learning approach, capable of efficiently\ndetermining optimal model caching and resource allocation decisions for AIGC\nservices in response to user mobility and time-varying channel conditions.\nNumerical results demonstrate that DDPG achieves a higher model hit ratio and\nprovides superior-quality, lower-latency AIGC services compared to other\nbenchmark solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of artificial intelligence (AI), generative AI\n(GenAI) has emerged as a transformative tool, enabling customized and\npersonalized AI-generated content (AIGC) services. However, GenAI models with\nbillions of parameters require substantial memory capacity and computational\npower for deployment and execution, presenting significant challenges to\nresource-limited edge networks. In this paper, we address the joint model\ncaching and resource allocation problem in GenAI-enabled wireless edge\nnetworks. Our objective is to balance the trade-off between delivering\nhigh-quality AIGC and minimizing the delay in AIGC service provisioning. To\ntackle this problem, we employ a deep deterministic policy gradient\n(DDPG)-based reinforcement learning approach, capable of efficiently\ndetermining optimal model caching and resource allocation decisions for AIGC\nservices in response to user mobility and time-varying channel conditions.\nNumerical results demonstrate that DDPG achieves a higher model hit ratio and\nprovides superior-quality, lower-latency AIGC services compared to other\nbenchmark solutions."
                },
                "authors": [
                    {
                        "name": "Zhang Liu"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Lianfen Huang"
                    },
                    {
                        "name": "Zhibin Gao"
                    },
                    {
                        "name": "Dusit Niyato"
                    }
                ],
                "author_detail": {
                    "name": "Dusit Niyato"
                },
                "author": "Dusit Niyato",
                "arxiv_comment": "conference paper with 6 pages and 5 figures. arXiv admin note: text\n  overlap with arXiv:2411.01458",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08312v1",
                "updated": "2024-11-13T03:28:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    3,
                    28,
                    44,
                    2,
                    318,
                    0
                ],
                "published": "2024-11-13T03:28:44Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    3,
                    28,
                    44,
                    2,
                    318,
                    0
                ],
                "title": "A Novel Extensible Simulation Framework for CXL-Enabled Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Extensible Simulation Framework for CXL-Enabled Systems"
                },
                "summary": "Compute Express Link (CXL) serves as a rising industry standard, delivering\nhigh-speed cache-coherent links to a variety of devices, including host CPUs,\ncomputational accelerators, and memory devices. It is designed to promote\nsystem scalability, enable peer-to-peer exchanges, and accelerate data\ntransmissions. To achieve these objectives, the most recent CXL protocol has\nbrought forth several innovative features, such as port-focused routing,\ndevice-handled coherence, and PCIe 6.0 compatibility. However, due to the\nlimited availability of hardware prototypes and simulators compatible with CXL,\nearlier CXL research has largely depended on emulating CXL devices using remote\nNUMA nodes. Unfortunately, these NUMA-based emulators have difficulties in\naccurately representing the new features due to fundamental differences in\nhardware and protocols. Moreover, the absence of support for non-tree topology\nand PCIe links makes it complex to merely adapt existing simulators for CXL\nsimulation. To overcome these problems, we introduce ESF, a simulation\nframework specifically designed for CXL systems. ESF has been developed to\naccurately reflect the unique features of the latest CXL protocol from the\nground up. It uses a specialized interconnect layer to facilitate connections\nwithin a wide range of system topologies and also includes key components to\ncarry out specific functions required by these features. By utilizing ESF, we\nthoroughly investigate various aspects of CXL systems, including system\ntopology, device-handled coherence, and the effects of PCIe characteristics,\nleading to important findings that can guide the creation of high-performance\nCXL systems. The ESF source codes are fully open-source and can be accessed at\nhttps://anonymous.4open.science/r/ESF-1CE3.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Express Link (CXL) serves as a rising industry standard, delivering\nhigh-speed cache-coherent links to a variety of devices, including host CPUs,\ncomputational accelerators, and memory devices. It is designed to promote\nsystem scalability, enable peer-to-peer exchanges, and accelerate data\ntransmissions. To achieve these objectives, the most recent CXL protocol has\nbrought forth several innovative features, such as port-focused routing,\ndevice-handled coherence, and PCIe 6.0 compatibility. However, due to the\nlimited availability of hardware prototypes and simulators compatible with CXL,\nearlier CXL research has largely depended on emulating CXL devices using remote\nNUMA nodes. Unfortunately, these NUMA-based emulators have difficulties in\naccurately representing the new features due to fundamental differences in\nhardware and protocols. Moreover, the absence of support for non-tree topology\nand PCIe links makes it complex to merely adapt existing simulators for CXL\nsimulation. To overcome these problems, we introduce ESF, a simulation\nframework specifically designed for CXL systems. ESF has been developed to\naccurately reflect the unique features of the latest CXL protocol from the\nground up. It uses a specialized interconnect layer to facilitate connections\nwithin a wide range of system topologies and also includes key components to\ncarry out specific functions required by these features. By utilizing ESF, we\nthoroughly investigate various aspects of CXL systems, including system\ntopology, device-handled coherence, and the effects of PCIe characteristics,\nleading to important findings that can guide the creation of high-performance\nCXL systems. The ESF source codes are fully open-source and can be accessed at\nhttps://anonymous.4open.science/r/ESF-1CE3."
                },
                "authors": [
                    {
                        "name": "Yuda An"
                    },
                    {
                        "name": "Shushu Yi"
                    },
                    {
                        "name": "Bo Mao"
                    },
                    {
                        "name": "Qiao Li"
                    },
                    {
                        "name": "Mingzhe Zhang"
                    },
                    {
                        "name": "Ke Zhou"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Guangyu Sun"
                    },
                    {
                        "name": "Xiaolin Wang"
                    },
                    {
                        "name": "Yingwei Luo"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "arxiv_affiliation": "Peking University",
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08203v1",
                "updated": "2024-11-12T21:50:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    12,
                    21,
                    50,
                    3,
                    1,
                    317,
                    0
                ],
                "published": "2024-11-12T21:50:03Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    21,
                    50,
                    3,
                    1,
                    317,
                    0
                ],
                "title": "FaaS and Furious: abstractions and differential caching for efficient\n  data pre-processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FaaS and Furious: abstractions and differential caching for efficient\n  data pre-processing"
                },
                "summary": "Data pre-processing pipelines are the bread and butter of any successful AI\nproject. We introduce a novel programming model for pipelines in a data\nlakehouse, allowing users to interact declaratively with assets in object\nstorage. Motivated by real-world industry usage patterns, we exploit these new\nabstractions with a columnar and differential cache to maximize iteration speed\nfor data scientists, who spent most of their time in pre-processing - adding or\nremoving features, restricting or relaxing time windows, wrangling current or\nolder datasets. We show how the new cache works transparently across\nprogramming languages, schemas and time windows, and provide preliminary\nevidence on its efficiency on standard data workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data pre-processing pipelines are the bread and butter of any successful AI\nproject. We introduce a novel programming model for pipelines in a data\nlakehouse, allowing users to interact declaratively with assets in object\nstorage. Motivated by real-world industry usage patterns, we exploit these new\nabstractions with a columnar and differential cache to maximize iteration speed\nfor data scientists, who spent most of their time in pre-processing - adding or\nremoving features, restricting or relaxing time windows, wrangling current or\nolder datasets. We show how the new cache works transparently across\nprogramming languages, schemas and time windows, and provide preliminary\nevidence on its efficiency on standard data workloads."
                },
                "authors": [
                    {
                        "name": "Jacopo Tagliabue"
                    },
                    {
                        "name": "Ryan Curtin"
                    },
                    {
                        "name": "Ciro Greco"
                    }
                ],
                "author_detail": {
                    "name": "Ciro Greco"
                },
                "author": "Ciro Greco",
                "arxiv_comment": "Pre-print of the paper accepted at DEMAI@IEEE Big Data 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.06219v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.06219v3",
                "updated": "2024-11-12T08:18:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    18,
                    45,
                    1,
                    317,
                    0
                ],
                "published": "2024-05-10T03:06:24Z",
                "published_parsed": [
                    2024,
                    5,
                    10,
                    3,
                    6,
                    24,
                    4,
                    131,
                    0
                ],
                "title": "SKVQ: Sliding-window Key and Value Cache Quantization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SKVQ: Sliding-window Key and Value Cache Quantization for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) can now handle longer sequences of tokens,\nenabling complex tasks like book understanding and generating lengthy novels.\nHowever, the key-value (KV) cache required for LLMs consumes substantial memory\nas context length increasing, becoming the bottleneck for deployment. In this\npaper, we present a strategy called SKVQ, which stands for sliding-window KV\ncache quantization, to address the issue of extremely low bitwidth KV cache\nquantization. To achieve this, SKVQ rearranges the channels of the KV cache in\norder to improve the similarity of channels in quantization groups, and applies\nclipped dynamic quantization at the group level. Additionally, SKVQ ensures\nthat the most recent window tokens in the KV cache are preserved with high\nprecision. This helps maintain the accuracy of a small but important portion of\nthe KV cache.SKVQ achieves high compression ratios while maintaining accuracy.\nOur evaluation on LLMs demonstrates that SKVQ surpasses previous quantization\napproaches, allowing for quantization of the KV cache to 2-bit keys and 1.5-bit\nvalues with minimal loss of accuracy. With SKVQ, it is possible to process\ncontext lengths of up to 1M on an 80GB memory GPU for a 7b model and up to 7\ntimes faster decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can now handle longer sequences of tokens,\nenabling complex tasks like book understanding and generating lengthy novels.\nHowever, the key-value (KV) cache required for LLMs consumes substantial memory\nas context length increasing, becoming the bottleneck for deployment. In this\npaper, we present a strategy called SKVQ, which stands for sliding-window KV\ncache quantization, to address the issue of extremely low bitwidth KV cache\nquantization. To achieve this, SKVQ rearranges the channels of the KV cache in\norder to improve the similarity of channels in quantization groups, and applies\nclipped dynamic quantization at the group level. Additionally, SKVQ ensures\nthat the most recent window tokens in the KV cache are preserved with high\nprecision. This helps maintain the accuracy of a small but important portion of\nthe KV cache.SKVQ achieves high compression ratios while maintaining accuracy.\nOur evaluation on LLMs demonstrates that SKVQ surpasses previous quantization\napproaches, allowing for quantization of the KV cache to 2-bit keys and 1.5-bit\nvalues with minimal loss of accuracy. With SKVQ, it is possible to process\ncontext lengths of up to 1M on an 80GB memory GPU for a 7b model and up to 7\ntimes faster decoding."
                },
                "authors": [
                    {
                        "name": "Haojie Duanmu"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Xiuhong Li"
                    },
                    {
                        "name": "Jiangfei Duan"
                    },
                    {
                        "name": "Xingcheng Zhang"
                    },
                    {
                        "name": "Dahua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Dahua Lin"
                },
                "author": "Dahua Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.06219v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.06219v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07627v1",
                "updated": "2024-11-12T08:17:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    17,
                    15,
                    1,
                    317,
                    0
                ],
                "published": "2024-11-12T08:17:15Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    17,
                    15,
                    1,
                    317,
                    0
                ],
                "title": "Leveraging Previous Steps: A Training-free Fast Solver for Flow\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Previous Steps: A Training-free Fast Solver for Flow\n  Diffusion"
                },
                "summary": "Flow diffusion models (FDMs) have recently shown potential in generation\ntasks due to the high generation quality. However, the current ordinary\ndifferential equation (ODE) solver for FDMs, e.g., the Euler solver, still\nsuffers from slow generation since ODE solvers need many number function\nevaluations (NFE) to keep high-quality generation. In this paper, we propose a\nnovel training-free flow-solver to reduce NFE while maintaining high-quality\ngeneration. The key insight for the flow-solver is to leverage the previous\nsteps to reduce the NFE, where a cache is created to reuse these results from\nthe previous steps. Specifically, the Taylor expansion is first used to\napproximate the ODE. To calculate the high-order derivatives of Taylor\nexpansion, the flow-solver proposes to use the previous steps and a polynomial\ninterpolation to approximate it, where the number of orders we could\napproximate equals the number of previous steps we cached. We also prove that\nthe flow-solver has a more minor approximation error and faster generation\nspeed. Experimental results on the CIFAR-10, CelebA-HQ, LSUN-Bedroom,\nLSUN-Church, ImageNet, and real text-to-image generation prove the efficiency\nof the flow-solver. Specifically, the flow-solver improves the FID-30K from\n13.79 to 6.75, from 46.64 to 19.49 with $\\text{NFE}=10$ on CIFAR-10 and\nLSUN-Church, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow diffusion models (FDMs) have recently shown potential in generation\ntasks due to the high generation quality. However, the current ordinary\ndifferential equation (ODE) solver for FDMs, e.g., the Euler solver, still\nsuffers from slow generation since ODE solvers need many number function\nevaluations (NFE) to keep high-quality generation. In this paper, we propose a\nnovel training-free flow-solver to reduce NFE while maintaining high-quality\ngeneration. The key insight for the flow-solver is to leverage the previous\nsteps to reduce the NFE, where a cache is created to reuse these results from\nthe previous steps. Specifically, the Taylor expansion is first used to\napproximate the ODE. To calculate the high-order derivatives of Taylor\nexpansion, the flow-solver proposes to use the previous steps and a polynomial\ninterpolation to approximate it, where the number of orders we could\napproximate equals the number of previous steps we cached. We also prove that\nthe flow-solver has a more minor approximation error and faster generation\nspeed. Experimental results on the CIFAR-10, CelebA-HQ, LSUN-Bedroom,\nLSUN-Church, ImageNet, and real text-to-image generation prove the efficiency\nof the flow-solver. Specifically, the flow-solver improves the FID-30K from\n13.79 to 6.75, from 46.64 to 19.49 with $\\text{NFE}=10$ on CIFAR-10 and\nLSUN-Church, respectively."
                },
                "authors": [
                    {
                        "name": "Kaiyu Song"
                    },
                    {
                        "name": "Hanjiang Lai"
                    }
                ],
                "author_detail": {
                    "name": "Hanjiang Lai"
                },
                "author": "Hanjiang Lai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06681v1",
                "updated": "2024-11-11T02:48:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    48,
                    0,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T02:48:00Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    48,
                    0,
                    0,
                    316,
                    0
                ],
                "title": "WDMoE: Wireless Distributed Mixture of Experts for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WDMoE: Wireless Distributed Mixture of Experts for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have achieved significant success in various\nnatural language processing tasks, but the role of wireless networks in\nsupporting LLMs has not been thoroughly explored. In this paper, we propose a\nwireless distributed Mixture of Experts (WDMoE) architecture to enable\ncollaborative deployment of LLMs across edge servers at the base station (BS)\nand mobile devices in wireless networks. Specifically, we decompose the MoE\nlayer in LLMs by placing the gating network and the preceding neural network\nlayer at BS, while distributing the expert networks among the devices. This\ndeployment leverages the parallel inference capabilities of expert networks on\nmobile devices, effectively utilizing the limited computing and caching\nresources of these devices. Accordingly, we develop a performance metric for\nWDMoE-based LLMs, which accounts for both model capability and latency. To\nminimize the latency while maintaining accuracy, we jointly optimize expert\nselection and bandwidth allocation based on the performance metric. Moreover,\nwe build a hardware testbed using NVIDIA Jetson kits to validate the\neffectiveness of WDMoE. Both theoretical simulations and practical hardware\nexperiments demonstrate that the proposed method can significantly reduce the\nlatency without compromising LLM performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved significant success in various\nnatural language processing tasks, but the role of wireless networks in\nsupporting LLMs has not been thoroughly explored. In this paper, we propose a\nwireless distributed Mixture of Experts (WDMoE) architecture to enable\ncollaborative deployment of LLMs across edge servers at the base station (BS)\nand mobile devices in wireless networks. Specifically, we decompose the MoE\nlayer in LLMs by placing the gating network and the preceding neural network\nlayer at BS, while distributing the expert networks among the devices. This\ndeployment leverages the parallel inference capabilities of expert networks on\nmobile devices, effectively utilizing the limited computing and caching\nresources of these devices. Accordingly, we develop a performance metric for\nWDMoE-based LLMs, which accounts for both model capability and latency. To\nminimize the latency while maintaining accuracy, we jointly optimize expert\nselection and bandwidth allocation based on the performance metric. Moreover,\nwe build a hardware testbed using NVIDIA Jetson kits to validate the\neffectiveness of WDMoE. Both theoretical simulations and practical hardware\nexperiments demonstrate that the proposed method can significantly reduce the\nlatency without compromising LLM performance."
                },
                "authors": [
                    {
                        "name": "Nan Xue"
                    },
                    {
                        "name": "Yaping Sun"
                    },
                    {
                        "name": "Zhiyong Chen"
                    },
                    {
                        "name": "Meixia Tao"
                    },
                    {
                        "name": "Xiaodong Xu"
                    },
                    {
                        "name": "Liang Qian"
                    },
                    {
                        "name": "Shuguang Cui"
                    },
                    {
                        "name": "Wenjun Zhang"
                    },
                    {
                        "name": "Ping Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Zhang"
                },
                "author": "Ping Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06680v1",
                "updated": "2024-11-11T02:47:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    47,
                    5,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T02:47:05Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    47,
                    5,
                    0,
                    316,
                    0
                ],
                "title": "Anchor Attention, Small Cache: Code Generation with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anchor Attention, Small Cache: Code Generation with Large Language\n  Models"
                },
                "summary": "The development of large language models (LLMs) has revolutionized automated\ncode generation. However, their high demand of computation resources has\nhindered a broader deployment and raised environmental concerns. A common\nstrategy for diminishing computational demands is to cache Key-Value (KV)\nstates from the attention mechanism which is adopted predominately by\nmainstream LLMs. It can mitigate the need of repeated attention computations,\nbut brings significant memory overhead. Current practices in NLP often use\nsparse attention which may, unfortunately, lead to substantial inaccuracies, or\nhallucinations, in code generation tasks. In this paper, we analyze the\nattention weights distribution within code generation models via an empirical\nstudy, uncovering a sparsity pattern, i.e., the aggregation of information at\nspecific anchor points. Based on this observation, we propose a novel approach,\nAnchorCoder, which features token-wise anchor attention designed to extract and\ncompress the contextual information, and layer-wise anchor attention enabling\ncross-layer communication to mitigate the issue of excessive superposition\ncaused by the compression. The extensive experiments across multiple benchmark\ndatasets confirm the effectiveness of AnchorCoder, which can consistently\nachieve a significant (at least 70%) reduction in KV cache requirements, while\npreserving the majority of model's performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of large language models (LLMs) has revolutionized automated\ncode generation. However, their high demand of computation resources has\nhindered a broader deployment and raised environmental concerns. A common\nstrategy for diminishing computational demands is to cache Key-Value (KV)\nstates from the attention mechanism which is adopted predominately by\nmainstream LLMs. It can mitigate the need of repeated attention computations,\nbut brings significant memory overhead. Current practices in NLP often use\nsparse attention which may, unfortunately, lead to substantial inaccuracies, or\nhallucinations, in code generation tasks. In this paper, we analyze the\nattention weights distribution within code generation models via an empirical\nstudy, uncovering a sparsity pattern, i.e., the aggregation of information at\nspecific anchor points. Based on this observation, we propose a novel approach,\nAnchorCoder, which features token-wise anchor attention designed to extract and\ncompress the contextual information, and layer-wise anchor attention enabling\ncross-layer communication to mitigate the issue of excessive superposition\ncaused by the compression. The extensive experiments across multiple benchmark\ndatasets confirm the effectiveness of AnchorCoder, which can consistently\nachieve a significant (at least 70%) reduction in KV cache requirements, while\npreserving the majority of model's performance."
                },
                "authors": [
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Guang Yang"
                    },
                    {
                        "name": "Harald C. Gall"
                    },
                    {
                        "name": "Taolue Chen"
                    }
                ],
                "author_detail": {
                    "name": "Taolue Chen"
                },
                "author": "Taolue Chen",
                "arxiv_comment": "14 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68N19",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06659v1",
                "updated": "2024-11-11T01:53:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    1,
                    53,
                    14,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T01:53:14Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    1,
                    53,
                    14,
                    0,
                    316,
                    0
                ],
                "title": "An Efficient Memory Module for Graph Few-Shot Class-Incremental Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Memory Module for Graph Few-Shot Class-Incremental Learning"
                },
                "summary": "Incremental graph learning has gained significant attention for its ability\nto address the catastrophic forgetting problem in graph representation\nlearning. However, traditional methods often rely on a large number of labels\nfor node classification, which is impractical in real-world applications. This\nmakes few-shot incremental learning on graphs a pressing need. Current methods\ntypically require extensive training samples from meta-learning to build memory\nand perform intensive fine-tuning of GNN parameters, leading to high memory\nconsumption and potential loss of previously learned knowledge. To tackle these\nchallenges, we introduce Mecoin, an efficient method for building and\nmaintaining memory. Mecoin employs Structured Memory Units to cache prototypes\nof learned categories, as well as Memory Construction Modules to update these\nprototypes for new categories through interactions between the nodes and the\ncached prototypes. Additionally, we have designed a Memory Representation\nAdaptation Module to store probabilities associated with each class prototype,\nreducing the need for parameter fine-tuning and lowering the forgetting rate.\nWhen a sample matches its corresponding class prototype, the relevant\nprobabilities are retrieved from the MRaM. Knowledge is then distilled back\ninto the GNN through a Graph Knowledge Distillation Module, preserving the\nmodel's memory. We analyze the effectiveness of Mecoin in terms of\ngeneralization error and explore the impact of different distillation\nstrategies on model performance through experiments and VC-dimension analysis.\nCompared to other related works, Mecoin shows superior performance in accuracy\nand forgetting rate. Our code is publicly available on the\nhttps://github.com/Arvin0313/Mecoin-GFSCIL.git .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incremental graph learning has gained significant attention for its ability\nto address the catastrophic forgetting problem in graph representation\nlearning. However, traditional methods often rely on a large number of labels\nfor node classification, which is impractical in real-world applications. This\nmakes few-shot incremental learning on graphs a pressing need. Current methods\ntypically require extensive training samples from meta-learning to build memory\nand perform intensive fine-tuning of GNN parameters, leading to high memory\nconsumption and potential loss of previously learned knowledge. To tackle these\nchallenges, we introduce Mecoin, an efficient method for building and\nmaintaining memory. Mecoin employs Structured Memory Units to cache prototypes\nof learned categories, as well as Memory Construction Modules to update these\nprototypes for new categories through interactions between the nodes and the\ncached prototypes. Additionally, we have designed a Memory Representation\nAdaptation Module to store probabilities associated with each class prototype,\nreducing the need for parameter fine-tuning and lowering the forgetting rate.\nWhen a sample matches its corresponding class prototype, the relevant\nprobabilities are retrieved from the MRaM. Knowledge is then distilled back\ninto the GNN through a Graph Knowledge Distillation Module, preserving the\nmodel's memory. We analyze the effectiveness of Mecoin in terms of\ngeneralization error and explore the impact of different distillation\nstrategies on model performance through experiments and VC-dimension analysis.\nCompared to other related works, Mecoin shows superior performance in accuracy\nand forgetting rate. Our code is publicly available on the\nhttps://github.com/Arvin0313/Mecoin-GFSCIL.git ."
                },
                "authors": [
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Aijia Zhang"
                    },
                    {
                        "name": "Junqi Gao"
                    },
                    {
                        "name": "Biqing Qi"
                    }
                ],
                "author_detail": {
                    "name": "Biqing Qi"
                },
                "author": "Biqing Qi",
                "arxiv_comment": "16 pages, 6 figures, 38th Conference on Neural Information Processing\n  Systems, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01783v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01783v2",
                "updated": "2024-11-10T23:04:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    23,
                    4,
                    12,
                    6,
                    315,
                    0
                ],
                "published": "2024-11-04T04:15:36Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    4,
                    15,
                    36,
                    0,
                    309,
                    0
                ],
                "title": "Context Parallelism for Scalable Million-Token Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context Parallelism for Scalable Million-Token Inference"
                },
                "summary": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth."
                },
                "authors": [
                    {
                        "name": "Amy Yang"
                    },
                    {
                        "name": "Jingyi Yang"
                    },
                    {
                        "name": "Aya Ibrahim"
                    },
                    {
                        "name": "Xinfeng Xie"
                    },
                    {
                        "name": "Bangsheng Tang"
                    },
                    {
                        "name": "Grigory Sizov"
                    },
                    {
                        "name": "Jeremy Reizenstein"
                    },
                    {
                        "name": "Jongsoo Park"
                    },
                    {
                        "name": "Jianyu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jianyu Huang"
                },
                "author": "Jianyu Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01783v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01783v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.14396v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.14396v4",
                "updated": "2024-11-10T15:58:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    15,
                    58,
                    7,
                    6,
                    315,
                    0
                ],
                "published": "2023-12-22T02:52:59Z",
                "published_parsed": [
                    2023,
                    12,
                    22,
                    2,
                    52,
                    59,
                    4,
                    356,
                    0
                ],
                "title": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing"
                },
                "summary": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation."
                },
                "authors": [
                    {
                        "name": "Hongfu Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongfu Li"
                },
                "author": "Hongfu Li",
                "arxiv_comment": "This paper was accepted by VLDB2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.14396v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.14396v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04873v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04873v2",
                "updated": "2024-11-10T10:08:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    10,
                    8,
                    37,
                    6,
                    315,
                    0
                ],
                "published": "2024-06-07T12:12:25Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    12,
                    12,
                    25,
                    4,
                    159,
                    0
                ],
                "title": "Ada-VE: Training-Free Consistent Video Editing Using Adaptive Motion\n  Prior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-VE: Training-Free Consistent Video Editing Using Adaptive Motion\n  Prior"
                },
                "summary": "Video-to-video synthesis poses significant challenges in maintaining\ncharacter consistency, smooth temporal transitions, and preserving visual\nquality during fast motion. While recent fully cross-frame self-attention\nmechanisms have improved character consistency across multiple frames, they\ncome with high computational costs and often include redundant operations,\nespecially for videos with higher frame rates. To address these inefficiencies,\nwe propose an adaptive motion-guided cross-frame attention mechanism that\nselectively reduces redundant computations. This enables a greater number of\ncross-frame attentions over more frames within the same computational budget,\nthereby enhancing both video quality and temporal coherence. Our method\nleverages optical flow to focus on moving regions while sparsely attending to\nstationary areas, allowing for the joint editing of more frames without\nincreasing computational demands. Traditional frame interpolation techniques\nstruggle with motion blur and flickering in intermediate frames, which\ncompromises visual fidelity. To mitigate this, we introduce KV-caching for\njointly edited frames, reusing keys and values across intermediate frames to\npreserve visual quality and maintain temporal consistency throughout the video.\nWith our adaptive cross-frame self-attention approach, we achieve a threefold\nincrease in the number of keyframes processed compared to existing methods, all\nwithin the same computational budget as fully cross-frame attention baselines.\nThis results in significant improvements in prediction accuracy and temporal\nconsistency, outperforming state-of-the-art approaches. Code will be made\npublicly available at https://github.com/tanvir-utexas/AdaVE/tree/main",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-to-video synthesis poses significant challenges in maintaining\ncharacter consistency, smooth temporal transitions, and preserving visual\nquality during fast motion. While recent fully cross-frame self-attention\nmechanisms have improved character consistency across multiple frames, they\ncome with high computational costs and often include redundant operations,\nespecially for videos with higher frame rates. To address these inefficiencies,\nwe propose an adaptive motion-guided cross-frame attention mechanism that\nselectively reduces redundant computations. This enables a greater number of\ncross-frame attentions over more frames within the same computational budget,\nthereby enhancing both video quality and temporal coherence. Our method\nleverages optical flow to focus on moving regions while sparsely attending to\nstationary areas, allowing for the joint editing of more frames without\nincreasing computational demands. Traditional frame interpolation techniques\nstruggle with motion blur and flickering in intermediate frames, which\ncompromises visual fidelity. To mitigate this, we introduce KV-caching for\njointly edited frames, reusing keys and values across intermediate frames to\npreserve visual quality and maintain temporal consistency throughout the video.\nWith our adaptive cross-frame self-attention approach, we achieve a threefold\nincrease in the number of keyframes processed compared to existing methods, all\nwithin the same computational budget as fully cross-frame attention baselines.\nThis results in significant improvements in prediction accuracy and temporal\nconsistency, outperforming state-of-the-art approaches. Code will be made\npublicly available at https://github.com/tanvir-utexas/AdaVE/tree/main"
                },
                "authors": [
                    {
                        "name": "Tanvir Mahmud"
                    },
                    {
                        "name": "Mustafa Munir"
                    },
                    {
                        "name": "Radu Marculescu"
                    },
                    {
                        "name": "Diana Marculescu"
                    }
                ],
                "author_detail": {
                    "name": "Diana Marculescu"
                },
                "author": "Diana Marculescu",
                "arxiv_comment": "Accepted in WACV 2025. Project page:\n  https://tanvir-utexas.github.io/AdaVE_Demo/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04873v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04873v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06364v1",
                "updated": "2024-11-10T05:12:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    5,
                    12,
                    51,
                    6,
                    315,
                    0
                ],
                "published": "2024-11-10T05:12:51Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    5,
                    12,
                    51,
                    6,
                    315,
                    0
                ],
                "title": "EcoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in\n  LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EcoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in\n  LLM Serving"
                },
                "summary": "As Large Language Models (LLMs) continue to grow, reducing costs and\nalleviating GPU demands has become increasingly critical. However, existing\nschedulers primarily target either GPU compute or Key-Value Cache (KVC)\nutilization, failing to fully optimize both GPU compute and KVC usage during\neach iteration or guarantee timely KVC allocations when needed. To address\nthese challenges, we conducted a trace-based experimental analysis and made\ninsightful observations, leading to the design of a system called EcoServe.\nEcoServe maximizes multi-resource utilization while ensuring service-level\nobjective (SLO) guarantees in LLM serving. To enable adding prompts to a batch\nto maximize GPU utilization in each iteration, EcoServe maintains separate\nwaiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It\nbatches GTs with the same predicted response lengths (RL) to save scheduling\ntime and allocates KVC space for the predicted RL to avoid KVC allocation\nfailures. It further has a novel KVC pipelining method, allowing sharing\nallocated but unused KVC space to enhance KVC utilization. In addition, it\nprioritizes queued requests that occupy more KVC to release KVC earlier and\nsatisfy request service-level-objective (SLO). Experimental results demonstrate\nthat EcoServe increases throughput by up to 4$\\times$ with the same level of\nlatency, generates up to 91\\% lower job completion time and up to 91\\% higher\nSLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs\nused in DistServe by up to 78\\% while maintaining the same level of goodput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) continue to grow, reducing costs and\nalleviating GPU demands has become increasingly critical. However, existing\nschedulers primarily target either GPU compute or Key-Value Cache (KVC)\nutilization, failing to fully optimize both GPU compute and KVC usage during\neach iteration or guarantee timely KVC allocations when needed. To address\nthese challenges, we conducted a trace-based experimental analysis and made\ninsightful observations, leading to the design of a system called EcoServe.\nEcoServe maximizes multi-resource utilization while ensuring service-level\nobjective (SLO) guarantees in LLM serving. To enable adding prompts to a batch\nto maximize GPU utilization in each iteration, EcoServe maintains separate\nwaiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It\nbatches GTs with the same predicted response lengths (RL) to save scheduling\ntime and allocates KVC space for the predicted RL to avoid KVC allocation\nfailures. It further has a novel KVC pipelining method, allowing sharing\nallocated but unused KVC space to enhance KVC utilization. In addition, it\nprioritizes queued requests that occupy more KVC to release KVC earlier and\nsatisfy request service-level-objective (SLO). Experimental results demonstrate\nthat EcoServe increases throughput by up to 4$\\times$ with the same level of\nlatency, generates up to 91\\% lower job completion time and up to 91\\% higher\nSLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs\nused in DistServe by up to 78\\% while maintaining the same level of goodput."
                },
                "authors": [
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Tanmoy Sen"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Sen"
                },
                "author": "Tanmoy Sen",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05646v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05646v2",
                "updated": "2024-11-08T16:29:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    16,
                    29,
                    33,
                    4,
                    313,
                    0
                ],
                "published": "2024-08-10T22:47:12Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "title": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression"
                },
                "summary": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance. Code is available at\nhttps://github.com/UtkarshSaxena1/EigenAttn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance. Code is available at\nhttps://github.com/UtkarshSaxena1/EigenAttn."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Gobinda Saha"
                    },
                    {
                        "name": "Sakshi Choudhary"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "12 page, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05646v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05646v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05555v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05555v1",
                "updated": "2024-11-08T13:24:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    24,
                    1,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T13:24:01Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    24,
                    1,
                    4,
                    313,
                    0
                ],
                "title": "AcceLLM: Accelerating LLM Inference using Redundancy for Load Balancing\n  and Data Locality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AcceLLM: Accelerating LLM Inference using Redundancy for Load Balancing\n  and Data Locality"
                },
                "summary": "Large Language Model (LLM) inference on large-scale systems is expected to\ndominate future cloud infrastructures. Efficient LLM inference in cloud\nenvironments with numerous AI accelerators is challenging, necessitating\nextensive optimizations for optimal performance. Current systems batch prefill\nand decoding to boost throughput but encounter latency issues, while others\ndisaggregate these phases, leading to resource underutilization. We propose\nAcceLLM, a novel method addressing latency and load balancing, inspired by the\ncache data management. It strategically utilizes redundant data to enhance\ninference via load balancing and optimal hardware use. Simulated evaluations on\nNvidia H100 GPU and Huawei Ascend 910B2 show AcceLLM surpasses state-of-the-art\nsystems up to 30% in latency and efficiency, handling diverse workloads\neffectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference on large-scale systems is expected to\ndominate future cloud infrastructures. Efficient LLM inference in cloud\nenvironments with numerous AI accelerators is challenging, necessitating\nextensive optimizations for optimal performance. Current systems batch prefill\nand decoding to boost throughput but encounter latency issues, while others\ndisaggregate these phases, leading to resource underutilization. We propose\nAcceLLM, a novel method addressing latency and load balancing, inspired by the\ncache data management. It strategically utilizes redundant data to enhance\ninference via load balancing and optimal hardware use. Simulated evaluations on\nNvidia H100 GPU and Huawei Ascend 910B2 show AcceLLM surpasses state-of-the-art\nsystems up to 30% in latency and efficiency, handling diverse workloads\neffectively."
                },
                "authors": [
                    {
                        "name": "Ilias Bournias"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    },
                    {
                        "name": "Georgios Zacharopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Georgios Zacharopoulos"
                },
                "author": "Georgios Zacharopoulos",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05555v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05555v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05276v1",
                "updated": "2024-11-08T02:21:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    21,
                    19,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T02:21:19Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    21,
                    19,
                    4,
                    313,
                    0
                ],
                "title": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching"
                },
                "summary": "Large Language Models (LLMs), such as GPT (Radford et al., 2019), have\nsignificantly advanced artificial intelligence by enabling sophisticated\nnatural language understanding and generation. However, the high computational\nand financial costs associated with frequent API calls to these models present\na substantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique reduces operational costs and improves response times, enhancing\nthe efficiency of LLM-powered applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as GPT (Radford et al., 2019), have\nsignificantly advanced artificial intelligence by enabling sophisticated\nnatural language understanding and generation. However, the high computational\nand financial costs associated with frequent API calls to these models present\na substantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique reduces operational costs and improves response times, enhancing\nthe efficiency of LLM-powered applications."
                },
                "authors": [
                    {
                        "name": "Sajal Regmi"
                    },
                    {
                        "name": "Chetan Phakami Pun"
                    }
                ],
                "author_detail": {
                    "name": "Chetan Phakami Pun"
                },
                "author": "Chetan Phakami Pun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02542v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02542v2",
                "updated": "2024-11-07T18:58:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    58,
                    50,
                    3,
                    312,
                    0
                ],
                "published": "2024-06-04T17:58:03Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    58,
                    3,
                    1,
                    156,
                    0
                ],
                "title": "Loki: Low-rank Keys for Efficient Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Loki: Low-rank Keys for Efficient Sparse Attention"
                },
                "summary": "Inference on large language models (LLMs) can be expensive in terms of the\ncompute and memory costs involved, especially when long sequence lengths are\nused. In particular, the self-attention mechanism used in LLM inference\ncontributes significantly to these costs, which has sparked an interest in\napproximating the self-attention computation to reduce such costs. In this\nwork, we propose to approximate self-attention by focusing on the\ndimensionality of key vectors computed in the attention block. Our analysis\nreveals that key vectors lie in a significantly lower-dimensional space,\nconsistently across several datasets and models. Exploiting this observation,\nwe propose Loki, a novel sparse attention method that ranks and selects tokens\nin the KV-cache based on attention scores computed in low-dimensional space.\nOur evaluations show that Loki is able to speed up the attention computation\ndue to reduced data movement (load/store) and compute costs while maintaining\nthe efficacy of the models better than other popular approximation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference on large language models (LLMs) can be expensive in terms of the\ncompute and memory costs involved, especially when long sequence lengths are\nused. In particular, the self-attention mechanism used in LLM inference\ncontributes significantly to these costs, which has sparked an interest in\napproximating the self-attention computation to reduce such costs. In this\nwork, we propose to approximate self-attention by focusing on the\ndimensionality of key vectors computed in the attention block. Our analysis\nreveals that key vectors lie in a significantly lower-dimensional space,\nconsistently across several datasets and models. Exploiting this observation,\nwe propose Loki, a novel sparse attention method that ranks and selects tokens\nin the KV-cache based on attention scores computed in low-dimensional space.\nOur evaluations show that Loki is able to speed up the attention computation\ndue to reduced data movement (load/store) and compute costs while maintaining\nthe efficacy of the models better than other popular approximation methods."
                },
                "authors": [
                    {
                        "name": "Prajwal Singhania"
                    },
                    {
                        "name": "Siddharth Singh"
                    },
                    {
                        "name": "Shwai He"
                    },
                    {
                        "name": "Soheil Feizi"
                    },
                    {
                        "name": "Abhinav Bhatele"
                    }
                ],
                "author_detail": {
                    "name": "Abhinav Bhatele"
                },
                "author": "Abhinav Bhatele",
                "arxiv_comment": "Proceedings of the Thirty-Eighth Annual Conference on Neural\n  Information Processing Systems (Main Conference Track)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02542v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02542v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04965v1",
                "updated": "2024-11-07T18:41:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    41,
                    50,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T18:41:50Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    41,
                    50,
                    3,
                    312,
                    0
                ],
                "title": "BitNet a4.8: 4-bit Activations for 1-bit LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitNet a4.8: 4-bit Activations for 1-bit LLMs"
                },
                "summary": "Recent research on the 1-bit Large Language Models (LLMs), such as BitNet\nb1.58, presents a promising direction for reducing the inference cost of LLMs\nwhile maintaining their performance. In this work, we introduce BitNet a4.8,\nenabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid\nquantization and sparsification strategy to mitigate the quantization errors\nintroduced by the outlier channels. Specifically, we utilize 4-bit activations\nfor inputs to the attention and feed-forward network layers, while sparsifying\nintermediate states followed with 8-bit quantization. Extensive experiments\ndemonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58\nwith equivalent training costs, while being faster in inference with enabling\n4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of\nparameters and supports 3-bit KV cache, further enhancing the efficiency of\nlarge-scale LLM deployment and inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research on the 1-bit Large Language Models (LLMs), such as BitNet\nb1.58, presents a promising direction for reducing the inference cost of LLMs\nwhile maintaining their performance. In this work, we introduce BitNet a4.8,\nenabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid\nquantization and sparsification strategy to mitigate the quantization errors\nintroduced by the outlier channels. Specifically, we utilize 4-bit activations\nfor inputs to the attention and feed-forward network layers, while sparsifying\nintermediate states followed with 8-bit quantization. Extensive experiments\ndemonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58\nwith equivalent training costs, while being faster in inference with enabling\n4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of\nparameters and supports 3-bit KV cache, further enhancing the efficiency of\nlarge-scale LLM deployment and inference."
                },
                "authors": [
                    {
                        "name": "Hongyu Wang"
                    },
                    {
                        "name": "Shuming Ma"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02397v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02397v2",
                "updated": "2024-11-07T17:06:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    17,
                    6,
                    32,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-04T18:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    59,
                    44,
                    0,
                    309,
                    0
                ],
                "title": "Adaptive Caching for Faster Video Generation with Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Caching for Faster Video Generation with Diffusion Transformers"
                },
                "summary": "Generating temporally-consistent high-fidelity videos can be computationally\nexpensive, especially over longer temporal spans. More-recent Diffusion\nTransformers (DiTs) -- despite making significant headway in this context --\nhave only heightened such challenges as they rely on larger models and heavier\nattention mechanisms, resulting in slower inference speeds. In this paper, we\nintroduce a training-free method to accelerate video DiTs, termed Adaptive\nCaching (AdaCache), which is motivated by the fact that \"not all videos are\ncreated equal\": meaning, some videos require fewer denoising steps to attain a\nreasonable quality than others. Building on this, we not only cache\ncomputations through the diffusion process, but also devise a caching schedule\ntailored to each video generation, maximizing the quality-latency trade-off. We\nfurther introduce a Motion Regularization (MoReg) scheme to utilize video\ninformation within AdaCache, essentially controlling the compute allocation\nbased on motion content. Altogether, our plug-and-play contributions grant\nsignificant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video\ngeneration) without sacrificing the generation quality, across multiple video\nDiT baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating temporally-consistent high-fidelity videos can be computationally\nexpensive, especially over longer temporal spans. More-recent Diffusion\nTransformers (DiTs) -- despite making significant headway in this context --\nhave only heightened such challenges as they rely on larger models and heavier\nattention mechanisms, resulting in slower inference speeds. In this paper, we\nintroduce a training-free method to accelerate video DiTs, termed Adaptive\nCaching (AdaCache), which is motivated by the fact that \"not all videos are\ncreated equal\": meaning, some videos require fewer denoising steps to attain a\nreasonable quality than others. Building on this, we not only cache\ncomputations through the diffusion process, but also devise a caching schedule\ntailored to each video generation, maximizing the quality-latency trade-off. We\nfurther introduce a Motion Regularization (MoReg) scheme to utilize video\ninformation within AdaCache, essentially controlling the compute allocation\nbased on motion content. Altogether, our plug-and-play contributions grant\nsignificant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video\ngeneration) without sacrificing the generation quality, across multiple video\nDiT baselines."
                },
                "authors": [
                    {
                        "name": "Kumara Kahatapitiya"
                    },
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Sen He"
                    },
                    {
                        "name": "Ding Liu"
                    },
                    {
                        "name": "Menglin Jia"
                    },
                    {
                        "name": "Chenyang Zhang"
                    },
                    {
                        "name": "Michael S. Ryoo"
                    },
                    {
                        "name": "Tian Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tian Xie"
                },
                "author": "Tian Xie",
                "arxiv_comment": "Project-page is available at https://adacache-dit.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02397v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16591v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16591v2",
                "updated": "2024-11-07T09:33:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    9,
                    33,
                    40,
                    3,
                    312,
                    0
                ],
                "published": "2024-05-26T14:50:40Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    14,
                    50,
                    40,
                    6,
                    147,
                    0
                ],
                "title": "CapS-Adapter: Caption-based MultiModal Adapter in Zero-Shot\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CapS-Adapter: Caption-based MultiModal Adapter in Zero-Shot\n  Classification"
                },
                "summary": "Recent advances in vision-language foundational models, such as CLIP, have\ndemonstrated significant strides in zero-shot classification. However, the\nextensive parameterization of models like CLIP necessitates a\nresource-intensive fine-tuning process. In response, TIP-Adapter and SuS-X have\nintroduced training-free methods aimed at bolstering the efficacy of downstream\ntasks. While these approaches incorporate support sets to maintain data\ndistribution consistency between knowledge cache and test sets, they often fall\nshort in terms of generalization on the test set, particularly when faced with\ntest data exhibiting substantial distributional variations. In this work, we\npresent CapS-Adapter, an innovative method that employs a caption-based support\nset, effectively harnessing both image and caption features to exceed existing\nstate-of-the-art techniques in training-free scenarios. CapS-Adapter adeptly\nconstructs support sets that closely mirror target distributions, utilizing\ninstance-level distribution features extracted from multimodal large models. By\nleveraging CLIP's single and cross-modal strengths, CapS-Adapter enhances\npredictive accuracy through the use of multimodal support sets. Our method\nachieves outstanding zero-shot classification results across 19 benchmark\ndatasets, improving accuracy by 2.19\\% over the previous leading method. Our\ncontributions are substantiated through extensive validation on multiple\nbenchmark datasets, demonstrating superior performance and robust\ngeneralization capabilities. Our code is made publicly available at\nhttps://github.com/WLuLi/CapS-Adapter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in vision-language foundational models, such as CLIP, have\ndemonstrated significant strides in zero-shot classification. However, the\nextensive parameterization of models like CLIP necessitates a\nresource-intensive fine-tuning process. In response, TIP-Adapter and SuS-X have\nintroduced training-free methods aimed at bolstering the efficacy of downstream\ntasks. While these approaches incorporate support sets to maintain data\ndistribution consistency between knowledge cache and test sets, they often fall\nshort in terms of generalization on the test set, particularly when faced with\ntest data exhibiting substantial distributional variations. In this work, we\npresent CapS-Adapter, an innovative method that employs a caption-based support\nset, effectively harnessing both image and caption features to exceed existing\nstate-of-the-art techniques in training-free scenarios. CapS-Adapter adeptly\nconstructs support sets that closely mirror target distributions, utilizing\ninstance-level distribution features extracted from multimodal large models. By\nleveraging CLIP's single and cross-modal strengths, CapS-Adapter enhances\npredictive accuracy through the use of multimodal support sets. Our method\nachieves outstanding zero-shot classification results across 19 benchmark\ndatasets, improving accuracy by 2.19\\% over the previous leading method. Our\ncontributions are substantiated through extensive validation on multiple\nbenchmark datasets, demonstrating superior performance and robust\ngeneralization capabilities. Our code is made publicly available at\nhttps://github.com/WLuLi/CapS-Adapter."
                },
                "authors": [
                    {
                        "name": "Qijie Wang"
                    },
                    {
                        "name": "Guandu Liu"
                    },
                    {
                        "name": "Bin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bin Wang"
                },
                "author": "Bin Wang",
                "arxiv_doi": "10.1145/3664647.3681566",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3664647.3681566",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.16591v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16591v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ACM Multimedia 2024 Poster",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01288v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01288v2",
                "updated": "2024-11-07T06:40:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    6,
                    40,
                    40,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-02T15:45:54Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    45,
                    54,
                    5,
                    307,
                    0
                ],
                "title": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01288v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01288v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02265v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02265v3",
                "updated": "2024-11-06T09:15:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    9,
                    15,
                    27,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-04T16:56:26Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    16,
                    56,
                    26,
                    0,
                    309,
                    0
                ],
                "title": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated\n  Parameters by Tencent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated\n  Parameters by Tencent"
                },
                "summary": "In this paper, we introduce Hunyuan-Large, which is currently the largest\nopen-source Transformer-based mixture of experts model, with a total of 389\nbillion parameters and 52 billion activation parameters, capable of handling up\nto 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior\nperformance across various benchmarks including language understanding and\ngeneration, logical reasoning, mathematical problem-solving, coding,\nlong-context, and aggregated tasks, where it outperforms LLama3.1-70B and\nexhibits comparable performance when compared to the significantly larger\nLLama3.1-405B model. Key practice of Hunyuan-Large include large-scale\nsynthetic data that is orders larger than in previous literature, a mixed\nexpert routing strategy, a key-value cache compression technique, and an\nexpert-specific learning rate strategy. Additionally, we also investigate the\nscaling laws and learning rate schedule of mixture of experts models, providing\nvaluable insights and guidances for future model development and optimization.\nThe code and checkpoints of Hunyuan-Large are released to facilitate future\ninnovations and applications.\n  Codes: https://github.com/Tencent/Hunyuan-Large\n  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Hunyuan-Large, which is currently the largest\nopen-source Transformer-based mixture of experts model, with a total of 389\nbillion parameters and 52 billion activation parameters, capable of handling up\nto 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior\nperformance across various benchmarks including language understanding and\ngeneration, logical reasoning, mathematical problem-solving, coding,\nlong-context, and aggregated tasks, where it outperforms LLama3.1-70B and\nexhibits comparable performance when compared to the significantly larger\nLLama3.1-405B model. Key practice of Hunyuan-Large include large-scale\nsynthetic data that is orders larger than in previous literature, a mixed\nexpert routing strategy, a key-value cache compression technique, and an\nexpert-specific learning rate strategy. Additionally, we also investigate the\nscaling laws and learning rate schedule of mixture of experts models, providing\nvaluable insights and guidances for future model development and optimization.\nThe code and checkpoints of Hunyuan-Large are released to facilitate future\ninnovations and applications.\n  Codes: https://github.com/Tencent/Hunyuan-Large\n  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large"
                },
                "authors": [
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Yanfeng Chen"
                    },
                    {
                        "name": "Yiqing Huang"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "Jiaqi Zhu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Shuaipeng Li"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Jonny Han"
                    },
                    {
                        "name": "Xiaobo Shu"
                    },
                    {
                        "name": "Jiahao Bu"
                    },
                    {
                        "name": "Zhongzhi Chen"
                    },
                    {
                        "name": "Xuemeng Huang"
                    },
                    {
                        "name": "Fengzong Lian"
                    },
                    {
                        "name": "Saiyong Yang"
                    },
                    {
                        "name": "Jianfeng Yan"
                    },
                    {
                        "name": "Yuyuan Zeng"
                    },
                    {
                        "name": "Xiaoqin Ren"
                    },
                    {
                        "name": "Chao Yu"
                    },
                    {
                        "name": "Lulu Wu"
                    },
                    {
                        "name": "Yue Mao"
                    },
                    {
                        "name": "Jun Xia"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Suncong Zheng"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Dian Jiao"
                    },
                    {
                        "name": "Jinbao Xue"
                    },
                    {
                        "name": "Xipeng Zhang"
                    },
                    {
                        "name": "Decheng Wu"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Dengpeng Wu"
                    },
                    {
                        "name": "Guanghui Xu"
                    },
                    {
                        "name": "Shaohua Chen"
                    },
                    {
                        "name": "Shuang Chen"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Yigeng Hong"
                    },
                    {
                        "name": "Junqiang Zheng"
                    },
                    {
                        "name": "Chengcheng Xu"
                    },
                    {
                        "name": "Zongwei Li"
                    },
                    {
                        "name": "Xiong Kuang"
                    },
                    {
                        "name": "Jianglu Hu"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Yuchi Deng"
                    },
                    {
                        "name": "Guiyang Li"
                    },
                    {
                        "name": "Ao Liu"
                    },
                    {
                        "name": "Chenchen Zhang"
                    },
                    {
                        "name": "Shihui Hu"
                    },
                    {
                        "name": "Zilong Zhao"
                    },
                    {
                        "name": "Zifan Wu"
                    },
                    {
                        "name": "Yao Ding"
                    },
                    {
                        "name": "Weichao Wang"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Roberts Wang"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Peijie Yu"
                    },
                    {
                        "name": "Ze Zhao"
                    },
                    {
                        "name": "Xun Cao"
                    },
                    {
                        "name": "Hai Wang"
                    },
                    {
                        "name": "Fusheng Xiang"
                    },
                    {
                        "name": "Mengyuan Huang"
                    },
                    {
                        "name": "Zhiyuan Xiong"
                    },
                    {
                        "name": "Bin Hu"
                    },
                    {
                        "name": "Xuebin Hou"
                    },
                    {
                        "name": "Lei Jiang"
                    },
                    {
                        "name": "Jianqiang Ma"
                    },
                    {
                        "name": "Jiajia Wu"
                    },
                    {
                        "name": "Yaping Deng"
                    },
                    {
                        "name": "Yi Shen"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Liang Dong"
                    },
                    {
                        "name": "Weiwen Jia"
                    },
                    {
                        "name": "Hu Chen"
                    },
                    {
                        "name": "Feifei Liu"
                    },
                    {
                        "name": "Rui Yuan"
                    },
                    {
                        "name": "Huilin Xu"
                    },
                    {
                        "name": "Zhenxiang Yan"
                    },
                    {
                        "name": "Tengfei Cao"
                    },
                    {
                        "name": "Zhichao Hu"
                    },
                    {
                        "name": "Xinhua Feng"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Tinghao Yu"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Feng Zhang"
                    },
                    {
                        "name": "Jianchen Zhu"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Xirui Li"
                    },
                    {
                        "name": "Chong Zha"
                    },
                    {
                        "name": "Wen Ouyang"
                    },
                    {
                        "name": "Yinben Xia"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Zekun He"
                    },
                    {
                        "name": "Rongpeng Chen"
                    },
                    {
                        "name": "Jiawei Song"
                    },
                    {
                        "name": "Ruibin Chen"
                    },
                    {
                        "name": "Fan Jiang"
                    },
                    {
                        "name": "Chongqing Zhao"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Hao Gong"
                    },
                    {
                        "name": "Rong Gan"
                    },
                    {
                        "name": "Winston Hu"
                    },
                    {
                        "name": "Zhanhui Kang"
                    },
                    {
                        "name": "Yong Yang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Jie Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Jiang"
                },
                "author": "Jie Jiang",
                "arxiv_comment": "17 pages, 4 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02265v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02265v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03731v1",
                "updated": "2024-11-06T07:53:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    53,
                    4,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T07:53:04Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    53,
                    4,
                    2,
                    311,
                    0
                ],
                "title": "Reducing Hyperparameter Tuning Costs in ML, Vision and Language Model\n  Training Pipelines via Memoization-Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing Hyperparameter Tuning Costs in ML, Vision and Language Model\n  Training Pipelines via Memoization-Awareness"
                },
                "summary": "The training or fine-tuning of machine learning, vision, and language models\nis often implemented as a pipeline: a sequence of stages encompassing data\npreparation, model training and evaluation. In this paper, we exploit pipeline\nstructures to reduce the cost of hyperparameter tuning for model\ntraining/fine-tuning, which is particularly valuable for language models given\ntheir high costs in GPU-days. We propose a \"memoization-aware\" Bayesian\nOptimization (BO) algorithm, EEIPU, that works in tandem with a pipeline\ncaching system, allowing it to evaluate significantly more hyperparameter\ncandidates per GPU-day than other tuning algorithms. The result is\nbetter-quality hyperparameters in the same amount of search time, or\nequivalently, reduced search time to reach the same hyperparameter quality. In\nour benchmarks on machine learning (model ensembles), vision (convolutional\narchitecture) and language (T5 architecture) pipelines, we compare EEIPU\nagainst recent BO algorithms: EEIPU produces an average of $103\\%$ more\nhyperparameter candidates (within the same budget), and increases the\nvalidation metric by an average of $108\\%$ more than other algorithms (where\nthe increase is measured starting from the end of warm-up iterations).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The training or fine-tuning of machine learning, vision, and language models\nis often implemented as a pipeline: a sequence of stages encompassing data\npreparation, model training and evaluation. In this paper, we exploit pipeline\nstructures to reduce the cost of hyperparameter tuning for model\ntraining/fine-tuning, which is particularly valuable for language models given\ntheir high costs in GPU-days. We propose a \"memoization-aware\" Bayesian\nOptimization (BO) algorithm, EEIPU, that works in tandem with a pipeline\ncaching system, allowing it to evaluate significantly more hyperparameter\ncandidates per GPU-day than other tuning algorithms. The result is\nbetter-quality hyperparameters in the same amount of search time, or\nequivalently, reduced search time to reach the same hyperparameter quality. In\nour benchmarks on machine learning (model ensembles), vision (convolutional\narchitecture) and language (T5 architecture) pipelines, we compare EEIPU\nagainst recent BO algorithms: EEIPU produces an average of $103\\%$ more\nhyperparameter candidates (within the same budget), and increases the\nvalidation metric by an average of $108\\%$ more than other algorithms (where\nthe increase is measured starting from the end of warm-up iterations)."
                },
                "authors": [
                    {
                        "name": "Abdelmajid Essofi"
                    },
                    {
                        "name": "Ridwan Salahuddeen"
                    },
                    {
                        "name": "Munachiso Nwadike"
                    },
                    {
                        "name": "Elnura Zhalieva"
                    },
                    {
                        "name": "Kun Zhang"
                    },
                    {
                        "name": "Eric Xing"
                    },
                    {
                        "name": "Willie Neiswanger"
                    },
                    {
                        "name": "Qirong Ho"
                    }
                ],
                "author_detail": {
                    "name": "Qirong Ho"
                },
                "author": "Qirong Ho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v2",
                "updated": "2024-11-06T07:12:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    12,
                    55,
                    2,
                    311,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_comment": "This work was submitted for review on Sept. 5, 2024, and the initial\n  version was uploaded to Arxiv on Sept. 30, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01433v2",
                "updated": "2024-11-06T01:49:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    1,
                    49,
                    45,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-03T04:25:46Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    4,
                    25,
                    46,
                    6,
                    308,
                    0
                ],
                "title": "HOBBIT: A Mixed Precision Expert Offloading System for Fast MoE\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HOBBIT: A Mixed Precision Expert Offloading System for Fast MoE\n  Inference"
                },
                "summary": "The Mixture-of-Experts (MoE) architecture has demonstrated significant\nadvantages in the era of Large Language Models (LLMs), offering enhanced\ncapabilities with reduced inference costs. However, deploying MoE-based LLMs on\nmemoryconstrained edge devices remains challenging due to their substantial\nmemory requirements. While existing expertoffloading methods alleviate the\nmemory requirements, they often incur significant expert-loading costs or\ncompromise model accuracy. We present HOBBIT, a mixed precision expert\noffloading system to enable flexible and efficient MoE inference. Our key\ninsight is that dynamically replacing less critical cache-miss experts with low\nprecision versions can substantially reduce expert-loading latency while\npreserving model accuracy. HOBBIT introduces three innovative techniques that\nmap the natural hierarchy of MoE computation: (1) a token-level dynamic expert\nloading mechanism, (2) a layer-level adaptive expert prefetching technique, and\n(3) a sequence-level multidimensional expert caching policy. These innovations\nfully leverage the benefits of mixedprecision expert inference. By implementing\nHOBBIT on top of the renowned LLM inference framework Llama.cpp, we evaluate\nits performance across different edge devices with representative MoE models.\nThe results demonstrate that HOBBIT achieves up to a 9.93x speedup in decoding\ncompared to state-of-the-art MoE offloading systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture-of-Experts (MoE) architecture has demonstrated significant\nadvantages in the era of Large Language Models (LLMs), offering enhanced\ncapabilities with reduced inference costs. However, deploying MoE-based LLMs on\nmemoryconstrained edge devices remains challenging due to their substantial\nmemory requirements. While existing expertoffloading methods alleviate the\nmemory requirements, they often incur significant expert-loading costs or\ncompromise model accuracy. We present HOBBIT, a mixed precision expert\noffloading system to enable flexible and efficient MoE inference. Our key\ninsight is that dynamically replacing less critical cache-miss experts with low\nprecision versions can substantially reduce expert-loading latency while\npreserving model accuracy. HOBBIT introduces three innovative techniques that\nmap the natural hierarchy of MoE computation: (1) a token-level dynamic expert\nloading mechanism, (2) a layer-level adaptive expert prefetching technique, and\n(3) a sequence-level multidimensional expert caching policy. These innovations\nfully leverage the benefits of mixedprecision expert inference. By implementing\nHOBBIT on top of the renowned LLM inference framework Llama.cpp, we evaluate\nits performance across different edge devices with representative MoE models.\nThe results demonstrate that HOBBIT achieves up to a 9.93x speedup in decoding\ncompared to state-of-the-art MoE offloading systems."
                },
                "authors": [
                    {
                        "name": "Peng Tang"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Xiaofeng Hou"
                    },
                    {
                        "name": "Yifei Pu"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Pheng-Ann Heng"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.05591v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.05591v3",
                "updated": "2024-11-05T08:34:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    8,
                    34,
                    44,
                    1,
                    310,
                    0
                ],
                "published": "2023-08-10T13:57:37Z",
                "published_parsed": [
                    2023,
                    8,
                    10,
                    13,
                    57,
                    37,
                    3,
                    222,
                    0
                ],
                "title": "Wireless Edge Content Broadcast via Integrated Terrestrial and\n  Non-terrestrial Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless Edge Content Broadcast via Integrated Terrestrial and\n  Non-terrestrial Networks"
                },
                "summary": "Non-terrestrial networks (NTN) have emerged as a transformative solution to\nbridge the digital divide and deliver essential services to remote and\nunderserved areas. In this context, low Earth orbit (LEO) satellite\nconstellations offer remarkable potential for efficient cache content broadcast\nin remote regions, thereby extending the reach of digital services. In this\npaper, we introduce a novel approach to optimize wireless edge content\nplacement using NTN. Despite wide coverage, the varying NTN transmission\ncapabilities must be carefully aligned with each content placement to maximize\nbroadcast efficiency. In this paper, we introduce a novel approach to optimize\nwireless edge content placement using NTN, positioning NTN as a complement to\nTN for achieving optimal content broadcasting. Specifically, we dynamically\nselect content for placement via NTN links. This selection is based on\npopularity and suitability for delivery through NTN, while considering the\norbital motion of LEO satellites. Our system-level case studies, based on a\npractical LEO constellation, demonstrate the significant improvement in\nplacement speed compared to existing methods, which neglect network mobility.\nWe also demonstrate that NTN links significantly outperform standalone wireless\nTN solutions, particularly in the early stages of content delivery. This\nadvantage is amplified when there is a higher correlation of content popularity\nacross geographical regions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-terrestrial networks (NTN) have emerged as a transformative solution to\nbridge the digital divide and deliver essential services to remote and\nunderserved areas. In this context, low Earth orbit (LEO) satellite\nconstellations offer remarkable potential for efficient cache content broadcast\nin remote regions, thereby extending the reach of digital services. In this\npaper, we introduce a novel approach to optimize wireless edge content\nplacement using NTN. Despite wide coverage, the varying NTN transmission\ncapabilities must be carefully aligned with each content placement to maximize\nbroadcast efficiency. In this paper, we introduce a novel approach to optimize\nwireless edge content placement using NTN, positioning NTN as a complement to\nTN for achieving optimal content broadcasting. Specifically, we dynamically\nselect content for placement via NTN links. This selection is based on\npopularity and suitability for delivery through NTN, while considering the\norbital motion of LEO satellites. Our system-level case studies, based on a\npractical LEO constellation, demonstrate the significant improvement in\nplacement speed compared to existing methods, which neglect network mobility.\nWe also demonstrate that NTN links significantly outperform standalone wireless\nTN solutions, particularly in the early stages of content delivery. This\nadvantage is amplified when there is a higher correlation of content popularity\nacross geographical regions."
                },
                "authors": [
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Giovanni Geraci"
                    },
                    {
                        "name": "Lingxiang Li"
                    },
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Q. S. Quek"
                },
                "author": "Tony Q. S. Quek",
                "arxiv_comment": "This work is expanded on our paper presented at IEEE Globecom 2023:\n  F. Wang, G. Geraci and T. Q. S. Quek, \"Optimizing Cache Content Placement in\n  Integrated Terrestrial and Non-terrestrial Networks,\" GLOBECOM 2023 - 2023\n  IEEE Global Communications Conference, Kuala Lumpur, Malaysia, 2023, pp.\n  6609-6614",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.05591v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.05591v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2410.16208v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16208v3",
                "updated": "2024-12-02T18:59:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    18,
                    59,
                    28,
                    0,
                    337,
                    0
                ],
                "published": "2024-10-21T17:11:21Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    17,
                    11,
                    21,
                    0,
                    295,
                    0
                ],
                "title": "Compute-Constrained Data Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute-Constrained Data Selection"
                },
                "summary": "Data selection can reduce the amount of training data needed to finetune\nLLMs; however, the efficacy of data selection scales directly with its compute.\nMotivated by the practical challenge of compute-constrained finetuning, we\nconsider the setting in which both the cost of selecting data and training are\nbudgeted for. We first formalize the problem of data selection with a\ncost-aware utility function, and model the data selection problem as trading\noff initial-selection cost for training gain. We run a comprehensive sweep of\nexperiments across multiple tasks, varying compute budget by scaling finetuning\ntokens, model sizes, and data selection compute. Interestingly we find that\nmany powerful data selection methods are almost never compute-optimal, and that\ncheaper data selection alternatives dominate both from a theoretical and\nempirical perspective. For compute-optimal training, we find that perplexity\nand gradient data selection require training-to-selection model size ratios of\n5x and 10x, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data selection can reduce the amount of training data needed to finetune\nLLMs; however, the efficacy of data selection scales directly with its compute.\nMotivated by the practical challenge of compute-constrained finetuning, we\nconsider the setting in which both the cost of selecting data and training are\nbudgeted for. We first formalize the problem of data selection with a\ncost-aware utility function, and model the data selection problem as trading\noff initial-selection cost for training gain. We run a comprehensive sweep of\nexperiments across multiple tasks, varying compute budget by scaling finetuning\ntokens, model sizes, and data selection compute. Interestingly we find that\nmany powerful data selection methods are almost never compute-optimal, and that\ncheaper data selection alternatives dominate both from a theoretical and\nempirical perspective. For compute-optimal training, we find that perplexity\nand gradient data selection require training-to-selection model size ratios of\n5x and 10x, respectively."
                },
                "authors": [
                    {
                        "name": "Junjie Oscar Yin"
                    },
                    {
                        "name": "Alexander M. Rush"
                    }
                ],
                "author_detail": {
                    "name": "Alexander M. Rush"
                },
                "author": "Alexander M. Rush",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16208v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16208v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17501v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17501v2",
                "updated": "2024-12-02T18:54:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    18,
                    54,
                    28,
                    0,
                    337,
                    0
                ],
                "published": "2024-11-26T15:13:06Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    13,
                    6,
                    1,
                    331,
                    0
                ],
                "title": "Inference Scaling fLaws: The Limits of LLM Resampling with Imperfect\n  Verifiers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference Scaling fLaws: The Limits of LLM Resampling with Imperfect\n  Verifiers"
                },
                "summary": "Recent research has generated hope that inference scaling could allow weaker\nlanguage models to match or exceed the accuracy of stronger models, such as by\nrepeatedly sampling solutions to a coding problem until it passes unit tests.\nThe central thesis of this paper is that there is no free lunch for inference\nscaling: indefinite accuracy improvement through resampling can only be\nrealized if the \"verifier\" (in this case, a set of unit tests) is perfect. When\nthe verifier is imperfect, as it almost always is in domains such as reasoning\nor coding (for example, unit tests have imperfect coverage), there is a nonzero\nprobability of false positives: incorrect solutions that pass the verifier.\nResampling cannot decrease this probability, so it imposes an upper bound to\nthe accuracy of resampling-based inference scaling even with an infinite\ncompute budget. We find that there is a very strong correlation between the\nmodel's single-sample accuracy (i.e. accuracy without unit tests) and its false\npositive rate on coding benchmarks HumanEval and MBPP, whose unit tests have\nlimited coverage. Therefore, no amount of inference scaling of weaker models\ncan enable them to match the single-sample accuracy of a sufficiently strong\nmodel (Fig. 1a). When we consider that false positives have a negative utility\ncompared to abstaining from producing a solution, it bends the inference\nscaling curve further downward. Empirically, we find that the optimal number of\nsamples can be less than 10 under realistic assumptions (Fig. 1b). Finally, we\nshow that beyond accuracy, false positives may have other undesirable\nqualities, such as poor adherence to coding style conventions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has generated hope that inference scaling could allow weaker\nlanguage models to match or exceed the accuracy of stronger models, such as by\nrepeatedly sampling solutions to a coding problem until it passes unit tests.\nThe central thesis of this paper is that there is no free lunch for inference\nscaling: indefinite accuracy improvement through resampling can only be\nrealized if the \"verifier\" (in this case, a set of unit tests) is perfect. When\nthe verifier is imperfect, as it almost always is in domains such as reasoning\nor coding (for example, unit tests have imperfect coverage), there is a nonzero\nprobability of false positives: incorrect solutions that pass the verifier.\nResampling cannot decrease this probability, so it imposes an upper bound to\nthe accuracy of resampling-based inference scaling even with an infinite\ncompute budget. We find that there is a very strong correlation between the\nmodel's single-sample accuracy (i.e. accuracy without unit tests) and its false\npositive rate on coding benchmarks HumanEval and MBPP, whose unit tests have\nlimited coverage. Therefore, no amount of inference scaling of weaker models\ncan enable them to match the single-sample accuracy of a sufficiently strong\nmodel (Fig. 1a). When we consider that false positives have a negative utility\ncompared to abstaining from producing a solution, it bends the inference\nscaling curve further downward. Empirically, we find that the optimal number of\nsamples can be less than 10 under realistic assumptions (Fig. 1b). Finally, we\nshow that beyond accuracy, false positives may have other undesirable\nqualities, such as poor adherence to coding style conventions."
                },
                "authors": [
                    {
                        "name": "Benedikt Stroebl"
                    },
                    {
                        "name": "Sayash Kapoor"
                    },
                    {
                        "name": "Arvind Narayanan"
                    }
                ],
                "author_detail": {
                    "name": "Arvind Narayanan"
                },
                "author": "Arvind Narayanan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17501v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17501v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05248v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05248v3",
                "updated": "2024-12-02T18:54:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    18,
                    54,
                    9,
                    0,
                    337,
                    0
                ],
                "published": "2023-12-08T18:55:40Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    18,
                    55,
                    40,
                    4,
                    342,
                    0
                ],
                "title": "Topology-Based Reconstruction Prevention for Decentralised Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topology-Based Reconstruction Prevention for Decentralised Learning"
                },
                "summary": "Decentralised learning has recently gained traction as an alternative to\nfederated learning in which both data and coordination are distributed. To\npreserve the confidentiality of users' data, decentralised learning relies on\ndifferential privacy, multi-party computation, or both. However, running\nmultiple privacy-preserving summations in sequence may allow adversaries to\nperform reconstruction attacks. Current reconstruction countermeasures either\ncannot trivially be adapted to the distributed setting, or add excessive\namounts of noise.\n  In this work, we first show that passive honest-but-curious adversaries can\ninfer other users' private data after several privacy-preserving summations.\nFor example, in subgraphs with 18 users, we show that only three passive\nhonest-but-curious adversaries succeed at reconstructing private data 11.0% of\nthe time, requiring an average of 8.8 summations per adversary. The success\nrate depends only on the adversaries' direct neighbourhood, and is independent\nof the size of the full network. We consider weak adversaries that do not\ncontrol the graph topology, cannot exploit the summation's inner workings, and\ndo not have auxiliary knowledge; and show that these adversaries can still\ninfer private data.\n  We analyse how reconstruction relates to topology and propose the first\ntopology-based decentralised defence against reconstruction attacks. We show\nthat reconstruction requires a number of adversaries linear in the length of\nthe network's shortest cycle. Consequently, exact attacks over\nprivacy-preserving summations are impossible in acyclic networks.\n  Our work is a stepping stone for a formal theory of topology-based\ndecentralised reconstruction defences. Such a theory would generalise our\ncountermeasure beyond summation, define confidentiality in terms of entropy,\nand describe the interactions with (topology-aware) differential privacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralised learning has recently gained traction as an alternative to\nfederated learning in which both data and coordination are distributed. To\npreserve the confidentiality of users' data, decentralised learning relies on\ndifferential privacy, multi-party computation, or both. However, running\nmultiple privacy-preserving summations in sequence may allow adversaries to\nperform reconstruction attacks. Current reconstruction countermeasures either\ncannot trivially be adapted to the distributed setting, or add excessive\namounts of noise.\n  In this work, we first show that passive honest-but-curious adversaries can\ninfer other users' private data after several privacy-preserving summations.\nFor example, in subgraphs with 18 users, we show that only three passive\nhonest-but-curious adversaries succeed at reconstructing private data 11.0% of\nthe time, requiring an average of 8.8 summations per adversary. The success\nrate depends only on the adversaries' direct neighbourhood, and is independent\nof the size of the full network. We consider weak adversaries that do not\ncontrol the graph topology, cannot exploit the summation's inner workings, and\ndo not have auxiliary knowledge; and show that these adversaries can still\ninfer private data.\n  We analyse how reconstruction relates to topology and propose the first\ntopology-based decentralised defence against reconstruction attacks. We show\nthat reconstruction requires a number of adversaries linear in the length of\nthe network's shortest cycle. Consequently, exact attacks over\nprivacy-preserving summations are impossible in acyclic networks.\n  Our work is a stepping stone for a formal theory of topology-based\ndecentralised reconstruction defences. Such a theory would generalise our\ncountermeasure beyond summation, define confidentiality in terms of entropy,\nand describe the interactions with (topology-aware) differential privacy."
                },
                "authors": [
                    {
                        "name": "Florine W. Dekker"
                    },
                    {
                        "name": "Zekeriya Erkin"
                    },
                    {
                        "name": "Mauro Conti"
                    }
                ],
                "author_detail": {
                    "name": "Mauro Conti"
                },
                "arxiv_affiliation": "Delft University of Technology, the Netherlands and",
                "author": "Mauro Conti",
                "arxiv_doi": "10.56553/popets-2025-0030",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.56553/popets-2025-0030",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.05248v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05248v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 19 figures, for associated experiment source code see\n  doi:10.4121/21572601.v2",
                "arxiv_journal_ref": "Proceedings on Privacy Enhancing Technologies 2025(1), 553-566",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.4; E.4; F.2.1; G.1.3; G.2.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24197v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24197v2",
                "updated": "2024-12-02T18:41:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    18,
                    41,
                    9,
                    0,
                    337,
                    0
                ],
                "published": "2024-10-31T17:55:02Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    55,
                    2,
                    3,
                    305,
                    0
                ],
                "title": "Generative modelling for mass-mapping with fast uncertainty\n  quantification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative modelling for mass-mapping with fast uncertainty\n  quantification"
                },
                "summary": "Understanding the nature of dark matter in the Universe is an important goal\nof modern cosmology. A key method for probing this distribution is via weak\ngravitational lensing mass-mapping - a challenging ill-posed inverse problem\nwhere one infers the convergence field from observed shear measurements.\nUpcoming stage IV surveys, such as those made by the Vera C. Rubin Observatory\nand Euclid satellite, will provide a greater quantity and precision of data for\nlensing analyses, necessitating high-fidelity mass-mapping methods that are\ncomputationally efficient and that also provide uncertainties for integration\ninto downstream cosmological analyses. In this work we introduce MMGAN, a novel\nmass-mapping method based on a regularised conditional generative adversarial\nnetwork (GAN) framework, which generates approximate posterior samples of the\nconvergence field given shear data. We adopt Wasserstein GANs to improve\ntraining stability and apply regularisation techniques to overcome mode\ncollapse, issues that otherwise are particularly acute for conditional GANs. We\ntrain and validate our model on a mock COSMOS-style dataset before applying it\nto true COSMOS survey data. Our approach significantly outperforms the\nKaiser-Squires technique and achieves similar reconstruction fidelity as\nalternative state-of-the-art deep learning approaches. Notably, while\nalternative approaches for generating samples from a learned posterior are slow\n(e.g. requiring $\\sim$10 GPU minutes per posterior sample), MMGAN can produce a\nhigh-quality convergence sample in less than a second.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the nature of dark matter in the Universe is an important goal\nof modern cosmology. A key method for probing this distribution is via weak\ngravitational lensing mass-mapping - a challenging ill-posed inverse problem\nwhere one infers the convergence field from observed shear measurements.\nUpcoming stage IV surveys, such as those made by the Vera C. Rubin Observatory\nand Euclid satellite, will provide a greater quantity and precision of data for\nlensing analyses, necessitating high-fidelity mass-mapping methods that are\ncomputationally efficient and that also provide uncertainties for integration\ninto downstream cosmological analyses. In this work we introduce MMGAN, a novel\nmass-mapping method based on a regularised conditional generative adversarial\nnetwork (GAN) framework, which generates approximate posterior samples of the\nconvergence field given shear data. We adopt Wasserstein GANs to improve\ntraining stability and apply regularisation techniques to overcome mode\ncollapse, issues that otherwise are particularly acute for conditional GANs. We\ntrain and validate our model on a mock COSMOS-style dataset before applying it\nto true COSMOS survey data. Our approach significantly outperforms the\nKaiser-Squires technique and achieves similar reconstruction fidelity as\nalternative state-of-the-art deep learning approaches. Notably, while\nalternative approaches for generating samples from a learned posterior are slow\n(e.g. requiring $\\sim$10 GPU minutes per posterior sample), MMGAN can produce a\nhigh-quality convergence sample in less than a second."
                },
                "authors": [
                    {
                        "name": "Jessica J. Whitney"
                    },
                    {
                        "name": "TobÃ­as I. Liaudat"
                    },
                    {
                        "name": "Matthew A. Price"
                    },
                    {
                        "name": "Matthijs Mars"
                    },
                    {
                        "name": "Jason D. McEwen"
                    }
                ],
                "author_detail": {
                    "name": "Jason D. McEwen"
                },
                "author": "Jason D. McEwen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24197v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24197v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02524v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02524v2",
                "updated": "2024-12-02T18:22:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    18,
                    22,
                    0,
                    0,
                    337,
                    0
                ],
                "published": "2024-04-03T07:26:15Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    7,
                    26,
                    15,
                    2,
                    94,
                    0
                ],
                "title": "Versatile Behavior Diffusion for Generalized Traffic Agent Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Versatile Behavior Diffusion for Generalized Traffic Agent Simulation"
                },
                "summary": "Existing traffic simulation models often fail to capture the complexities of\nreal-world scenarios, limiting the effective evaluation of autonomous driving\nsystems. We introduce Versatile Behavior Diffusion (VBD), a novel traffic\nscenario generation framework that utilizes diffusion generative models to\npredict scene-consistent and controllable multi-agent interactions in\nclosed-loop settings. VBD achieves state-of-the-art performance on the Waymo\nSim Agents Benchmark and can effectively produce realistic and coherent traffic\nbehaviors with complex agent interactions under diverse environmental\nconditions. Furthermore, VBD offers inference-time scenario editing through\nmulti-step refinement guided by behavior priors and model-based optimization\nobjectives. This capability allows for controllable multi-agent behavior\ngeneration, accommodating a wide range of user requirements across various\ntraffic simulation applications. Despite being trained solely on publicly\navailable datasets representing typical traffic conditions, we introduce\nconflict-prior and game-theoretic guidance approaches that enable the creation\nof interactive, long-tail safety-critical scenarios, which is essential for\ncomprehensive testing and validation of autonomous vehicles. Lastly, we provide\nin-depth insights into effective training and inference strategies for\ndiffusion-based traffic scenario generation models, highlighting best practices\nand common pitfalls. Our work significantly advances the ability to simulate\ncomplex traffic environments, offering a powerful tool for the development and\nassessment of autonomous driving technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing traffic simulation models often fail to capture the complexities of\nreal-world scenarios, limiting the effective evaluation of autonomous driving\nsystems. We introduce Versatile Behavior Diffusion (VBD), a novel traffic\nscenario generation framework that utilizes diffusion generative models to\npredict scene-consistent and controllable multi-agent interactions in\nclosed-loop settings. VBD achieves state-of-the-art performance on the Waymo\nSim Agents Benchmark and can effectively produce realistic and coherent traffic\nbehaviors with complex agent interactions under diverse environmental\nconditions. Furthermore, VBD offers inference-time scenario editing through\nmulti-step refinement guided by behavior priors and model-based optimization\nobjectives. This capability allows for controllable multi-agent behavior\ngeneration, accommodating a wide range of user requirements across various\ntraffic simulation applications. Despite being trained solely on publicly\navailable datasets representing typical traffic conditions, we introduce\nconflict-prior and game-theoretic guidance approaches that enable the creation\nof interactive, long-tail safety-critical scenarios, which is essential for\ncomprehensive testing and validation of autonomous vehicles. Lastly, we provide\nin-depth insights into effective training and inference strategies for\ndiffusion-based traffic scenario generation models, highlighting best practices\nand common pitfalls. Our work significantly advances the ability to simulate\ncomplex traffic environments, offering a powerful tool for the development and\nassessment of autonomous driving technologies."
                },
                "authors": [
                    {
                        "name": "Zhiyu Huang"
                    },
                    {
                        "name": "Zixu Zhang"
                    },
                    {
                        "name": "Ameya Vaidya"
                    },
                    {
                        "name": "Yuxiao Chen"
                    },
                    {
                        "name": "Chen Lv"
                    },
                    {
                        "name": "Jaime FernÃ¡ndez Fisac"
                    }
                ],
                "author_detail": {
                    "name": "Jaime FernÃ¡ndez Fisac"
                },
                "author": "Jaime FernÃ¡ndez Fisac",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02524v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02524v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17593v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17593v3",
                "updated": "2024-12-02T17:43:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    17,
                    43,
                    20,
                    0,
                    337,
                    0
                ],
                "published": "2024-11-26T17:01:27Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    1,
                    27,
                    1,
                    331,
                    0
                ],
                "title": "What Differentiates Educational Literature? A Multimodal Fusion Approach\n  of Transformers and Computational Linguistics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Differentiates Educational Literature? A Multimodal Fusion Approach\n  of Transformers and Computational Linguistics"
                },
                "summary": "The integration of new literature into the English curriculum remains a\nchallenge since educators often lack scalable tools to rapidly evaluate\nreadability and adapt texts for diverse classroom needs. This study proposes to\naddress this gap through a multimodal approach that combines transformer-based\ntext classification with linguistic feature analysis to align texts with UK Key\nStages. Eight state-of-the-art Transformers were fine-tuned on segmented text\ndata, with BERT achieving the highest unimodal F1 score of 0.75. In parallel,\n500 deep neural network topologies were searched for the classification of\nlinguistic characteristics, achieving an F1 score of 0.392. The fusion of these\nmodalities shows a significant improvement, with every multimodal approach\noutperforming all unimodal models. In particular, the ELECTRA Transformer fused\nwith the neural network achieved an F1 score of 0.996. Unimodal and multimodal\napproaches are shown to have statistically significant differences in all\nvalidation metrics (accuracy, precision, recall, F1 score) except for inference\ntime. The proposed approach is finally encapsulated in a stakeholder-facing web\napplication, providing non-technical stakeholder access to real-time insights\non text complexity, reading difficulty, curriculum alignment, and\nrecommendations for learning age range. The application empowers data-driven\ndecision making and reduces manual workload by integrating AI-based\nrecommendations into lesson planning for English literature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of new literature into the English curriculum remains a\nchallenge since educators often lack scalable tools to rapidly evaluate\nreadability and adapt texts for diverse classroom needs. This study proposes to\naddress this gap through a multimodal approach that combines transformer-based\ntext classification with linguistic feature analysis to align texts with UK Key\nStages. Eight state-of-the-art Transformers were fine-tuned on segmented text\ndata, with BERT achieving the highest unimodal F1 score of 0.75. In parallel,\n500 deep neural network topologies were searched for the classification of\nlinguistic characteristics, achieving an F1 score of 0.392. The fusion of these\nmodalities shows a significant improvement, with every multimodal approach\noutperforming all unimodal models. In particular, the ELECTRA Transformer fused\nwith the neural network achieved an F1 score of 0.996. Unimodal and multimodal\napproaches are shown to have statistically significant differences in all\nvalidation metrics (accuracy, precision, recall, F1 score) except for inference\ntime. The proposed approach is finally encapsulated in a stakeholder-facing web\napplication, providing non-technical stakeholder access to real-time insights\non text complexity, reading difficulty, curriculum alignment, and\nrecommendations for learning age range. The application empowers data-driven\ndecision making and reduces manual workload by integrating AI-based\nrecommendations into lesson planning for English literature."
                },
                "authors": [
                    {
                        "name": "Jordan J. Bird"
                    }
                ],
                "author_detail": {
                    "name": "Jordan J. Bird"
                },
                "author": "Jordan J. Bird",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17593v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17593v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.15257v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.15257v2",
                "updated": "2024-12-02T17:31:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    17,
                    31,
                    47,
                    0,
                    337,
                    0
                ],
                "published": "2023-11-26T10:06:27Z",
                "published_parsed": [
                    2023,
                    11,
                    26,
                    10,
                    6,
                    27,
                    6,
                    330,
                    0
                ],
                "title": "Career Modeling with Missing Data and Traces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Career Modeling with Missing Data and Traces"
                },
                "summary": "Many social scientists study the career trajectories of populations of\ninterest, such as economic and administrative elites. However, data to document\nsuch processes are rarely completely available, which motivates the adoption of\ninference tools that can account for large numbers of missing values. Taking\nthe example of public-private paths of elite civil servants in France, we\nintroduce binary Markov switching models to perform Bayesian data augmentation.\nOur procedure relies on two data sources: (1) detailed observations of a small\nnumber of individual trajectories, and (2) less informative ``traces'' left by\nall individuals, which we model for imputation of missing data. An advantage of\nthis model class is that it maintains the properties of hidden Markov models\nand enables a tailored sampler to target the posterior, while allowing for\nvarying parameters across individuals and time. We provide two applied studies\nwhich demonstrate this can be used to properly test substantive hypotheses, and\nexpand the social scientific literature in various ways. We notably show that\nthe rate at which ENA graduates exit the French public sector has not increased\nsince 1990, but that the rate at which they come back has increased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many social scientists study the career trajectories of populations of\ninterest, such as economic and administrative elites. However, data to document\nsuch processes are rarely completely available, which motivates the adoption of\ninference tools that can account for large numbers of missing values. Taking\nthe example of public-private paths of elite civil servants in France, we\nintroduce binary Markov switching models to perform Bayesian data augmentation.\nOur procedure relies on two data sources: (1) detailed observations of a small\nnumber of individual trajectories, and (2) less informative ``traces'' left by\nall individuals, which we model for imputation of missing data. An advantage of\nthis model class is that it maintains the properties of hidden Markov models\nand enables a tailored sampler to target the posterior, while allowing for\nvarying parameters across individuals and time. We provide two applied studies\nwhich demonstrate this can be used to properly test substantive hypotheses, and\nexpand the social scientific literature in various ways. We notably show that\nthe rate at which ENA graduates exit the French public sector has not increased\nsince 1990, but that the rate at which they come back has increased."
                },
                "authors": [
                    {
                        "name": "ThÃ©o Voldoire"
                    },
                    {
                        "name": "Robin J. Ryder"
                    },
                    {
                        "name": "Ryan Lahfa"
                    }
                ],
                "author_detail": {
                    "name": "Ryan Lahfa"
                },
                "author": "Ryan Lahfa",
                "arxiv_comment": "33 pages, 5 figures and 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.15257v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.15257v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62P25 (Primary) 62F15, 62M05 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01493v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01493v3",
                "updated": "2024-12-02T17:10:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    17,
                    10,
                    34,
                    0,
                    337,
                    0
                ],
                "published": "2024-06-03T16:20:24Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    16,
                    20,
                    24,
                    0,
                    155,
                    0
                ],
                "title": "Learning Temporally Consistent Video Depth from Video Diffusion Priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Temporally Consistent Video Depth from Video Diffusion Priors"
                },
                "summary": "This work addresses the challenge of streamed video depth estimation, which\nexpects not only per-frame accuracy but, more importantly, cross-frame\nconsistency. We argue that sharing contextual information between frames or\nclips is pivotal in fostering temporal consistency. Thus, instead of directly\ndeveloping a depth estimator from scratch, we reformulate this predictive task\ninto a conditional generation problem to provide contextual information within\na clip and across clips. Specifically, we propose a consistent context-aware\ntraining and inference strategy for arbitrarily long videos to provide\ncross-clip context. We sample independent noise levels for each frame within a\nclip during training while using a sliding window strategy and initializing\noverlapping frames with previously predicted frames without adding noise.\nMoreover, we design an effective training strategy to provide context within a\nclip. Extensive experimental results validate our design choices and\ndemonstrate the superiority of our approach, dubbed ChronoDepth. Project page:\nhttps://xdimlab.github.io/ChronoDepth/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work addresses the challenge of streamed video depth estimation, which\nexpects not only per-frame accuracy but, more importantly, cross-frame\nconsistency. We argue that sharing contextual information between frames or\nclips is pivotal in fostering temporal consistency. Thus, instead of directly\ndeveloping a depth estimator from scratch, we reformulate this predictive task\ninto a conditional generation problem to provide contextual information within\na clip and across clips. Specifically, we propose a consistent context-aware\ntraining and inference strategy for arbitrarily long videos to provide\ncross-clip context. We sample independent noise levels for each frame within a\nclip during training while using a sliding window strategy and initializing\noverlapping frames with previously predicted frames without adding noise.\nMoreover, we design an effective training strategy to provide context within a\nclip. Extensive experimental results validate our design choices and\ndemonstrate the superiority of our approach, dubbed ChronoDepth. Project page:\nhttps://xdimlab.github.io/ChronoDepth/."
                },
                "authors": [
                    {
                        "name": "Jiahao Shao"
                    },
                    {
                        "name": "Yuanbo Yang"
                    },
                    {
                        "name": "Hongyu Zhou"
                    },
                    {
                        "name": "Youmin Zhang"
                    },
                    {
                        "name": "Yujun Shen"
                    },
                    {
                        "name": "Vitor Guizilini"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Matteo Poggi"
                    },
                    {
                        "name": "Yiyi Liao"
                    }
                ],
                "author_detail": {
                    "name": "Yiyi Liao"
                },
                "author": "Yiyi Liao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01493v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01493v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.09110v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.09110v2",
                "updated": "2024-12-02T16:30:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    16,
                    30,
                    40,
                    0,
                    337,
                    0
                ],
                "published": "2024-01-17T10:23:51Z",
                "published_parsed": [
                    2024,
                    1,
                    17,
                    10,
                    23,
                    51,
                    2,
                    17,
                    0
                ],
                "title": "Global and Local Error-Tolerant Decentralized State Estimation under\n  Partially Ordered Observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Global and Local Error-Tolerant Decentralized State Estimation under\n  Partially Ordered Observations"
                },
                "summary": "We investigate decentralized state estimation for a discrete event system in\na setting where the information received at a coordinator may be corrupted or\ntampered by a malicious attacker. Specifically, a system is observed by a set\nof (local) observation sites (OSs) which occasionally send their recorded\nsequences of observations to the coordinator that is in charge of estimating\nthe system state. The malfunctions and attacks, referred to as errors in this\npaper, include symbol deletions, insertions and replacements, each of which\nbears a positive cost. Two types of errors, global errors and local errors, are\nproposed to describe the impact of errors on decentralized information\nprocessing. Global errors occur when all OSs record the same error, while local\nerrors occur when different OSs record different errors. Distinguishing these\ntypes of errors is important for a proper design of decentralized information\nprocessing (so as to be more resilient and better equipped to handle attacks\nand failures). For each type of error, we propose two methods to efficiently\nperform state estimation: one based on appropriately modifying the original\nsystem and the other based on inferring the matching behavior of the original\nsystem. For each method, we adopt an estimation-by-release methodology to\ndesign an algorithm for constructing a corresponding synchronizer for state\nestimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate decentralized state estimation for a discrete event system in\na setting where the information received at a coordinator may be corrupted or\ntampered by a malicious attacker. Specifically, a system is observed by a set\nof (local) observation sites (OSs) which occasionally send their recorded\nsequences of observations to the coordinator that is in charge of estimating\nthe system state. The malfunctions and attacks, referred to as errors in this\npaper, include symbol deletions, insertions and replacements, each of which\nbears a positive cost. Two types of errors, global errors and local errors, are\nproposed to describe the impact of errors on decentralized information\nprocessing. Global errors occur when all OSs record the same error, while local\nerrors occur when different OSs record different errors. Distinguishing these\ntypes of errors is important for a proper design of decentralized information\nprocessing (so as to be more resilient and better equipped to handle attacks\nand failures). For each type of error, we propose two methods to efficiently\nperform state estimation: one based on appropriately modifying the original\nsystem and the other based on inferring the matching behavior of the original\nsystem. For each method, we adopt an estimation-by-release methodology to\ndesign an algorithm for constructing a corresponding synchronizer for state\nestimation."
                },
                "authors": [
                    {
                        "name": "Dajiang Sun"
                    },
                    {
                        "name": "Christoforos N. Hadjicostis"
                    },
                    {
                        "name": "Zhiwu Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhiwu Li"
                },
                "author": "Zhiwu Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.09110v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.09110v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19839v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19839v3",
                "updated": "2024-12-02T16:27:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    16,
                    27,
                    16,
                    0,
                    337,
                    0
                ],
                "published": "2024-09-30T00:41:51Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    0,
                    41,
                    51,
                    0,
                    274,
                    0
                ],
                "title": "ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities"
                },
                "summary": "Forecasts of future events are essential inputs into informed\ndecision-making. Machine learning (ML) systems have the potential to deliver\nforecasts at scale, but there is no framework for evaluating the accuracy of ML\nsystems on a standardized set of forecasting questions. To address this gap, we\nintroduce ForecastBench: a dynamic benchmark that evaluates the accuracy of ML\nsystems on an automatically generated and regularly updated set of 1,000\nforecasting questions. To avoid any possibility of data leakage, ForecastBench\nis comprised solely of questions about future events that have no known answer\nat the time of submission. We quantify the capabilities of current ML systems\nby collecting forecasts from expert (human) forecasters, the general public,\nand LLMs on a random subset of questions from the benchmark ($N=200$). While\nLLMs have achieved super-human performance on many benchmarks, they perform\nless well here: expert forecasters outperform the top-performing LLM (p-value\n$<0.01$). We display system and human scores in a public leaderboard at\nwww.forecastbench.org.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasts of future events are essential inputs into informed\ndecision-making. Machine learning (ML) systems have the potential to deliver\nforecasts at scale, but there is no framework for evaluating the accuracy of ML\nsystems on a standardized set of forecasting questions. To address this gap, we\nintroduce ForecastBench: a dynamic benchmark that evaluates the accuracy of ML\nsystems on an automatically generated and regularly updated set of 1,000\nforecasting questions. To avoid any possibility of data leakage, ForecastBench\nis comprised solely of questions about future events that have no known answer\nat the time of submission. We quantify the capabilities of current ML systems\nby collecting forecasts from expert (human) forecasters, the general public,\nand LLMs on a random subset of questions from the benchmark ($N=200$). While\nLLMs have achieved super-human performance on many benchmarks, they perform\nless well here: expert forecasters outperform the top-performing LLM (p-value\n$<0.01$). We display system and human scores in a public leaderboard at\nwww.forecastbench.org."
                },
                "authors": [
                    {
                        "name": "Ezra Karger"
                    },
                    {
                        "name": "Houtan Bastani"
                    },
                    {
                        "name": "Chen Yueh-Han"
                    },
                    {
                        "name": "Zachary Jacobs"
                    },
                    {
                        "name": "Danny Halawi"
                    },
                    {
                        "name": "Fred Zhang"
                    },
                    {
                        "name": "Philip E. Tetlock"
                    }
                ],
                "author_detail": {
                    "name": "Philip E. Tetlock"
                },
                "author": "Philip E. Tetlock",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19839v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19839v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14539v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14539v3",
                "updated": "2024-12-02T16:26:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    16,
                    26,
                    57,
                    0,
                    337,
                    0
                ],
                "published": "2024-06-20T17:49:11Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    17,
                    49,
                    11,
                    3,
                    172,
                    0
                ],
                "title": "Invertible Consistency Distillation for Text-Guided Image Editing in\n  Around 7 Steps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Invertible Consistency Distillation for Text-Guided Image Editing in\n  Around 7 Steps"
                },
                "summary": "Diffusion distillation represents a highly promising direction for achieving\nfaithful text-to-image generation in a few sampling steps. However, despite\nrecent successes, existing distilled models still do not provide the full\nspectrum of diffusion abilities, such as real image inversion, which enables\nmany precise image manipulation methods. This work aims to enrich distilled\ntext-to-image diffusion models with the ability to effectively encode real\nimages into their latent space. To this end, we introduce invertible\nConsistency Distillation (iCD), a generalized consistency distillation\nframework that facilitates both high-quality image synthesis and accurate image\nencoding in only 3-4 inference steps. Though the inversion problem for\ntext-to-image diffusion models gets exacerbated by high classifier-free\nguidance scales, we notice that dynamic guidance significantly reduces\nreconstruction errors without noticeable degradation in generation performance.\nAs a result, we demonstrate that iCD equipped with dynamic guidance may serve\nas a highly effective tool for zero-shot text-guided image editing, competing\nwith more expensive state-of-the-art alternatives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion distillation represents a highly promising direction for achieving\nfaithful text-to-image generation in a few sampling steps. However, despite\nrecent successes, existing distilled models still do not provide the full\nspectrum of diffusion abilities, such as real image inversion, which enables\nmany precise image manipulation methods. This work aims to enrich distilled\ntext-to-image diffusion models with the ability to effectively encode real\nimages into their latent space. To this end, we introduce invertible\nConsistency Distillation (iCD), a generalized consistency distillation\nframework that facilitates both high-quality image synthesis and accurate image\nencoding in only 3-4 inference steps. Though the inversion problem for\ntext-to-image diffusion models gets exacerbated by high classifier-free\nguidance scales, we notice that dynamic guidance significantly reduces\nreconstruction errors without noticeable degradation in generation performance.\nAs a result, we demonstrate that iCD equipped with dynamic guidance may serve\nas a highly effective tool for zero-shot text-guided image editing, competing\nwith more expensive state-of-the-art alternatives."
                },
                "authors": [
                    {
                        "name": "Nikita Starodubcev"
                    },
                    {
                        "name": "Mikhail Khoroshikh"
                    },
                    {
                        "name": "Artem Babenko"
                    },
                    {
                        "name": "Dmitry Baranchuk"
                    }
                ],
                "author_detail": {
                    "name": "Dmitry Baranchuk"
                },
                "author": "Dmitry Baranchuk",
                "arxiv_comment": "Project page: https://yandex-research.github.io/invertible-cd/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14539v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14539v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17607v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17607v2",
                "updated": "2024-12-02T16:13:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    16,
                    13,
                    24,
                    0,
                    337,
                    0
                ],
                "published": "2024-11-26T17:19:09Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    19,
                    9,
                    1,
                    331,
                    0
                ],
                "title": "Scaling Speech-Text Pre-training with Synthetic Interleaved Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Speech-Text Pre-training with Synthetic Interleaved Data"
                },
                "summary": "Speech language models (SpeechLMs) accept speech input and produce speech\noutput, allowing for more natural human-computer interaction compared to\ntext-based large language models (LLMs). Traditional approaches for developing\nSpeechLMs are constrained by the limited availability of unsupervised speech\ndata and parallel speech-text data, which are significantly less abundant than\ntext pre-training data, thereby limiting their scalability as LLMs. We propose\na novel approach to scaling speech-text pre-training by leveraging large-scale\nsynthetic interleaved data derived from text corpora, eliminating the need for\nparallel speech-text datasets. Our method efficiently constructs speech-text\ninterleaved data by sampling text spans from existing text corpora and\nsynthesizing corresponding speech spans using a text-to-token model, bypassing\nthe need to generate actual speech. We also employ a supervised speech\ntokenizer derived from an automatic speech recognition (ASR) model by\nincorporating a vector-quantized bottleneck into the encoder. This supervised\ntraining approach results in discrete speech tokens with strong semantic\npreservation even at lower frame rates (e.g. 12.5Hz), while still maintaining\nspeech reconstruction quality. Starting from a pre-trained language model and\nscaling our pre-training to 1 trillion tokens (with 600B synthetic interleaved\nspeech-text data), we achieve state-of-the-art performance in speech language\nmodeling and spoken question answering, improving performance on spoken\nquestions tasks from the previous SOTA of 13% (Moshi) to 31%. We further\ndemonstrate that by fine-tuning the pre-trained model with speech dialogue\ndata, we can develop an end-to-end spoken chatbot that achieves competitive\nperformance comparable to existing baselines in both conversational abilities\nand speech quality, even operating exclusively in the speech domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech language models (SpeechLMs) accept speech input and produce speech\noutput, allowing for more natural human-computer interaction compared to\ntext-based large language models (LLMs). Traditional approaches for developing\nSpeechLMs are constrained by the limited availability of unsupervised speech\ndata and parallel speech-text data, which are significantly less abundant than\ntext pre-training data, thereby limiting their scalability as LLMs. We propose\na novel approach to scaling speech-text pre-training by leveraging large-scale\nsynthetic interleaved data derived from text corpora, eliminating the need for\nparallel speech-text datasets. Our method efficiently constructs speech-text\ninterleaved data by sampling text spans from existing text corpora and\nsynthesizing corresponding speech spans using a text-to-token model, bypassing\nthe need to generate actual speech. We also employ a supervised speech\ntokenizer derived from an automatic speech recognition (ASR) model by\nincorporating a vector-quantized bottleneck into the encoder. This supervised\ntraining approach results in discrete speech tokens with strong semantic\npreservation even at lower frame rates (e.g. 12.5Hz), while still maintaining\nspeech reconstruction quality. Starting from a pre-trained language model and\nscaling our pre-training to 1 trillion tokens (with 600B synthetic interleaved\nspeech-text data), we achieve state-of-the-art performance in speech language\nmodeling and spoken question answering, improving performance on spoken\nquestions tasks from the previous SOTA of 13% (Moshi) to 31%. We further\ndemonstrate that by fine-tuning the pre-trained model with speech dialogue\ndata, we can develop an end-to-end spoken chatbot that achieves competitive\nperformance comparable to existing baselines in both conversational abilities\nand speech quality, even operating exclusively in the speech domain."
                },
                "authors": [
                    {
                        "name": "Aohan Zeng"
                    },
                    {
                        "name": "Zhengxiao Du"
                    },
                    {
                        "name": "Mingdao Liu"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Shengmin Jiang"
                    },
                    {
                        "name": "Yuxiao Dong"
                    },
                    {
                        "name": "Jie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Tang"
                },
                "author": "Jie Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17607v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17607v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16867v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16867v2",
                "updated": "2024-12-02T15:42:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    15,
                    42,
                    53,
                    0,
                    337,
                    0
                ],
                "published": "2024-07-23T22:23:47Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    22,
                    23,
                    47,
                    1,
                    205,
                    0
                ],
                "title": "From Text to Insight: Large Language Models for Materials Science Data\n  Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Text to Insight: Large Language Models for Materials Science Data\n  Extraction"
                },
                "summary": "The vast majority of materials science knowledge exists in unstructured\nnatural language, yet structured data is crucial for innovative and systematic\nmaterials design. Traditionally, the field has relied on manual curation and\npartial automation for data extraction for specific use cases. The advent of\nlarge language models (LLMs) represents a significant shift, potentially\nenabling efficient extraction of structured, actionable data from unstructured\ntext by non-experts. While applying LLMs to materials science data extraction\npresents unique challenges, domain knowledge offers opportunities to guide and\nvalidate LLM outputs. This review provides a comprehensive overview of\nLLM-based structured data extraction in materials science, synthesizing current\nknowledge and outlining future directions. We address the lack of standardized\nguidelines and present frameworks for leveraging the synergy between LLMs and\nmaterials science expertise. This work serves as a foundational resource for\nresearchers aiming to harness LLMs for data-driven materials research. The\ninsights presented here could significantly enhance how researchers across\ndisciplines access and utilize scientific information, potentially accelerating\nthe development of novel materials for critical societal needs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The vast majority of materials science knowledge exists in unstructured\nnatural language, yet structured data is crucial for innovative and systematic\nmaterials design. Traditionally, the field has relied on manual curation and\npartial automation for data extraction for specific use cases. The advent of\nlarge language models (LLMs) represents a significant shift, potentially\nenabling efficient extraction of structured, actionable data from unstructured\ntext by non-experts. While applying LLMs to materials science data extraction\npresents unique challenges, domain knowledge offers opportunities to guide and\nvalidate LLM outputs. This review provides a comprehensive overview of\nLLM-based structured data extraction in materials science, synthesizing current\nknowledge and outlining future directions. We address the lack of standardized\nguidelines and present frameworks for leveraging the synergy between LLMs and\nmaterials science expertise. This work serves as a foundational resource for\nresearchers aiming to harness LLMs for data-driven materials research. The\ninsights presented here could significantly enhance how researchers across\ndisciplines access and utilize scientific information, potentially accelerating\nthe development of novel materials for critical societal needs."
                },
                "authors": [
                    {
                        "name": "Mara Schilling-Wilhelmi"
                    },
                    {
                        "name": "MartiÃ±o RÃ­os-GarcÃ­a"
                    },
                    {
                        "name": "Sherjeel Shabih"
                    },
                    {
                        "name": "MarÃ­a Victoria Gil"
                    },
                    {
                        "name": "Santiago Miret"
                    },
                    {
                        "name": "Christoph T. Koch"
                    },
                    {
                        "name": "JosÃ© A. MÃ¡rquez"
                    },
                    {
                        "name": "Kevin Maik Jablonka"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Maik Jablonka"
                },
                "author": "Kevin Maik Jablonka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16867v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16867v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.10886v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.10886v2",
                "updated": "2024-12-02T15:40:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    15,
                    40,
                    45,
                    0,
                    337,
                    0
                ],
                "published": "2024-02-16T18:43:10Z",
                "published_parsed": [
                    2024,
                    2,
                    16,
                    18,
                    43,
                    10,
                    4,
                    47,
                    0
                ],
                "title": "Reviewer2: Optimizing Review Generation Through Prompt Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reviewer2: Optimizing Review Generation Through Prompt Generation"
                },
                "summary": "Recent developments in LLMs offer new opportunities for assisting authors in\nimproving their work. In this paper, we envision a use case where authors can\nreceive LLM-generated reviews that uncover weak points in the current draft.\nWhile initial methods for automated review generation already exist, these\nmethods tend to produce reviews that lack detail, and they do not cover the\nrange of opinions that human reviewers produce. To address this shortcoming, we\npropose an efficient two-stage review generation framework called Reviewer2.\nUnlike prior work, this approach explicitly models the distribution of possible\naspects that the review may address. We show that this leads to more detailed\nreviews that better cover the range of aspects that human reviewers identify in\nthe draft. As part of the research, we generate a large-scale review dataset of\n27k papers and 99k reviews that we annotate with aspect prompts, which we make\navailable as a resource for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent developments in LLMs offer new opportunities for assisting authors in\nimproving their work. In this paper, we envision a use case where authors can\nreceive LLM-generated reviews that uncover weak points in the current draft.\nWhile initial methods for automated review generation already exist, these\nmethods tend to produce reviews that lack detail, and they do not cover the\nrange of opinions that human reviewers produce. To address this shortcoming, we\npropose an efficient two-stage review generation framework called Reviewer2.\nUnlike prior work, this approach explicitly models the distribution of possible\naspects that the review may address. We show that this leads to more detailed\nreviews that better cover the range of aspects that human reviewers identify in\nthe draft. As part of the research, we generate a large-scale review dataset of\n27k papers and 99k reviews that we annotate with aspect prompts, which we make\navailable as a resource for future research."
                },
                "authors": [
                    {
                        "name": "Zhaolin Gao"
                    },
                    {
                        "name": "KiantÃ© Brantley"
                    },
                    {
                        "name": "Thorsten Joachims"
                    }
                ],
                "author_detail": {
                    "name": "Thorsten Joachims"
                },
                "author": "Thorsten Joachims",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.10886v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.10886v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08751v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08751v2",
                "updated": "2024-12-02T15:16:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    15,
                    16,
                    3,
                    0,
                    337,
                    0
                ],
                "published": "2024-06-27T13:47:06Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    13,
                    47,
                    6,
                    3,
                    179,
                    0
                ],
                "title": "Latent Diffusion for Neural Spiking Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent Diffusion for Neural Spiking Data"
                },
                "summary": "Modern datasets in neuroscience enable unprecedented inquiries into the\nrelationship between complex behaviors and the activity of many simultaneously\nrecorded neurons. While latent variable models can successfully extract\nlow-dimensional embeddings from such recordings, using them to generate\nrealistic spiking data, especially in a behavior-dependent manner, still poses\na challenge. Here, we present Latent Diffusion for Neural Spiking data (LDNS),\na diffusion-based generative model with a low-dimensional latent space: LDNS\nemploys an autoencoder with structured state-space (S4) layers to project\ndiscrete high-dimensional spiking data into continuous time-aligned latents. On\nthese inferred latents, we train expressive (conditional) diffusion models,\nenabling us to sample neural activity with realistic single-neuron and\npopulation spiking statistics. We validate LDNS on synthetic data, accurately\nrecovering latent structure, firing rates, and spiking statistics. Next, we\ndemonstrate its flexibility by generating variable-length data that mimics\nhuman cortical activity during attempted speech. We show how to equip LDNS with\nan expressive observation model that accounts for single-neuron dynamics not\nmediated by the latent state, further increasing the realism of generated\nsamples. Finally, conditional LDNS trained on motor cortical activity during\ndiverse reaching behaviors can generate realistic spiking data given reach\ndirection or unseen reach trajectories. In summary, LDNS simultaneously enables\ninference of low-dimensional latents and realistic conditional generation of\nneural spiking datasets, opening up further possibilities for simulating\nexperimentally testable hypotheses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern datasets in neuroscience enable unprecedented inquiries into the\nrelationship between complex behaviors and the activity of many simultaneously\nrecorded neurons. While latent variable models can successfully extract\nlow-dimensional embeddings from such recordings, using them to generate\nrealistic spiking data, especially in a behavior-dependent manner, still poses\na challenge. Here, we present Latent Diffusion for Neural Spiking data (LDNS),\na diffusion-based generative model with a low-dimensional latent space: LDNS\nemploys an autoencoder with structured state-space (S4) layers to project\ndiscrete high-dimensional spiking data into continuous time-aligned latents. On\nthese inferred latents, we train expressive (conditional) diffusion models,\nenabling us to sample neural activity with realistic single-neuron and\npopulation spiking statistics. We validate LDNS on synthetic data, accurately\nrecovering latent structure, firing rates, and spiking statistics. Next, we\ndemonstrate its flexibility by generating variable-length data that mimics\nhuman cortical activity during attempted speech. We show how to equip LDNS with\nan expressive observation model that accounts for single-neuron dynamics not\nmediated by the latent state, further increasing the realism of generated\nsamples. Finally, conditional LDNS trained on motor cortical activity during\ndiverse reaching behaviors can generate realistic spiking data given reach\ndirection or unseen reach trajectories. In summary, LDNS simultaneously enables\ninference of low-dimensional latents and realistic conditional generation of\nneural spiking datasets, opening up further possibilities for simulating\nexperimentally testable hypotheses."
                },
                "authors": [
                    {
                        "name": "Jaivardhan Kapoor"
                    },
                    {
                        "name": "Auguste Schulz"
                    },
                    {
                        "name": "Julius Vetter"
                    },
                    {
                        "name": "Felix Pei"
                    },
                    {
                        "name": "Richard Gao"
                    },
                    {
                        "name": "Jakob H. Macke"
                    }
                ],
                "author_detail": {
                    "name": "Jakob H. Macke"
                },
                "author": "Jakob H. Macke",
                "arxiv_comment": "38th Conference on Neural Information Processing Systems (NeurIPS\n  2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08751v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08751v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.06390v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.06390v2",
                "updated": "2024-12-02T15:04:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    15,
                    4,
                    25,
                    0,
                    337,
                    0
                ],
                "published": "2024-02-09T13:11:57Z",
                "published_parsed": [
                    2024,
                    2,
                    9,
                    13,
                    11,
                    57,
                    4,
                    40,
                    0
                ],
                "title": "Deepfake for the Good: Generating Avatars through Face-Swapping with\n  Implicit Deepfake Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deepfake for the Good: Generating Avatars through Face-Swapping with\n  Implicit Deepfake Generation"
                },
                "summary": "Numerous emerging deep-learning techniques have had a substantial impact on\ncomputer graphics. Among the most promising breakthroughs are the rise of\nNeural Radiance Fields (NeRFs) and Gaussian Splatting (GS). NeRFs encode the\nobject's shape and color in neural network weights using a handful of images\nwith known camera positions to generate novel views. In contrast, GS provides\naccelerated training and inference without a decrease in rendering quality by\nencoding the object's characteristics in a collection of Gaussian\ndistributions. These two techniques have found many use cases in spatial\ncomputing and other domains. On the other hand, the emergence of deepfake\nmethods has sparked considerable controversy. Deepfakes refers to artificial\nintelligence-generated videos that closely mimic authentic footage. Using\ngenerative models, they can modify facial features, enabling the creation of\naltered identities or expressions that exhibit a remarkably realistic\nappearance to a real person. Despite these controversies, deepfake can offer a\nnext-generation solution for avatar creation and gaming when of desirable\nquality. To that end, we show how to combine all these emerging technologies to\nobtain a more plausible outcome. Our ImplicitDeepfake uses the classical\ndeepfake algorithm to modify all training images separately and then train NeRF\nand GS on modified faces. Such simple strategies can produce plausible 3D\ndeepfake-based avatars.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerous emerging deep-learning techniques have had a substantial impact on\ncomputer graphics. Among the most promising breakthroughs are the rise of\nNeural Radiance Fields (NeRFs) and Gaussian Splatting (GS). NeRFs encode the\nobject's shape and color in neural network weights using a handful of images\nwith known camera positions to generate novel views. In contrast, GS provides\naccelerated training and inference without a decrease in rendering quality by\nencoding the object's characteristics in a collection of Gaussian\ndistributions. These two techniques have found many use cases in spatial\ncomputing and other domains. On the other hand, the emergence of deepfake\nmethods has sparked considerable controversy. Deepfakes refers to artificial\nintelligence-generated videos that closely mimic authentic footage. Using\ngenerative models, they can modify facial features, enabling the creation of\naltered identities or expressions that exhibit a remarkably realistic\nappearance to a real person. Despite these controversies, deepfake can offer a\nnext-generation solution for avatar creation and gaming when of desirable\nquality. To that end, we show how to combine all these emerging technologies to\nobtain a more plausible outcome. Our ImplicitDeepfake uses the classical\ndeepfake algorithm to modify all training images separately and then train NeRF\nand GS on modified faces. Such simple strategies can produce plausible 3D\ndeepfake-based avatars."
                },
                "authors": [
                    {
                        "name": "Georgii Stanishevskii"
                    },
                    {
                        "name": "Jakub Steczkiewicz"
                    },
                    {
                        "name": "Tomasz Szczepanik"
                    },
                    {
                        "name": "SÅawomir Tadeja"
                    },
                    {
                        "name": "Jacek Tabor"
                    },
                    {
                        "name": "PrzemysÅaw Spurek"
                    }
                ],
                "author_detail": {
                    "name": "PrzemysÅaw Spurek"
                },
                "author": "PrzemysÅaw Spurek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.06390v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.06390v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13729v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13729v2",
                "updated": "2024-12-02T14:59:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    14,
                    59,
                    8,
                    0,
                    337,
                    0
                ],
                "published": "2024-09-10T01:20:22Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    1,
                    20,
                    22,
                    1,
                    254,
                    0
                ],
                "title": "MathGLM-Vision: Solving Mathematical Problems with Multi-Modal Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MathGLM-Vision: Solving Mathematical Problems with Multi-Modal Large\n  Language Model"
                },
                "summary": "Large language models (LLMs) have demonstrated significant capabilities in\nmathematical reasoning, particularly with text-based mathematical problems.\nHowever, current multi-modal large language models (MLLMs), especially those\nspecialized in mathematics, tend to focus predominantly on solving geometric\nproblems but ignore the diversity of visual information available in other\nareas of mathematics. Moreover, the geometric information for these specialized\nmathematical MLLMs is derived from several public datasets, which are typically\nlimited in diversity and complexity. To address these limitations, we aim to\nconstruct a fine-tuning dataset named MathVL, and develop a series of\nspecialized mathematical MLLMs termed MathGLM-Vision by conducting Supervised\nFine-Tuning (SFT) on MathVL with various parameter-scale backbones. To\nextensively evaluate the effectiveness of MathGLM-Vision, we conduct\nexperiments on several public benchmarks and our curated MathVL-test consisting\nof 2,000 problems. Experimental results demonstrate that MathGLM-Vision\nachieves significant improvements compared with some existing models, including\nbackbone models and open-source mathematical MLLMs. These findings indicate the\nimportance of diversity dataset in enhancing the mathematical reasoning\nabilities of MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated significant capabilities in\nmathematical reasoning, particularly with text-based mathematical problems.\nHowever, current multi-modal large language models (MLLMs), especially those\nspecialized in mathematics, tend to focus predominantly on solving geometric\nproblems but ignore the diversity of visual information available in other\nareas of mathematics. Moreover, the geometric information for these specialized\nmathematical MLLMs is derived from several public datasets, which are typically\nlimited in diversity and complexity. To address these limitations, we aim to\nconstruct a fine-tuning dataset named MathVL, and develop a series of\nspecialized mathematical MLLMs termed MathGLM-Vision by conducting Supervised\nFine-Tuning (SFT) on MathVL with various parameter-scale backbones. To\nextensively evaluate the effectiveness of MathGLM-Vision, we conduct\nexperiments on several public benchmarks and our curated MathVL-test consisting\nof 2,000 problems. Experimental results demonstrate that MathGLM-Vision\nachieves significant improvements compared with some existing models, including\nbackbone models and open-source mathematical MLLMs. These findings indicate the\nimportance of diversity dataset in enhancing the mathematical reasoning\nabilities of MLLMs."
                },
                "authors": [
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Jinhao Chen"
                    },
                    {
                        "name": "Zhengxiao Du"
                    },
                    {
                        "name": "Wenmeng Yu"
                    },
                    {
                        "name": "Weihan Wang"
                    },
                    {
                        "name": "Wenyi Hong"
                    },
                    {
                        "name": "Zhihuan Jiang"
                    },
                    {
                        "name": "Bin Xu"
                    },
                    {
                        "name": "Jie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Tang"
                },
                "author": "Jie Tang",
                "arxiv_comment": "30 pages,19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13729v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13729v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17773v2",
                "updated": "2024-12-02T14:55:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    14,
                    55,
                    49,
                    0,
                    337,
                    0
                ],
                "published": "2024-11-26T09:36:02Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    9,
                    36,
                    2,
                    1,
                    331,
                    0
                ],
                "title": "Efficient Multi-modal Large Language Models via Visual Token Grouping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Multi-modal Large Language Models via Visual Token Grouping"
                },
                "summary": "The development of Multi-modal Large Language Models (MLLMs) enhances Large\nLanguage Models (LLMs) with the ability to perceive data formats beyond text,\nsignificantly advancing a range of downstream applications, such as visual\nquestion answering and image captioning. However, the substantial computational\ncosts associated with processing high-resolution images and videos pose a\nbarrier to their broader adoption. To address this challenge, compressing\nvision tokens in MLLMs has emerged as a promising approach to reduce inference\ncosts. While existing methods conduct token reduction in the feature alignment\nphase. In this paper, we introduce VisToG, a novel grouping mechanism that\nleverages the capabilities of pre-trained vision encoders to group similar\nimage segments without the need for segmentation masks. Specifically, we\nconcatenate semantic tokens to represent image semantic segments after the\nlinear projection layer before feeding into the vision encoder. Besides, with\nthe isolated attention we adopt, VisToG can identify and eliminate redundant\nvisual tokens utilizing the prior knowledge in the pre-trained vision encoder,\nwhich effectively reduces computational demands. Extensive experiments\ndemonstrate the effectiveness of VisToG, maintaining 98.1% of the original\nperformance while achieving a reduction of over 27\\% inference time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of Multi-modal Large Language Models (MLLMs) enhances Large\nLanguage Models (LLMs) with the ability to perceive data formats beyond text,\nsignificantly advancing a range of downstream applications, such as visual\nquestion answering and image captioning. However, the substantial computational\ncosts associated with processing high-resolution images and videos pose a\nbarrier to their broader adoption. To address this challenge, compressing\nvision tokens in MLLMs has emerged as a promising approach to reduce inference\ncosts. While existing methods conduct token reduction in the feature alignment\nphase. In this paper, we introduce VisToG, a novel grouping mechanism that\nleverages the capabilities of pre-trained vision encoders to group similar\nimage segments without the need for segmentation masks. Specifically, we\nconcatenate semantic tokens to represent image semantic segments after the\nlinear projection layer before feeding into the vision encoder. Besides, with\nthe isolated attention we adopt, VisToG can identify and eliminate redundant\nvisual tokens utilizing the prior knowledge in the pre-trained vision encoder,\nwhich effectively reduces computational demands. Extensive experiments\ndemonstrate the effectiveness of VisToG, maintaining 98.1% of the original\nperformance while achieving a reduction of over 27\\% inference time."
                },
                "authors": [
                    {
                        "name": "Minbin Huang"
                    },
                    {
                        "name": "Runhui Huang"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Chuanyang Zheng"
                    },
                    {
                        "name": "Xiangguo Sun"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Hong Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Hong Cheng"
                },
                "author": "Hong Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06154v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06154v2",
                "updated": "2024-12-02T14:49:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    14,
                    49,
                    55,
                    0,
                    337,
                    0
                ],
                "published": "2024-10-08T15:55:40Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    15,
                    55,
                    40,
                    1,
                    282,
                    0
                ],
                "title": "GLOV: Guided Large Language Models as Implicit Optimizers for Vision\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GLOV: Guided Large Language Models as Implicit Optimizers for Vision\n  Language Models"
                },
                "summary": "In this work, we propose a novel method (GLOV) enabling Large Language Models\n(LLMs) to act as implicit Optimizers for Vision-Langugage Models (VLMs) to\nenhance downstream vision tasks. Our GLOV meta-prompts an LLM with the\ndownstream task description, querying it for suitable VLM prompts (e.g., for\nzero-shot classification with CLIP). These prompts are ranked according to a\npurity measure obtained through a fitness function. In each respective\noptimization step, the ranked prompts are fed as in-context examples (with\ntheir accuracies) to equip the LLM with the knowledge of the type of text\nprompts preferred by the downstream VLM. Furthermore, we also explicitly steer\nthe LLM generation process in each optimization step by specifically adding an\noffset difference vector of the embeddings from the positive and negative\nsolutions found by the LLM, in previous optimization steps, to the intermediate\nlayer of the network for the next generation step. This offset vector steers\nthe LLM generation toward the type of language preferred by the downstream VLM,\nresulting in enhanced performance on the downstream vision tasks. We\ncomprehensively evaluate our GLOV on 16 diverse datasets using two families of\nVLMs, i.e., dual-encoder (e.g., CLIP) and encoder-decoder (e.g., LLaVa) models\n-- showing that the discovered solutions can enhance the recognition\nperformance by up to 15.0% and 57.5% (3.8% and 21.6% on average) for these\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we propose a novel method (GLOV) enabling Large Language Models\n(LLMs) to act as implicit Optimizers for Vision-Langugage Models (VLMs) to\nenhance downstream vision tasks. Our GLOV meta-prompts an LLM with the\ndownstream task description, querying it for suitable VLM prompts (e.g., for\nzero-shot classification with CLIP). These prompts are ranked according to a\npurity measure obtained through a fitness function. In each respective\noptimization step, the ranked prompts are fed as in-context examples (with\ntheir accuracies) to equip the LLM with the knowledge of the type of text\nprompts preferred by the downstream VLM. Furthermore, we also explicitly steer\nthe LLM generation process in each optimization step by specifically adding an\noffset difference vector of the embeddings from the positive and negative\nsolutions found by the LLM, in previous optimization steps, to the intermediate\nlayer of the network for the next generation step. This offset vector steers\nthe LLM generation toward the type of language preferred by the downstream VLM,\nresulting in enhanced performance on the downstream vision tasks. We\ncomprehensively evaluate our GLOV on 16 diverse datasets using two families of\nVLMs, i.e., dual-encoder (e.g., CLIP) and encoder-decoder (e.g., LLaVa) models\n-- showing that the discovered solutions can enhance the recognition\nperformance by up to 15.0% and 57.5% (3.8% and 21.6% on average) for these\nmodels."
                },
                "authors": [
                    {
                        "name": "M. Jehanzeb Mirza"
                    },
                    {
                        "name": "Mengjie Zhao"
                    },
                    {
                        "name": "Zhuoyuan Mao"
                    },
                    {
                        "name": "Sivan Doveh"
                    },
                    {
                        "name": "Wei Lin"
                    },
                    {
                        "name": "Paul Gavrikov"
                    },
                    {
                        "name": "Michael Dorkenwald"
                    },
                    {
                        "name": "Shiqi Yang"
                    },
                    {
                        "name": "Saurav Jha"
                    },
                    {
                        "name": "Hiromi Wakaki"
                    },
                    {
                        "name": "Yuki Mitsufuji"
                    },
                    {
                        "name": "Horst Possegger"
                    },
                    {
                        "name": "Rogerio Feris"
                    },
                    {
                        "name": "Leonid Karlinsky"
                    },
                    {
                        "name": "James Glass"
                    }
                ],
                "author_detail": {
                    "name": "James Glass"
                },
                "author": "James Glass",
                "arxiv_comment": "Code: https://github.com/jmiemirza/GLOV",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06154v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06154v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19655v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19655v2",
                "updated": "2024-12-02T14:28:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    14,
                    28,
                    7,
                    0,
                    337,
                    0
                ],
                "published": "2024-11-29T12:21:15Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    12,
                    21,
                    15,
                    4,
                    334,
                    0
                ],
                "title": "Truth or Mirage? Towards End-to-End Factuality Evaluation with LLM-Oasis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Truth or Mirage? Towards End-to-End Factuality Evaluation with LLM-Oasis"
                },
                "summary": "After the introduction of Large Language Models (LLMs), there have been\nsubstantial improvements in the performance of Natural Language Generation\n(NLG) tasks, including Text Summarization and Machine Translation. However,\nLLMs still produce outputs containing hallucinations, that is, content not\ngrounded in factual information. Therefore, developing methods to assess the\nfactuality of LLMs has become urgent.\n  Indeed, resources for factuality evaluation have recently emerged. Although\nchallenging, these resources face one or more of the following limitations: (i)\nthey are tailored to a specific task or domain; (ii) they are limited in size,\nthereby preventing the training of new factuality evaluators; (iii) they are\ndesigned for simpler verification tasks, such as claim verification.\n  To address these issues, we introduce LLM-Oasis, to the best of our knowledge\nthe largest resource for training end-to-end factuality evaluators. LLM-Oasis\nis constructed by extracting claims from Wikipedia, falsifying a subset of\nthese claims, and generating pairs of factual and unfactual texts. We then rely\non human annotators to both validate the quality of our dataset and to create a\ngold standard test set for benchmarking factuality evaluation systems.\n  Our experiments demonstrate that LLM-Oasis presents a significant challenge\nfor state-of-the-art LLMs, with GPT-4o achieving up to 60% accuracy in our\nproposed end-to-end factuality evaluation task, highlighting its potential to\ndrive future research in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "After the introduction of Large Language Models (LLMs), there have been\nsubstantial improvements in the performance of Natural Language Generation\n(NLG) tasks, including Text Summarization and Machine Translation. However,\nLLMs still produce outputs containing hallucinations, that is, content not\ngrounded in factual information. Therefore, developing methods to assess the\nfactuality of LLMs has become urgent.\n  Indeed, resources for factuality evaluation have recently emerged. Although\nchallenging, these resources face one or more of the following limitations: (i)\nthey are tailored to a specific task or domain; (ii) they are limited in size,\nthereby preventing the training of new factuality evaluators; (iii) they are\ndesigned for simpler verification tasks, such as claim verification.\n  To address these issues, we introduce LLM-Oasis, to the best of our knowledge\nthe largest resource for training end-to-end factuality evaluators. LLM-Oasis\nis constructed by extracting claims from Wikipedia, falsifying a subset of\nthese claims, and generating pairs of factual and unfactual texts. We then rely\non human annotators to both validate the quality of our dataset and to create a\ngold standard test set for benchmarking factuality evaluation systems.\n  Our experiments demonstrate that LLM-Oasis presents a significant challenge\nfor state-of-the-art LLMs, with GPT-4o achieving up to 60% accuracy in our\nproposed end-to-end factuality evaluation task, highlighting its potential to\ndrive future research in the field."
                },
                "authors": [
                    {
                        "name": "Alessandro ScirÃ¨"
                    },
                    {
                        "name": "Andrei Stefan Bejgu"
                    },
                    {
                        "name": "Simone Tedeschi"
                    },
                    {
                        "name": "Karim Ghonim"
                    },
                    {
                        "name": "Federico Martelli"
                    },
                    {
                        "name": "Roberto Navigli"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Navigli"
                },
                "author": "Roberto Navigli",
                "arxiv_comment": "15 pages. To be submitted to CL journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19655v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19655v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01639v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01639v2",
                "updated": "2024-12-02T14:25:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    14,
                    25,
                    30,
                    0,
                    337,
                    0
                ],
                "published": "2024-10-02T15:09:36Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    15,
                    9,
                    36,
                    2,
                    276,
                    0
                ],
                "title": "Moral Alignment for LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Moral Alignment for LLM Agents"
                },
                "summary": "Decision-making agents based on pre-trained Large Language Models (LLMs) are\nincreasingly being deployed across various domains of human activity. While\ntheir applications are currently rather specialized, several research efforts\nare under way to develop more generalist agents. As LLM-based systems become\nmore agentic, their influence on human activity will grow and the transparency\nof this will decrease. Consequently, developing effective methods for aligning\nthem to human values is vital.\n  The prevailing practice in alignment often relies on human preference data\n(e.g., in RLHF or DPO), in which values are implicit and are essentially\ndeduced from relative preferences over different model outputs. In this work,\ninstead of relying on human feedback, we introduce the design of reward\nfunctions that explicitly encode core human values for Reinforcement\nLearning-based fine-tuning of foundation agent models. Specifically, we use\nintrinsic rewards for the moral alignment of LLM agents.\n  We evaluate our approach using the traditional philosophical frameworks of\nDeontological Ethics and Utilitarianism, quantifying moral rewards for agents\nin terms of actions and consequences on the Iterated Prisoner's Dilemma (IPD)\nenvironment. We also show how moral fine-tuning can be deployed to enable an\nagent to unlearn a previously developed selfish strategy. Finally, we find that\ncertain moral strategies learned on the IPD game generalize to several other\nmatrix game environments. In summary, we demonstrate that fine-tuning with\nintrinsic rewards is a promising general solution for aligning LLM agents to\nhuman values, and it might represent a more transparent and cost-effective\nalternative to currently predominant alignment techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision-making agents based on pre-trained Large Language Models (LLMs) are\nincreasingly being deployed across various domains of human activity. While\ntheir applications are currently rather specialized, several research efforts\nare under way to develop more generalist agents. As LLM-based systems become\nmore agentic, their influence on human activity will grow and the transparency\nof this will decrease. Consequently, developing effective methods for aligning\nthem to human values is vital.\n  The prevailing practice in alignment often relies on human preference data\n(e.g., in RLHF or DPO), in which values are implicit and are essentially\ndeduced from relative preferences over different model outputs. In this work,\ninstead of relying on human feedback, we introduce the design of reward\nfunctions that explicitly encode core human values for Reinforcement\nLearning-based fine-tuning of foundation agent models. Specifically, we use\nintrinsic rewards for the moral alignment of LLM agents.\n  We evaluate our approach using the traditional philosophical frameworks of\nDeontological Ethics and Utilitarianism, quantifying moral rewards for agents\nin terms of actions and consequences on the Iterated Prisoner's Dilemma (IPD)\nenvironment. We also show how moral fine-tuning can be deployed to enable an\nagent to unlearn a previously developed selfish strategy. Finally, we find that\ncertain moral strategies learned on the IPD game generalize to several other\nmatrix game environments. In summary, we demonstrate that fine-tuning with\nintrinsic rewards is a promising general solution for aligning LLM agents to\nhuman values, and it might represent a more transparent and cost-effective\nalternative to currently predominant alignment techniques."
                },
                "authors": [
                    {
                        "name": "Elizaveta Tennant"
                    },
                    {
                        "name": "Stephen Hailes"
                    },
                    {
                        "name": "Mirco Musolesi"
                    }
                ],
                "author_detail": {
                    "name": "Mirco Musolesi"
                },
                "author": "Mirco Musolesi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01639v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01639v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.10495v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.10495v2",
                "updated": "2024-12-02T13:56:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    13,
                    56,
                    32,
                    0,
                    337,
                    0
                ],
                "published": "2024-02-16T07:56:40Z",
                "published_parsed": [
                    2024,
                    2,
                    16,
                    7,
                    56,
                    40,
                    4,
                    47,
                    0
                ],
                "title": "Transport and fusion of Majorana zero modes in the presence of\n  nonadiabatic transitions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transport and fusion of Majorana zero modes in the presence of\n  nonadiabatic transitions"
                },
                "summary": "We perform simulations for transport and nontrivial fusion of Majorana zero\nmodes in topological superconducting quantum wires. We uncover interesting\nbehaviors of nonadiabatic transition associated with the transport through\nmini-gate-controlled multiple-segments modulations. Owing to breaking of the\ninitial fermion parity induced by nonadiabatic transitions, a deviation from\nthe statistics of outcomes of nontrivial fusion arises and is analyzed.\nMoreover, we develop a measurement scheme to infer the amount of fermion parity\nbreaking and nonadiabatic transition probability to excited states, based on\nthe characteristic spectrum of measurement current by a quantum-point-contact\ndetector, by measurement of the charge occupation dynamics in a\nfusion-outcome-probing quantum dot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We perform simulations for transport and nontrivial fusion of Majorana zero\nmodes in topological superconducting quantum wires. We uncover interesting\nbehaviors of nonadiabatic transition associated with the transport through\nmini-gate-controlled multiple-segments modulations. Owing to breaking of the\ninitial fermion parity induced by nonadiabatic transitions, a deviation from\nthe statistics of outcomes of nontrivial fusion arises and is analyzed.\nMoreover, we develop a measurement scheme to infer the amount of fermion parity\nbreaking and nonadiabatic transition probability to excited states, based on\nthe characteristic spectrum of measurement current by a quantum-point-contact\ndetector, by measurement of the charge occupation dynamics in a\nfusion-outcome-probing quantum dot."
                },
                "authors": [
                    {
                        "name": "Qiongyao Wang"
                    },
                    {
                        "name": "Jing Bai"
                    },
                    {
                        "name": "Luting Xu"
                    },
                    {
                        "name": "Wei Feng"
                    },
                    {
                        "name": "Xin-Qi Li"
                    }
                ],
                "author_detail": {
                    "name": "Xin-Qi Li"
                },
                "author": "Xin-Qi Li",
                "arxiv_doi": "10.1103/PhysRevB.110.115402",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevB.110.115402",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.10495v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.10495v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "9 pages, 6 figures",
                "arxiv_journal_ref": "Phys. Rev. B 110, 115402 (2024)",
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03235v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03235v2",
                "updated": "2024-12-02T13:21:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    13,
                    21,
                    36,
                    0,
                    337,
                    0
                ],
                "published": "2024-10-04T09:00:06Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    9,
                    0,
                    6,
                    4,
                    278,
                    0
                ],
                "title": "Enriching Ontologies with Disjointness Axioms using Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enriching Ontologies with Disjointness Axioms using Large Language\n  Models"
                },
                "summary": "Ontologies often lack explicit disjointness declarations between classes,\ndespite their usefulness for sophisticated reasoning and consistency checking\nin Knowledge Graphs. In this study, we explore the potential of Large Language\nModels (LLMs) to enrich ontologies by identifying and asserting class\ndisjointness axioms. Our approach aims at leveraging the implicit knowledge\nembedded in LLMs, using prompt engineering to elicit this knowledge for\nclassifying ontological disjointness. We validate our methodology on the\nDBpedia ontology, focusing on open-source LLMs. Our findings suggest that LLMs,\nwhen guided by effective prompt strategies, can reliably identify disjoint\nclass relationships, thus streamlining the process of ontology completion\nwithout extensive manual input. For comprehensive disjointness enrichment, we\npropose a process that takes logical relationships between disjointness and\nsubclass statements into account in order to maintain satisfiability and reduce\nthe number of calls to the LLM. This work provides a foundation for future\napplications of LLMs in automated ontology enhancement and offers insights into\noptimizing LLM performance through strategic prompt design. Our code is\npublicly available on GitHub at https://github.com/n28div/llm-disjointness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontologies often lack explicit disjointness declarations between classes,\ndespite their usefulness for sophisticated reasoning and consistency checking\nin Knowledge Graphs. In this study, we explore the potential of Large Language\nModels (LLMs) to enrich ontologies by identifying and asserting class\ndisjointness axioms. Our approach aims at leveraging the implicit knowledge\nembedded in LLMs, using prompt engineering to elicit this knowledge for\nclassifying ontological disjointness. We validate our methodology on the\nDBpedia ontology, focusing on open-source LLMs. Our findings suggest that LLMs,\nwhen guided by effective prompt strategies, can reliably identify disjoint\nclass relationships, thus streamlining the process of ontology completion\nwithout extensive manual input. For comprehensive disjointness enrichment, we\npropose a process that takes logical relationships between disjointness and\nsubclass statements into account in order to maintain satisfiability and reduce\nthe number of calls to the LLM. This work provides a foundation for future\napplications of LLMs in automated ontology enhancement and offers insights into\noptimizing LLM performance through strategic prompt design. Our code is\npublicly available on GitHub at https://github.com/n28div/llm-disjointness."
                },
                "authors": [
                    {
                        "name": "Elias Crum"
                    },
                    {
                        "name": "Antonio De Santis"
                    },
                    {
                        "name": "Manon Ovide"
                    },
                    {
                        "name": "Jiaxin Pan"
                    },
                    {
                        "name": "Alessia Pisu"
                    },
                    {
                        "name": "Nicolas Lazzari"
                    },
                    {
                        "name": "Sebastian Rudolph"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Rudolph"
                },
                "author": "Sebastian Rudolph",
                "arxiv_comment": "Accepted at KBC-LM'24 workshop at ISWC 2024,\n  https://ceur-ws.org/Vol-3853/paper1.pdf",
                "arxiv_journal_ref": "Proc. of 2nd Workshop on Knowledge Base Construction from\n  Pre-Trained Language Models (KBC-LM 2024) co-located with ISWC 2024,\n  Baltimore, USA, November 12, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03235v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03235v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07123v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07123v2",
                "updated": "2024-12-02T13:04:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    13,
                    4,
                    18,
                    0,
                    337,
                    0
                ],
                "published": "2024-09-11T09:21:20Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    9,
                    21,
                    20,
                    2,
                    255,
                    0
                ],
                "title": "Cross-Refine: Improving Natural Language Explanation Generation by\n  Learning in Tandem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Refine: Improving Natural Language Explanation Generation by\n  Learning in Tandem"
                },
                "summary": "Natural language explanations (NLEs) are vital for elucidating the reasoning\nbehind large language model (LLM) decisions. Many techniques have been\ndeveloped to generate NLEs using LLMs. However, like humans, LLMs might not\nalways produce optimal NLEs on first attempt. Inspired by human learning\nprocesses, we introduce Cross-Refine, which employs role modeling by deploying\ntwo LLMs as generator and critic, respectively. The generator outputs a first\nNLE and then refines this initial explanation using feedback and suggestions\nprovided by the critic. Cross-Refine does not require any supervised training\ndata or additional training. We validate Cross-Refine across three NLP tasks\nusing three state-of-the-art open-source LLMs through automatic and human\nevaluation. We select Self-Refine (Madaan et al., 2023) as the baseline, which\nonly utilizes self-feedback to refine the explanations. Our findings from\nautomatic evaluation and a user study indicate that Cross-Refine outperforms\nSelf-Refine. Meanwhile, Cross-Refine can perform effectively with less powerful\nLLMs, whereas Self-Refine only yields strong results with ChatGPT.\nAdditionally, we conduct an ablation study to assess the importance of feedback\nand suggestions. Both of them play an important role in refining explanations.\nWe further evaluate Cross-Refine on a bilingual dataset in English and German.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural language explanations (NLEs) are vital for elucidating the reasoning\nbehind large language model (LLM) decisions. Many techniques have been\ndeveloped to generate NLEs using LLMs. However, like humans, LLMs might not\nalways produce optimal NLEs on first attempt. Inspired by human learning\nprocesses, we introduce Cross-Refine, which employs role modeling by deploying\ntwo LLMs as generator and critic, respectively. The generator outputs a first\nNLE and then refines this initial explanation using feedback and suggestions\nprovided by the critic. Cross-Refine does not require any supervised training\ndata or additional training. We validate Cross-Refine across three NLP tasks\nusing three state-of-the-art open-source LLMs through automatic and human\nevaluation. We select Self-Refine (Madaan et al., 2023) as the baseline, which\nonly utilizes self-feedback to refine the explanations. Our findings from\nautomatic evaluation and a user study indicate that Cross-Refine outperforms\nSelf-Refine. Meanwhile, Cross-Refine can perform effectively with less powerful\nLLMs, whereas Self-Refine only yields strong results with ChatGPT.\nAdditionally, we conduct an ablation study to assess the importance of feedback\nand suggestions. Both of them play an important role in refining explanations.\nWe further evaluate Cross-Refine on a bilingual dataset in English and German."
                },
                "authors": [
                    {
                        "name": "Qianli Wang"
                    },
                    {
                        "name": "Tatiana Anikina"
                    },
                    {
                        "name": "Nils Feldhus"
                    },
                    {
                        "name": "Simon Ostermann"
                    },
                    {
                        "name": "Sebastian MÃ¶ller"
                    },
                    {
                        "name": "Vera Schmitt"
                    }
                ],
                "author_detail": {
                    "name": "Vera Schmitt"
                },
                "author": "Vera Schmitt",
                "arxiv_comment": "Accepted at COLING 2025; long paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07123v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07123v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.15798v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.15798v4",
                "updated": "2024-12-02T12:58:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    12,
                    58,
                    23,
                    0,
                    337,
                    0
                ],
                "published": "2023-05-25T07:28:28Z",
                "published_parsed": [
                    2023,
                    5,
                    25,
                    7,
                    28,
                    28,
                    3,
                    145,
                    0
                ],
                "title": "BK-SDM: A Lightweight, Fast, and Cheap Version of Stable Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BK-SDM: A Lightweight, Fast, and Cheap Version of Stable Diffusion"
                },
                "summary": "Text-to-image (T2I) generation with Stable Diffusion models (SDMs) involves\nhigh computing demands due to billion-scale parameters. To enhance efficiency,\nrecent studies have reduced sampling steps and applied network quantization\nwhile retaining the original architectures. The lack of architectural reduction\nattempts may stem from worries over expensive retraining for such massive\nmodels. In this work, we uncover the surprising potential of block pruning and\nfeature distillation for low-cost general-purpose T2I. By removing several\nresidual and attention blocks from the U-Net of SDMs, we achieve 30%~50%\nreduction in model size, MACs, and latency. We show that distillation\nretraining is effective even under limited resources: using only 13 A100 days\nand a tiny dataset, our compact models can imitate the original SDMs (v1.4 and\nv2.1-base with over 6,000 A100 days). Benefiting from the transferred\nknowledge, our BK-SDMs deliver competitive results on zero-shot MS-COCO against\nlarger multi-billion parameter models. We further demonstrate the applicability\nof our lightweight backbones in personalized generation and image-to-image\ntranslation. Deployment of our models on edge devices attains 4-second\ninference. Code and models can be found at:\nhttps://github.com/Nota-NetsPresso/BK-SDM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image (T2I) generation with Stable Diffusion models (SDMs) involves\nhigh computing demands due to billion-scale parameters. To enhance efficiency,\nrecent studies have reduced sampling steps and applied network quantization\nwhile retaining the original architectures. The lack of architectural reduction\nattempts may stem from worries over expensive retraining for such massive\nmodels. In this work, we uncover the surprising potential of block pruning and\nfeature distillation for low-cost general-purpose T2I. By removing several\nresidual and attention blocks from the U-Net of SDMs, we achieve 30%~50%\nreduction in model size, MACs, and latency. We show that distillation\nretraining is effective even under limited resources: using only 13 A100 days\nand a tiny dataset, our compact models can imitate the original SDMs (v1.4 and\nv2.1-base with over 6,000 A100 days). Benefiting from the transferred\nknowledge, our BK-SDMs deliver competitive results on zero-shot MS-COCO against\nlarger multi-billion parameter models. We further demonstrate the applicability\nof our lightweight backbones in personalized generation and image-to-image\ntranslation. Deployment of our models on edge devices attains 4-second\ninference. Code and models can be found at:\nhttps://github.com/Nota-NetsPresso/BK-SDM"
                },
                "authors": [
                    {
                        "name": "Bo-Kyeong Kim"
                    },
                    {
                        "name": "Hyoung-Kyu Song"
                    },
                    {
                        "name": "Thibault Castells"
                    },
                    {
                        "name": "Shinkook Choi"
                    }
                ],
                "author_detail": {
                    "name": "Shinkook Choi"
                },
                "author": "Shinkook Choi",
                "arxiv_doi": "10.1007/978-3-031-72949-2",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-72949-2",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2305.15798v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.15798v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ECCV 2024 Camera-Ready Version",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04037v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04037v3",
                "updated": "2024-12-02T12:51:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    12,
                    51,
                    48,
                    0,
                    337,
                    0
                ],
                "published": "2024-11-06T16:34:59Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    16,
                    34,
                    59,
                    2,
                    311,
                    0
                ],
                "title": "Investigating the heterogenous effects of a massive content moderation\n  intervention via Difference-in-Differences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating the heterogenous effects of a massive content moderation\n  intervention via Difference-in-Differences"
                },
                "summary": "In today's online environments, users encounter harm and abuse on a daily\nbasis. Therefore, content moderation is crucial to ensure their safety and\nwell-being. However, the effectiveness of many moderation interventions is\nstill uncertain. Here, we apply a causal inference approach to shed light on\nthe effectiveness of The Great Ban, a massive social media deplatforming\nintervention. We analyze 53M comments shared by nearly 34K users, providing\nin-depth results on both the intended and unintended consequences of the ban.\nOur causal analyses reveal that 15.6% of the moderated users abandoned the\nplatform while the remaining ones decreased their overall toxicity by 4.1%.\nNonetheless, a subset of those users increased their toxicity by 70% after the\nintervention. However, the increases in toxicity did not lead to marked\nincreases in activity or engagement, meaning that the most toxic users had an\noverall limited impact. Our findings bring to light new insights on the\neffectiveness of deplatforming moderation interventions. Furthermore, they also\ncontribute to informing future content moderation strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In today's online environments, users encounter harm and abuse on a daily\nbasis. Therefore, content moderation is crucial to ensure their safety and\nwell-being. However, the effectiveness of many moderation interventions is\nstill uncertain. Here, we apply a causal inference approach to shed light on\nthe effectiveness of The Great Ban, a massive social media deplatforming\nintervention. We analyze 53M comments shared by nearly 34K users, providing\nin-depth results on both the intended and unintended consequences of the ban.\nOur causal analyses reveal that 15.6% of the moderated users abandoned the\nplatform while the remaining ones decreased their overall toxicity by 4.1%.\nNonetheless, a subset of those users increased their toxicity by 70% after the\nintervention. However, the increases in toxicity did not lead to marked\nincreases in activity or engagement, meaning that the most toxic users had an\noverall limited impact. Our findings bring to light new insights on the\neffectiveness of deplatforming moderation interventions. Furthermore, they also\ncontribute to informing future content moderation strategies."
                },
                "authors": [
                    {
                        "name": "Lorenzo Cima"
                    },
                    {
                        "name": "Benedetta Tessa"
                    },
                    {
                        "name": "Stefano Cresci"
                    },
                    {
                        "name": "Amaury Trujillo"
                    },
                    {
                        "name": "Marco Avvenuti"
                    }
                ],
                "author_detail": {
                    "name": "Marco Avvenuti"
                },
                "author": "Marco Avvenuti",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2401.11254 This work is an\n  extension of this conference paper: Cima, L., Trujillo, A., Avvenuti, M., &\n  Cresci, S. (2024, May). The Great Ban: Efficacy and Unintended Consequences\n  of a Massive Deplatforming Operation on Reddit. In Companion Publication of\n  the 16th ACM Web Science Conference (pp. 85-93)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04037v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04037v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16886v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16886v3",
                "updated": "2024-12-02T12:39:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    12,
                    39,
                    7,
                    0,
                    337,
                    0
                ],
                "published": "2024-08-29T20:19:10Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    20,
                    19,
                    10,
                    3,
                    242,
                    0
                ],
                "title": "LV-UNet: A Lightweight and Vanilla Model for Medical Image Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LV-UNet: A Lightweight and Vanilla Model for Medical Image Segmentation"
                },
                "summary": "While large models have achieved significant progress in computer vision,\nchallenges such as optimization complexity, the intricacy of transformer\narchitectures, computational constraints, and practical application demands\nhighlight the importance of simpler model designs in medical image\nsegmentation. This need is particularly pronounced in mobile medical devices,\nwhich require lightweight, deployable models with real-time performance.\nHowever, existing lightweight models often suffer from poor robustness across\ndatasets, limiting their widespread adoption. To address these challenges, this\npaper introduces LV-UNet, a lightweight and vanilla model that leverages\npre-trained MobileNetv3-Large backbones and incorporates fusible modules.\nLV-UNet employs an enhanced deep training strategy and switches to a deployment\nmode during inference by re-parametrization, significantly reducing parameter\ncount and computational overhead. Experimental results on ISIC 2016, BUSI,\nCVC-ClinicDB, CVC-ColonDB, and Kvair-SEG datasets demonstrate a better\ntrade-off between performance and the computational load. The code will be\nreleased at \\url{https://github.com/juntaoJianggavin/LV-UNet}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large models have achieved significant progress in computer vision,\nchallenges such as optimization complexity, the intricacy of transformer\narchitectures, computational constraints, and practical application demands\nhighlight the importance of simpler model designs in medical image\nsegmentation. This need is particularly pronounced in mobile medical devices,\nwhich require lightweight, deployable models with real-time performance.\nHowever, existing lightweight models often suffer from poor robustness across\ndatasets, limiting their widespread adoption. To address these challenges, this\npaper introduces LV-UNet, a lightweight and vanilla model that leverages\npre-trained MobileNetv3-Large backbones and incorporates fusible modules.\nLV-UNet employs an enhanced deep training strategy and switches to a deployment\nmode during inference by re-parametrization, significantly reducing parameter\ncount and computational overhead. Experimental results on ISIC 2016, BUSI,\nCVC-ClinicDB, CVC-ColonDB, and Kvair-SEG datasets demonstrate a better\ntrade-off between performance and the computational load. The code will be\nreleased at \\url{https://github.com/juntaoJianggavin/LV-UNet}."
                },
                "authors": [
                    {
                        "name": "Juntao Jiang"
                    },
                    {
                        "name": "Mengmeng Wang"
                    },
                    {
                        "name": "Huizhong Tian"
                    },
                    {
                        "name": "Lingbo Cheng"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "Accepted by IEEE BIBM2024 ML4BMI workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16886v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16886v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14655v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14655v2",
                "updated": "2024-12-02T12:37:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    12,
                    37,
                    46,
                    0,
                    337,
                    0
                ],
                "published": "2024-05-23T14:53:54Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    14,
                    53,
                    54,
                    3,
                    144,
                    0
                ],
                "title": "Multi-turn Reinforcement Learning from Preference Human Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn Reinforcement Learning from Preference Human Feedback"
                },
                "summary": "Reinforcement Learning from Human Feedback (RLHF) has become the standard\napproach for aligning Large Language Models (LLMs) with human preferences,\nallowing LLMs to demonstrate remarkable abilities in various tasks. Existing\nmethods work by emulating the preferences at the single decision (turn) level,\nlimiting their capabilities in settings that require planning or multi-turn\ninteractions to achieve a long-term goal. In this paper, we address this issue\nby developing novel methods for Reinforcement Learning (RL) from preference\nfeedback between two full multi-turn conversations. In the tabular setting, we\npresent a novel mirror-descent-based policy optimization algorithm for the\ngeneral multi-turn preference-based RL problem, and prove its convergence to\nNash equilibrium. To evaluate performance, we create a new environment,\nEducation Dialogue, where a teacher agent guides a student in learning a random\ntopic, and show that a deep RL variant of our algorithm outperforms RLHF\nbaselines. Finally, we show that in an environment with explicit rewards, our\nalgorithm recovers the same performance as a reward-based RL baseline, despite\nrelying solely on a weaker preference signal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Human Feedback (RLHF) has become the standard\napproach for aligning Large Language Models (LLMs) with human preferences,\nallowing LLMs to demonstrate remarkable abilities in various tasks. Existing\nmethods work by emulating the preferences at the single decision (turn) level,\nlimiting their capabilities in settings that require planning or multi-turn\ninteractions to achieve a long-term goal. In this paper, we address this issue\nby developing novel methods for Reinforcement Learning (RL) from preference\nfeedback between two full multi-turn conversations. In the tabular setting, we\npresent a novel mirror-descent-based policy optimization algorithm for the\ngeneral multi-turn preference-based RL problem, and prove its convergence to\nNash equilibrium. To evaluate performance, we create a new environment,\nEducation Dialogue, where a teacher agent guides a student in learning a random\ntopic, and show that a deep RL variant of our algorithm outperforms RLHF\nbaselines. Finally, we show that in an environment with explicit rewards, our\nalgorithm recovers the same performance as a reward-based RL baseline, despite\nrelying solely on a weaker preference signal."
                },
                "authors": [
                    {
                        "name": "Lior Shani"
                    },
                    {
                        "name": "Aviv Rosenberg"
                    },
                    {
                        "name": "Asaf Cassel"
                    },
                    {
                        "name": "Oran Lang"
                    },
                    {
                        "name": "Daniele Calandriello"
                    },
                    {
                        "name": "Avital Zipori"
                    },
                    {
                        "name": "Hila Noga"
                    },
                    {
                        "name": "Orgad Keller"
                    },
                    {
                        "name": "Bilal Piot"
                    },
                    {
                        "name": "Idan Szpektor"
                    },
                    {
                        "name": "Avinatan Hassidim"
                    },
                    {
                        "name": "Yossi Matias"
                    },
                    {
                        "name": "RÃ©mi Munos"
                    }
                ],
                "author_detail": {
                    "name": "RÃ©mi Munos"
                },
                "author": "RÃ©mi Munos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14655v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14655v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17109v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17109v2",
                "updated": "2024-12-02T12:36:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    12,
                    36,
                    32,
                    0,
                    337,
                    0
                ],
                "published": "2024-05-27T12:25:34Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    12,
                    25,
                    34,
                    0,
                    148,
                    0
                ],
                "title": "Left-Linear Completion with AC Axioms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Left-Linear Completion with AC Axioms"
                },
                "summary": "We revisit completion modulo equational theories for left-linear term rewrite\nsystems where unification modulo the theory is avoided and the normal rewrite\nrelation can be used in order to decide validity questions. To that end, we\ngive a new correctness proof for finite runs and establish a simulation result\nbetween the two inference systems known from the literature. Given a concrete\nreduction order, novel canonicity results show that the resulting complete\nsystems are unique up to the representation of their rules' right-hand sides.\nFurthermore, we show how left-linear AC completion can be simulated by general\nAC completion. In particular, this result allows us to switch from the former\nto the latter at any point during a completion process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We revisit completion modulo equational theories for left-linear term rewrite\nsystems where unification modulo the theory is avoided and the normal rewrite\nrelation can be used in order to decide validity questions. To that end, we\ngive a new correctness proof for finite runs and establish a simulation result\nbetween the two inference systems known from the literature. Given a concrete\nreduction order, novel canonicity results show that the resulting complete\nsystems are unique up to the representation of their rules' right-hand sides.\nFurthermore, we show how left-linear AC completion can be simulated by general\nAC completion. In particular, this result allows us to switch from the former\nto the latter at any point during a completion process."
                },
                "authors": [
                    {
                        "name": "Johannes Niederhauser"
                    },
                    {
                        "name": "Nao Hirokawa"
                    },
                    {
                        "name": "Aart Middeldorp"
                    }
                ],
                "author_detail": {
                    "name": "Aart Middeldorp"
                },
                "author": "Aart Middeldorp",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17109v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17109v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02272v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02272v4",
                "updated": "2024-12-02T12:36:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    12,
                    36,
                    30,
                    0,
                    337,
                    0
                ],
                "published": "2024-11-04T17:03:55Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    3,
                    55,
                    0,
                    309,
                    0
                ],
                "title": "Combining Induction and Transduction for Abstract Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining Induction and Transduction for Abstract Reasoning"
                },
                "summary": "When learning an input-output mapping from very few examples, is it better to\nfirst infer a latent function that explains the examples, or is it better to\ndirectly predict new test outputs, e.g. using a neural network? We study this\nquestion on ARC by training neural models for induction (inferring latent\nfunctions) and transduction (directly predicting the test output for a given\ntest input). We train on synthetically generated variations of Python programs\nthat solve ARC training tasks. We find inductive and transductive models solve\ndifferent kinds of test problems, despite having the same training problems and\nsharing the same neural architecture: Inductive program synthesis excels at\nprecise computations, and at composing multiple concepts, while transduction\nsucceeds on fuzzier perceptual concepts. Ensembling them approaches human-level\nperformance on ARC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When learning an input-output mapping from very few examples, is it better to\nfirst infer a latent function that explains the examples, or is it better to\ndirectly predict new test outputs, e.g. using a neural network? We study this\nquestion on ARC by training neural models for induction (inferring latent\nfunctions) and transduction (directly predicting the test output for a given\ntest input). We train on synthetically generated variations of Python programs\nthat solve ARC training tasks. We find inductive and transductive models solve\ndifferent kinds of test problems, despite having the same training problems and\nsharing the same neural architecture: Inductive program synthesis excels at\nprecise computations, and at composing multiple concepts, while transduction\nsucceeds on fuzzier perceptual concepts. Ensembling them approaches human-level\nperformance on ARC."
                },
                "authors": [
                    {
                        "name": "Wen-Ding Li"
                    },
                    {
                        "name": "Keya Hu"
                    },
                    {
                        "name": "Carter Larsen"
                    },
                    {
                        "name": "Yuqing Wu"
                    },
                    {
                        "name": "Simon Alford"
                    },
                    {
                        "name": "Caleb Woo"
                    },
                    {
                        "name": "Spencer M. Dunn"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Michelangelo Naim"
                    },
                    {
                        "name": "Dat Nguyen"
                    },
                    {
                        "name": "Wei-Long Zheng"
                    },
                    {
                        "name": "Zenna Tavares"
                    },
                    {
                        "name": "Yewen Pu"
                    },
                    {
                        "name": "Kevin Ellis"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Ellis"
                },
                "author": "Kevin Ellis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02272v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02272v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.07818v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.07818v5",
                "updated": "2024-12-02T12:29:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    12,
                    29,
                    47,
                    0,
                    337,
                    0
                ],
                "published": "2024-02-12T17:24:15Z",
                "published_parsed": [
                    2024,
                    2,
                    12,
                    17,
                    24,
                    15,
                    0,
                    43,
                    0
                ],
                "title": "Differentially Private Zeroth-Order Methods for Scalable Large Language\n  Model Finetuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentially Private Zeroth-Order Methods for Scalable Large Language\n  Model Finetuning"
                },
                "summary": "Fine-tuning on task-specific datasets is a widely-embraced paradigm of\nharnessing the powerful capability of pretrained LLMs for various downstream\ntasks. Due to the popularity of LLMs fine-tuning and its accompanying privacy\nconcerns, differentially private (DP) fine-tuning of pretrained LLMs has been\nwidely used to safeguarding the privacy of task-specific datasets. Lying at the\ndesign core of DP LLM fine-tuning methods is the satisfactory tradeoff among\nprivacy, utility, and scalability. Most existing methods build upon the seminal\nwork of DP-SGD. Despite pushing the scalability of DP-SGD to its limit,\nDP-SGD-based fine-tuning methods are unfortunately limited by the inherent\ninefficiency of SGD.\n  In this paper, we investigate the potential of DP zeroth-order methods for\nLLM pretraining, which avoids the scalability bottleneck of SGD by\napproximating the gradient with the more efficient zeroth-order gradient.\nRather than treating the zeroth-order method as a drop-in replacement for SGD,\nthis paper presents a comprehensive study both theoretically and empirically.\nFirst, we propose the stagewise DP zeroth-order method (DP-ZOSO) that\ndynamically schedules key hyperparameters. This design is grounded on the\nsynergy between DP random perturbation and the gradient approximation error of\nthe zeroth-order method, and its effect on fine-tuning trajectory.\n  We provide theoretical analysis for both proposed methods. We conduct\nextensive empirical analysis on both encoder-only masked language model and\ndecoder-only autoregressive language model, achieving impressive results in\nterms of scalability and utility regardless of the class of tasks (compared\nwith DPZero, DP-ZOPO improves $4.5\\%$ on SST-5, $5.5\\%$ on MNLI with\nRoBERTa-Large and 9.2\\% on CB, 3.9\\% on BoolQ with OPT-2.7b when $\\epsilon=4$,\ndemonstrates more significant enhancement in performance on more complicated\ntasks).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning on task-specific datasets is a widely-embraced paradigm of\nharnessing the powerful capability of pretrained LLMs for various downstream\ntasks. Due to the popularity of LLMs fine-tuning and its accompanying privacy\nconcerns, differentially private (DP) fine-tuning of pretrained LLMs has been\nwidely used to safeguarding the privacy of task-specific datasets. Lying at the\ndesign core of DP LLM fine-tuning methods is the satisfactory tradeoff among\nprivacy, utility, and scalability. Most existing methods build upon the seminal\nwork of DP-SGD. Despite pushing the scalability of DP-SGD to its limit,\nDP-SGD-based fine-tuning methods are unfortunately limited by the inherent\ninefficiency of SGD.\n  In this paper, we investigate the potential of DP zeroth-order methods for\nLLM pretraining, which avoids the scalability bottleneck of SGD by\napproximating the gradient with the more efficient zeroth-order gradient.\nRather than treating the zeroth-order method as a drop-in replacement for SGD,\nthis paper presents a comprehensive study both theoretically and empirically.\nFirst, we propose the stagewise DP zeroth-order method (DP-ZOSO) that\ndynamically schedules key hyperparameters. This design is grounded on the\nsynergy between DP random perturbation and the gradient approximation error of\nthe zeroth-order method, and its effect on fine-tuning trajectory.\n  We provide theoretical analysis for both proposed methods. We conduct\nextensive empirical analysis on both encoder-only masked language model and\ndecoder-only autoregressive language model, achieving impressive results in\nterms of scalability and utility regardless of the class of tasks (compared\nwith DPZero, DP-ZOPO improves $4.5\\%$ on SST-5, $5.5\\%$ on MNLI with\nRoBERTa-Large and 9.2\\% on CB, 3.9\\% on BoolQ with OPT-2.7b when $\\epsilon=4$,\ndemonstrates more significant enhancement in performance on more complicated\ntasks)."
                },
                "authors": [
                    {
                        "name": "Z Liu"
                    },
                    {
                        "name": "J Lou"
                    },
                    {
                        "name": "W Bao"
                    },
                    {
                        "name": "Y Hu"
                    },
                    {
                        "name": "B Li"
                    },
                    {
                        "name": "Z Qin"
                    },
                    {
                        "name": "K Ren"
                    }
                ],
                "author_detail": {
                    "name": "K Ren"
                },
                "author": "K Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.07818v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.07818v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11883v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11883v3",
                "updated": "2024-12-02T12:09:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    12,
                    9,
                    5,
                    0,
                    337,
                    0
                ],
                "published": "2024-10-11T10:09:46Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    10,
                    9,
                    46,
                    4,
                    285,
                    0
                ],
                "title": "Simulation-based inference with scattering representations: scattering\n  is all you need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation-based inference with scattering representations: scattering\n  is all you need"
                },
                "summary": "We demonstrate the successful use of scattering representations without\nfurther compression for simulation-based inference (SBI) with images (i.e.\nfield-level), illustrated with a cosmological case study. Scattering\nrepresentations provide a highly effective representational space for\nsubsequent learning tasks, although the higher dimensional compressed space\nintroduces challenges. We overcome these through spatial averaging, coupled\nwith more expressive density estimators. Compared to alternative methods, such\nan approach does not require additional simulations for either training or\ncomputing derivatives, is interpretable, and resilient to covariate shift. As\nexpected, we show that a scattering only approach extracts more information\nthan traditional second order summary statistics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We demonstrate the successful use of scattering representations without\nfurther compression for simulation-based inference (SBI) with images (i.e.\nfield-level), illustrated with a cosmological case study. Scattering\nrepresentations provide a highly effective representational space for\nsubsequent learning tasks, although the higher dimensional compressed space\nintroduces challenges. We overcome these through spatial averaging, coupled\nwith more expressive density estimators. Compared to alternative methods, such\nan approach does not require additional simulations for either training or\ncomputing derivatives, is interpretable, and resilient to covariate shift. As\nexpected, we show that a scattering only approach extracts more information\nthan traditional second order summary statistics."
                },
                "authors": [
                    {
                        "name": "Kiyam Lin"
                    },
                    {
                        "name": "Benjamin Joachimi"
                    },
                    {
                        "name": "Jason D. McEwen"
                    }
                ],
                "author_detail": {
                    "name": "Jason D. McEwen"
                },
                "author": "Jason D. McEwen",
                "arxiv_comment": "9 pages, 2 figures, accepted by NeurIPS workshop on Machine Learning\n  and the Physical Sciences",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11883v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11883v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18773v2",
                "updated": "2024-12-02T12:08:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    12,
                    8,
                    13,
                    0,
                    337,
                    0
                ],
                "published": "2024-11-27T21:53:15Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    21,
                    53,
                    15,
                    2,
                    332,
                    0
                ],
                "title": "Inference on Dynamic Spatial Autoregressive Models with Change Point\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference on Dynamic Spatial Autoregressive Models with Change Point\n  Detection"
                },
                "summary": "We analyze a varying-coefficient dynamic spatial autoregressive model with\nspatial fixed effects. One salient feature of the model is the incorporation of\nmultiple spatial weight matrices through their linear combinations with varying\ncoefficients, which help solve the problem of choosing the most \"correct\" one\nfor applied econometricians who often face the availability of multiple expert\nspatial weight matrices. We estimate and make inferences on the model\ncoefficients and coefficients in basis expansions of the varying coefficients\nthrough penalized estimations, establishing the oracle properties of the\nestimators and the consistency of the overall estimated spatial weight matrix,\nwhich can be time-dependent. We further consider two applications of our model\nin change point detections in dynamic spatial autoregressive models, providing\ntheoretical justifications in consistent change point locations estimation and\npractical implementations. Simulation experiments demonstrate the performance\nof our proposed methodology, and a real data analysis is also carried out.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We analyze a varying-coefficient dynamic spatial autoregressive model with\nspatial fixed effects. One salient feature of the model is the incorporation of\nmultiple spatial weight matrices through their linear combinations with varying\ncoefficients, which help solve the problem of choosing the most \"correct\" one\nfor applied econometricians who often face the availability of multiple expert\nspatial weight matrices. We estimate and make inferences on the model\ncoefficients and coefficients in basis expansions of the varying coefficients\nthrough penalized estimations, establishing the oracle properties of the\nestimators and the consistency of the overall estimated spatial weight matrix,\nwhich can be time-dependent. We further consider two applications of our model\nin change point detections in dynamic spatial autoregressive models, providing\ntheoretical justifications in consistent change point locations estimation and\npractical implementations. Simulation experiments demonstrate the performance\nof our proposed methodology, and a real data analysis is also carried out."
                },
                "authors": [
                    {
                        "name": "Zetai Cen"
                    },
                    {
                        "name": "Yudong Chen"
                    },
                    {
                        "name": "Clifford Lam"
                    }
                ],
                "author_detail": {
                    "name": "Clifford Lam"
                },
                "author": "Clifford Lam",
                "arxiv_comment": "68 pages, 5 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62F30",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.09810v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.09810v2",
                "updated": "2024-12-02T11:30:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    30,
                    51,
                    0,
                    337,
                    0
                ],
                "published": "2024-07-13T09:11:59Z",
                "published_parsed": [
                    2024,
                    7,
                    13,
                    9,
                    11,
                    59,
                    5,
                    195,
                    0
                ],
                "title": "Euclid and KiDS-1000: Quantifying the impact of source-lens clustering\n  on cosmic shear analyses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Euclid and KiDS-1000: Quantifying the impact of source-lens clustering\n  on cosmic shear analyses"
                },
                "summary": "The transition from current Stage-III surveys such as the Kilo-Degree Survey\n(KiDS) to the increased area and redshift range of Stage IV surveys such as\nEuclid will significantly increase the precision of weak lensing analyses.\nHowever, with increasing precision, the accuracy of model assumptions needs to\nbe evaluated. In this study, we quantify the impact of the correlated\nclustering of weak lensing source galaxies with the surrounding large-scale\nstructure, known as source-lens clustering (SLC), which is commonly neglected.\nFor this, we use simulated cosmological datasets with realistically distributed\ngalaxies and measure shear correlation functions for both clustered and\nuniformly distributed source galaxies. Cosmological analyses are performed for\nboth scenarios to quantify the impact of SLC on parameter inference for a\nKiDS-like and a Euclid-like setting. We find for Stage III surveys, SLC has a\nminor impact when accounting for nuisance parameters for intrinsic alignments\nand shifts of tomographic bins, as these nuisance parameters absorb the effect\nof SLC, thus changing their original meaning. For KiDS (Euclid), the inferred\nintrinsic alignment amplitude $A_{IA}$ changes from $0.11_{-0.46}^{+0.44}$\n($-0.009_{-0.080}^{+0.079}$) for data without SLC to $0.28_{-0.44}^{+0.42}$\n($0.022_{-0.082}^{+0.081}$) with SLC. However, fixed nuisance parameters lead\nto shifts in $S_8$ and $\\Omega_{m}$, emphasizing the need for including SLC in\nthe modelling. For Euclid, we find that $\\sigma_8$, $\\Omega_m$, and $w_0$ are\nshifted by $0.19$, $0.12$, and $0.12\\, \\sigma$, respectively, when including\nfree nuisance parameters, and by $0.20$, $0.16$, and $0.32\\,\\sigma$ when fixing\nthe nuisance parameters. Consequently, SLC on its own has only a small impact\non the inferred parameter inference when using uninformative priors for\nnuisance parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transition from current Stage-III surveys such as the Kilo-Degree Survey\n(KiDS) to the increased area and redshift range of Stage IV surveys such as\nEuclid will significantly increase the precision of weak lensing analyses.\nHowever, with increasing precision, the accuracy of model assumptions needs to\nbe evaluated. In this study, we quantify the impact of the correlated\nclustering of weak lensing source galaxies with the surrounding large-scale\nstructure, known as source-lens clustering (SLC), which is commonly neglected.\nFor this, we use simulated cosmological datasets with realistically distributed\ngalaxies and measure shear correlation functions for both clustered and\nuniformly distributed source galaxies. Cosmological analyses are performed for\nboth scenarios to quantify the impact of SLC on parameter inference for a\nKiDS-like and a Euclid-like setting. We find for Stage III surveys, SLC has a\nminor impact when accounting for nuisance parameters for intrinsic alignments\nand shifts of tomographic bins, as these nuisance parameters absorb the effect\nof SLC, thus changing their original meaning. For KiDS (Euclid), the inferred\nintrinsic alignment amplitude $A_{IA}$ changes from $0.11_{-0.46}^{+0.44}$\n($-0.009_{-0.080}^{+0.079}$) for data without SLC to $0.28_{-0.44}^{+0.42}$\n($0.022_{-0.082}^{+0.081}$) with SLC. However, fixed nuisance parameters lead\nto shifts in $S_8$ and $\\Omega_{m}$, emphasizing the need for including SLC in\nthe modelling. For Euclid, we find that $\\sigma_8$, $\\Omega_m$, and $w_0$ are\nshifted by $0.19$, $0.12$, and $0.12\\, \\sigma$, respectively, when including\nfree nuisance parameters, and by $0.20$, $0.16$, and $0.32\\,\\sigma$ when fixing\nthe nuisance parameters. Consequently, SLC on its own has only a small impact\non the inferred parameter inference when using uninformative priors for\nnuisance parameters."
                },
                "authors": [
                    {
                        "name": "L. Linke"
                    },
                    {
                        "name": "S. Unruh"
                    },
                    {
                        "name": "A. Wittje"
                    },
                    {
                        "name": "T. Schrabback"
                    },
                    {
                        "name": "S. Grandis"
                    },
                    {
                        "name": "M. Asgari"
                    },
                    {
                        "name": "A. Dvornik"
                    },
                    {
                        "name": "H. Hildebrandt"
                    },
                    {
                        "name": "H. Hoekstra"
                    },
                    {
                        "name": "B. Joachimi"
                    },
                    {
                        "name": "R. Reischke"
                    },
                    {
                        "name": "J. L. van den Busch"
                    },
                    {
                        "name": "A. H. Wright"
                    },
                    {
                        "name": "P. Schneider"
                    },
                    {
                        "name": "N. Aghanim"
                    },
                    {
                        "name": "B. Altieri"
                    },
                    {
                        "name": "A. Amara"
                    },
                    {
                        "name": "S. Andreon"
                    },
                    {
                        "name": "N. Auricchio"
                    },
                    {
                        "name": "C. Baccigalupi"
                    },
                    {
                        "name": "M. Baldi"
                    },
                    {
                        "name": "S. Bardelli"
                    },
                    {
                        "name": "D. Bonino"
                    },
                    {
                        "name": "E. Branchini"
                    },
                    {
                        "name": "M. Brescia"
                    },
                    {
                        "name": "J. Brinchmann"
                    },
                    {
                        "name": "S. Camera"
                    },
                    {
                        "name": "V. Capobianco"
                    },
                    {
                        "name": "C. Carbone"
                    },
                    {
                        "name": "V. F. Cardone"
                    },
                    {
                        "name": "J. Carretero"
                    },
                    {
                        "name": "S. Casas"
                    },
                    {
                        "name": "F. J. Castander"
                    },
                    {
                        "name": "M. Castellano"
                    },
                    {
                        "name": "S. Cavuoti"
                    },
                    {
                        "name": "A. Cimatti"
                    },
                    {
                        "name": "G. Congedo"
                    },
                    {
                        "name": "C. J. Conselice"
                    },
                    {
                        "name": "L. Conversi"
                    },
                    {
                        "name": "Y. Copin"
                    },
                    {
                        "name": "F. Courbin"
                    },
                    {
                        "name": "H. M. Courtois"
                    },
                    {
                        "name": "A. Da Silva"
                    },
                    {
                        "name": "H. Degaudenzi"
                    },
                    {
                        "name": "J. Dinis"
                    },
                    {
                        "name": "M. Douspis"
                    },
                    {
                        "name": "F. Dubath"
                    },
                    {
                        "name": "X. Dupac"
                    },
                    {
                        "name": "S. Dusini"
                    },
                    {
                        "name": "M. Farina"
                    },
                    {
                        "name": "S. Farrens"
                    },
                    {
                        "name": "S. Ferriol"
                    },
                    {
                        "name": "P. Fosalba"
                    },
                    {
                        "name": "M. Frailis"
                    },
                    {
                        "name": "E. Franceschi"
                    },
                    {
                        "name": "M. Fumana"
                    },
                    {
                        "name": "S. Galeotta"
                    },
                    {
                        "name": "B. Gillis"
                    },
                    {
                        "name": "C. Giocoli"
                    },
                    {
                        "name": "A. Grazian"
                    },
                    {
                        "name": "F. Grupp"
                    },
                    {
                        "name": "L. Guzzo"
                    },
                    {
                        "name": "S. V. H. Haugan"
                    },
                    {
                        "name": "W. Holmes"
                    },
                    {
                        "name": "I. Hook"
                    },
                    {
                        "name": "F. Hormuth"
                    },
                    {
                        "name": "A. Hornstrup"
                    },
                    {
                        "name": "P. Hudelot"
                    },
                    {
                        "name": "K. Jahnke"
                    },
                    {
                        "name": "E. KeihÃ¤nen"
                    },
                    {
                        "name": "S. Kermiche"
                    },
                    {
                        "name": "A. Kiessling"
                    },
                    {
                        "name": "M. Kilbinger"
                    },
                    {
                        "name": "T. Kitching"
                    },
                    {
                        "name": "B. Kubik"
                    },
                    {
                        "name": "K. Kuijken"
                    },
                    {
                        "name": "M. KÃ¼mmel"
                    },
                    {
                        "name": "M. Kunz"
                    },
                    {
                        "name": "H. Kurki-Suonio"
                    },
                    {
                        "name": "S. Ligori"
                    },
                    {
                        "name": "P. B. Lilje"
                    },
                    {
                        "name": "V. Lindholm"
                    },
                    {
                        "name": "I. Lloro"
                    },
                    {
                        "name": "D. Maino"
                    },
                    {
                        "name": "E. Maiorano"
                    },
                    {
                        "name": "O. Mansutti"
                    },
                    {
                        "name": "O. Marggraf"
                    },
                    {
                        "name": "K. Markovic"
                    },
                    {
                        "name": "N. Martinet"
                    },
                    {
                        "name": "F. Marulli"
                    },
                    {
                        "name": "R. Massey"
                    },
                    {
                        "name": "H. J. McCracken"
                    },
                    {
                        "name": "E. Medinaceli"
                    },
                    {
                        "name": "S. Mei"
                    },
                    {
                        "name": "Y. Mellier"
                    },
                    {
                        "name": "M. Meneghetti"
                    },
                    {
                        "name": "E. Merlin"
                    },
                    {
                        "name": "G. Meylan"
                    },
                    {
                        "name": "M. Moresco"
                    },
                    {
                        "name": "L. Moscardini"
                    },
                    {
                        "name": "E. Munari"
                    },
                    {
                        "name": "R. Nakajima"
                    },
                    {
                        "name": "R. C. Nichol"
                    },
                    {
                        "name": "S. -M. Niemi"
                    },
                    {
                        "name": "J. W. Nightingale"
                    },
                    {
                        "name": "C. Padilla"
                    },
                    {
                        "name": "S. Paltani"
                    },
                    {
                        "name": "F. Pasian"
                    },
                    {
                        "name": "K. Pedersen"
                    },
                    {
                        "name": "V. Pettorino"
                    },
                    {
                        "name": "S. Pires"
                    },
                    {
                        "name": "G. Polenta"
                    },
                    {
                        "name": "M. Poncet"
                    },
                    {
                        "name": "L. A. Popa"
                    },
                    {
                        "name": "F. Raison"
                    },
                    {
                        "name": "R. Rebolo"
                    },
                    {
                        "name": "A. Renzi"
                    },
                    {
                        "name": "J. Rhodes"
                    },
                    {
                        "name": "G. Riccio"
                    },
                    {
                        "name": "E. Romelli"
                    },
                    {
                        "name": "M. Roncarelli"
                    },
                    {
                        "name": "R. Saglia"
                    },
                    {
                        "name": "Z. Sakr"
                    },
                    {
                        "name": "D. Sapone"
                    },
                    {
                        "name": "B. Sartoris"
                    },
                    {
                        "name": "M. Schirmer"
                    },
                    {
                        "name": "A. Secroun"
                    },
                    {
                        "name": "G. Seidel"
                    },
                    {
                        "name": "S. Serrano"
                    },
                    {
                        "name": "C. Sirignano"
                    },
                    {
                        "name": "G. Sirri"
                    },
                    {
                        "name": "L. Stanco"
                    },
                    {
                        "name": "J. -L. Starck"
                    },
                    {
                        "name": "P. Tallada-CrespÃ­"
                    },
                    {
                        "name": "A. N. Taylor"
                    },
                    {
                        "name": "I. Tereno"
                    },
                    {
                        "name": "R. Toledo-Moreo"
                    },
                    {
                        "name": "F. Torradeflot"
                    },
                    {
                        "name": "I. Tutusaus"
                    },
                    {
                        "name": "L. Valenziano"
                    },
                    {
                        "name": "T. Vassallo"
                    },
                    {
                        "name": "G. Verdoes Kleijn"
                    },
                    {
                        "name": "A. Veropalumbo"
                    },
                    {
                        "name": "Y. Wang"
                    },
                    {
                        "name": "J. Weller"
                    },
                    {
                        "name": "G. Zamorani"
                    },
                    {
                        "name": "E. Zucca"
                    },
                    {
                        "name": "C. Burigana"
                    },
                    {
                        "name": "A. Pezzotta"
                    },
                    {
                        "name": "C. Porciani"
                    },
                    {
                        "name": "V. Scottez"
                    },
                    {
                        "name": "M. Viel"
                    },
                    {
                        "name": "A. M. C. Le Brun"
                    }
                ],
                "author_detail": {
                    "name": "A. M. C. Le Brun"
                },
                "arxiv_affiliation": "Laboratoire Univers et ThÃ©orie, Observatoire de Paris, UniversitÃ© PSL, UniversitÃ© Paris CitÃ©, CNRS, 92190 Meudon, France",
                "author": "A. M. C. Le Brun",
                "arxiv_comment": "17 pages plus appendix, 10 figures, replaced by version accepted by\n  Astronomy & Astrophysics. Abstract abridged on arXiv",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.09810v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.09810v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.06892v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.06892v2",
                "updated": "2024-12-02T11:24:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    24,
                    20,
                    0,
                    337,
                    0
                ],
                "published": "2024-03-11T16:48:25Z",
                "published_parsed": [
                    2024,
                    3,
                    11,
                    16,
                    48,
                    25,
                    0,
                    71,
                    0
                ],
                "title": "Real-time Transformer-based Open-Vocabulary Detection with Efficient\n  Fusion Head",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time Transformer-based Open-Vocabulary Detection with Efficient\n  Fusion Head"
                },
                "summary": "End-to-end transformer-based detectors (DETRs) have shown exceptional\nperformance in both closed-set and open-vocabulary object detection (OVD) tasks\nthrough the integration of language modalities. However, their demanding\ncomputational requirements have hindered their practical application in\nreal-time object detection (OD) scenarios. In this paper, we scrutinize the\nlimitations of two leading models in the OVDEval benchmark, OmDet and\nGrounding-DINO, and introduce OmDet-Turbo. This novel transformer-based\nreal-time OVD model features an innovative Efficient Fusion Head (EFH) module\ndesigned to alleviate the bottlenecks observed in OmDet and Grounding-DINO.\nNotably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with\nTensorRT and language cache techniques applied. Notably, in zero-shot scenarios\non COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on\npar with current state-of-the-art supervised models. Furthermore, it\nestablishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting an\nAP of 30.1 and an NMS-AP of 26.86, respectively. The practicality of\nOmDet-Turbo in industrial applications is underscored by its exceptional\nperformance on benchmark datasets and superior inference speed, positioning it\nas a compelling choice for real-time object detection tasks. Code:\n\\url{https://github.com/om-ai-lab/OmDet}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-end transformer-based detectors (DETRs) have shown exceptional\nperformance in both closed-set and open-vocabulary object detection (OVD) tasks\nthrough the integration of language modalities. However, their demanding\ncomputational requirements have hindered their practical application in\nreal-time object detection (OD) scenarios. In this paper, we scrutinize the\nlimitations of two leading models in the OVDEval benchmark, OmDet and\nGrounding-DINO, and introduce OmDet-Turbo. This novel transformer-based\nreal-time OVD model features an innovative Efficient Fusion Head (EFH) module\ndesigned to alleviate the bottlenecks observed in OmDet and Grounding-DINO.\nNotably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with\nTensorRT and language cache techniques applied. Notably, in zero-shot scenarios\non COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on\npar with current state-of-the-art supervised models. Furthermore, it\nestablishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting an\nAP of 30.1 and an NMS-AP of 26.86, respectively. The practicality of\nOmDet-Turbo in industrial applications is underscored by its exceptional\nperformance on benchmark datasets and superior inference speed, positioning it\nas a compelling choice for real-time object detection tasks. Code:\n\\url{https://github.com/om-ai-lab/OmDet}"
                },
                "authors": [
                    {
                        "name": "Tiancheng Zhao"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Xuan He"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Kyusong Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kyusong Lee"
                },
                "author": "Kyusong Lee",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.06892v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.06892v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.11932v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.11932v3",
                "updated": "2024-12-02T10:57:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    10,
                    57,
                    58,
                    0,
                    337,
                    0
                ],
                "published": "2024-05-20T10:16:26Z",
                "published_parsed": [
                    2024,
                    5,
                    20,
                    10,
                    16,
                    26,
                    0,
                    141,
                    0
                ],
                "title": "Nonequilbrium physics of generative diffusion models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonequilbrium physics of generative diffusion models"
                },
                "summary": "Generative diffusion models apply the concept of Langevin dynamics in physics\nto machine leaning, attracting a lot of interests from engineering, statistics\nand physics, but a complete picture about inherent mechanisms is still lacking.\nIn this paper, we provide a transparent physics analysis of diffusion models,\nformulating the fluctuation theorem, entropy production, equilibrium measure,\nand Franz-Parisi potential to understand the dynamic process and intrinsic\nphase transitions. Our analysis is rooted in a path integral representation of\nboth forward and backward dynamics, and in treating the reverse diffusion\ngenerative process as a statistical inference, where the time-dependent state\nvariables serve as quenched disorder akin to that in spin glass theory. Our\nstudy thus links stochastic thermodynamics, statistical inference and geometry\nbased analysis together to yield a coherent picture about how the generative\ndiffusion models work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative diffusion models apply the concept of Langevin dynamics in physics\nto machine leaning, attracting a lot of interests from engineering, statistics\nand physics, but a complete picture about inherent mechanisms is still lacking.\nIn this paper, we provide a transparent physics analysis of diffusion models,\nformulating the fluctuation theorem, entropy production, equilibrium measure,\nand Franz-Parisi potential to understand the dynamic process and intrinsic\nphase transitions. Our analysis is rooted in a path integral representation of\nboth forward and backward dynamics, and in treating the reverse diffusion\ngenerative process as a statistical inference, where the time-dependent state\nvariables serve as quenched disorder akin to that in spin glass theory. Our\nstudy thus links stochastic thermodynamics, statistical inference and geometry\nbased analysis together to yield a coherent picture about how the generative\ndiffusion models work."
                },
                "authors": [
                    {
                        "name": "Zhendong Yu"
                    },
                    {
                        "name": "Haiping Huang"
                    }
                ],
                "author_detail": {
                    "name": "Haiping Huang"
                },
                "author": "Haiping Huang",
                "arxiv_comment": "26 pages, 11 figures, 31 refs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.11932v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.11932v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14708v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14708v2",
                "updated": "2024-12-02T10:52:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    10,
                    52,
                    21,
                    0,
                    337,
                    0
                ],
                "published": "2024-11-22T03:33:51Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    3,
                    33,
                    51,
                    4,
                    327,
                    0
                ],
                "title": "Understanding LLM Embeddings for Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding LLM Embeddings for Regression"
                },
                "summary": "With the rise of large language models (LLMs) for flexibly processing\ninformation as strings, a natural application is regression, specifically by\npreprocessing string representations into LLM embeddings as downstream features\nfor metric prediction. In this paper, we provide one of the first comprehensive\ninvestigations into embedding-based regression and demonstrate that LLM\nembeddings as features can be better for high-dimensional regression tasks than\nusing traditional feature engineering. This regression performance can be\nexplained in part due to LLM embeddings over numeric data inherently preserving\nLipschitz continuity over the feature space. Furthermore, we quantify the\ncontribution of different model effects, most notably model size and language\nunderstanding, which we find surprisingly do not always improve regression\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of large language models (LLMs) for flexibly processing\ninformation as strings, a natural application is regression, specifically by\npreprocessing string representations into LLM embeddings as downstream features\nfor metric prediction. In this paper, we provide one of the first comprehensive\ninvestigations into embedding-based regression and demonstrate that LLM\nembeddings as features can be better for high-dimensional regression tasks than\nusing traditional feature engineering. This regression performance can be\nexplained in part due to LLM embeddings over numeric data inherently preserving\nLipschitz continuity over the feature space. Furthermore, we quantify the\ncontribution of different model effects, most notably model size and language\nunderstanding, which we find surprisingly do not always improve regression\nperformance."
                },
                "authors": [
                    {
                        "name": "Eric Tang"
                    },
                    {
                        "name": "Bangding Yang"
                    },
                    {
                        "name": "Xingyou Song"
                    }
                ],
                "author_detail": {
                    "name": "Xingyou Song"
                },
                "author": "Xingyou Song",
                "arxiv_comment": "16 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14708v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14708v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.19211v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.19211v2",
                "updated": "2024-12-02T10:44:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    10,
                    44,
                    8,
                    0,
                    337,
                    0
                ],
                "published": "2024-03-28T08:19:33Z",
                "published_parsed": [
                    2024,
                    3,
                    28,
                    8,
                    19,
                    33,
                    3,
                    88,
                    0
                ],
                "title": "Dual-Personalizing Adapter for Federated Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dual-Personalizing Adapter for Federated Foundation Models"
                },
                "summary": "Recently, foundation models, particularly large language models (LLMs), have\ndemonstrated an impressive ability to adapt to various tasks by fine-tuning\ndiverse instruction data. Notably, federated foundation models (FedFM) emerge\nas a privacy preservation method to fine-tune models collaboratively under\nfederated learning (FL) settings by leveraging many distributed datasets with\nnon-IID data. To alleviate communication and computation overhead,\nparameter-efficient methods are introduced for efficiency, and some research\nadapted personalization methods to FedFM for better user preferences alignment.\nHowever, a critical gap in existing research is the neglect of test-time\ndistribution shifts in real-world applications, and conventional methods for\ntest-time distribution shifts in personalized FL are less effective for FedFM\ndue to their failure to adapt to complex distribution shift scenarios and the\nrequirement to train all parameters. To bridge this gap, we refine the setting\nin FedFM, termed test-time personalization, which aims to learn personalized\nfederated foundation models on clients while effectively handling test-time\ndistribution shifts simultaneously. To address challenges in this setting, we\nexplore a simple yet effective solution, a Federated Dual-Personalizing Adapter\n(FedDPA) architecture. By co-working with a foundation model, a global adapter\nand a local adapter jointly tackle the test-time distribution shifts and\nclient-specific personalization. Additionally, we introduce an instance-wise\ndynamic weighting mechanism that dynamically integrates the global and local\nadapters for each test instance during inference, facilitating effective\ntest-time personalization. The effectiveness of the proposed method has been\nevaluated on benchmark datasets across different NLP tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, foundation models, particularly large language models (LLMs), have\ndemonstrated an impressive ability to adapt to various tasks by fine-tuning\ndiverse instruction data. Notably, federated foundation models (FedFM) emerge\nas a privacy preservation method to fine-tune models collaboratively under\nfederated learning (FL) settings by leveraging many distributed datasets with\nnon-IID data. To alleviate communication and computation overhead,\nparameter-efficient methods are introduced for efficiency, and some research\nadapted personalization methods to FedFM for better user preferences alignment.\nHowever, a critical gap in existing research is the neglect of test-time\ndistribution shifts in real-world applications, and conventional methods for\ntest-time distribution shifts in personalized FL are less effective for FedFM\ndue to their failure to adapt to complex distribution shift scenarios and the\nrequirement to train all parameters. To bridge this gap, we refine the setting\nin FedFM, termed test-time personalization, which aims to learn personalized\nfederated foundation models on clients while effectively handling test-time\ndistribution shifts simultaneously. To address challenges in this setting, we\nexplore a simple yet effective solution, a Federated Dual-Personalizing Adapter\n(FedDPA) architecture. By co-working with a foundation model, a global adapter\nand a local adapter jointly tackle the test-time distribution shifts and\nclient-specific personalization. Additionally, we introduce an instance-wise\ndynamic weighting mechanism that dynamically integrates the global and local\nadapters for each test instance during inference, facilitating effective\ntest-time personalization. The effectiveness of the proposed method has been\nevaluated on benchmark datasets across different NLP tasks."
                },
                "authors": [
                    {
                        "name": "Yiyuan Yang"
                    },
                    {
                        "name": "Guodong Long"
                    },
                    {
                        "name": "Tao Shen"
                    },
                    {
                        "name": "Jing Jiang"
                    },
                    {
                        "name": "Michael Blumenstein"
                    }
                ],
                "author_detail": {
                    "name": "Michael Blumenstein"
                },
                "author": "Michael Blumenstein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.19211v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.19211v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.06125v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.06125v2",
                "updated": "2024-12-02T10:02:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    10,
                    2,
                    18,
                    0,
                    337,
                    0
                ],
                "published": "2024-07-08T17:00:51Z",
                "published_parsed": [
                    2024,
                    7,
                    8,
                    17,
                    0,
                    51,
                    0,
                    190,
                    0
                ],
                "title": "Depression Detection and Analysis using Large Language Models on Textual\n  and Audio-Visual Modalities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Depression Detection and Analysis using Large Language Models on Textual\n  and Audio-Visual Modalities"
                },
                "summary": "Depression has proven to be a significant public health issue, profoundly\naffecting the psychological well-being of individuals. If it remains\nundiagnosed, depression can lead to severe health issues, which can manifest\nphysically and even lead to suicide. Generally, Diagnosing depression or any\nother mental disorder involves conducting semi-structured interviews alongside\nsupplementary questionnaires, including variants of the Patient Health\nQuestionnaire (PHQ) by Clinicians and mental health professionals. This\napproach places significant reliance on the experience and judgment of trained\nphysicians, making the diagnosis susceptible to personal biases. Given that the\nunderlying mechanisms causing depression are still being actively researched,\nphysicians often face challenges in diagnosing and treating the condition,\nparticularly in its early stages of clinical presentation. Recently,\nsignificant strides have been made in Artificial neural computing to solve\nproblems involving text, image, and speech in various domains. Our analysis has\naimed to leverage these state-of-the-art (SOTA) models in our experiments to\nachieve optimal outcomes leveraging multiple modalities. The experiments were\nperformed on the Extended Distress Analysis Interview Corpus Wizard of Oz\ndataset (E-DAIC) corpus presented in the Audio/Visual Emotion Challenge (AVEC)\n2019 Challenge. The proposed solutions demonstrate better results achieved by\nProprietary and Open-source Large Language Models (LLMs), which achieved a Root\nMean Square Error (RMSE) score of 3.98 on Textual Modality, beating the AVEC\n2019 challenge baseline results and current SOTA regression analysis\narchitectures. Additionally, the proposed solution achieved an accuracy of\n71.43% in the classification task. The paper also includes a novel audio-visual\nmulti-modal network that predicts PHQ-8 scores with an RMSE of 6.51.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Depression has proven to be a significant public health issue, profoundly\naffecting the psychological well-being of individuals. If it remains\nundiagnosed, depression can lead to severe health issues, which can manifest\nphysically and even lead to suicide. Generally, Diagnosing depression or any\nother mental disorder involves conducting semi-structured interviews alongside\nsupplementary questionnaires, including variants of the Patient Health\nQuestionnaire (PHQ) by Clinicians and mental health professionals. This\napproach places significant reliance on the experience and judgment of trained\nphysicians, making the diagnosis susceptible to personal biases. Given that the\nunderlying mechanisms causing depression are still being actively researched,\nphysicians often face challenges in diagnosing and treating the condition,\nparticularly in its early stages of clinical presentation. Recently,\nsignificant strides have been made in Artificial neural computing to solve\nproblems involving text, image, and speech in various domains. Our analysis has\naimed to leverage these state-of-the-art (SOTA) models in our experiments to\nachieve optimal outcomes leveraging multiple modalities. The experiments were\nperformed on the Extended Distress Analysis Interview Corpus Wizard of Oz\ndataset (E-DAIC) corpus presented in the Audio/Visual Emotion Challenge (AVEC)\n2019 Challenge. The proposed solutions demonstrate better results achieved by\nProprietary and Open-source Large Language Models (LLMs), which achieved a Root\nMean Square Error (RMSE) score of 3.98 on Textual Modality, beating the AVEC\n2019 challenge baseline results and current SOTA regression analysis\narchitectures. Additionally, the proposed solution achieved an accuracy of\n71.43% in the classification task. The paper also includes a novel audio-visual\nmulti-modal network that predicts PHQ-8 scores with an RMSE of 6.51."
                },
                "authors": [
                    {
                        "name": "Chayan Tank"
                    },
                    {
                        "name": "Sarthak Pol"
                    },
                    {
                        "name": "Vinayak Katoch"
                    },
                    {
                        "name": "Shaina Mehta"
                    },
                    {
                        "name": "Avinash Anand"
                    },
                    {
                        "name": "Rajiv Ratn Shah"
                    }
                ],
                "author_detail": {
                    "name": "Rajiv Ratn Shah"
                },
                "author": "Rajiv Ratn Shah",
                "arxiv_comment": "12 pages, 9 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.06125v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.06125v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.10807v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.10807v3",
                "updated": "2024-12-02T09:59:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    9,
                    59,
                    54,
                    0,
                    337,
                    0
                ],
                "published": "2023-12-17T20:13:20Z",
                "published_parsed": [
                    2023,
                    12,
                    17,
                    20,
                    13,
                    20,
                    6,
                    351,
                    0
                ],
                "title": "Bridging Language and Action: A Survey of Language-Conditioned Robot\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Language and Action: A Survey of Language-Conditioned Robot\n  Manipulation"
                },
                "summary": "Language-conditioned robot manipulation is an emerging field aimed at\nenabling seamless communication and cooperation between humans and robotic\nagents by teaching robots to comprehend and execute instructions conveyed in\nnatural language. This interdisciplinary area integrates scene understanding,\nlanguage processing, and policy learning to bridge the gap between human\ninstructions and robotic actions. In this comprehensive survey, we\nsystematically explore recent advancements in language-conditioned robotic\nmanipulation. We categorize existing methods into language-conditioned reward\nshaping, language-conditioned policy learning, neuro-symbolic artificial\nintelligence, and the utilization of foundational models (FMs) such as large\nlanguage models (LLMs) and vision-language models (VLMs). Specifically, we\nanalyze state-of-the-art techniques concerning semantic information extraction,\nenvironment and evaluation, auxiliary tasks, and task representation\nstrategies. By conducting a comparative analysis, we highlight the strengths\nand limitations of current approaches in bridging language instructions with\nrobot actions. Finally, we discuss open challenges and future research\ndirections, focusing on potentially enhancing generalization capabilities and\naddressing safety issues in language-conditioned robot manipulators. The GitHub\nrepository of this paper can be found at\nhttps://github.com/hk-zh/language-conditioned-robot-manipulation-models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-conditioned robot manipulation is an emerging field aimed at\nenabling seamless communication and cooperation between humans and robotic\nagents by teaching robots to comprehend and execute instructions conveyed in\nnatural language. This interdisciplinary area integrates scene understanding,\nlanguage processing, and policy learning to bridge the gap between human\ninstructions and robotic actions. In this comprehensive survey, we\nsystematically explore recent advancements in language-conditioned robotic\nmanipulation. We categorize existing methods into language-conditioned reward\nshaping, language-conditioned policy learning, neuro-symbolic artificial\nintelligence, and the utilization of foundational models (FMs) such as large\nlanguage models (LLMs) and vision-language models (VLMs). Specifically, we\nanalyze state-of-the-art techniques concerning semantic information extraction,\nenvironment and evaluation, auxiliary tasks, and task representation\nstrategies. By conducting a comparative analysis, we highlight the strengths\nand limitations of current approaches in bridging language instructions with\nrobot actions. Finally, we discuss open challenges and future research\ndirections, focusing on potentially enhancing generalization capabilities and\naddressing safety issues in language-conditioned robot manipulators. The GitHub\nrepository of this paper can be found at\nhttps://github.com/hk-zh/language-conditioned-robot-manipulation-models."
                },
                "authors": [
                    {
                        "name": "Hongkuan Zhou"
                    },
                    {
                        "name": "Xiangtong Yao"
                    },
                    {
                        "name": "Oier Mees"
                    },
                    {
                        "name": "Yuan Meng"
                    },
                    {
                        "name": "Ted Xiao"
                    },
                    {
                        "name": "Yonatan Bisk"
                    },
                    {
                        "name": "Jean Oh"
                    },
                    {
                        "name": "Edward Johns"
                    },
                    {
                        "name": "Mohit Shridhar"
                    },
                    {
                        "name": "Dhruv Shah"
                    },
                    {
                        "name": "Jesse Thomason"
                    },
                    {
                        "name": "Kai Huang"
                    },
                    {
                        "name": "Joyce Chai"
                    },
                    {
                        "name": "Zhenshan Bing"
                    },
                    {
                        "name": "Alois Knoll"
                    }
                ],
                "author_detail": {
                    "name": "Alois Knoll"
                },
                "author": "Alois Knoll",
                "arxiv_comment": "37 pages, 15 figures, 4 tables, 353 citations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.10807v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.10807v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14377v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14377v2",
                "updated": "2024-12-02T09:48:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    9,
                    48,
                    21,
                    0,
                    337,
                    0
                ],
                "published": "2024-05-23T09:52:15Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    9,
                    52,
                    15,
                    3,
                    144,
                    0
                ],
                "title": "CoMERA: Computing- and Memory-Efficient Training via Rank-Adaptive\n  Tensor Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoMERA: Computing- and Memory-Efficient Training via Rank-Adaptive\n  Tensor Optimization"
                },
                "summary": "Training large AI models such as LLMs and DLRMs costs massive GPUs and\ncomputing time. The high training cost has become only affordable to big tech\ncompanies, meanwhile also causing increasing concerns about the environmental\nimpact. This paper presents CoMERA, a Computing- and Memory-Efficient training\nmethod via Rank-Adaptive tensor optimization. CoMERA achieves rank-adaptive\ntensor-compressed (pre)-training via a multi-objective optimization formulation\nand improves the training to provide both a high compression ratio and\nexcellent accuracy in the training process. Our optimized numerical computation\n(e.g., optimized tensorized embedding and tensor-network contractions) and GPU\nimplementation eliminate part of the run-time overhead in the tensorized\ntraining on GPU. This leads to, for the first time, $2-3\\times$ speedup per\ntraining epoch compared with standard training. CoMERA also outperforms the\nrecent GaLore in terms of both memory and computing efficiency. Specifically,\nCoMERA is $2\\times$ faster per training epoch and $9\\times$ more\nmemory-efficient than GaLore on a tested six-encoder transformer with\nsingle-batch training. Our method also shows $\\sim 2\\times$ speedup than\nstandard pre-training on a BERT-like code-generation LLM while achieving\n$4.23\\times$ compression ratio in pre-training. With further HPC optimization,\nCoMERA may reduce the pre-training cost of many other LLMs. An implementation\nof CoMERA is available at https://github.com/ziyangjoy/CoMERA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large AI models such as LLMs and DLRMs costs massive GPUs and\ncomputing time. The high training cost has become only affordable to big tech\ncompanies, meanwhile also causing increasing concerns about the environmental\nimpact. This paper presents CoMERA, a Computing- and Memory-Efficient training\nmethod via Rank-Adaptive tensor optimization. CoMERA achieves rank-adaptive\ntensor-compressed (pre)-training via a multi-objective optimization formulation\nand improves the training to provide both a high compression ratio and\nexcellent accuracy in the training process. Our optimized numerical computation\n(e.g., optimized tensorized embedding and tensor-network contractions) and GPU\nimplementation eliminate part of the run-time overhead in the tensorized\ntraining on GPU. This leads to, for the first time, $2-3\\times$ speedup per\ntraining epoch compared with standard training. CoMERA also outperforms the\nrecent GaLore in terms of both memory and computing efficiency. Specifically,\nCoMERA is $2\\times$ faster per training epoch and $9\\times$ more\nmemory-efficient than GaLore on a tested six-encoder transformer with\nsingle-batch training. Our method also shows $\\sim 2\\times$ speedup than\nstandard pre-training on a BERT-like code-generation LLM while achieving\n$4.23\\times$ compression ratio in pre-training. With further HPC optimization,\nCoMERA may reduce the pre-training cost of many other LLMs. An implementation\nof CoMERA is available at https://github.com/ziyangjoy/CoMERA."
                },
                "authors": [
                    {
                        "name": "Zi Yang"
                    },
                    {
                        "name": "Ziyue Liu"
                    },
                    {
                        "name": "Samridhi Choudhary"
                    },
                    {
                        "name": "Xinfeng Xie"
                    },
                    {
                        "name": "Cao Gao"
                    },
                    {
                        "name": "Siegfried Kunzmann"
                    },
                    {
                        "name": "Zheng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Zhang"
                },
                "author": "Zheng Zhang",
                "arxiv_comment": "Accepted by Neurips 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14377v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14377v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15344v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15344v2",
                "updated": "2024-12-02T09:45:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    9,
                    45,
                    7,
                    0,
                    337,
                    0
                ],
                "published": "2024-09-10T07:04:48Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    7,
                    4,
                    48,
                    1,
                    254,
                    0
                ],
                "title": "Video-Driven Graph Network-Based Simulators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-Driven Graph Network-Based Simulators"
                },
                "summary": "Lifelike visualizations in design, cinematography, and gaming rely on precise\nphysics simulations, typically requiring extensive computational resources and\ndetailed physical input. This paper presents a method that can infer a system's\nphysical properties from a short video, eliminating the need for explicit\nparameter input, provided it is close to the training condition. The learned\nrepresentation is then used within a Graph Network-based Simulator to emulate\nthe trajectories of physical systems. We demonstrate that the video-derived\nencodings effectively capture the physical properties of the system and\nshowcase a linear dependence between some of the encodings and the system's\nmotion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lifelike visualizations in design, cinematography, and gaming rely on precise\nphysics simulations, typically requiring extensive computational resources and\ndetailed physical input. This paper presents a method that can infer a system's\nphysical properties from a short video, eliminating the need for explicit\nparameter input, provided it is close to the training condition. The learned\nrepresentation is then used within a Graph Network-based Simulator to emulate\nthe trajectories of physical systems. We demonstrate that the video-derived\nencodings effectively capture the physical properties of the system and\nshowcase a linear dependence between some of the encodings and the system's\nmotion."
                },
                "authors": [
                    {
                        "name": "Franciszek Szewczyk"
                    },
                    {
                        "name": "Gilles Louppe"
                    },
                    {
                        "name": "Matthia Sabatelli"
                    }
                ],
                "author_detail": {
                    "name": "Matthia Sabatelli"
                },
                "author": "Matthia Sabatelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15344v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15344v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04125v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04125v2",
                "updated": "2024-12-02T09:42:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    9,
                    42,
                    24,
                    0,
                    337,
                    0
                ],
                "published": "2024-07-04T18:54:30Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    18,
                    54,
                    30,
                    3,
                    186,
                    0
                ],
                "title": "Query-Guided Self-Supervised Summarization of Nursing Notes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query-Guided Self-Supervised Summarization of Nursing Notes"
                },
                "summary": "Nursing notes, an important part of Electronic Health Records (EHRs), track a\npatient's health during a care episode. Summarizing key information in nursing\nnotes can help clinicians quickly understand patients' conditions. However,\nexisting summarization methods in the clinical setting, especially abstractive\nmethods, have overlooked nursing notes and require reference summaries for\ntraining. We introduce QGSumm, a novel query-guided self-supervised domain\nadaptation approach for abstractive nursing note summarization. The method uses\npatient-related clinical queries for guidance, and hence does not need\nreference summaries for training. Through automatic experiments and manual\nevaluation by an expert clinician, we study our approach and other\nstate-of-the-art Large Language Models (LLMs) for nursing note summarization.\nOur experiments show: 1) GPT-4 is competitive in maintaining information in the\noriginal nursing notes, 2) QGSumm can generate high-quality summaries with a\ngood balance between recall of the original content and hallucination rate\nlower than other top methods. Ultimately, our work offers a new perspective on\nconditional text summarization, tailored to clinical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nursing notes, an important part of Electronic Health Records (EHRs), track a\npatient's health during a care episode. Summarizing key information in nursing\nnotes can help clinicians quickly understand patients' conditions. However,\nexisting summarization methods in the clinical setting, especially abstractive\nmethods, have overlooked nursing notes and require reference summaries for\ntraining. We introduce QGSumm, a novel query-guided self-supervised domain\nadaptation approach for abstractive nursing note summarization. The method uses\npatient-related clinical queries for guidance, and hence does not need\nreference summaries for training. Through automatic experiments and manual\nevaluation by an expert clinician, we study our approach and other\nstate-of-the-art Large Language Models (LLMs) for nursing note summarization.\nOur experiments show: 1) GPT-4 is competitive in maintaining information in the\noriginal nursing notes, 2) QGSumm can generate high-quality summaries with a\ngood balance between recall of the original content and hallucination rate\nlower than other top methods. Ultimately, our work offers a new perspective on\nconditional text summarization, tailored to clinical applications."
                },
                "authors": [
                    {
                        "name": "Ya Gao"
                    },
                    {
                        "name": "Hans Moen"
                    },
                    {
                        "name": "Saila Koivusalo"
                    },
                    {
                        "name": "Miika Koskinen"
                    },
                    {
                        "name": "Pekka Marttinen"
                    }
                ],
                "author_detail": {
                    "name": "Pekka Marttinen"
                },
                "author": "Pekka Marttinen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04125v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04125v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19876v2",
                "updated": "2024-12-02T08:58:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    58,
                    20,
                    0,
                    337,
                    0
                ],
                "published": "2024-11-29T17:38:56Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    17,
                    38,
                    56,
                    4,
                    334,
                    0
                ],
                "title": "LUMIA: Linear probing for Unimodal and MultiModal Membership Inference\n  Attacks leveraging internal LLM states",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LUMIA: Linear probing for Unimodal and MultiModal Membership Inference\n  Attacks leveraging internal LLM states"
                },
                "summary": "Large Language Models (LLMs) are increasingly used in a variety of\napplications, but concerns around membership inference have grown in parallel.\nPrevious efforts focus on black-to-grey-box models, thus neglecting the\npotential benefit from internal LLM information. To address this, we propose\nthe use of Linear Probes (LPs) as a method to detect Membership Inference\nAttacks (MIAs) by examining internal activations of LLMs. Our approach, dubbed\nLUMIA, applies LPs layer-by-layer to get fine-grained data on the model inner\nworkings. We test this method across several model architectures, sizes and\ndatasets, including unimodal and multimodal tasks. In unimodal MIA, LUMIA\nachieves an average gain of 15.71 % in Area Under the Curve (AUC) over previous\ntechniques. Remarkably, LUMIA reaches AUC>60% in 65.33% of cases -- an\nincrement of 46.80% against the state of the art. Furthermore, our approach\nreveals key insights, such as the model layers where MIAs are most detectable.\nIn multimodal models, LPs indicate that visual inputs can significantly\ncontribute to detect MIAs -- AUC>60% is reached in 85.90% of experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used in a variety of\napplications, but concerns around membership inference have grown in parallel.\nPrevious efforts focus on black-to-grey-box models, thus neglecting the\npotential benefit from internal LLM information. To address this, we propose\nthe use of Linear Probes (LPs) as a method to detect Membership Inference\nAttacks (MIAs) by examining internal activations of LLMs. Our approach, dubbed\nLUMIA, applies LPs layer-by-layer to get fine-grained data on the model inner\nworkings. We test this method across several model architectures, sizes and\ndatasets, including unimodal and multimodal tasks. In unimodal MIA, LUMIA\nachieves an average gain of 15.71 % in Area Under the Curve (AUC) over previous\ntechniques. Remarkably, LUMIA reaches AUC>60% in 65.33% of cases -- an\nincrement of 46.80% against the state of the art. Furthermore, our approach\nreveals key insights, such as the model layers where MIAs are most detectable.\nIn multimodal models, LPs indicate that visual inputs can significantly\ncontribute to detect MIAs -- AUC>60% is reached in 85.90% of experiments."
                },
                "authors": [
                    {
                        "name": "Luis Ibanez-Lissen"
                    },
                    {
                        "name": "Lorena Gonzalez-Manzano"
                    },
                    {
                        "name": "Jose Maria de Fuentes"
                    },
                    {
                        "name": "Nicolas Anciaux"
                    },
                    {
                        "name": "Joaquin Garcia-Alfaro"
                    }
                ],
                "author_detail": {
                    "name": "Joaquin Garcia-Alfaro"
                },
                "author": "Joaquin Garcia-Alfaro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19916v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19916v3",
                "updated": "2024-12-02T08:56:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    56,
                    26,
                    0,
                    337,
                    0
                ],
                "published": "2024-09-30T03:37:10Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    3,
                    37,
                    10,
                    0,
                    274,
                    0
                ],
                "title": "Deep Learning and Machine Learning, Advancing Big Data Analytics and\n  Management: Object-Oriented Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning and Machine Learning, Advancing Big Data Analytics and\n  Management: Object-Oriented Programming"
                },
                "summary": "Object-Oriented Programming (OOP) has become a crucial paradigm for managing\nthe growing complexity of modern software systems, particularly in fields like\nmachine learning, deep learning, large language models (LLM), and data\nanalytics. This work provides a comprehensive introduction to the integration\nof OOP techniques within these domains, with a focus on improving code\nmodularity, maintainability, and scalability. We begin by outlining the\nevolution of computing and the rise of OOP, followed by an in-depth discussion\nof key OOP principles such as encapsulation, inheritance, polymorphism, and\nabstraction. The practical application of these principles is demonstrated\nusing Python, a widely adopted language in AI and data science. Furthermore, we\nexamine how design patterns and modular programming can be employed to enhance\nthe structure and efficiency of machine learning systems. In subsequent\nsections, we apply these OOP concepts to real-world AI tasks, including the\nencapsulation of preprocessing workflows, machine learning model training, and\nevaluation. Detailed examples illustrate how OOP can be used to build reusable,\nscalable machine learning systems while maintaining code clarity and reducing\nredundancy.This work is intended to serve as a bridge for both beginners and\nexperienced developers, equipping them with the necessary knowledge to apply\nOOP methodologies in AI-driven projects, ultimately fostering the development\nof more robust and maintainable systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object-Oriented Programming (OOP) has become a crucial paradigm for managing\nthe growing complexity of modern software systems, particularly in fields like\nmachine learning, deep learning, large language models (LLM), and data\nanalytics. This work provides a comprehensive introduction to the integration\nof OOP techniques within these domains, with a focus on improving code\nmodularity, maintainability, and scalability. We begin by outlining the\nevolution of computing and the rise of OOP, followed by an in-depth discussion\nof key OOP principles such as encapsulation, inheritance, polymorphism, and\nabstraction. The practical application of these principles is demonstrated\nusing Python, a widely adopted language in AI and data science. Furthermore, we\nexamine how design patterns and modular programming can be employed to enhance\nthe structure and efficiency of machine learning systems. In subsequent\nsections, we apply these OOP concepts to real-world AI tasks, including the\nencapsulation of preprocessing workflows, machine learning model training, and\nevaluation. Detailed examples illustrate how OOP can be used to build reusable,\nscalable machine learning systems while maintaining code clarity and reducing\nredundancy.This work is intended to serve as a bridge for both beginners and\nexperienced developers, equipping them with the necessary knowledge to apply\nOOP methodologies in AI-driven projects, ultimately fostering the development\nof more robust and maintainable systems."
                },
                "authors": [
                    {
                        "name": "Tianyang Wang"
                    },
                    {
                        "name": "Ziqian Bi"
                    },
                    {
                        "name": "Keyu Chen"
                    },
                    {
                        "name": "Jiawei Xu"
                    },
                    {
                        "name": "Qian Niu"
                    },
                    {
                        "name": "Junyu Liu"
                    },
                    {
                        "name": "Benji Peng"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Sen Zhang"
                    },
                    {
                        "name": "Xuanhe Pan"
                    },
                    {
                        "name": "Jinlang Wang"
                    },
                    {
                        "name": "Pohsun Feng"
                    },
                    {
                        "name": "Caitlyn Heqi Yin"
                    },
                    {
                        "name": "Yizhu Wen"
                    },
                    {
                        "name": "Ming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ming Liu"
                },
                "author": "Ming Liu",
                "arxiv_comment": "49pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19916v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19916v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03274v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03274v3",
                "updated": "2024-12-02T08:53:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    53,
                    40,
                    0,
                    337,
                    0
                ],
                "published": "2024-09-05T06:31:37Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    6,
                    31,
                    37,
                    3,
                    249,
                    0
                ],
                "title": "Recent Advances in Attack and Defense Approaches of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Advances in Attack and Defense Approaches of Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) have revolutionized artificial intelligence and\nmachine learning through their advanced text processing and generating\ncapabilities. However, their widespread deployment has raised significant\nsafety and reliability concerns. Established vulnerabilities in deep neural\nnetworks, coupled with emerging threat models, may compromise security\nevaluations and create a false sense of security. Given the extensive research\nin the field of LLM security, we believe that summarizing the current state of\naffairs will help the research community better understand the present\nlandscape and inform future developments. This paper reviews current research\non LLM vulnerabilities and threats, and evaluates the effectiveness of\ncontemporary defense mechanisms. We analyze recent studies on attack vectors\nand model weaknesses, providing insights into attack mechanisms and the\nevolving threat landscape. We also examine current defense strategies,\nhighlighting their strengths and limitations. By contrasting advancements in\nattack and defense methodologies, we identify research gaps and propose future\ndirections to enhance LLM security. Our goal is to advance the understanding of\nLLM safety challenges and guide the development of more robust security\nmeasures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized artificial intelligence and\nmachine learning through their advanced text processing and generating\ncapabilities. However, their widespread deployment has raised significant\nsafety and reliability concerns. Established vulnerabilities in deep neural\nnetworks, coupled with emerging threat models, may compromise security\nevaluations and create a false sense of security. Given the extensive research\nin the field of LLM security, we believe that summarizing the current state of\naffairs will help the research community better understand the present\nlandscape and inform future developments. This paper reviews current research\non LLM vulnerabilities and threats, and evaluates the effectiveness of\ncontemporary defense mechanisms. We analyze recent studies on attack vectors\nand model weaknesses, providing insights into attack mechanisms and the\nevolving threat landscape. We also examine current defense strategies,\nhighlighting their strengths and limitations. By contrasting advancements in\nattack and defense methodologies, we identify research gaps and propose future\ndirections to enhance LLM security. Our goal is to advance the understanding of\nLLM safety challenges and guide the development of more robust security\nmeasures."
                },
                "authors": [
                    {
                        "name": "Jing Cui"
                    },
                    {
                        "name": "Yishi Xu"
                    },
                    {
                        "name": "Zhewei Huang"
                    },
                    {
                        "name": "Shuchang Zhou"
                    },
                    {
                        "name": "Jianbin Jiao"
                    },
                    {
                        "name": "Junge Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Junge Zhang"
                },
                "author": "Junge Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03274v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03274v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16605v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16605v2",
                "updated": "2024-12-02T08:41:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    41,
                    46,
                    0,
                    337,
                    0
                ],
                "published": "2024-05-26T15:31:09Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    15,
                    31,
                    9,
                    6,
                    147,
                    0
                ],
                "title": "Demystify Mamba in Vision: A Linear Attention Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystify Mamba in Vision: A Linear Attention Perspective"
                },
                "summary": "Mamba is an effective state space model with linear computation complexity.\nIt has recently shown impressive efficiency in dealing with high-resolution\ninputs across various vision tasks. In this paper, we reveal that the powerful\nMamba model shares surprising similarities with linear attention Transformer,\nwhich typically underperform conventional Transformer in practice. By exploring\nthe similarities and disparities between the effective Mamba and subpar linear\nattention Transformer, we provide comprehensive analyses to demystify the key\nfactors behind Mamba's success. Specifically, we reformulate the selective\nstate space model and linear attention within a unified formulation, rephrasing\nMamba as a variant of linear attention Transformer with six major distinctions:\ninput gate, forget gate, shortcut, no attention normalization, single-head, and\nmodified block design. For each design, we meticulously analyze its pros and\ncons, and empirically evaluate its impact on model performance in vision tasks.\nInterestingly, the results highlight the forget gate and block design as the\ncore contributors to Mamba's success, while the other four designs are less\ncrucial. Based on these findings, we propose a Mamba-Inspired Linear Attention\n(MILA) model by incorporating the merits of these two key designs into linear\nattention. The resulting model outperforms various vision Mamba models in both\nimage classification and high-resolution dense prediction tasks, while enjoying\nparallelizable computation and fast inference speed. Code is available at\nhttps://github.com/LeapLabTHU/MLLA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mamba is an effective state space model with linear computation complexity.\nIt has recently shown impressive efficiency in dealing with high-resolution\ninputs across various vision tasks. In this paper, we reveal that the powerful\nMamba model shares surprising similarities with linear attention Transformer,\nwhich typically underperform conventional Transformer in practice. By exploring\nthe similarities and disparities between the effective Mamba and subpar linear\nattention Transformer, we provide comprehensive analyses to demystify the key\nfactors behind Mamba's success. Specifically, we reformulate the selective\nstate space model and linear attention within a unified formulation, rephrasing\nMamba as a variant of linear attention Transformer with six major distinctions:\ninput gate, forget gate, shortcut, no attention normalization, single-head, and\nmodified block design. For each design, we meticulously analyze its pros and\ncons, and empirically evaluate its impact on model performance in vision tasks.\nInterestingly, the results highlight the forget gate and block design as the\ncore contributors to Mamba's success, while the other four designs are less\ncrucial. Based on these findings, we propose a Mamba-Inspired Linear Attention\n(MILA) model by incorporating the merits of these two key designs into linear\nattention. The resulting model outperforms various vision Mamba models in both\nimage classification and high-resolution dense prediction tasks, while enjoying\nparallelizable computation and fast inference speed. Code is available at\nhttps://github.com/LeapLabTHU/MLLA."
                },
                "authors": [
                    {
                        "name": "Dongchen Han"
                    },
                    {
                        "name": "Ziyi Wang"
                    },
                    {
                        "name": "Zhuofan Xia"
                    },
                    {
                        "name": "Yizeng Han"
                    },
                    {
                        "name": "Yifan Pu"
                    },
                    {
                        "name": "Chunjiang Ge"
                    },
                    {
                        "name": "Jun Song"
                    },
                    {
                        "name": "Shiji Song"
                    },
                    {
                        "name": "Bo Zheng"
                    },
                    {
                        "name": "Gao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Gao Huang"
                },
                "author": "Gao Huang",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16605v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16605v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.09710v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.09710v2",
                "updated": "2024-12-02T08:05:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    5,
                    45,
                    0,
                    337,
                    0
                ],
                "published": "2024-02-15T05:06:53Z",
                "published_parsed": [
                    2024,
                    2,
                    15,
                    5,
                    6,
                    53,
                    3,
                    46,
                    0
                ],
                "title": "Preserving Data Privacy for ML-driven Applications in Open Radio Access\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preserving Data Privacy for ML-driven Applications in Open Radio Access\n  Networks"
                },
                "summary": "Deep learning offers a promising solution to improve spectrum access\ntechniques by utilizing data-driven approaches to manage and share limited\nspectrum resources for emerging applications. For several of these\napplications, the sensitive wireless data (such as spectrograms) are stored in\na shared database or multistakeholder cloud environment and are therefore prone\nto privacy leaks. This paper aims to address such privacy concerns by examining\nthe representative case study of shared database scenarios in 5G Open Radio\nAccess Network (O-RAN) networks where we have a shared database within the\nnear-real-time (near-RT) RAN intelligent controller. We focus on securing the\ndata that can be used by machine learning (ML) models for spectrum sharing and\ninterference mitigation applications without compromising the model and network\nperformances. The underlying idea is to leverage a (i) Shuffling-based\nlearnable encryption technique to encrypt the data, following which, (ii)\nemploy a custom Vision transformer (ViT) as the trained ML model that is\ncapable of performing accurate inferences on such encrypted data. The paper\noffers a thorough analysis and comparisons with analogous convolutional neural\nnetworks (CNN) as well as deeper architectures (such as ResNet-50) as\nbaselines. Our experiments showcase that the proposed approach significantly\noutperforms the baseline CNN with an improvement of 24.5% and 23.9% for the\npercent accuracy and F1-Score respectively when operated on encrypted data.\nThough deeper ResNet-50 architecture is obtained as a slightly more accurate\nmodel, with an increase of 4.4%, the proposed approach boasts a reduction of\nparameters by 99.32%, and thus, offers a much-improved prediction time by\nnearly 60%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning offers a promising solution to improve spectrum access\ntechniques by utilizing data-driven approaches to manage and share limited\nspectrum resources for emerging applications. For several of these\napplications, the sensitive wireless data (such as spectrograms) are stored in\na shared database or multistakeholder cloud environment and are therefore prone\nto privacy leaks. This paper aims to address such privacy concerns by examining\nthe representative case study of shared database scenarios in 5G Open Radio\nAccess Network (O-RAN) networks where we have a shared database within the\nnear-real-time (near-RT) RAN intelligent controller. We focus on securing the\ndata that can be used by machine learning (ML) models for spectrum sharing and\ninterference mitigation applications without compromising the model and network\nperformances. The underlying idea is to leverage a (i) Shuffling-based\nlearnable encryption technique to encrypt the data, following which, (ii)\nemploy a custom Vision transformer (ViT) as the trained ML model that is\ncapable of performing accurate inferences on such encrypted data. The paper\noffers a thorough analysis and comparisons with analogous convolutional neural\nnetworks (CNN) as well as deeper architectures (such as ResNet-50) as\nbaselines. Our experiments showcase that the proposed approach significantly\noutperforms the baseline CNN with an improvement of 24.5% and 23.9% for the\npercent accuracy and F1-Score respectively when operated on encrypted data.\nThough deeper ResNet-50 architecture is obtained as a slightly more accurate\nmodel, with an increase of 4.4%, the proposed approach boasts a reduction of\nparameters by 99.32%, and thus, offers a much-improved prediction time by\nnearly 60%."
                },
                "authors": [
                    {
                        "name": "Pranshav Gajjar"
                    },
                    {
                        "name": "Azuka Chiejina"
                    },
                    {
                        "name": "Vijay K. Shah"
                    }
                ],
                "author_detail": {
                    "name": "Vijay K. Shah"
                },
                "author": "Vijay K. Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.09710v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.09710v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.02408v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.02408v2",
                "updated": "2024-12-02T07:47:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    7,
                    47,
                    0,
                    0,
                    337,
                    0
                ],
                "published": "2024-02-04T08:57:54Z",
                "published_parsed": [
                    2024,
                    2,
                    4,
                    8,
                    57,
                    54,
                    6,
                    35,
                    0
                ],
                "title": "GLaPE: Gold Label-agnostic Prompt Evaluation and Optimization for Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GLaPE: Gold Label-agnostic Prompt Evaluation and Optimization for Large\n  Language Model"
                },
                "summary": "Despite the rapid progress of large language models (LLMs), their task\nperformance remains sensitive to prompt design. Recent studies have explored\nleveraging the LLM itself as an optimizer to identify optimal prompts that\nmaximize task accuracy. However, when evaluating prompts, such approaches\nheavily rely on elusive manually annotated gold labels to calculate task\naccuracy for each candidate prompt, which hinders the widespread implementation\nand generality. To overcome the limitation, this work proposes a gold\nlabel-agnostic prompt evaluation (GLaPE) to alleviate dependence on gold\nlabels. Motivated by the observed correlation between self-consistency and the\naccuracy of the answer, we adopt self-consistency as the initial evaluation\nscore. Subsequently, we refine the scores of prompts producing identical\nanswers to be mutually consistent. Experimental results show that GLaPE\nprovides reliable evaluations uniform with accuracy, even in the absence of\ngold labels. Moreover, on six popular reasoning tasks, our GLaPE-based prompt\noptimization yields effective prompts comparable to accuracy-based ones. The\ncode is publicly available at https://github.com/thunderous77/GLaPE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the rapid progress of large language models (LLMs), their task\nperformance remains sensitive to prompt design. Recent studies have explored\nleveraging the LLM itself as an optimizer to identify optimal prompts that\nmaximize task accuracy. However, when evaluating prompts, such approaches\nheavily rely on elusive manually annotated gold labels to calculate task\naccuracy for each candidate prompt, which hinders the widespread implementation\nand generality. To overcome the limitation, this work proposes a gold\nlabel-agnostic prompt evaluation (GLaPE) to alleviate dependence on gold\nlabels. Motivated by the observed correlation between self-consistency and the\naccuracy of the answer, we adopt self-consistency as the initial evaluation\nscore. Subsequently, we refine the scores of prompts producing identical\nanswers to be mutually consistent. Experimental results show that GLaPE\nprovides reliable evaluations uniform with accuracy, even in the absence of\ngold labels. Moreover, on six popular reasoning tasks, our GLaPE-based prompt\noptimization yields effective prompts comparable to accuracy-based ones. The\ncode is publicly available at https://github.com/thunderous77/GLaPE."
                },
                "authors": [
                    {
                        "name": "Xuanchang Zhang"
                    },
                    {
                        "name": "Zhuosheng Zhang"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.02408v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.02408v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16316v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16316v3",
                "updated": "2024-12-02T07:05:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    7,
                    5,
                    29,
                    0,
                    337,
                    0
                ],
                "published": "2024-11-25T12:09:43Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    9,
                    43,
                    0,
                    330,
                    0
                ],
                "title": "Monocular Lane Detection Based on Deep Learning: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monocular Lane Detection Based on Deep Learning: A Survey"
                },
                "summary": "Lane detection plays an important role in autonomous driving perception\nsystems. As deep learning algorithms gain popularity, monocular lane detection\nmethods based on them have demonstrated superior performance and emerged as a\nkey research direction in autonomous driving perception. The core designs of\nthese algorithmic frameworks can be summarized as follows: (1) Task paradigm,\nfocusing on lane instance-level discrimination; (2) Lane modeling, representing\nlanes as a set of learnable parameters in the neural network; (3) Global\ncontext supplementation, enhancing inference on the obscure lanes; (4)\nPerspective effect elimination, providing accurate 3D lanes for downstream\napplications. From these perspectives, this paper presents a comprehensive\noverview of existing methods, encompassing both the increasingly mature 2D lane\ndetection approaches and the developing 3D lane detection works. Besides, this\npaper compares the performance of mainstream methods on different benchmarks\nand investigates their inference speed under a unified setting for fair\ncomparison. Moreover, we present some extended works on lane detection,\nincluding multi-task perception, video lane detection, online high-definition\nmap construction, and lane topology reasoning, to offer readers a comprehensive\nroadmap for the evolution of lane detection. Finally, we point out some\npotential future research directions in this field. We exhaustively collect the\npapers and codes of existing works at\nhttps://github.com/Core9724/Awesome-Lane-Detection and will keep tracing the\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lane detection plays an important role in autonomous driving perception\nsystems. As deep learning algorithms gain popularity, monocular lane detection\nmethods based on them have demonstrated superior performance and emerged as a\nkey research direction in autonomous driving perception. The core designs of\nthese algorithmic frameworks can be summarized as follows: (1) Task paradigm,\nfocusing on lane instance-level discrimination; (2) Lane modeling, representing\nlanes as a set of learnable parameters in the neural network; (3) Global\ncontext supplementation, enhancing inference on the obscure lanes; (4)\nPerspective effect elimination, providing accurate 3D lanes for downstream\napplications. From these perspectives, this paper presents a comprehensive\noverview of existing methods, encompassing both the increasingly mature 2D lane\ndetection approaches and the developing 3D lane detection works. Besides, this\npaper compares the performance of mainstream methods on different benchmarks\nand investigates their inference speed under a unified setting for fair\ncomparison. Moreover, we present some extended works on lane detection,\nincluding multi-task perception, video lane detection, online high-definition\nmap construction, and lane topology reasoning, to offer readers a comprehensive\nroadmap for the evolution of lane detection. Finally, we point out some\npotential future research directions in this field. We exhaustively collect the\npapers and codes of existing works at\nhttps://github.com/Core9724/Awesome-Lane-Detection and will keep tracing the\nresearch."
                },
                "authors": [
                    {
                        "name": "Xin He"
                    },
                    {
                        "name": "Haiyun Guo"
                    },
                    {
                        "name": "Kuan Zhu"
                    },
                    {
                        "name": "Bingke Zhu"
                    },
                    {
                        "name": "Xu Zhao"
                    },
                    {
                        "name": "Jianwu Fang"
                    },
                    {
                        "name": "Jinqiao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jinqiao Wang"
                },
                "author": "Jinqiao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16316v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16316v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18363v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18363v2",
                "updated": "2024-12-02T07:04:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    7,
                    4,
                    40,
                    0,
                    337,
                    0
                ],
                "published": "2024-11-27T14:11:10Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    14,
                    11,
                    10,
                    2,
                    332,
                    0
                ],
                "title": "ChatRex: Taming Multimodal LLM for Joint Perception and Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatRex: Taming Multimodal LLM for Joint Perception and Understanding"
                },
                "summary": "Perception and understanding are two pillars of computer vision. While\nmultimodal large language models (MLLM) have demonstrated remarkable visual\nunderstanding capabilities, they arguably lack accurate perception abilities,\ne.g. the stage-of-the-art model Qwen2-VL only achieves a 43.9 recall rate on\nthe COCO dataset, limiting many tasks requiring the combination of perception\nand understanding. In this work, we aim to bridge this perception gap from both\nmodel designing and data development perspectives. We first introduce ChatRex,\nan MLLM with a decoupled perception design. Instead of having the LLM directly\npredict box coordinates, we feed the output boxes from a universal proposal\nnetwork into the LLM, allowing it to output the corresponding box indices to\nrepresent its detection results, turning the regression task into a\nretrieval-based task that LLM handles more proficiently. From the data\nperspective, we build a fully automated data engine and construct the\nRexverse-2M dataset which possesses multiple granularities to support the joint\ntraining of perception and understanding. After standard two-stage training,\nChatRex demonstrates strong perception capabilities while preserving multimodal\nunderstanding performance. The combination of these two capabilities\nsimultaneously unlocks many attractive applications, demonstrating the\ncomplementary roles of both perception and understanding in MLLM. Code is\navailable at \\url{https://github.com/IDEA-Research/ChatRex}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perception and understanding are two pillars of computer vision. While\nmultimodal large language models (MLLM) have demonstrated remarkable visual\nunderstanding capabilities, they arguably lack accurate perception abilities,\ne.g. the stage-of-the-art model Qwen2-VL only achieves a 43.9 recall rate on\nthe COCO dataset, limiting many tasks requiring the combination of perception\nand understanding. In this work, we aim to bridge this perception gap from both\nmodel designing and data development perspectives. We first introduce ChatRex,\nan MLLM with a decoupled perception design. Instead of having the LLM directly\npredict box coordinates, we feed the output boxes from a universal proposal\nnetwork into the LLM, allowing it to output the corresponding box indices to\nrepresent its detection results, turning the regression task into a\nretrieval-based task that LLM handles more proficiently. From the data\nperspective, we build a fully automated data engine and construct the\nRexverse-2M dataset which possesses multiple granularities to support the joint\ntraining of perception and understanding. After standard two-stage training,\nChatRex demonstrates strong perception capabilities while preserving multimodal\nunderstanding performance. The combination of these two capabilities\nsimultaneously unlocks many attractive applications, demonstrating the\ncomplementary roles of both perception and understanding in MLLM. Code is\navailable at \\url{https://github.com/IDEA-Research/ChatRex}."
                },
                "authors": [
                    {
                        "name": "Qing Jiang"
                    },
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Yuqin Yang"
                    },
                    {
                        "name": "Yuda Xiong"
                    },
                    {
                        "name": "Yihao Chen"
                    },
                    {
                        "name": "Zhaoyang Zeng"
                    },
                    {
                        "name": "Tianhe Ren"
                    },
                    {
                        "name": "Lei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zhang"
                },
                "author": "Lei Zhang",
                "arxiv_comment": "35 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18363v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18363v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10825v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10825v3",
                "updated": "2024-12-02T07:00:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    7,
                    0,
                    57,
                    0,
                    337,
                    0
                ],
                "published": "2024-09-17T01:37:57Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    1,
                    37,
                    57,
                    1,
                    261,
                    0
                ],
                "title": "Unveiling and Mitigating Bias in Large Language Model Recommendations: A\n  Path to Fairness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling and Mitigating Bias in Large Language Model Recommendations: A\n  Path to Fairness"
                },
                "summary": "excel in delivering comprehensive suggestions by deeply analyzing content and\nuser behavior. However, they often inherit biases from skewed training data,\nfavoring mainstream content while underrepresenting diverse or non-traditional\noptions. This study explores the interplay between bias and LLM-based\nrecommendation systems, focusing on music, song, and book recommendations\nacross diverse demographic and cultural groups. This paper analyzes bias in\nLLM-based recommendation systems across multiple models (GPT, LLaMA, and\nGemini), revealing its deep and pervasive impact on outcomes. Intersecting\nidentities and contextual factors, like socioeconomic status, further amplify\nbiases, complicating fair recommendations across diverse groups. Our findings\nreveal that bias in these systems is deeply ingrained, yet even simple\ninterventions like prompt engineering can significantly reduce it. We further\npropose a retrieval-augmented generation strategy to mitigate bias more\neffectively. Numerical experiments validate these strategies, demonstrating\nboth the pervasive nature of bias and the impact of the proposed solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "excel in delivering comprehensive suggestions by deeply analyzing content and\nuser behavior. However, they often inherit biases from skewed training data,\nfavoring mainstream content while underrepresenting diverse or non-traditional\noptions. This study explores the interplay between bias and LLM-based\nrecommendation systems, focusing on music, song, and book recommendations\nacross diverse demographic and cultural groups. This paper analyzes bias in\nLLM-based recommendation systems across multiple models (GPT, LLaMA, and\nGemini), revealing its deep and pervasive impact on outcomes. Intersecting\nidentities and contextual factors, like socioeconomic status, further amplify\nbiases, complicating fair recommendations across diverse groups. Our findings\nreveal that bias in these systems is deeply ingrained, yet even simple\ninterventions like prompt engineering can significantly reduce it. We further\npropose a retrieval-augmented generation strategy to mitigate bias more\neffectively. Numerical experiments validate these strategies, demonstrating\nboth the pervasive nature of bias and the impact of the proposed solutions."
                },
                "authors": [
                    {
                        "name": "Anindya Bijoy Das"
                    },
                    {
                        "name": "Shahnewaz Karim Sakib"
                    }
                ],
                "author_detail": {
                    "name": "Shahnewaz Karim Sakib"
                },
                "author": "Shahnewaz Karim Sakib",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10825v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10825v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19951v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19951v2",
                "updated": "2024-12-02T06:54:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    6,
                    54,
                    47,
                    0,
                    337,
                    0
                ],
                "published": "2024-11-29T18:59:54Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    18,
                    59,
                    54,
                    4,
                    334,
                    0
                ],
                "title": "T2Vid: Translating Long Text into Multi-Image is the Catalyst for\n  Video-LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T2Vid: Translating Long Text into Multi-Image is the Catalyst for\n  Video-LLMs"
                },
                "summary": "The success of Multimodal Large Language Models (MLLMs) in the image domain\nhas garnered wide attention from the research community. Drawing on previous\nsuccessful experiences, researchers have recently explored extending the\nsuccess to the video understanding realms. Apart from training from scratch, an\nefficient way is to utilize the pre-trained image-LLMs, leading to two\nmainstream approaches, i.e. zero-shot inference and further fine-tuning with\nvideo data. In this work, our study of these approaches harvests an effective\ndata augmentation method. We first make a deeper inspection of the zero-shot\ninference way and identify two limitations, i.e. limited generalization and\nlack of temporal understanding capabilities. Thus, we further investigate the\nfine-tuning approach and find a low learning efficiency when simply using all\nthe video data samples, which can be attributed to a lack of instruction\ndiversity. Aiming at this issue, we develop a method called T2Vid to synthesize\nvideo-like samples to enrich the instruction diversity in the training corpus.\nIntegrating these data enables a simple and efficient training scheme, which\nachieves performance comparable to or even superior to using full video\ndatasets by training with just 15% the sample size. Meanwhile, we find that the\nproposed scheme can boost the performance of long video understanding without\ntraining with long video samples. We hope our study will spark more thinking\nabout using MLLMs for video understanding and curation of high-quality data.\nThe code is released at https://github.com/xjtupanda/T2Vid.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of Multimodal Large Language Models (MLLMs) in the image domain\nhas garnered wide attention from the research community. Drawing on previous\nsuccessful experiences, researchers have recently explored extending the\nsuccess to the video understanding realms. Apart from training from scratch, an\nefficient way is to utilize the pre-trained image-LLMs, leading to two\nmainstream approaches, i.e. zero-shot inference and further fine-tuning with\nvideo data. In this work, our study of these approaches harvests an effective\ndata augmentation method. We first make a deeper inspection of the zero-shot\ninference way and identify two limitations, i.e. limited generalization and\nlack of temporal understanding capabilities. Thus, we further investigate the\nfine-tuning approach and find a low learning efficiency when simply using all\nthe video data samples, which can be attributed to a lack of instruction\ndiversity. Aiming at this issue, we develop a method called T2Vid to synthesize\nvideo-like samples to enrich the instruction diversity in the training corpus.\nIntegrating these data enables a simple and efficient training scheme, which\nachieves performance comparable to or even superior to using full video\ndatasets by training with just 15% the sample size. Meanwhile, we find that the\nproposed scheme can boost the performance of long video understanding without\ntraining with long video samples. We hope our study will spark more thinking\nabout using MLLMs for video understanding and curation of high-quality data.\nThe code is released at https://github.com/xjtupanda/T2Vid."
                },
                "authors": [
                    {
                        "name": "Shukang Yin"
                    },
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Sirui Zhao"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Chunjiang Ge"
                    },
                    {
                        "name": "Yan Yang"
                    },
                    {
                        "name": "Zuwei Long"
                    },
                    {
                        "name": "Yuhan Dai"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Ran He"
                    },
                    {
                        "name": "Caifeng Shan"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "arxiv_comment": "Project page: https://github.com/xjtupanda/T2Vid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19951v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19951v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13025v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13025v2",
                "updated": "2024-12-02T06:40:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    6,
                    40,
                    50,
                    0,
                    337,
                    0
                ],
                "published": "2024-10-16T20:33:06Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    20,
                    33,
                    6,
                    2,
                    290,
                    0
                ],
                "title": "LoRA Soups: Merging LoRAs for Practical Skill Composition Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRA Soups: Merging LoRAs for Practical Skill Composition Tasks"
                },
                "summary": "Low-Rank Adaptation (LoRA) is a popular technique for parameter-efficient\nfine-tuning of Large Language Models (LLMs). We study how different LoRA\nmodules can be merged to achieve skill composition -- testing the performance\nof the merged model on a target task that involves combining multiple skills,\neach skill coming from a single LoRA. This setup is favorable when it is\ndifficult to obtain training data for the target task and when it can be\ndecomposed into multiple skills. First, we identify practically occurring\nuse-cases that can be studied under the realm of skill composition, e.g.\nsolving hard math-word problems with code, creating a bot to answer questions\non proprietary manuals or about domain-specialized corpora. Our main\ncontribution is to show that concatenation of LoRAs (CAT), which optimally\nweights LoRAs that were individually trained on different skills, outperforms\nexisting model- and data- merging techniques; for instance on math-word\nproblems, CAT beats these methods by an average of 43% and 12% respectively.\nThus, this paper advocates model merging as an efficient way to solve\ncompositional tasks and underscores CAT as a simple, compute-friendly and\neffective procedure. To our knowledge, this is the first work demonstrating the\nsuperiority of model merging over data mixing for binary skill composition\ntasks. Code and data are available at https://github.com/aksh555/LoRA-Soups",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) is a popular technique for parameter-efficient\nfine-tuning of Large Language Models (LLMs). We study how different LoRA\nmodules can be merged to achieve skill composition -- testing the performance\nof the merged model on a target task that involves combining multiple skills,\neach skill coming from a single LoRA. This setup is favorable when it is\ndifficult to obtain training data for the target task and when it can be\ndecomposed into multiple skills. First, we identify practically occurring\nuse-cases that can be studied under the realm of skill composition, e.g.\nsolving hard math-word problems with code, creating a bot to answer questions\non proprietary manuals or about domain-specialized corpora. Our main\ncontribution is to show that concatenation of LoRAs (CAT), which optimally\nweights LoRAs that were individually trained on different skills, outperforms\nexisting model- and data- merging techniques; for instance on math-word\nproblems, CAT beats these methods by an average of 43% and 12% respectively.\nThus, this paper advocates model merging as an efficient way to solve\ncompositional tasks and underscores CAT as a simple, compute-friendly and\neffective procedure. To our knowledge, this is the first work demonstrating the\nsuperiority of model merging over data mixing for binary skill composition\ntasks. Code and data are available at https://github.com/aksh555/LoRA-Soups"
                },
                "authors": [
                    {
                        "name": "Akshara Prabhakar"
                    },
                    {
                        "name": "Yuanzhi Li"
                    },
                    {
                        "name": "Karthik Narasimhan"
                    },
                    {
                        "name": "Sham Kakade"
                    },
                    {
                        "name": "Eran Malach"
                    },
                    {
                        "name": "Samy Jelassi"
                    }
                ],
                "author_detail": {
                    "name": "Samy Jelassi"
                },
                "author": "Samy Jelassi",
                "arxiv_comment": "COLING 2025 Industry track; 9 pages plus references and appendices",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13025v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13025v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19943v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19943v2",
                "updated": "2024-12-02T06:26:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    6,
                    26,
                    38,
                    0,
                    337,
                    0
                ],
                "published": "2024-11-29T18:58:22Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    18,
                    58,
                    22,
                    4,
                    334,
                    0
                ],
                "title": "Critical Tokens Matter: Token-Level Contrastive Estimation Enhances\n  LLM's Reasoning Capability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critical Tokens Matter: Token-Level Contrastive Estimation Enhances\n  LLM's Reasoning Capability"
                },
                "summary": "Large Language Models (LLMs) have exhibited remarkable performance on\nreasoning tasks. They utilize autoregressive token generation to construct\nreasoning trajectories, enabling the development of a coherent chain of\nthought. In this work, we explore the impact of individual tokens on the final\noutcomes of reasoning tasks. We identify the existence of ``critical tokens''\nthat lead to incorrect reasoning trajectories in LLMs. Specifically, we find\nthat LLMs tend to produce positive outcomes when forced to decode other tokens\ninstead of critical tokens. Motivated by this observation, we propose a novel\napproach - cDPO - designed to automatically recognize and conduct token-level\nrewards for the critical tokens during the alignment process. Specifically, we\ndevelop a contrastive estimation approach to automatically identify critical\ntokens. It is achieved by comparing the generation likelihood of positive and\nnegative models. To achieve this, we separately fine-tune the positive and\nnegative models on various reasoning trajectories, consequently, they are\ncapable of identifying identify critical tokens within incorrect trajectories\nthat contribute to erroneous outcomes. Moreover, to further align the model\nwith the critical token information during the alignment process, we extend the\nconventional DPO algorithms to token-level DPO and utilize the differential\nlikelihood from the aforementioned positive and negative model as important\nweight for token-level DPO learning.Experimental results on GSM8K and MATH500\nbenchmarks with two-widely used models Llama-3 (8B and 70B) and deepseek-math\n(7B) demonstrate the effectiveness of the propsoed approach cDPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited remarkable performance on\nreasoning tasks. They utilize autoregressive token generation to construct\nreasoning trajectories, enabling the development of a coherent chain of\nthought. In this work, we explore the impact of individual tokens on the final\noutcomes of reasoning tasks. We identify the existence of ``critical tokens''\nthat lead to incorrect reasoning trajectories in LLMs. Specifically, we find\nthat LLMs tend to produce positive outcomes when forced to decode other tokens\ninstead of critical tokens. Motivated by this observation, we propose a novel\napproach - cDPO - designed to automatically recognize and conduct token-level\nrewards for the critical tokens during the alignment process. Specifically, we\ndevelop a contrastive estimation approach to automatically identify critical\ntokens. It is achieved by comparing the generation likelihood of positive and\nnegative models. To achieve this, we separately fine-tune the positive and\nnegative models on various reasoning trajectories, consequently, they are\ncapable of identifying identify critical tokens within incorrect trajectories\nthat contribute to erroneous outcomes. Moreover, to further align the model\nwith the critical token information during the alignment process, we extend the\nconventional DPO algorithms to token-level DPO and utilize the differential\nlikelihood from the aforementioned positive and negative model as important\nweight for token-level DPO learning.Experimental results on GSM8K and MATH500\nbenchmarks with two-widely used models Llama-3 (8B and 70B) and deepseek-math\n(7B) demonstrate the effectiveness of the propsoed approach cDPO."
                },
                "authors": [
                    {
                        "name": "Zicheng Lin"
                    },
                    {
                        "name": "Tian Liang"
                    },
                    {
                        "name": "Jiahao Xu"
                    },
                    {
                        "name": "Xing Wang"
                    },
                    {
                        "name": "Ruilin Luo"
                    },
                    {
                        "name": "Chufan Shi"
                    },
                    {
                        "name": "Siheng Li"
                    },
                    {
                        "name": "Yujiu Yang"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhaopeng Tu"
                },
                "author": "Zhaopeng Tu",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19943v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19943v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17244v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17244v2",
                "updated": "2024-12-02T05:55:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    5,
                    55,
                    38,
                    0,
                    337,
                    0
                ],
                "published": "2024-11-26T09:18:58Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    9,
                    18,
                    58,
                    1,
                    331,
                    0
                ],
                "title": "The apparent and cosmic rates of short gamma-ray bursts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The apparent and cosmic rates of short gamma-ray bursts"
                },
                "summary": "The short gamma-ray burst (sGRB), GRB~170817A, is often considered a rare\nevent. However, its inferred event rate, $\\mathcal{O}(100s)\\ \\text{Gpc}^{-3}\\\n\\text{yr}^{-1}$, exceeds cosmic sGRB rate estimates from high-redshift samples\nby an order of magnitude. This discrepancy can be explained by geometric\neffects related to the structure of the relativistic jet. We first illustrate\nhow adopting a detector flux threshold point estimate rather than an efficiency\nfunction, can lead to a large variation in rate estimates. Simulating the\nFermi-GBM sGRB detection efficiency, we then show that for a given a universal\nstructured jet profile, one can model a geometric bias with redshift. Assuming\ndifferent jet profiles, we show a geometrically scaled rate of GRB~170817A is\nconsistent with the cosmic beaming uncorrected rate estimates of short\n$\\gamma$-ray bursts (sGRBs) and that geometry can boost observational rates\nwithin $\\mathcal{O}(100s)$\\,Mpc. We find an apparent GRB~170817A rate of\n$303_{-300}^{+1580}$ $\\mathrm{Gpc}^{-3}\\, \\mathrm{yr}^{-1} $ which when\ncorrected for geometry yields $6.15_{-6.06}^{+31.2}$ $\\mathrm{Gpc}^{-3}\\,\n\\mathrm{yr}^{-1} $ and $3.34_{-3.29}^{+16.7}$ $\\mathrm{Gpc}^{-3}\\,\n\\mathrm{yr}^{-1} $ for two different jet profiles, consistent with pre-2017\nestimates of the isotropic sGRB rate. Our study shows how jet structure can\nimpact rate estimations and could allow one to test structured jet profiles. We\nfinally show that modelling the maximum structured jet viewing angle with\nredshift can transform a cosmic beaming uncorrected rate to a representative\nestimate of the binary neutron star merger rate. We suggest this framework can\nbe used to demonstrate parity with merger rates or to yield estimates of the\nsuccessful jet fraction of sGRBs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The short gamma-ray burst (sGRB), GRB~170817A, is often considered a rare\nevent. However, its inferred event rate, $\\mathcal{O}(100s)\\ \\text{Gpc}^{-3}\\\n\\text{yr}^{-1}$, exceeds cosmic sGRB rate estimates from high-redshift samples\nby an order of magnitude. This discrepancy can be explained by geometric\neffects related to the structure of the relativistic jet. We first illustrate\nhow adopting a detector flux threshold point estimate rather than an efficiency\nfunction, can lead to a large variation in rate estimates. Simulating the\nFermi-GBM sGRB detection efficiency, we then show that for a given a universal\nstructured jet profile, one can model a geometric bias with redshift. Assuming\ndifferent jet profiles, we show a geometrically scaled rate of GRB~170817A is\nconsistent with the cosmic beaming uncorrected rate estimates of short\n$\\gamma$-ray bursts (sGRBs) and that geometry can boost observational rates\nwithin $\\mathcal{O}(100s)$\\,Mpc. We find an apparent GRB~170817A rate of\n$303_{-300}^{+1580}$ $\\mathrm{Gpc}^{-3}\\, \\mathrm{yr}^{-1} $ which when\ncorrected for geometry yields $6.15_{-6.06}^{+31.2}$ $\\mathrm{Gpc}^{-3}\\,\n\\mathrm{yr}^{-1} $ and $3.34_{-3.29}^{+16.7}$ $\\mathrm{Gpc}^{-3}\\,\n\\mathrm{yr}^{-1} $ for two different jet profiles, consistent with pre-2017\nestimates of the isotropic sGRB rate. Our study shows how jet structure can\nimpact rate estimations and could allow one to test structured jet profiles. We\nfinally show that modelling the maximum structured jet viewing angle with\nredshift can transform a cosmic beaming uncorrected rate to a representative\nestimate of the binary neutron star merger rate. We suggest this framework can\nbe used to demonstrate parity with merger rates or to yield estimates of the\nsuccessful jet fraction of sGRBs."
                },
                "authors": [
                    {
                        "name": "E. J. Howell"
                    },
                    {
                        "name": "E. Burns"
                    },
                    {
                        "name": "A. Goldstein"
                    }
                ],
                "author_detail": {
                    "name": "A. Goldstein"
                },
                "author": "A. Goldstein",
                "arxiv_comment": "Submitted to MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17244v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17244v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11285v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11285v2",
                "updated": "2024-12-02T05:22:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    5,
                    22,
                    1,
                    0,
                    337,
                    0
                ],
                "published": "2024-06-17T07:46:45Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    7,
                    46,
                    45,
                    0,
                    169,
                    0
                ],
                "title": "Self and Cross-Model Distillation for LLMs: Effective Methods for\n  Refusal Pattern Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self and Cross-Model Distillation for LLMs: Effective Methods for\n  Refusal Pattern Alignment"
                },
                "summary": "Large Language Models (LLMs) like OpenAI's GPT series, Anthropic's Claude,\nand Meta's LLaMa have shown remarkable capabilities in text generation.\nHowever, their susceptibility to toxic prompts presents significant security\nchallenges. This paper investigates alignment techniques, including Supervised\nFine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), to\nmitigate these risks. We conduct an empirical study on refusal patterns across\nnine LLMs, revealing that models with uniform refusal patterns, such as\nClaude3, exhibit higher security. Based on these findings, we propose\nself-distilling and cross-model distilling methods to enhance LLM security. Our\nresults show that these methods significantly improve refusal rates and reduce\nunsafe content, with cross-model distilling achieving refusal rates close to\nClaude3's 94.51%. These findings underscore the potential of distillation-based\nalignment in securing LLMs against toxic prompts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) like OpenAI's GPT series, Anthropic's Claude,\nand Meta's LLaMa have shown remarkable capabilities in text generation.\nHowever, their susceptibility to toxic prompts presents significant security\nchallenges. This paper investigates alignment techniques, including Supervised\nFine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), to\nmitigate these risks. We conduct an empirical study on refusal patterns across\nnine LLMs, revealing that models with uniform refusal patterns, such as\nClaude3, exhibit higher security. Based on these findings, we propose\nself-distilling and cross-model distilling methods to enhance LLM security. Our\nresults show that these methods significantly improve refusal rates and reduce\nunsafe content, with cross-model distilling achieving refusal rates close to\nClaude3's 94.51%. These findings underscore the potential of distillation-based\nalignment in securing LLMs against toxic prompts."
                },
                "authors": [
                    {
                        "name": "Jie Li"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Chongyang Liu"
                    },
                    {
                        "name": "Xiaoning Ren"
                    },
                    {
                        "name": "Ling Shi"
                    },
                    {
                        "name": "Weisong Sun"
                    },
                    {
                        "name": "Yinxing Xue"
                    }
                ],
                "author_detail": {
                    "name": "Yinxing Xue"
                },
                "author": "Yinxing Xue",
                "arxiv_comment": "The method used in the paper has obvious problems and ambiguities.\n  The security enhancement method we used cannot be considered distillation,\n  but it is described as distillation in the paper, and the experiment lacks\n  comparison and baseline, which has been criticized by many peers. In order to\n  avoid further dissemination, we have decided to withdraw the paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11285v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11285v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07656v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07656v2",
                "updated": "2024-12-02T04:36:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    4,
                    36,
                    45,
                    0,
                    337,
                    0
                ],
                "published": "2024-11-12T09:14:16Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    9,
                    14,
                    16,
                    1,
                    317,
                    0
                ],
                "title": "Mitigating Bias in Queer Representation within Large Language Models: A\n  Collaborative Agent Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Bias in Queer Representation within Large Language Models: A\n  Collaborative Agent Approach"
                },
                "summary": "Large Language Models (LLMs) often perpetuate biases in pronoun usage,\nleading to misrepresentation or exclusion of queer individuals. This paper\naddresses the specific problem of biased pronoun usage in LLM outputs,\nparticularly the inappropriate use of traditionally gendered pronouns (\"he,\"\n\"she\") when inclusive language is needed to accurately represent all\nidentities. We introduce a collaborative agent pipeline designed to mitigate\nthese biases by analyzing and optimizing pronoun usage for inclusivity. Our\nmulti-agent framework includes specialized agents for both bias detection and\ncorrection. Experimental evaluations using the Tango dataset-a benchmark\nfocused on gender pronoun usage-demonstrate that our approach significantly\nimproves inclusive pronoun classification, achieving a 32.6 percentage point\nincrease over GPT-4o in correctly disagreeing with inappropriate traditionally\ngendered pronouns $(\\chi^2 = 38.57, p < 0.0001)$. These results accentuate the\npotential of agent-driven frameworks in enhancing fairness and inclusivity in\nAI-generated content, demonstrating their efficacy in reducing biases and\npromoting socially responsible AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often perpetuate biases in pronoun usage,\nleading to misrepresentation or exclusion of queer individuals. This paper\naddresses the specific problem of biased pronoun usage in LLM outputs,\nparticularly the inappropriate use of traditionally gendered pronouns (\"he,\"\n\"she\") when inclusive language is needed to accurately represent all\nidentities. We introduce a collaborative agent pipeline designed to mitigate\nthese biases by analyzing and optimizing pronoun usage for inclusivity. Our\nmulti-agent framework includes specialized agents for both bias detection and\ncorrection. Experimental evaluations using the Tango dataset-a benchmark\nfocused on gender pronoun usage-demonstrate that our approach significantly\nimproves inclusive pronoun classification, achieving a 32.6 percentage point\nincrease over GPT-4o in correctly disagreeing with inappropriate traditionally\ngendered pronouns $(\\chi^2 = 38.57, p < 0.0001)$. These results accentuate the\npotential of agent-driven frameworks in enhancing fairness and inclusivity in\nAI-generated content, demonstrating their efficacy in reducing biases and\npromoting socially responsible AI."
                },
                "authors": [
                    {
                        "name": "Tianyi Huang"
                    },
                    {
                        "name": "Arya Somasundaram"
                    }
                ],
                "author_detail": {
                    "name": "Arya Somasundaram"
                },
                "author": "Arya Somasundaram",
                "arxiv_comment": "NeurIPS 2024 Queer in AI Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07656v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07656v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05600v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05600v3",
                "updated": "2024-12-02T03:51:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    3,
                    51,
                    34,
                    0,
                    337,
                    0
                ],
                "published": "2024-06-09T00:23:20Z",
                "published_parsed": [
                    2024,
                    6,
                    9,
                    0,
                    23,
                    20,
                    6,
                    161,
                    0
                ],
                "title": "61A Bot Report: AI Assistants in CS1 Save Students Homework Time and\n  Reduce Demands on Staff. (Now What?)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "61A Bot Report: AI Assistants in CS1 Save Students Homework Time and\n  Reduce Demands on Staff. (Now What?)"
                },
                "summary": "LLM-based chatbots enable students to get immediate, interactive help on\nhomework assignments, but even a thoughtfully-designed bot may not serve all\npedagogical goals. We report here on the development and deployment of a\nGPT-4-based interactive homework assistant (\"61A Bot\") for students in a large\nCS1 course; over 2000 students made over 100,000 requests of our Bot across two\nsemesters. Our assistant offers one-shot, contextual feedback within the\ncommand-line \"autograder\" students use to test their code. Our Bot wraps\nstudent code in a custom prompt that supports our pedagogical goals and avoids\nproviding solutions directly. Analyzing student feedback, questions, and\nautograder data, we find reductions in homework-related question rates in our\ncourse forum, as well as reductions in homework completion time when our Bot is\navailable. For students in the 50th-80th percentile, reductions can exceed 30\nminutes per assignment, up to 50% less time than students at the same\npercentile rank in prior semesters. Finally, we discuss these observations,\npotential impacts on student learning, and other potential costs and benefits\nof AI assistance in CS1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based chatbots enable students to get immediate, interactive help on\nhomework assignments, but even a thoughtfully-designed bot may not serve all\npedagogical goals. We report here on the development and deployment of a\nGPT-4-based interactive homework assistant (\"61A Bot\") for students in a large\nCS1 course; over 2000 students made over 100,000 requests of our Bot across two\nsemesters. Our assistant offers one-shot, contextual feedback within the\ncommand-line \"autograder\" students use to test their code. Our Bot wraps\nstudent code in a custom prompt that supports our pedagogical goals and avoids\nproviding solutions directly. Analyzing student feedback, questions, and\nautograder data, we find reductions in homework-related question rates in our\ncourse forum, as well as reductions in homework completion time when our Bot is\navailable. For students in the 50th-80th percentile, reductions can exceed 30\nminutes per assignment, up to 50% less time than students at the same\npercentile rank in prior semesters. Finally, we discuss these observations,\npotential impacts on student learning, and other potential costs and benefits\nof AI assistance in CS1."
                },
                "authors": [
                    {
                        "name": "J. D. Zamfirescu-Pereira"
                    },
                    {
                        "name": "Laryn Qi"
                    },
                    {
                        "name": "BjÃ¶rn Hartmann"
                    },
                    {
                        "name": "John DeNero"
                    },
                    {
                        "name": "Narges Norouzi"
                    }
                ],
                "author_detail": {
                    "name": "Narges Norouzi"
                },
                "author": "Narges Norouzi",
                "arxiv_doi": "10.1145/3641554.3701864",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3641554.3701864",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.05600v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05600v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "6 pages, 3 figures, 1 table, 1 page of references",
                "arxiv_journal_ref": "SIGCSE TS 2025, February 26-March 1, 2025, Pittsburgh, PA, USA",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15950v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15950v2",
                "updated": "2024-12-02T03:48:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    3,
                    48,
                    43,
                    0,
                    337,
                    0
                ],
                "published": "2024-08-28T17:08:56Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    8,
                    56,
                    2,
                    241,
                    0
                ],
                "title": "Atari-GPT: Benchmarking Multimodal Large Language Models as Low-Level\n  Policies in Atari Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Atari-GPT: Benchmarking Multimodal Large Language Models as Low-Level\n  Policies in Atari Games"
                },
                "summary": "Recent advancements in large language models (LLMs) have expanded their\ncapabilities beyond traditional text-based tasks to multimodal domains,\nintegrating visual, auditory, and textual data. While multimodal LLMs have been\nextensively explored for high-level planning in domains like robotics and\ngames, their potential as low-level controllers remains largely untapped. In\nthis paper, we introduce a novel benchmark aimed at testing the emergent\ncapabilities of multimodal LLMs as low-level policies in Atari games. Unlike\ntraditional reinforcement learning (RL) methods that require training for each\nnew environment and reward function specification, these LLMs utilize\npre-existing multimodal knowledge to directly engage with game environments.\nOur study assesses the performances of multiple multimodal LLMs against\ntraditional RL agents, human players, and random agents, focusing on their\nability to understand and interact with complex visual scenes and formulate\nstrategic responses. Our results show that these multimodal LLMs are not yet\ncapable of being zero-shot low-level policies. Furthermore, we see that this\nis, in part, due to their visual and spatial reasoning. Additional results and\nvideos are available on our project webpage:\nhttps://dev1nw.github.io/atari-gpt/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have expanded their\ncapabilities beyond traditional text-based tasks to multimodal domains,\nintegrating visual, auditory, and textual data. While multimodal LLMs have been\nextensively explored for high-level planning in domains like robotics and\ngames, their potential as low-level controllers remains largely untapped. In\nthis paper, we introduce a novel benchmark aimed at testing the emergent\ncapabilities of multimodal LLMs as low-level policies in Atari games. Unlike\ntraditional reinforcement learning (RL) methods that require training for each\nnew environment and reward function specification, these LLMs utilize\npre-existing multimodal knowledge to directly engage with game environments.\nOur study assesses the performances of multiple multimodal LLMs against\ntraditional RL agents, human players, and random agents, focusing on their\nability to understand and interact with complex visual scenes and formulate\nstrategic responses. Our results show that these multimodal LLMs are not yet\ncapable of being zero-shot low-level policies. Furthermore, we see that this\nis, in part, due to their visual and spatial reasoning. Additional results and\nvideos are available on our project webpage:\nhttps://dev1nw.github.io/atari-gpt/."
                },
                "authors": [
                    {
                        "name": "Nicholas R. Waytowich"
                    },
                    {
                        "name": "Devin White"
                    },
                    {
                        "name": "MD Sunbeam"
                    },
                    {
                        "name": "Vinicius G. Goecks"
                    }
                ],
                "author_detail": {
                    "name": "Vinicius G. Goecks"
                },
                "author": "Vinicius G. Goecks",
                "arxiv_comment": "Currently under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15950v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15950v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01245v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01245v3",
                "updated": "2024-12-02T03:27:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    3,
                    27,
                    10,
                    0,
                    337,
                    0
                ],
                "published": "2024-04-01T17:03:41Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    17,
                    3,
                    41,
                    0,
                    92,
                    0
                ],
                "title": "A Statistical Framework of Watermarks for Large Language Models: Pivot,\n  Detection Efficiency and Optimal Rules",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Statistical Framework of Watermarks for Large Language Models: Pivot,\n  Detection Efficiency and Optimal Rules"
                },
                "summary": "Since ChatGPT was introduced in November 2022, embedding (nearly)\nunnoticeable statistical signals into text generated by large language models\n(LLMs), also known as watermarking, has been used as a principled approach to\nprovable detection of LLM-generated text from its human-written counterpart. In\nthis paper, we introduce a general and flexible framework for reasoning about\nthe statistical efficiency of watermarks and designing powerful detection\nrules. Inspired by the hypothesis testing formulation of watermark detection,\nour framework starts by selecting a pivotal statistic of the text and a secret\nkey -- provided by the LLM to the verifier -- to enable controlling the false\npositive rate (the error of mistakenly detecting human-written text as\nLLM-generated). Next, this framework allows one to evaluate the power of\nwatermark detection rules by obtaining a closed-form expression of the\nasymptotic false negative rate (the error of incorrectly classifying\nLLM-generated text as human-written). Our framework further reduces the problem\nof determining the optimal detection rule to solving a minimax optimization\nprogram. We apply this framework to two representative watermarks -- one of\nwhich has been internally implemented at OpenAI -- and obtain several findings\nthat can be instrumental in guiding the practice of implementing watermarks. In\nparticular, we derive optimal detection rules for these watermarks under our\nframework. These theoretically derived detection rules are demonstrated to be\ncompetitive and sometimes enjoy a higher power than existing detection\napproaches through numerical experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since ChatGPT was introduced in November 2022, embedding (nearly)\nunnoticeable statistical signals into text generated by large language models\n(LLMs), also known as watermarking, has been used as a principled approach to\nprovable detection of LLM-generated text from its human-written counterpart. In\nthis paper, we introduce a general and flexible framework for reasoning about\nthe statistical efficiency of watermarks and designing powerful detection\nrules. Inspired by the hypothesis testing formulation of watermark detection,\nour framework starts by selecting a pivotal statistic of the text and a secret\nkey -- provided by the LLM to the verifier -- to enable controlling the false\npositive rate (the error of mistakenly detecting human-written text as\nLLM-generated). Next, this framework allows one to evaluate the power of\nwatermark detection rules by obtaining a closed-form expression of the\nasymptotic false negative rate (the error of incorrectly classifying\nLLM-generated text as human-written). Our framework further reduces the problem\nof determining the optimal detection rule to solving a minimax optimization\nprogram. We apply this framework to two representative watermarks -- one of\nwhich has been internally implemented at OpenAI -- and obtain several findings\nthat can be instrumental in guiding the practice of implementing watermarks. In\nparticular, we derive optimal detection rules for these watermarks under our\nframework. These theoretically derived detection rules are demonstrated to be\ncompetitive and sometimes enjoy a higher power than existing detection\napproaches through numerical experiments."
                },
                "authors": [
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Feng Ruan"
                    },
                    {
                        "name": "Huiyuan Wang"
                    },
                    {
                        "name": "Qi Long"
                    },
                    {
                        "name": "Weijie J. Su"
                    }
                ],
                "author_detail": {
                    "name": "Weijie J. Su"
                },
                "author": "Weijie J. Su",
                "arxiv_comment": "To appear in the Annals of Statistics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01245v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01245v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.13174v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.13174v2",
                "updated": "2024-12-02T02:55:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    2,
                    55,
                    29,
                    0,
                    337,
                    0
                ],
                "published": "2024-01-24T01:41:26Z",
                "published_parsed": [
                    2024,
                    1,
                    24,
                    1,
                    41,
                    26,
                    2,
                    24,
                    0
                ],
                "title": "Towards Complementary Knowledge Distillation for Efficient Dense Image\n  Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Complementary Knowledge Distillation for Efficient Dense Image\n  Prediction"
                },
                "summary": "It has been revealed that small efficient dense image prediction (EDIP)\nmodels, trained using the knowledge distillation (KD) framework, encounter two\nkey challenges, including maintaining boundary region completeness and\npreserving target region connectivity, despite their favorable capacity to\nrecognize main object regions. In this work, we propose a complementary\nboundary and context distillation (BCD) method within the KD framework for\nEDIPs, which facilitates the targeted knowledge transfer from large accurate\nteacher models to compact efficient student models. Specifically, the boundary\ndistillation component focuses on extracting explicit object-level semantic\nboundaries from the hierarchical feature maps of the backbone network to\nenhance the student model's mask quality in boundary regions. Concurrently, the\ncontext distillation component leverages self-relations as a bridge to transfer\nimplicit pixel-level contexts from the teacher model to the student model,\nensuring strong connectivity in target regions. Our proposed BCD method is\nspecifically designed for EDIP tasks and is characterized by its simplicity and\nefficiency. Extensive experimental results across semantic segmentation, object\ndetection, and instance segmentation on various representative datasets\ndemonstrate that our method can outperform existing methods without requiring\nextra supervisions or incurring increased inference costs, resulting in\nwell-defined object boundaries and smooth connecting regions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It has been revealed that small efficient dense image prediction (EDIP)\nmodels, trained using the knowledge distillation (KD) framework, encounter two\nkey challenges, including maintaining boundary region completeness and\npreserving target region connectivity, despite their favorable capacity to\nrecognize main object regions. In this work, we propose a complementary\nboundary and context distillation (BCD) method within the KD framework for\nEDIPs, which facilitates the targeted knowledge transfer from large accurate\nteacher models to compact efficient student models. Specifically, the boundary\ndistillation component focuses on extracting explicit object-level semantic\nboundaries from the hierarchical feature maps of the backbone network to\nenhance the student model's mask quality in boundary regions. Concurrently, the\ncontext distillation component leverages self-relations as a bridge to transfer\nimplicit pixel-level contexts from the teacher model to the student model,\nensuring strong connectivity in target regions. Our proposed BCD method is\nspecifically designed for EDIP tasks and is characterized by its simplicity and\nefficiency. Extensive experimental results across semantic segmentation, object\ndetection, and instance segmentation on various representative datasets\ndemonstrate that our method can outperform existing methods without requiring\nextra supervisions or incurring increased inference costs, resulting in\nwell-defined object boundaries and smooth connecting regions."
                },
                "authors": [
                    {
                        "name": "Dong Zhang"
                    },
                    {
                        "name": "Pingcheng Dong"
                    },
                    {
                        "name": "Xinting Hu"
                    },
                    {
                        "name": "Long Chen"
                    },
                    {
                        "name": "Kwang-Ting Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Kwang-Ting Cheng"
                },
                "author": "Kwang-Ting Cheng",
                "arxiv_comment": "under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.13174v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.13174v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17912v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17912v2",
                "updated": "2024-12-02T02:42:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    2,
                    42,
                    5,
                    0,
                    337,
                    0
                ],
                "published": "2024-11-26T22:06:39Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    22,
                    6,
                    39,
                    1,
                    331,
                    0
                ],
                "title": "Can LLMs plan paths in the real world?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs plan paths in the real world?"
                },
                "summary": "As large language models (LLMs) increasingly integrate into vehicle\nnavigation systems, understanding their path-planning capability is crucial. We\ntested three LLMs through six real-world path-planning scenarios in various\nsettings and with various difficulties. Our experiments showed that all LLMs\nmade numerous errors in all scenarios, revealing that they are unreliable path\nplanners. We suggest that future work focus on implementing mechanisms for\nreality checks, enhancing model transparency, and developing smaller models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) increasingly integrate into vehicle\nnavigation systems, understanding their path-planning capability is crucial. We\ntested three LLMs through six real-world path-planning scenarios in various\nsettings and with various difficulties. Our experiments showed that all LLMs\nmade numerous errors in all scenarios, revealing that they are unreliable path\nplanners. We suggest that future work focus on implementing mechanisms for\nreality checks, enhancing model transparency, and developing smaller models."
                },
                "authors": [
                    {
                        "name": "Wanyi Chen"
                    },
                    {
                        "name": "Meng-Wen Su"
                    },
                    {
                        "name": "Nafisa Mehjabin"
                    },
                    {
                        "name": "Mary L. Cummings"
                    }
                ],
                "author_detail": {
                    "name": "Mary L. Cummings"
                },
                "author": "Mary L. Cummings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17912v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17912v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12735v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12735v4",
                "updated": "2024-12-02T02:34:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    2,
                    34,
                    47,
                    0,
                    337,
                    0
                ],
                "published": "2024-07-17T16:55:42Z",
                "published_parsed": [
                    2024,
                    7,
                    17,
                    16,
                    55,
                    42,
                    2,
                    199,
                    0
                ],
                "title": "EchoSight: Advancing Visual-Language Models with Wiki Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EchoSight: Advancing Visual-Language Models with Wiki Knowledge"
                },
                "summary": "Knowledge-based Visual Question Answering (KVQA) tasks require answering\nquestions about images using extensive background knowledge. Despite\nsignificant advancements, generative models often struggle with these tasks due\nto the limited integration of external knowledge. In this paper, we introduce\nEchoSight, a novel multimodal Retrieval-Augmented Generation (RAG) framework\nthat enables large language models (LLMs) to answer visual questions requiring\nfine-grained encyclopedic knowledge. To strive for high-performing retrieval,\nEchoSight first searches wiki articles by using visual-only information,\nsubsequently, these candidate articles are further reranked according to their\nrelevance to the combined text-image query. This approach significantly\nimproves the integration of multimodal knowledge, leading to enhanced retrieval\noutcomes and more accurate VQA responses. Our experimental results on the\nEncyclopedic VQA and InfoSeek datasets demonstrate that EchoSight establishes\nnew state-of-the-art results in knowledge-based VQA, achieving an accuracy of\n41.8% on Encyclopedic VQA and 31.3% on InfoSeek.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-based Visual Question Answering (KVQA) tasks require answering\nquestions about images using extensive background knowledge. Despite\nsignificant advancements, generative models often struggle with these tasks due\nto the limited integration of external knowledge. In this paper, we introduce\nEchoSight, a novel multimodal Retrieval-Augmented Generation (RAG) framework\nthat enables large language models (LLMs) to answer visual questions requiring\nfine-grained encyclopedic knowledge. To strive for high-performing retrieval,\nEchoSight first searches wiki articles by using visual-only information,\nsubsequently, these candidate articles are further reranked according to their\nrelevance to the combined text-image query. This approach significantly\nimproves the integration of multimodal knowledge, leading to enhanced retrieval\noutcomes and more accurate VQA responses. Our experimental results on the\nEncyclopedic VQA and InfoSeek datasets demonstrate that EchoSight establishes\nnew state-of-the-art results in knowledge-based VQA, achieving an accuracy of\n41.8% on Encyclopedic VQA and 31.3% on InfoSeek."
                },
                "authors": [
                    {
                        "name": "Yibin Yan"
                    },
                    {
                        "name": "Weidi Xie"
                    }
                ],
                "author_detail": {
                    "name": "Weidi Xie"
                },
                "author": "Weidi Xie",
                "arxiv_doi": "10.18653/v1/2024.findings-emnlp.83",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2024.findings-emnlp.83",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.12735v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12735v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by EMNLP 2024 findings; Project Page:\n  https://go2heart.github.io/echosight",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11266v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11266v3",
                "updated": "2024-12-02T02:27:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    2,
                    27,
                    17,
                    0,
                    337,
                    0
                ],
                "published": "2024-11-18T03:45:34Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    3,
                    45,
                    34,
                    0,
                    323,
                    0
                ],
                "title": "VersaTune: An Efficient Data Composition Framework for Training\n  Multi-Capability LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VersaTune: An Efficient Data Composition Framework for Training\n  Multi-Capability LLMs"
                },
                "summary": "Large-scale pretrained models, particularly Large Language Models (LLMs),\nhave exhibited remarkable capabilities in handling multiple tasks across\ndomains due to their emergent properties. These capabilities are further\naugmented during the Supervised Fine-Tuning (SFT) phase. Despite their\npotential, existing work mainly focuses on domain-specific enhancements during\nfine-tuning, the challenge of which lies in catastrophic forgetting of\nknowledge across other domains. In this study, we introduce VersaTune, a novel\ndata composition framework designed for enhancing LLMs' overall multi-ability\nperformances during training. We categorize knowledge into distinct domains\nincluding law, medicine, finance, science, code, etc. We begin with detecting\nthe distribution of domain-specific knowledge within the base model, followed\nby the training data composition that aligns with the model's existing\nknowledge distribution. During the training process, domain weights are\ndynamically adjusted based on their learnable potential and forgetting degree.\nExperimental results demonstrate that VersaTune achieves significant\nimprovements in multi-domain performance, with an 35.21% enhancement in\ncomprehensive multi-domain tasks. Additionally, in scenarios where specific\ndomain optimization is required, VersaTune reduces the degradation of\nperformance in other domains by 38.77%, without compromising the target\ndomain's training efficacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale pretrained models, particularly Large Language Models (LLMs),\nhave exhibited remarkable capabilities in handling multiple tasks across\ndomains due to their emergent properties. These capabilities are further\naugmented during the Supervised Fine-Tuning (SFT) phase. Despite their\npotential, existing work mainly focuses on domain-specific enhancements during\nfine-tuning, the challenge of which lies in catastrophic forgetting of\nknowledge across other domains. In this study, we introduce VersaTune, a novel\ndata composition framework designed for enhancing LLMs' overall multi-ability\nperformances during training. We categorize knowledge into distinct domains\nincluding law, medicine, finance, science, code, etc. We begin with detecting\nthe distribution of domain-specific knowledge within the base model, followed\nby the training data composition that aligns with the model's existing\nknowledge distribution. During the training process, domain weights are\ndynamically adjusted based on their learnable potential and forgetting degree.\nExperimental results demonstrate that VersaTune achieves significant\nimprovements in multi-domain performance, with an 35.21% enhancement in\ncomprehensive multi-domain tasks. Additionally, in scenarios where specific\ndomain optimization is required, VersaTune reduces the degradation of\nperformance in other domains by 38.77%, without compromising the target\ndomain's training efficacy."
                },
                "authors": [
                    {
                        "name": "Keer Lu"
                    },
                    {
                        "name": "Keshi Zhao"
                    },
                    {
                        "name": "Zheng Liang"
                    },
                    {
                        "name": "Da Pan"
                    },
                    {
                        "name": "Shusen Zhang"
                    },
                    {
                        "name": "Xin Wu"
                    },
                    {
                        "name": "Weipeng Chen"
                    },
                    {
                        "name": "Zenan Zhou"
                    },
                    {
                        "name": "Guosheng Dong"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11266v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11266v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18977v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18977v2",
                "updated": "2024-12-02T02:01:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    2,
                    1,
                    5,
                    0,
                    337,
                    0
                ],
                "published": "2024-11-28T07:58:30Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    7,
                    58,
                    30,
                    3,
                    333,
                    0
                ],
                "title": "Det-SAM2:Technical Report on the Self-Prompting Segmentation Framework\n  Based on Segment Anything Model 2",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Det-SAM2:Technical Report on the Self-Prompting Segmentation Framework\n  Based on Segment Anything Model 2"
                },
                "summary": "Segment Anything Model 2 (SAM2) demonstrates exceptional performance in video\nsegmentation and refinement of segmentation results. We anticipate that it can\nfurther evolve to achieve higher levels of automation for practical\napplications. Building upon SAM2, we conducted a series of practices that\nultimately led to the development of a fully automated pipeline, termed\nDet-SAM2, in which object prompts are automatically generated by a detection\nmodel to facilitate inference and refinement by SAM2. This pipeline enables\ninference on infinitely long video streams with constant VRAM and RAM usage,\nall while preserving the same efficiency and accuracy as the original SAM2.\n  This technical report focuses on the construction of the overall Det-SAM2\nframework and the subsequent engineering optimization applied to SAM2. We\npresent a case demonstrating an application built on the Det-SAM2 framework: AI\nrefereeing in a billiards scenario, derived from our business context. The\nproject at \\url{https://github.com/motern88/Det-SAM2}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Segment Anything Model 2 (SAM2) demonstrates exceptional performance in video\nsegmentation and refinement of segmentation results. We anticipate that it can\nfurther evolve to achieve higher levels of automation for practical\napplications. Building upon SAM2, we conducted a series of practices that\nultimately led to the development of a fully automated pipeline, termed\nDet-SAM2, in which object prompts are automatically generated by a detection\nmodel to facilitate inference and refinement by SAM2. This pipeline enables\ninference on infinitely long video streams with constant VRAM and RAM usage,\nall while preserving the same efficiency and accuracy as the original SAM2.\n  This technical report focuses on the construction of the overall Det-SAM2\nframework and the subsequent engineering optimization applied to SAM2. We\npresent a case demonstrating an application built on the Det-SAM2 framework: AI\nrefereeing in a billiards scenario, derived from our business context. The\nproject at \\url{https://github.com/motern88/Det-SAM2}."
                },
                "authors": [
                    {
                        "name": "Zhiting Wang"
                    },
                    {
                        "name": "Qiangong Zhou"
                    },
                    {
                        "name": "Zongyang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zongyang Liu"
                },
                "author": "Zongyang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18977v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18977v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.02326v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.02326v2",
                "updated": "2024-12-02T01:59:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    1,
                    59,
                    30,
                    0,
                    337,
                    0
                ],
                "published": "2024-04-23T18:55:49Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    18,
                    55,
                    49,
                    1,
                    114,
                    0
                ],
                "title": "Evaluating LLMs for Hardware Design and Test",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLMs for Hardware Design and Test"
                },
                "summary": "Large Language Models (LLMs) have demonstrated capabilities for producing\ncode in Hardware Description Languages (HDLs). However, most of the focus\nremains on their abilities to write functional code, not test code. The\nhardware design process consists of both design and test, and so eschewing\nvalidation and verification leaves considerable potential benefit unexplored,\ngiven that a design and test framework may allow for progress towards full\nautomation of the digital design pipeline. In this work, we perform one of the\nfirst studies exploring how a LLM can both design and test hardware modules\nfrom provided specifications. Using a suite of 8 representative benchmarks, we\nexamined the capabilities and limitations of the state-of-the-art\nconversational LLMs when producing Verilog for functional and verification\npurposes. We taped out the benchmarks on a Skywater 130nm shuttle and received\nthe functional chip.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated capabilities for producing\ncode in Hardware Description Languages (HDLs). However, most of the focus\nremains on their abilities to write functional code, not test code. The\nhardware design process consists of both design and test, and so eschewing\nvalidation and verification leaves considerable potential benefit unexplored,\ngiven that a design and test framework may allow for progress towards full\nautomation of the digital design pipeline. In this work, we perform one of the\nfirst studies exploring how a LLM can both design and test hardware modules\nfrom provided specifications. Using a suite of 8 representative benchmarks, we\nexamined the capabilities and limitations of the state-of-the-art\nconversational LLMs when producing Verilog for functional and verification\npurposes. We taped out the benchmarks on a Skywater 130nm shuttle and received\nthe functional chip."
                },
                "authors": [
                    {
                        "name": "Jason Blocklove"
                    },
                    {
                        "name": "Siddharth Garg"
                    },
                    {
                        "name": "Ramesh Karri"
                    },
                    {
                        "name": "Hammond Pearce"
                    }
                ],
                "author_detail": {
                    "name": "Hammond Pearce"
                },
                "author": "Hammond Pearce",
                "arxiv_doi": "10.1109/LAD62341.2024.10691811",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/LAD62341.2024.10691811",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.02326v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.02326v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14846v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14846v3",
                "updated": "2024-12-02T01:24:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    1,
                    24,
                    2,
                    0,
                    337,
                    0
                ],
                "published": "2024-10-18T19:50:32Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    19,
                    50,
                    32,
                    4,
                    292,
                    0
                ],
                "title": "Early Bright Galaxies from Helium Enhancements in High-Redshift Star\n  Clusters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early Bright Galaxies from Helium Enhancements in High-Redshift Star\n  Clusters"
                },
                "summary": "The first few cycles of JWST have identified an overabundance of UV-bright\ngalaxies and a general excess of UV luminosity density at $z\\gtrsim10$ compared\nto expectations from most (pre-JWST) theoretical models. Moreover, some of the\nbrightest high-redshift spectroscopically confirmed galaxies exhibit peculiar\nchemical abundance patterns, most notably extremely high N/O ratios. Since N/O\nhas been empirically shown to scale strongly with He/H, as expected for hot\nhydrogen burning, these same bright high-redshift galaxies are likely also\nhelium-enhanced. Under simplistic assumptions for stellar evolution, the\nbolometric luminosity of a star scales as\n$L\\propto(2-\\frac{5}{4}Y)^{-4}(2-Y)^{-1}$ -- hence a higher He/H leads to\nbrighter stars. In this Letter, we evolve a series of MESA models to the\nzero-age main-sequence and highlight that the helium enhancements at the levels\nmeasured and inferred for high-redshift galaxies can boost the 1500\n$\\mathring{\\rm A}$ UV luminosity by up to $\\sim50\\%$, while simultaneously\nincreasing the stellar effective temperature. The combination of helium\nenhancements with nebular continuum emission expected for intense bursts of\nstar formation have the potential to help reduce the tension between JWST\nobservations and certain galaxy formation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The first few cycles of JWST have identified an overabundance of UV-bright\ngalaxies and a general excess of UV luminosity density at $z\\gtrsim10$ compared\nto expectations from most (pre-JWST) theoretical models. Moreover, some of the\nbrightest high-redshift spectroscopically confirmed galaxies exhibit peculiar\nchemical abundance patterns, most notably extremely high N/O ratios. Since N/O\nhas been empirically shown to scale strongly with He/H, as expected for hot\nhydrogen burning, these same bright high-redshift galaxies are likely also\nhelium-enhanced. Under simplistic assumptions for stellar evolution, the\nbolometric luminosity of a star scales as\n$L\\propto(2-\\frac{5}{4}Y)^{-4}(2-Y)^{-1}$ -- hence a higher He/H leads to\nbrighter stars. In this Letter, we evolve a series of MESA models to the\nzero-age main-sequence and highlight that the helium enhancements at the levels\nmeasured and inferred for high-redshift galaxies can boost the 1500\n$\\mathring{\\rm A}$ UV luminosity by up to $\\sim50\\%$, while simultaneously\nincreasing the stellar effective temperature. The combination of helium\nenhancements with nebular continuum emission expected for intense bursts of\nstar formation have the potential to help reduce the tension between JWST\nobservations and certain galaxy formation models."
                },
                "authors": [
                    {
                        "name": "Harley Katz"
                    },
                    {
                        "name": "Alexander P. Ji"
                    },
                    {
                        "name": "O. Grace Telford"
                    },
                    {
                        "name": "Peter Senchyna"
                    }
                ],
                "author_detail": {
                    "name": "Peter Senchyna"
                },
                "author": "Peter Senchyna",
                "arxiv_comment": "Note that this is an erratum, fixing an error in equation 2 where we\n  erroneously dropped the exponent of $\\mu$. All conclusions and subsequent\n  calculations remain unchanged and corrected equations are now provided",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14846v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14846v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12269v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12269v2",
                "updated": "2024-12-01T23:07:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    23,
                    7,
                    27,
                    6,
                    336,
                    0
                ],
                "published": "2024-07-17T02:35:24Z",
                "published_parsed": [
                    2024,
                    7,
                    17,
                    2,
                    35,
                    24,
                    2,
                    199,
                    0
                ],
                "title": "UTG: Towards a Unified View of Snapshot and Event Based Models for\n  Temporal Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UTG: Towards a Unified View of Snapshot and Event Based Models for\n  Temporal Graphs"
                },
                "summary": "Many real world graphs are inherently dynamic, constantly evolving with node\nand edge additions. These graphs can be represented by temporal graphs, either\nthrough a stream of edge events or a sequence of graph snapshots. Until now,\nthe development of machine learning methods for both types has occurred largely\nin isolation, resulting in limited experimental comparison and theoretical\ncrosspollination between the two. In this paper, we introduce Unified Temporal\nGraph (UTG), a framework that unifies snapshot-based and event-based machine\nlearning models under a single umbrella, enabling models developed for one\nrepresentation to be applied effectively to datasets of the other. We also\npropose a novel UTG training procedure to boost the performance of\nsnapshot-based models in the streaming setting. We comprehensively evaluate\nboth snapshot and event-based models across both types of temporal graphs on\nthe temporal link prediction task. Our main findings are threefold: first, when\ncombined with UTG training, snapshot-based models can perform competitively\nwith event-based models such as TGN and GraphMixer even on event datasets.\nSecond, snapshot-based models are at least an order of magnitude faster than\nmost event-based models during inference. Third, while event-based methods such\nas NAT and DyGFormer outperforms snapshot-based methods on both types of\ntemporal graphs, this is because they leverage joint neighborhood structural\nfeatures thus emphasizing the potential to incorporate these features into\nsnapshotbased models as well. These findings highlight the importance of\ncomparing model architectures independent of the data format and suggest the\npotential of combining the efficiency of snapshot-based models with the\nperformance of event-based models in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many real world graphs are inherently dynamic, constantly evolving with node\nand edge additions. These graphs can be represented by temporal graphs, either\nthrough a stream of edge events or a sequence of graph snapshots. Until now,\nthe development of machine learning methods for both types has occurred largely\nin isolation, resulting in limited experimental comparison and theoretical\ncrosspollination between the two. In this paper, we introduce Unified Temporal\nGraph (UTG), a framework that unifies snapshot-based and event-based machine\nlearning models under a single umbrella, enabling models developed for one\nrepresentation to be applied effectively to datasets of the other. We also\npropose a novel UTG training procedure to boost the performance of\nsnapshot-based models in the streaming setting. We comprehensively evaluate\nboth snapshot and event-based models across both types of temporal graphs on\nthe temporal link prediction task. Our main findings are threefold: first, when\ncombined with UTG training, snapshot-based models can perform competitively\nwith event-based models such as TGN and GraphMixer even on event datasets.\nSecond, snapshot-based models are at least an order of magnitude faster than\nmost event-based models during inference. Third, while event-based methods such\nas NAT and DyGFormer outperforms snapshot-based methods on both types of\ntemporal graphs, this is because they leverage joint neighborhood structural\nfeatures thus emphasizing the potential to incorporate these features into\nsnapshotbased models as well. These findings highlight the importance of\ncomparing model architectures independent of the data format and suggest the\npotential of combining the efficiency of snapshot-based models with the\nperformance of event-based models in the future."
                },
                "authors": [
                    {
                        "name": "Shenyang Huang"
                    },
                    {
                        "name": "Farimah Poursafaei"
                    },
                    {
                        "name": "Reihaneh Rabbany"
                    },
                    {
                        "name": "Guillaume Rabusseau"
                    },
                    {
                        "name": "Emanuele Rossi"
                    }
                ],
                "author_detail": {
                    "name": "Emanuele Rossi"
                },
                "author": "Emanuele Rossi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12269v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12269v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13166v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13166v2",
                "updated": "2024-12-01T22:34:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    22,
                    34,
                    5,
                    6,
                    336,
                    0
                ],
                "published": "2024-06-19T02:45:32Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    2,
                    45,
                    32,
                    2,
                    171,
                    0
                ],
                "title": "Enhancing supply chain security with automated machine learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing supply chain security with automated machine learning"
                },
                "summary": "The increasing scale and complexity of global supply chains have led to new\nchallenges spanning various fields, such as supply chain disruptions due to\nlong waiting lines at the ports, material shortages, and inflation. Coupled\nwith the size of supply chains and the availability of vast amounts of data,\nefforts towards tackling such challenges have led to an increasing interest in\napplying machine learning methods in many aspects of supply chains. Unlike\nother solutions, ML techniques, including Random Forest, XGBoost, LightGBM, and\nNeural Networks, make predictions and approximate optimal solutions faster.\nThis paper presents an automated ML framework to enhance supply chain security\nby detecting fraudulent activities, predicting maintenance needs, and\nforecasting material backorders. Using datasets of varying sizes, results show\nthat fraud detection achieves an 88% accuracy rate using sampling methods,\nmachine failure prediction reaches 93.4% accuracy, and material backorder\nprediction achieves 89.3% accuracy. Hyperparameter tuning significantly\nimproved the performance of these models, with certain supervised techniques\nlike XGBoost and LightGBM reaching up to 100% precision. This research\ncontributes to supply chain security by streamlining data preprocessing,\nfeature selection, model optimization, and inference deployment, addressing\ncritical challenges and boosting operational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing scale and complexity of global supply chains have led to new\nchallenges spanning various fields, such as supply chain disruptions due to\nlong waiting lines at the ports, material shortages, and inflation. Coupled\nwith the size of supply chains and the availability of vast amounts of data,\nefforts towards tackling such challenges have led to an increasing interest in\napplying machine learning methods in many aspects of supply chains. Unlike\nother solutions, ML techniques, including Random Forest, XGBoost, LightGBM, and\nNeural Networks, make predictions and approximate optimal solutions faster.\nThis paper presents an automated ML framework to enhance supply chain security\nby detecting fraudulent activities, predicting maintenance needs, and\nforecasting material backorders. Using datasets of varying sizes, results show\nthat fraud detection achieves an 88% accuracy rate using sampling methods,\nmachine failure prediction reaches 93.4% accuracy, and material backorder\nprediction achieves 89.3% accuracy. Hyperparameter tuning significantly\nimproved the performance of these models, with certain supervised techniques\nlike XGBoost and LightGBM reaching up to 100% precision. This research\ncontributes to supply chain security by streamlining data preprocessing,\nfeature selection, model optimization, and inference deployment, addressing\ncritical challenges and boosting operational efficiency."
                },
                "authors": [
                    {
                        "name": "Haibo Wang"
                    },
                    {
                        "name": "Lutfu S. Sua"
                    },
                    {
                        "name": "Bahram Alidaee"
                    }
                ],
                "author_detail": {
                    "name": "Bahram Alidaee"
                },
                "author": "Bahram Alidaee",
                "arxiv_comment": "26 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13166v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13166v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.10792v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.10792v8",
                "updated": "2024-12-01T22:01:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    22,
                    1,
                    51,
                    6,
                    336,
                    0
                ],
                "published": "2023-08-21T15:35:16Z",
                "published_parsed": [
                    2023,
                    8,
                    21,
                    15,
                    35,
                    16,
                    0,
                    233,
                    0
                ],
                "title": "Instruction Tuning for Large Language Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction Tuning for Large Language Models: A Survey"
                },
                "summary": "This paper surveys research works in the quickly advancing field of\ninstruction tuning (IT), which can also be referred to as supervised\nfine-tuning (SFT)\\footnote{In this paper, unless specified otherwise,\nsupervised fine-tuning (SFT) and instruction tuning (IT) are used\ninterchangeably.}, a crucial technique to enhance the capabilities and\ncontrollability of large language models (LLMs). Instruction tuning refers to\nthe process of further training LLMs on a dataset consisting of\n\\textsc{(instruction, output)} pairs in a supervised fashion, which bridges the\ngap between the next-word prediction objective of LLMs and the users' objective\nof having LLMs adhere to human instructions. In this work, we make a systematic\nreview of the literature, including the general methodology of SFT, the\nconstruction of SFT datasets, the training of SFT models, and applications to\ndifferent modalities, domains and application, along with analysis on aspects\nthat influence the outcome of SFT (e.g., generation of instruction outputs,\nsize of the instruction dataset, etc). We also review the potential pitfalls of\nSFT along with criticism against it, along with efforts pointing out current\ndeficiencies of existing strategies and suggest some avenues for fruitful\nresearch. Project Page: github.com/xiaoya-li/Instruction-Tuning-Survey",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper surveys research works in the quickly advancing field of\ninstruction tuning (IT), which can also be referred to as supervised\nfine-tuning (SFT)\\footnote{In this paper, unless specified otherwise,\nsupervised fine-tuning (SFT) and instruction tuning (IT) are used\ninterchangeably.}, a crucial technique to enhance the capabilities and\ncontrollability of large language models (LLMs). Instruction tuning refers to\nthe process of further training LLMs on a dataset consisting of\n\\textsc{(instruction, output)} pairs in a supervised fashion, which bridges the\ngap between the next-word prediction objective of LLMs and the users' objective\nof having LLMs adhere to human instructions. In this work, we make a systematic\nreview of the literature, including the general methodology of SFT, the\nconstruction of SFT datasets, the training of SFT models, and applications to\ndifferent modalities, domains and application, along with analysis on aspects\nthat influence the outcome of SFT (e.g., generation of instruction outputs,\nsize of the instruction dataset, etc). We also review the potential pitfalls of\nSFT along with criticism against it, along with efforts pointing out current\ndeficiencies of existing strategies and suggest some avenues for fruitful\nresearch. Project Page: github.com/xiaoya-li/Instruction-Tuning-Survey"
                },
                "authors": [
                    {
                        "name": "Shengyu Zhang"
                    },
                    {
                        "name": "Linfeng Dong"
                    },
                    {
                        "name": "Xiaoya Li"
                    },
                    {
                        "name": "Sen Zhang"
                    },
                    {
                        "name": "Xiaofei Sun"
                    },
                    {
                        "name": "Shuhe Wang"
                    },
                    {
                        "name": "Jiwei Li"
                    },
                    {
                        "name": "Runyi Hu"
                    },
                    {
                        "name": "Tianwei Zhang"
                    },
                    {
                        "name": "Fei Wu"
                    },
                    {
                        "name": "Guoyin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guoyin Wang"
                },
                "author": "Guoyin Wang",
                "arxiv_comment": "V5; Last update: Dec. 1, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.10792v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.10792v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.09905v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.09905v2",
                "updated": "2024-12-01T21:42:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    21,
                    42,
                    37,
                    6,
                    336,
                    0
                ],
                "published": "2024-03-14T22:33:22Z",
                "published_parsed": [
                    2024,
                    3,
                    14,
                    22,
                    33,
                    22,
                    3,
                    74,
                    0
                ],
                "title": "Right Place, Right Time! Generalizing ObjectNav to Dynamic Environments\n  with Portable Targets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Right Place, Right Time! Generalizing ObjectNav to Dynamic Environments\n  with Portable Targets"
                },
                "summary": "ObjectNav is a popular task in Embodied AI, where an agent navigates to a\ntarget object in an unseen environment. Prior literature makes the assumption\nof a static environment with stationary objects, which lacks realism. To\naddress this, we present a novel formulation to generalize ObjectNav to dynamic\nenvironments with non-stationary objects, and refer to it as Portable ObjectNav\nor P-ObjectNav. In our formulation, we first address several challenging issues\nwith dynamizing existing topological scene graphs by developing a novel method\nthat introduces multiple transition behaviors to portable objects in the scene.\nWe use this technique to dynamize Matterport3D, a popular simulator for\nevaluating embodied tasks. We then present a benchmark for P-ObjectNav using a\ncombination of heuristic, reinforcement learning, and Large Language Model\n(LLM)-based navigation approaches on the dynamized environment, while\nintroducing novel evaluation metrics tailored for our task. Our work\nfundamentally challenges the \"static-environment\" notion of prior ObjectNav\nwork; the code and dataset for P-ObjectNav will be made publicly available to\nfoster research on embodied navigation in dynamic scenes. We provide an\nanonymized repository for our code and dataset:\nhttps://anonymous.4open.science/r/PObjectNav-1C6D.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ObjectNav is a popular task in Embodied AI, where an agent navigates to a\ntarget object in an unseen environment. Prior literature makes the assumption\nof a static environment with stationary objects, which lacks realism. To\naddress this, we present a novel formulation to generalize ObjectNav to dynamic\nenvironments with non-stationary objects, and refer to it as Portable ObjectNav\nor P-ObjectNav. In our formulation, we first address several challenging issues\nwith dynamizing existing topological scene graphs by developing a novel method\nthat introduces multiple transition behaviors to portable objects in the scene.\nWe use this technique to dynamize Matterport3D, a popular simulator for\nevaluating embodied tasks. We then present a benchmark for P-ObjectNav using a\ncombination of heuristic, reinforcement learning, and Large Language Model\n(LLM)-based navigation approaches on the dynamized environment, while\nintroducing novel evaluation metrics tailored for our task. Our work\nfundamentally challenges the \"static-environment\" notion of prior ObjectNav\nwork; the code and dataset for P-ObjectNav will be made publicly available to\nfoster research on embodied navigation in dynamic scenes. We provide an\nanonymized repository for our code and dataset:\nhttps://anonymous.4open.science/r/PObjectNav-1C6D."
                },
                "authors": [
                    {
                        "name": "Vishnu Sashank Dorbala"
                    },
                    {
                        "name": "Bhrij Patel"
                    },
                    {
                        "name": "Amrit Singh Bedi"
                    },
                    {
                        "name": "Dinesh Manocha"
                    }
                ],
                "author_detail": {
                    "name": "Dinesh Manocha"
                },
                "author": "Dinesh Manocha",
                "arxiv_comment": "19",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.09905v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.09905v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17855v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17855v2",
                "updated": "2024-12-01T19:02:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    19,
                    2,
                    28,
                    6,
                    336,
                    0
                ],
                "published": "2024-11-26T20:11:46Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    20,
                    11,
                    46,
                    1,
                    331,
                    0
                ],
                "title": "\"Give me the code\" -- Log Analysis of First-Year CS Students'\n  Interactions With GPT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Give me the code\" -- Log Analysis of First-Year CS Students'\n  Interactions With GPT"
                },
                "summary": "The impact of Large Language Models (LLMs) like GPT-3, GPT-4, and Bard in\ncomputer science (CS) education is expected to be profound. Students now have\nthe power to generate code solutions for a wide array of programming\nassignments. For first-year students, this may be particularly problematic\nsince the foundational skills are still in development and an over-reliance on\ngenerative AI tools can hinder their ability to grasp essential programming\nconcepts. This paper analyzes the prompts used by 69 freshmen undergraduate\nstudents to solve a certain programming problem within a project assignment,\nwithout giving them prior prompt training. We also present the rules of the\nexercise that motivated the prompts, designed to foster critical thinking\nskills during the interaction. Despite using unsophisticated prompting\ntechniques, our findings suggest that the majority of students successfully\nleveraged GPT, incorporating the suggested solutions into their projects.\nAdditionally, half of the students demonstrated the ability to exercise\njudgment in selecting from multiple GPT-generated solutions, showcasing the\ndevelopment of their critical thinking skills in evaluating AI-generated code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The impact of Large Language Models (LLMs) like GPT-3, GPT-4, and Bard in\ncomputer science (CS) education is expected to be profound. Students now have\nthe power to generate code solutions for a wide array of programming\nassignments. For first-year students, this may be particularly problematic\nsince the foundational skills are still in development and an over-reliance on\ngenerative AI tools can hinder their ability to grasp essential programming\nconcepts. This paper analyzes the prompts used by 69 freshmen undergraduate\nstudents to solve a certain programming problem within a project assignment,\nwithout giving them prior prompt training. We also present the rules of the\nexercise that motivated the prompts, designed to foster critical thinking\nskills during the interaction. Despite using unsophisticated prompting\ntechniques, our findings suggest that the majority of students successfully\nleveraged GPT, incorporating the suggested solutions into their projects.\nAdditionally, half of the students demonstrated the ability to exercise\njudgment in selecting from multiple GPT-generated solutions, showcasing the\ndevelopment of their critical thinking skills in evaluating AI-generated code."
                },
                "authors": [
                    {
                        "name": "Pedro Alves"
                    },
                    {
                        "name": "Bruno Pereira Cipriano"
                    }
                ],
                "author_detail": {
                    "name": "Bruno Pereira Cipriano"
                },
                "author": "Bruno Pereira Cipriano",
                "arxiv_comment": "This is the author's version of the work. It is posted here for your\n  personal use. Not for redistribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17855v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17855v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12644v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12644v3",
                "updated": "2024-12-01T17:45:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    17,
                    45,
                    28,
                    6,
                    336,
                    0
                ],
                "published": "2024-06-18T14:12:27Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    14,
                    12,
                    27,
                    1,
                    170,
                    0
                ],
                "title": "Hierarchical Prompting Taxonomy: A Universal Evaluation Framework for\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Prompting Taxonomy: A Universal Evaluation Framework for\n  Large Language Models"
                },
                "summary": "Assessing the effectiveness of large language models (LLMs) in performing\ndifferent tasks is crucial for understanding their strengths and weaknesses.\nThis paper presents the Hierarchical Prompting Taxonomy (HPT), grounded on\nhuman cognitive principles and designed to assess LLMs by examining the\ncognitive demands of various tasks. The HPT uses the Hierarchical Prompting\nFramework (HPF), a prompt selection framework that organizes five distinct\nprompting strategies by their cognitive load on LLMs. This study introduces the\nHierarchical Prompting Index (HPI) to measure task complexity, which\ndemonstrates LLMs' abilities across different datasets and serves as a\nuniversal metric for task complexity. The HPT offers a reliable method for\nevaluating LLMs' problem-solving skills in diverse scenarios, leading to\nclearer conclusions. Extensive experiments with multiple datasets and LLMs show\nthat the HPF enhances LLM performance by 2\\% to 63\\% compared to standard\nbenchmark datasets, confirming the effectiveness of the HPT. To support future\nresearch in this domain, the implementations of HPT and HPF are publicly\navailable",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the effectiveness of large language models (LLMs) in performing\ndifferent tasks is crucial for understanding their strengths and weaknesses.\nThis paper presents the Hierarchical Prompting Taxonomy (HPT), grounded on\nhuman cognitive principles and designed to assess LLMs by examining the\ncognitive demands of various tasks. The HPT uses the Hierarchical Prompting\nFramework (HPF), a prompt selection framework that organizes five distinct\nprompting strategies by their cognitive load on LLMs. This study introduces the\nHierarchical Prompting Index (HPI) to measure task complexity, which\ndemonstrates LLMs' abilities across different datasets and serves as a\nuniversal metric for task complexity. The HPT offers a reliable method for\nevaluating LLMs' problem-solving skills in diverse scenarios, leading to\nclearer conclusions. Extensive experiments with multiple datasets and LLMs show\nthat the HPF enhances LLM performance by 2\\% to 63\\% compared to standard\nbenchmark datasets, confirming the effectiveness of the HPT. To support future\nresearch in this domain, the implementations of HPT and HPF are publicly\navailable"
                },
                "authors": [
                    {
                        "name": "Devichand Budagam"
                    },
                    {
                        "name": "Ashutosh Kumar"
                    },
                    {
                        "name": "Mahsa Khoshnoodi"
                    },
                    {
                        "name": "Sankalp KJ"
                    },
                    {
                        "name": "Vinija Jain"
                    },
                    {
                        "name": "Aman Chadha"
                    }
                ],
                "author_detail": {
                    "name": "Aman Chadha"
                },
                "author": "Aman Chadha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12644v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12644v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01357v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01357v2",
                "updated": "2024-12-01T16:18:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    16,
                    18,
                    23,
                    6,
                    336,
                    0
                ],
                "published": "2024-11-02T20:27:51Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    20,
                    27,
                    51,
                    5,
                    307,
                    0
                ],
                "title": "WaKA: Data Attribution using K-Nearest Neighbors and Membership Privacy\n  Principles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaKA: Data Attribution using K-Nearest Neighbors and Membership Privacy\n  Principles"
                },
                "summary": "In this paper, we introduce WaKA (Wasserstein K-nearest-neighbors\nAttribution), a novel attribution method that leverages principles from the\nLiRA (Likelihood Ratio Attack) framework and k-nearest neighbors classifiers\n(k-NN). WaKA efficiently measures the contribution of individual data points to\nthe model's loss distribution, analyzing every possible k-NN that can be\nconstructed using the training set, without requiring to sample subsets of the\ntraining set. WaKA is versatile and can be used a posteriori as a membership\ninference attack (MIA) to assess privacy risks or a priori for privacy\ninfluence measurement and data valuation. Thus, WaKA can be seen as bridging\nthe gap between data attribution and membership inference attack (MIA) by\nproviding a unified framework to distinguish between a data point's value and\nits privacy risk. For instance, we have shown that self-attribution values are\nmore strongly correlated with the attack success rate than the contribution of\na point to the model generalization. WaKA's different usage were also evaluated\nacross diverse real-world datasets, demonstrating performance very close to\nLiRA when used as an MIA on k-NN classifiers, but with greater computational\nefficiency. Additionally, WaKA shows greater robustness than Shapley Values for\ndata minimization tasks (removal or addition) on imbalanced datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce WaKA (Wasserstein K-nearest-neighbors\nAttribution), a novel attribution method that leverages principles from the\nLiRA (Likelihood Ratio Attack) framework and k-nearest neighbors classifiers\n(k-NN). WaKA efficiently measures the contribution of individual data points to\nthe model's loss distribution, analyzing every possible k-NN that can be\nconstructed using the training set, without requiring to sample subsets of the\ntraining set. WaKA is versatile and can be used a posteriori as a membership\ninference attack (MIA) to assess privacy risks or a priori for privacy\ninfluence measurement and data valuation. Thus, WaKA can be seen as bridging\nthe gap between data attribution and membership inference attack (MIA) by\nproviding a unified framework to distinguish between a data point's value and\nits privacy risk. For instance, we have shown that self-attribution values are\nmore strongly correlated with the attack success rate than the contribution of\na point to the model generalization. WaKA's different usage were also evaluated\nacross diverse real-world datasets, demonstrating performance very close to\nLiRA when used as an MIA on k-NN classifiers, but with greater computational\nefficiency. Additionally, WaKA shows greater robustness than Shapley Values for\ndata minimization tasks (removal or addition) on imbalanced datasets."
                },
                "authors": [
                    {
                        "name": "Patrick Mesana"
                    },
                    {
                        "name": "ClÃ©ment BÃ©nesse"
                    },
                    {
                        "name": "Hadrien Lautraite"
                    },
                    {
                        "name": "Gilles Caporossi"
                    },
                    {
                        "name": "SÃ©bastien Gambs"
                    }
                ],
                "author_detail": {
                    "name": "SÃ©bastien Gambs"
                },
                "author": "SÃ©bastien Gambs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01357v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01357v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01444v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01444v2",
                "updated": "2024-12-01T16:11:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    16,
                    11,
                    18,
                    6,
                    336,
                    0
                ],
                "published": "2024-07-21T16:11:00Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    16,
                    11,
                    0,
                    6,
                    203,
                    0
                ],
                "title": "No Size Fits All: The Perils and Pitfalls of Leveraging LLMs Vary with\n  Company Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No Size Fits All: The Perils and Pitfalls of Leveraging LLMs Vary with\n  Company Size"
                },
                "summary": "Large language models (LLMs) are playing a pivotal role in deploying\nstrategic use cases across a range of organizations, from large pan-continental\ncompanies to emerging startups. The issues and challenges involved in the\nsuccessful utilization of LLMs can vary significantly depending on the size of\nthe organization. It is important to study and discuss these pertinent issues\nof LLM adaptation with a focus on the scale of the industrial concerns and\nbrainstorm possible solutions and prospective directions. Such a study has not\nbeen prominently featured in the current research literature. In this study, we\nadopt a threefold strategy: first, we conduct a case study with industry\npractitioners to formulate the key research questions; second, we examine\nexisting industrial publications to address these questions; and finally, we\nprovide a practical guide for industries to utilize LLMs more efficiently. We\nrelease the\nGitHub\\footnote{\\url{https://github.com/vinayakcse/IndustrialLLMsPapers}}\nrepository with the most recent papers in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are playing a pivotal role in deploying\nstrategic use cases across a range of organizations, from large pan-continental\ncompanies to emerging startups. The issues and challenges involved in the\nsuccessful utilization of LLMs can vary significantly depending on the size of\nthe organization. It is important to study and discuss these pertinent issues\nof LLM adaptation with a focus on the scale of the industrial concerns and\nbrainstorm possible solutions and prospective directions. Such a study has not\nbeen prominently featured in the current research literature. In this study, we\nadopt a threefold strategy: first, we conduct a case study with industry\npractitioners to formulate the key research questions; second, we examine\nexisting industrial publications to address these questions; and finally, we\nprovide a practical guide for industries to utilize LLMs more efficiently. We\nrelease the\nGitHub\\footnote{\\url{https://github.com/vinayakcse/IndustrialLLMsPapers}}\nrepository with the most recent papers in the field."
                },
                "authors": [
                    {
                        "name": "Ashok Urlana"
                    },
                    {
                        "name": "Charaka Vinayak Kumar"
                    },
                    {
                        "name": "Bala Mallikarjunarao Garlapati"
                    },
                    {
                        "name": "Ajeet Kumar Singh"
                    },
                    {
                        "name": "Rahul Mishra"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Mishra"
                },
                "author": "Rahul Mishra",
                "arxiv_comment": "COLING2025 Industry track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01444v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01444v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.02511v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.02511v3",
                "updated": "2024-12-01T15:55:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    55,
                    50,
                    6,
                    336,
                    0
                ],
                "published": "2024-02-04T14:51:49Z",
                "published_parsed": [
                    2024,
                    2,
                    4,
                    14,
                    51,
                    49,
                    6,
                    35,
                    0
                ],
                "title": "PoCo: Policy Composition from and for Heterogeneous Robot Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PoCo: Policy Composition from and for Heterogeneous Robot Learning"
                },
                "summary": "Training general robotic policies from heterogeneous data for different tasks\nis a significant challenge. Existing robotic datasets vary in different\nmodalities such as color, depth, tactile, and proprioceptive information, and\ncollected in different domains such as simulation, real robots, and human\nvideos. Current methods usually collect and pool all data from one domain to\ntrain a single policy to handle such heterogeneity in tasks and domains, which\nis prohibitively expensive and difficult. In this work, we present a flexible\napproach, dubbed Policy Composition, to combine information across such diverse\nmodalities and domains for learning scene-level and task-level generalized\nmanipulation skills, by composing different data distributions represented with\ndiffusion models. Our method can use task-level composition for multi-task\nmanipulation and be composed with analytic cost functions to adapt policy\nbehaviors at inference time. We train our method on simulation, human, and real\nrobot data and evaluate in tool-use tasks. The composed policy achieves robust\nand dexterous performance under varying scenes and tasks and outperforms\nbaselines from a single data source in both simulation and real-world\nexperiments. See https://liruiw.github.io/policycomp for more details .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training general robotic policies from heterogeneous data for different tasks\nis a significant challenge. Existing robotic datasets vary in different\nmodalities such as color, depth, tactile, and proprioceptive information, and\ncollected in different domains such as simulation, real robots, and human\nvideos. Current methods usually collect and pool all data from one domain to\ntrain a single policy to handle such heterogeneity in tasks and domains, which\nis prohibitively expensive and difficult. In this work, we present a flexible\napproach, dubbed Policy Composition, to combine information across such diverse\nmodalities and domains for learning scene-level and task-level generalized\nmanipulation skills, by composing different data distributions represented with\ndiffusion models. Our method can use task-level composition for multi-task\nmanipulation and be composed with analytic cost functions to adapt policy\nbehaviors at inference time. We train our method on simulation, human, and real\nrobot data and evaluate in tool-use tasks. The composed policy achieves robust\nand dexterous performance under varying scenes and tasks and outperforms\nbaselines from a single data source in both simulation and real-world\nexperiments. See https://liruiw.github.io/policycomp for more details ."
                },
                "authors": [
                    {
                        "name": "Lirui Wang"
                    },
                    {
                        "name": "Jialiang Zhao"
                    },
                    {
                        "name": "Yilun Du"
                    },
                    {
                        "name": "Edward H. Adelson"
                    },
                    {
                        "name": "Russ Tedrake"
                    }
                ],
                "author_detail": {
                    "name": "Russ Tedrake"
                },
                "author": "Russ Tedrake",
                "arxiv_comment": "R:SS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.02511v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.02511v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19128v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19128v3",
                "updated": "2024-12-01T15:07:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    7,
                    29,
                    6,
                    336,
                    0
                ],
                "published": "2024-10-24T19:56:28Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    19,
                    56,
                    28,
                    3,
                    298,
                    0
                ],
                "title": "Retrieving Implicit and Explicit Emotional Events Using Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieving Implicit and Explicit Emotional Events Using Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have garnered significant attention in recent\nyears due to their impressive performance. While considerable research has\nevaluated these models from various perspectives, the extent to which LLMs can\nperform implicit and explicit emotion retrieval remains largely unexplored. To\naddress this gap, this study investigates LLMs' emotion retrieval capabilities\nin commonsense. Through extensive experiments involving multiple models, we\nsystematically evaluate the ability of LLMs on emotion retrieval. Specifically,\nwe propose a supervised contrastive probing method to verify LLMs' performance\nfor implicit and explicit emotion retrieval, as well as the diversity of the\nemotional events they retrieve. The results offer valuable insights into the\nstrengths and limitations of LLMs in handling emotion retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have garnered significant attention in recent\nyears due to their impressive performance. While considerable research has\nevaluated these models from various perspectives, the extent to which LLMs can\nperform implicit and explicit emotion retrieval remains largely unexplored. To\naddress this gap, this study investigates LLMs' emotion retrieval capabilities\nin commonsense. Through extensive experiments involving multiple models, we\nsystematically evaluate the ability of LLMs on emotion retrieval. Specifically,\nwe propose a supervised contrastive probing method to verify LLMs' performance\nfor implicit and explicit emotion retrieval, as well as the diversity of the\nemotional events they retrieve. The results offer valuable insights into the\nstrengths and limitations of LLMs in handling emotion retrieval."
                },
                "authors": [
                    {
                        "name": "Guimin Hu"
                    },
                    {
                        "name": "Hasti Seifi"
                    }
                ],
                "author_detail": {
                    "name": "Hasti Seifi"
                },
                "author": "Hasti Seifi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19128v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19128v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.15656v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.15656v2",
                "updated": "2024-12-01T14:46:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    14,
                    46,
                    4,
                    6,
                    336,
                    0
                ],
                "published": "2023-07-28T16:33:21Z",
                "published_parsed": [
                    2023,
                    7,
                    28,
                    16,
                    33,
                    21,
                    4,
                    209,
                    0
                ],
                "title": "Axisymmetric membrane nano-resonators: A comparison of nonlinear\n  reduced-order models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Axisymmetric membrane nano-resonators: A comparison of nonlinear\n  reduced-order models"
                },
                "summary": "The shift in the backbone of the frequency--response curve and the\n`jump-down' observed at a critical frequency observed in nano-resonators are\ncaused by their nonlinear mechanical response. The shift and jump-down point\nare therefore often used to infer the mechanical properties that underlie the\nnonlinear response, particularly the resonator's stretching modulus. To\nfacilitate this, the resonators's dynamics are often modelled using a\nGalerkin-type numerical approach or lumped ordinary differential equations like\nthe Duffing equation, that incorporate an appropriate nonlinearity. To\nunderstand the source of the problem's nonlinearities, we first develop an\naxisymmetric but spatially-varying model of a membrane resonator subject to a\nuniform oscillatory load with linear damping. We then derive asymptotic\nsolutions for the resulting partial differential equations (PDEs) using the\nMethod of Multiple Scales (MS), which allows a systematic reduction to a\nDuffing-like equation with analytically determined coefficients. We also solve\nthe PDEs numerically via the method of lines. By comparing the numerical\nsolutions with the asymptotic results, we demonstrate that the numerical\napproach reveals a non-constant maximum compliance with increasing load, which\ncontradicts the predictions of the MS analysis. In contrast, we show that\ncombining a Galerkin decomposition with the Harmonic Balance Method accurately\ncaptures the non-constant maximum compliance and reliably predicts jump-down\nbehaviour. We analyze the resulting frequency-response predictions derived from\nthese methods. We also argue that fitting based on the jump-down point may be\nsensitive to noise and discuss strategies for fitting frequency-response curves\nfrom experimental data to theory that are robust to this.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The shift in the backbone of the frequency--response curve and the\n`jump-down' observed at a critical frequency observed in nano-resonators are\ncaused by their nonlinear mechanical response. The shift and jump-down point\nare therefore often used to infer the mechanical properties that underlie the\nnonlinear response, particularly the resonator's stretching modulus. To\nfacilitate this, the resonators's dynamics are often modelled using a\nGalerkin-type numerical approach or lumped ordinary differential equations like\nthe Duffing equation, that incorporate an appropriate nonlinearity. To\nunderstand the source of the problem's nonlinearities, we first develop an\naxisymmetric but spatially-varying model of a membrane resonator subject to a\nuniform oscillatory load with linear damping. We then derive asymptotic\nsolutions for the resulting partial differential equations (PDEs) using the\nMethod of Multiple Scales (MS), which allows a systematic reduction to a\nDuffing-like equation with analytically determined coefficients. We also solve\nthe PDEs numerically via the method of lines. By comparing the numerical\nsolutions with the asymptotic results, we demonstrate that the numerical\napproach reveals a non-constant maximum compliance with increasing load, which\ncontradicts the predictions of the MS analysis. In contrast, we show that\ncombining a Galerkin decomposition with the Harmonic Balance Method accurately\ncaptures the non-constant maximum compliance and reliably predicts jump-down\nbehaviour. We analyze the resulting frequency-response predictions derived from\nthese methods. We also argue that fitting based on the jump-down point may be\nsensitive to noise and discuss strategies for fitting frequency-response curves\nfrom experimental data to theory that are robust to this."
                },
                "authors": [
                    {
                        "name": "Safvan Palathingal"
                    },
                    {
                        "name": "Dominic Vella"
                    }
                ],
                "author_detail": {
                    "name": "Dominic Vella"
                },
                "author": "Dominic Vella",
                "arxiv_doi": "10.1016/j.ijnonlinmec.2024.104933",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.ijnonlinmec.2024.104933",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2307.15656v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.15656v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Revised version, with title change (among others) following review",
                "arxiv_journal_ref": "Int. J. Nonlin. Mech. 168, 104933 (2025)",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17017v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17017v2",
                "updated": "2024-12-01T14:37:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    14,
                    37,
                    22,
                    6,
                    336,
                    0
                ],
                "published": "2024-11-26T01:00:09Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    1,
                    0,
                    9,
                    1,
                    331,
                    0
                ],
                "title": "TED-VITON: Transformer-Empowered Diffusion Models for Virtual Try-On",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TED-VITON: Transformer-Empowered Diffusion Models for Virtual Try-On"
                },
                "summary": "Recent advancements in Virtual Try-On (VTO) have demonstrated exceptional\nefficacy in generating realistic images and preserving garment details, largely\nattributed to the robust generative capabilities of text-to-image (T2I)\ndiffusion backbones. However, the T2I models that underpin these methods have\nbecome outdated, thereby limiting the potential for further improvement in VTO.\nAdditionally, current methods face notable challenges in accurately rendering\ntext on garments without distortion and preserving fine-grained details, such\nas textures and material fidelity. The emergence of Diffusion Transformer (DiT)\nbased T2I models has showcased impressive performance and offers a promising\nopportunity for advancing VTO. Directly applying existing VTO techniques to\ntransformer-based T2I models is ineffective due to substantial architectural\ndifferences, which hinder their ability to fully leverage the models' advanced\ncapabilities for improved text generation. To address these challenges and\nunlock the full potential of DiT-based T2I models for VTO, we propose\nTED-VITON, a novel framework that integrates a Garment Semantic (GS) Adapter\nfor enhancing garment-specific features, a Text Preservation Loss to ensure\naccurate and distortion-free text rendering, and a constraint mechanism to\ngenerate prompts by optimizing Large Language Model (LLM). These innovations\nenable state-of-the-art (SOTA) performance in visual quality and text fidelity,\nestablishing a new benchmark for VTO task. Project page:\n\\url{https://zhenchenwan.github.io/TED-VITON/}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Virtual Try-On (VTO) have demonstrated exceptional\nefficacy in generating realistic images and preserving garment details, largely\nattributed to the robust generative capabilities of text-to-image (T2I)\ndiffusion backbones. However, the T2I models that underpin these methods have\nbecome outdated, thereby limiting the potential for further improvement in VTO.\nAdditionally, current methods face notable challenges in accurately rendering\ntext on garments without distortion and preserving fine-grained details, such\nas textures and material fidelity. The emergence of Diffusion Transformer (DiT)\nbased T2I models has showcased impressive performance and offers a promising\nopportunity for advancing VTO. Directly applying existing VTO techniques to\ntransformer-based T2I models is ineffective due to substantial architectural\ndifferences, which hinder their ability to fully leverage the models' advanced\ncapabilities for improved text generation. To address these challenges and\nunlock the full potential of DiT-based T2I models for VTO, we propose\nTED-VITON, a novel framework that integrates a Garment Semantic (GS) Adapter\nfor enhancing garment-specific features, a Text Preservation Loss to ensure\naccurate and distortion-free text rendering, and a constraint mechanism to\ngenerate prompts by optimizing Large Language Model (LLM). These innovations\nenable state-of-the-art (SOTA) performance in visual quality and text fidelity,\nestablishing a new benchmark for VTO task. Project page:\n\\url{https://zhenchenwan.github.io/TED-VITON/}"
                },
                "authors": [
                    {
                        "name": "Zhenchen Wan"
                    },
                    {
                        "name": "Yanwu Xu"
                    },
                    {
                        "name": "Zhaoqing Wang"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Tongliang Liu"
                    },
                    {
                        "name": "Mingming Gong"
                    }
                ],
                "author_detail": {
                    "name": "Mingming Gong"
                },
                "author": "Mingming Gong",
                "arxiv_comment": "Project page: \\href{https://github.com/ZhenchenWan/TED-VITON}{this\n  URL}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17017v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17017v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.14225v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.14225v3",
                "updated": "2024-12-01T13:55:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    13,
                    55,
                    56,
                    6,
                    336,
                    0
                ],
                "published": "2023-05-23T16:40:07Z",
                "published_parsed": [
                    2023,
                    5,
                    23,
                    16,
                    40,
                    7,
                    1,
                    143,
                    0
                ],
                "title": "ManiTweet: A New Benchmark for Identifying Manipulation of News on\n  Social Media",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ManiTweet: A New Benchmark for Identifying Manipulation of News on\n  Social Media"
                },
                "summary": "Considerable advancements have been made to tackle the misrepresentation of\ninformation derived from reference articles in the domains of fact-checking and\nfaithful summarization. However, an unaddressed aspect remains - the\nidentification of social media posts that manipulate information within\nassociated news articles. This task presents a significant challenge, primarily\ndue to the prevalence of personal opinions in such posts. We present a novel\ntask, identifying manipulation of news on social media, which aims to detect\nmanipulation in social media posts and identify manipulated or inserted\ninformation. To study this task, we have proposed a data collection schema and\ncurated a dataset called ManiTweet, consisting of 3.6K pairs of tweets and\ncorresponding articles. Our analysis demonstrates that this task is highly\nchallenging, with large language models (LLMs) yielding unsatisfactory\nperformance. Additionally, we have developed a simple yet effective basic model\nthat outperforms LLMs significantly on the ManiTweet dataset. Finally, we have\nconducted an exploratory analysis of human-written tweets, unveiling intriguing\nconnections between manipulation and the domain and factuality of news\narticles, as well as revealing that manipulated sentences are more likely to\nencapsulate the main story or consequences of a news outlet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Considerable advancements have been made to tackle the misrepresentation of\ninformation derived from reference articles in the domains of fact-checking and\nfaithful summarization. However, an unaddressed aspect remains - the\nidentification of social media posts that manipulate information within\nassociated news articles. This task presents a significant challenge, primarily\ndue to the prevalence of personal opinions in such posts. We present a novel\ntask, identifying manipulation of news on social media, which aims to detect\nmanipulation in social media posts and identify manipulated or inserted\ninformation. To study this task, we have proposed a data collection schema and\ncurated a dataset called ManiTweet, consisting of 3.6K pairs of tweets and\ncorresponding articles. Our analysis demonstrates that this task is highly\nchallenging, with large language models (LLMs) yielding unsatisfactory\nperformance. Additionally, we have developed a simple yet effective basic model\nthat outperforms LLMs significantly on the ManiTweet dataset. Finally, we have\nconducted an exploratory analysis of human-written tweets, unveiling intriguing\nconnections between manipulation and the domain and factuality of news\narticles, as well as revealing that manipulated sentences are more likely to\nencapsulate the main story or consequences of a news outlet."
                },
                "authors": [
                    {
                        "name": "Kung-Hsiang Huang"
                    },
                    {
                        "name": "Hou Pong Chan"
                    },
                    {
                        "name": "Kathleen McKeown"
                    },
                    {
                        "name": "Heng Ji"
                    }
                ],
                "author_detail": {
                    "name": "Heng Ji"
                },
                "author": "Heng Ji",
                "arxiv_comment": "COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.14225v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.14225v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06237v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06237v2",
                "updated": "2024-12-01T13:31:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    13,
                    31,
                    14,
                    6,
                    336,
                    0
                ],
                "published": "2024-11-09T17:38:01Z",
                "published_parsed": [
                    2024,
                    11,
                    9,
                    17,
                    38,
                    1,
                    5,
                    314,
                    0
                ],
                "title": "Leveraging Retrieval-Augmented Generation for Persian University\n  Knowledge Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Retrieval-Augmented Generation for Persian University\n  Knowledge Retrieval"
                },
                "summary": "This paper introduces an innovative approach using Retrieval-Augmented\nGeneration (RAG) pipelines with Large Language Models (LLMs) to enhance\ninformation retrieval and query response systems for university-related\nquestion answering. By systematically extracting data from the university\nofficial webpage and employing advanced prompt engineering techniques, we\ngenerate accurate, contextually relevant responses to user queries.\n  We developed a comprehensive university benchmark, UniversityQuestionBench\n(UQB), to rigorously evaluate our system performance, based on common key\nmetrics in the filed of RAG pipelines, assessing accuracy and reliability\nthrough various metrics and real-world scenarios. Our experimental results\ndemonstrate significant improvements in the precision and relevance of\ngenerated responses, enhancing user experience and reducing the time required\nto obtain relevant answers. In summary, this paper presents a novel application\nof RAG pipelines and LLMs, supported by a meticulously prepared university\nbenchmark, offering valuable insights into advanced AI techniques for academic\ndata retrieval and setting the stage for future research in this domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an innovative approach using Retrieval-Augmented\nGeneration (RAG) pipelines with Large Language Models (LLMs) to enhance\ninformation retrieval and query response systems for university-related\nquestion answering. By systematically extracting data from the university\nofficial webpage and employing advanced prompt engineering techniques, we\ngenerate accurate, contextually relevant responses to user queries.\n  We developed a comprehensive university benchmark, UniversityQuestionBench\n(UQB), to rigorously evaluate our system performance, based on common key\nmetrics in the filed of RAG pipelines, assessing accuracy and reliability\nthrough various metrics and real-world scenarios. Our experimental results\ndemonstrate significant improvements in the precision and relevance of\ngenerated responses, enhancing user experience and reducing the time required\nto obtain relevant answers. In summary, this paper presents a novel application\nof RAG pipelines and LLMs, supported by a meticulously prepared university\nbenchmark, offering valuable insights into advanced AI techniques for academic\ndata retrieval and setting the stage for future research in this domain."
                },
                "authors": [
                    {
                        "name": "Arshia Hemmat"
                    },
                    {
                        "name": "Kianoosh Vadaei"
                    },
                    {
                        "name": "Mohammad Hassan Heydari"
                    },
                    {
                        "name": "Afsaneh Fatemi"
                    }
                ],
                "author_detail": {
                    "name": "Afsaneh Fatemi"
                },
                "author": "Afsaneh Fatemi",
                "arxiv_comment": "6 pages, 2 figures, 1 table, Submitted to 15th IKT conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06237v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06237v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09111v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09111v3",
                "updated": "2024-12-01T13:08:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    13,
                    8,
                    57,
                    6,
                    336,
                    0
                ],
                "published": "2024-11-14T00:59:13Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    0,
                    59,
                    13,
                    3,
                    319,
                    0
                ],
                "title": "Reducing Reasoning Costs -- The Path of Optimization for Chain of\n  Thought via Sparse Attention Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing Reasoning Costs -- The Path of Optimization for Chain of\n  Thought via Sparse Attention Mechanism"
                },
                "summary": "In order to address the chain of thought in the large language model\ninference cost surge, this research proposes to use a sparse attention\nmechanism that only focuses on a few relevant tokens. The researcher\nconstructed a new attention mechanism and used GiantRabbit trained with custom\nGPTs as an experimental tool. The experiment tested and compared the reasoning\ntime, correctness score and chain of thought length of this model and o1\nPreview in solving the linear algebra test questions of MIT OpenCourseWare. The\nresults show that GiantRabbit's reasoning time and chain of thought length are\nsignificantly lower than o1 Preview. It verifies the feasibility of sparse\nattention mechanism for optimizing chain of thought reasoning. Detailed\narchitectural details and experimental process have been uploaded to Github,\nthe link is:https://github.com/brucewang123456789/GeniusTrail.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In order to address the chain of thought in the large language model\ninference cost surge, this research proposes to use a sparse attention\nmechanism that only focuses on a few relevant tokens. The researcher\nconstructed a new attention mechanism and used GiantRabbit trained with custom\nGPTs as an experimental tool. The experiment tested and compared the reasoning\ntime, correctness score and chain of thought length of this model and o1\nPreview in solving the linear algebra test questions of MIT OpenCourseWare. The\nresults show that GiantRabbit's reasoning time and chain of thought length are\nsignificantly lower than o1 Preview. It verifies the feasibility of sparse\nattention mechanism for optimizing chain of thought reasoning. Detailed\narchitectural details and experimental process have been uploaded to Github,\nthe link is:https://github.com/brucewang123456789/GeniusTrail.git."
                },
                "authors": [
                    {
                        "name": "Libo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Libo Wang"
                },
                "author": "Libo Wang",
                "arxiv_comment": "The main text is 5 pages, totaling 9 pages; 4 figures, 1 table. It\n  have been submitted to NeurIPS 2024 Workshop MusIML and OpenReview",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09111v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09111v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.06090v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.06090v4",
                "updated": "2024-12-01T12:13:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    12,
                    13,
                    57,
                    6,
                    336,
                    0
                ],
                "published": "2024-03-10T04:23:24Z",
                "published_parsed": [
                    2024,
                    3,
                    10,
                    4,
                    23,
                    24,
                    6,
                    70,
                    0
                ],
                "title": "What Matters When Repurposing Diffusion Models for General Dense\n  Perception Tasks?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Matters When Repurposing Diffusion Models for General Dense\n  Perception Tasks?"
                },
                "summary": "Extensive pre-training with large data is indispensable for downstream\ngeometry and semantic visual perception tasks. Thanks to large-scale\ntext-to-image (T2I) pretraining, recent works show promising results by simply\nfine-tuning T2I diffusion models for dense perception tasks. However, several\ncrucial design decisions in this process still lack comprehensive\njustification, encompassing the necessity of the multi-step stochastic\ndiffusion mechanism, training strategy, inference ensemble strategy, and\nfine-tuning data quality. In this work, we conduct a thorough investigation\ninto critical factors that affect transfer efficiency and performance when\nusing diffusion priors. Our key findings are: 1) High-quality fine-tuning data\nis paramount for both semantic and geometry perception tasks. 2) The stochastic\nnature of diffusion models has a slightly negative impact on deterministic\nvisual perception tasks. 3) Apart from fine-tuning the diffusion model with\nonly latent space supervision, task-specific image-level supervision is\nbeneficial to enhance fine-grained details. These observations culminate in the\ndevelopment of GenPercept, an effective deterministic one-step fine-tuning\nparadigm tailed for dense visual perception tasks. Different from the previous\nmulti-step methods, our paradigm has a much faster inference speed, and can be\nseamlessly integrated with customized perception decoders and loss functions\nfor image-level supervision, which is critical to improving the fine-grained\ndetails of predictions. Comprehensive experiments on diverse dense visual\nperceptual tasks, including monocular depth estimation, surface normal\nestimation, image segmentation, and matting, are performed to demonstrate the\nremarkable adaptability and effectiveness of our proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extensive pre-training with large data is indispensable for downstream\ngeometry and semantic visual perception tasks. Thanks to large-scale\ntext-to-image (T2I) pretraining, recent works show promising results by simply\nfine-tuning T2I diffusion models for dense perception tasks. However, several\ncrucial design decisions in this process still lack comprehensive\njustification, encompassing the necessity of the multi-step stochastic\ndiffusion mechanism, training strategy, inference ensemble strategy, and\nfine-tuning data quality. In this work, we conduct a thorough investigation\ninto critical factors that affect transfer efficiency and performance when\nusing diffusion priors. Our key findings are: 1) High-quality fine-tuning data\nis paramount for both semantic and geometry perception tasks. 2) The stochastic\nnature of diffusion models has a slightly negative impact on deterministic\nvisual perception tasks. 3) Apart from fine-tuning the diffusion model with\nonly latent space supervision, task-specific image-level supervision is\nbeneficial to enhance fine-grained details. These observations culminate in the\ndevelopment of GenPercept, an effective deterministic one-step fine-tuning\nparadigm tailed for dense visual perception tasks. Different from the previous\nmulti-step methods, our paradigm has a much faster inference speed, and can be\nseamlessly integrated with customized perception decoders and loss functions\nfor image-level supervision, which is critical to improving the fine-grained\ndetails of predictions. Comprehensive experiments on diverse dense visual\nperceptual tasks, including monocular depth estimation, surface normal\nestimation, image segmentation, and matting, are performed to demonstrate the\nremarkable adaptability and effectiveness of our proposed method."
                },
                "authors": [
                    {
                        "name": "Guangkai Xu"
                    },
                    {
                        "name": "Yongtao Ge"
                    },
                    {
                        "name": "Mingyu Liu"
                    },
                    {
                        "name": "Chengxiang Fan"
                    },
                    {
                        "name": "Kangyang Xie"
                    },
                    {
                        "name": "Zhiyue Zhao"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Chunhua Shen"
                    }
                ],
                "author_detail": {
                    "name": "Chunhua Shen"
                },
                "author": "Chunhua Shen",
                "arxiv_comment": "Code is at: https://github.com/aim-uofa/GenPercept",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.06090v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.06090v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19456v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19456v2",
                "updated": "2024-12-01T11:27:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    11,
                    27,
                    9,
                    6,
                    336,
                    0
                ],
                "published": "2024-10-25T10:30:21Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    10,
                    30,
                    21,
                    4,
                    299,
                    0
                ],
                "title": "Computational Bottlenecks of Training Small-scale Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational Bottlenecks of Training Small-scale Large Language Models"
                },
                "summary": "While large language models (LLMs) dominate the AI landscape, Small-scale\nlarge Language Models (SLMs) are gaining attention due to cost and efficiency\ndemands from consumers. However, there is limited research on the training\nbehavior and computational requirements of SLMs. In this study, we explore the\ncomputational bottlenecks of training SLMs (up to 2B parameters) by examining\nthe effects of various hyperparameters and configurations, including GPU type,\nbatch size, model size, communication protocol, attention type, and the number\nof GPUs. We assess these factors on popular cloud services using metrics such\nas loss per dollar and tokens per second. Our findings aim to support the\nbroader adoption and optimization of language model training for low-resource\nAI research institutes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) dominate the AI landscape, Small-scale\nlarge Language Models (SLMs) are gaining attention due to cost and efficiency\ndemands from consumers. However, there is limited research on the training\nbehavior and computational requirements of SLMs. In this study, we explore the\ncomputational bottlenecks of training SLMs (up to 2B parameters) by examining\nthe effects of various hyperparameters and configurations, including GPU type,\nbatch size, model size, communication protocol, attention type, and the number\nof GPUs. We assess these factors on popular cloud services using metrics such\nas loss per dollar and tokens per second. Our findings aim to support the\nbroader adoption and optimization of language model training for low-resource\nAI research institutes."
                },
                "authors": [
                    {
                        "name": "Saleh Ashkboos"
                    },
                    {
                        "name": "Iman Mirzadeh"
                    },
                    {
                        "name": "Keivan Alizadeh"
                    },
                    {
                        "name": "Mohammad Hossein Sekhavat"
                    },
                    {
                        "name": "Moin Nabi"
                    },
                    {
                        "name": "Mehrdad Farajtabar"
                    },
                    {
                        "name": "Fartash Faghri"
                    }
                ],
                "author_detail": {
                    "name": "Fartash Faghri"
                },
                "author": "Fartash Faghri",
                "arxiv_comment": "8 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19456v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19456v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18207v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18207v2",
                "updated": "2024-12-01T10:23:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    10,
                    23,
                    18,
                    6,
                    336,
                    0
                ],
                "published": "2024-11-27T10:33:51Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    10,
                    33,
                    51,
                    2,
                    332,
                    0
                ],
                "title": "From Open Vocabulary to Open World: Teaching Vision Language Models to\n  Detect Novel Objects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Open Vocabulary to Open World: Teaching Vision Language Models to\n  Detect Novel Objects"
                },
                "summary": "Traditional object detection methods operate under the closed-set assumption,\nwhere models can only detect a fixed number of objects predefined in the\ntraining set. Recent works on open vocabulary object detection (OVD) enable the\ndetection of objects defined by an unbounded vocabulary, which reduces the cost\nof training models for specific tasks. However, OVD heavily relies on accurate\nprompts provided by an ''oracle'', which limits their use in critical\napplications such as driving scene perception. OVD models tend to misclassify\nnear-out-of-distribution (NOOD) objects that have similar semantics to known\nclasses, and ignore far-out-of-distribution (FOOD) objects. To address theses\nlimitations, we propose a framework that enables OVD models to operate in open\nworld settings, by identifying and incrementally learning novel objects. To\ndetect FOOD objects, we propose Open World Embedding Learning (OWEL) and\nintroduce the concept of Pseudo Unknown Embedding which infers the location of\nunknown classes in a continuous semantic space based on the information of\nknown classes. We also propose Multi-Scale Contrastive Anchor Learning (MSCAL),\nwhich enables the identification of misclassified unknown objects by promoting\nthe intra-class consistency of object embeddings at different scales. The\nproposed method achieves state-of-the-art performance in common open world\nobject detection and autonomous driving benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional object detection methods operate under the closed-set assumption,\nwhere models can only detect a fixed number of objects predefined in the\ntraining set. Recent works on open vocabulary object detection (OVD) enable the\ndetection of objects defined by an unbounded vocabulary, which reduces the cost\nof training models for specific tasks. However, OVD heavily relies on accurate\nprompts provided by an ''oracle'', which limits their use in critical\napplications such as driving scene perception. OVD models tend to misclassify\nnear-out-of-distribution (NOOD) objects that have similar semantics to known\nclasses, and ignore far-out-of-distribution (FOOD) objects. To address theses\nlimitations, we propose a framework that enables OVD models to operate in open\nworld settings, by identifying and incrementally learning novel objects. To\ndetect FOOD objects, we propose Open World Embedding Learning (OWEL) and\nintroduce the concept of Pseudo Unknown Embedding which infers the location of\nunknown classes in a continuous semantic space based on the information of\nknown classes. We also propose Multi-Scale Contrastive Anchor Learning (MSCAL),\nwhich enables the identification of misclassified unknown objects by promoting\nthe intra-class consistency of object embeddings at different scales. The\nproposed method achieves state-of-the-art performance in common open world\nobject detection and autonomous driving benchmarks."
                },
                "authors": [
                    {
                        "name": "Zizhao Li"
                    },
                    {
                        "name": "Zhengkang Xiang"
                    },
                    {
                        "name": "Joseph West"
                    },
                    {
                        "name": "Kourosh Khoshelham"
                    }
                ],
                "author_detail": {
                    "name": "Kourosh Khoshelham"
                },
                "author": "Kourosh Khoshelham",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18207v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18207v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17305v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17305v2",
                "updated": "2024-12-01T09:02:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    9,
                    2,
                    35,
                    6,
                    336,
                    0
                ],
                "published": "2024-06-25T06:24:50Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    6,
                    24,
                    50,
                    1,
                    177,
                    0
                ],
                "title": "Retrieval Augmented Instruction Tuning for Open NER with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Instruction Tuning for Open NER with Large Language\n  Models"
                },
                "summary": "The strong capability of large language models (LLMs) has been applied to\ninformation extraction (IE) through either retrieval augmented prompting or\ninstruction tuning (IT). However, the best way to incorporate information with\nLLMs for IE remains an open question. In this paper, we explore Retrieval\nAugmented Instruction Tuning (RA-IT) for IE, focusing on the task of open named\nentity recognition (NER). Specifically, for each training sample, we retrieve\nsemantically similar examples from the training dataset as the context and\nprepend them to the input of the original instruction. To evaluate our RA-IT\napproach more thoroughly, we construct a Chinese IT dataset for open NER and\nevaluate RA-IT in both English and Chinese scenarios. Experimental results\nverify the effectiveness of RA-IT across various data sizes and in both English\nand Chinese scenarios. We also conduct thorough studies to explore the impacts\nof various retrieval strategies in the proposed RA-IT framework. Code and data\nare available at: https://github.com/Emma1066/Retrieval-Augmented-IT-OpenNER",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The strong capability of large language models (LLMs) has been applied to\ninformation extraction (IE) through either retrieval augmented prompting or\ninstruction tuning (IT). However, the best way to incorporate information with\nLLMs for IE remains an open question. In this paper, we explore Retrieval\nAugmented Instruction Tuning (RA-IT) for IE, focusing on the task of open named\nentity recognition (NER). Specifically, for each training sample, we retrieve\nsemantically similar examples from the training dataset as the context and\nprepend them to the input of the original instruction. To evaluate our RA-IT\napproach more thoroughly, we construct a Chinese IT dataset for open NER and\nevaluate RA-IT in both English and Chinese scenarios. Experimental results\nverify the effectiveness of RA-IT across various data sizes and in both English\nand Chinese scenarios. We also conduct thorough studies to explore the impacts\nof various retrieval strategies in the proposed RA-IT framework. Code and data\nare available at: https://github.com/Emma1066/Retrieval-Augmented-IT-OpenNER"
                },
                "authors": [
                    {
                        "name": "Tingyu Xie"
                    },
                    {
                        "name": "Jian Zhang"
                    },
                    {
                        "name": "Yan Zhang"
                    },
                    {
                        "name": "Yuanyuan Liang"
                    },
                    {
                        "name": "Qi Li"
                    },
                    {
                        "name": "Hongwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hongwei Wang"
                },
                "author": "Hongwei Wang",
                "arxiv_comment": "To be appeared at COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17305v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17305v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.13578v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.13578v3",
                "updated": "2024-12-01T08:38:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    8,
                    38,
                    17,
                    6,
                    336,
                    0
                ],
                "published": "2024-01-24T16:43:35Z",
                "published_parsed": [
                    2024,
                    1,
                    24,
                    16,
                    43,
                    35,
                    2,
                    24,
                    0
                ],
                "title": "WPDA: Frequency-based Backdoor Attack with Wavelet Packet Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WPDA: Frequency-based Backdoor Attack with Wavelet Packet Decomposition"
                },
                "summary": "This work explores an emerging security threat against deep neural networks\n(DNNs) based image classification, i.e., backdoor attack. In this scenario, the\nattacker aims to inject a backdoor into the model by manipulating training\ndata, such that the backdoor could be activated by a particular trigger and\nbootstraps the model to make a target prediction at inference. Currently, most\nexisting data poisoning-based attacks struggle to achieve success at low\npoisoning ratios, increasing the risk of being defended by defense methods. In\nthis paper, we propose a novel frequency-based backdoor attack via Wavelet\nPacket Decomposition (WPD), WPD decomposes the original image signal to a\nspectrogram that contains frequency information with different semantic\nmeanings. We leverage WPD to statistically analyze the frequency distribution\nof the dataset to infer the key frequency regions the DNNs would focus on, and\nthe trigger information is only injected into the key frequency regions. Our\nmethod mainly includes three parts: 1) the selection of the poisoning frequency\nregions in spectrogram; 2) trigger generation; 3) the generation of the\npoisoned dataset. Our method is stealthy and precise, evidenced by the 98.12%\nAttack Success Rate (ASR) on CIFAR-10 with the extremely low poisoning ratio\n0.004% (i.e., only 2 poisoned samples among 50,000 training samples) and can\nbypass most existing defense methods. Besides, we also provide visualization\nanalyses to explain why our method works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores an emerging security threat against deep neural networks\n(DNNs) based image classification, i.e., backdoor attack. In this scenario, the\nattacker aims to inject a backdoor into the model by manipulating training\ndata, such that the backdoor could be activated by a particular trigger and\nbootstraps the model to make a target prediction at inference. Currently, most\nexisting data poisoning-based attacks struggle to achieve success at low\npoisoning ratios, increasing the risk of being defended by defense methods. In\nthis paper, we propose a novel frequency-based backdoor attack via Wavelet\nPacket Decomposition (WPD), WPD decomposes the original image signal to a\nspectrogram that contains frequency information with different semantic\nmeanings. We leverage WPD to statistically analyze the frequency distribution\nof the dataset to infer the key frequency regions the DNNs would focus on, and\nthe trigger information is only injected into the key frequency regions. Our\nmethod mainly includes three parts: 1) the selection of the poisoning frequency\nregions in spectrogram; 2) trigger generation; 3) the generation of the\npoisoned dataset. Our method is stealthy and precise, evidenced by the 98.12%\nAttack Success Rate (ASR) on CIFAR-10 with the extremely low poisoning ratio\n0.004% (i.e., only 2 poisoned samples among 50,000 training samples) and can\nbypass most existing defense methods. Besides, we also provide visualization\nanalyses to explain why our method works."
                },
                "authors": [
                    {
                        "name": "Zhengyao Song"
                    },
                    {
                        "name": "Yongqiang Li"
                    },
                    {
                        "name": "Danni Yuan"
                    },
                    {
                        "name": "Li Liu"
                    },
                    {
                        "name": "Shaokui Wei"
                    },
                    {
                        "name": "Baoyuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Baoyuan Wu"
                },
                "author": "Baoyuan Wu",
                "arxiv_comment": "13 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.13578v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.13578v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.9",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14491v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14491v3",
                "updated": "2024-12-01T08:37:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    8,
                    37,
                    51,
                    6,
                    336,
                    0
                ],
                "published": "2024-11-20T12:34:44Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    12,
                    34,
                    44,
                    2,
                    325,
                    0
                ],
                "title": "A Survey on Human-Centric LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Human-Centric LLMs"
                },
                "summary": "The rapid evolution of large language models (LLMs) and their capacity to\nsimulate human cognition and behavior has given rise to LLM-based frameworks\nand tools that are evaluated and applied based on their ability to perform\ntasks traditionally performed by humans, namely those involving cognition,\ndecision-making, and social interaction. This survey provides a comprehensive\nexamination of such human-centric LLM capabilities, focusing on their\nperformance in both individual tasks (where an LLM acts as a stand-in for a\nsingle human) and collective tasks (where multiple LLMs coordinate to mimic\ngroup dynamics). We first evaluate LLM competencies across key areas including\nreasoning, perception, and social cognition, comparing their abilities to\nhuman-like skills. Then, we explore real-world applications of LLMs in\nhuman-centric domains such as behavioral science, political science, and\nsociology, assessing their effectiveness in replicating human behaviors and\ninteractions. Finally, we identify challenges and future research directions,\nsuch as improving LLM adaptability, emotional intelligence, and cultural\nsensitivity, while addressing inherent biases and enhancing frameworks for\nhuman-AI collaboration. This survey aims to provide a foundational\nunderstanding of LLMs from a human-centric perspective, offering insights into\ntheir current capabilities and potential for future development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of large language models (LLMs) and their capacity to\nsimulate human cognition and behavior has given rise to LLM-based frameworks\nand tools that are evaluated and applied based on their ability to perform\ntasks traditionally performed by humans, namely those involving cognition,\ndecision-making, and social interaction. This survey provides a comprehensive\nexamination of such human-centric LLM capabilities, focusing on their\nperformance in both individual tasks (where an LLM acts as a stand-in for a\nsingle human) and collective tasks (where multiple LLMs coordinate to mimic\ngroup dynamics). We first evaluate LLM competencies across key areas including\nreasoning, perception, and social cognition, comparing their abilities to\nhuman-like skills. Then, we explore real-world applications of LLMs in\nhuman-centric domains such as behavioral science, political science, and\nsociology, assessing their effectiveness in replicating human behaviors and\ninteractions. Finally, we identify challenges and future research directions,\nsuch as improving LLM adaptability, emotional intelligence, and cultural\nsensitivity, while addressing inherent biases and enhancing frameworks for\nhuman-AI collaboration. This survey aims to provide a foundational\nunderstanding of LLMs from a human-centric perspective, offering insights into\ntheir current capabilities and potential for future development."
                },
                "authors": [
                    {
                        "name": "Jing Yi Wang"
                    },
                    {
                        "name": "Nicholas Sukiennik"
                    },
                    {
                        "name": "Tong Li"
                    },
                    {
                        "name": "Weikang Su"
                    },
                    {
                        "name": "Qianyue Hao"
                    },
                    {
                        "name": "Jingbo Xu"
                    },
                    {
                        "name": "Zihan Huang"
                    },
                    {
                        "name": "Fengli Xu"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14491v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14491v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23402v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23402v2",
                "updated": "2024-12-01T07:55:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    7,
                    55,
                    12,
                    6,
                    336,
                    0
                ],
                "published": "2024-10-30T19:07:01Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    19,
                    7,
                    1,
                    2,
                    304,
                    0
                ],
                "title": "VISUALCODER: Guiding Large Language Models in Code Execution with\n  Fine-grained Multimodal Chain-of-Thought Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VISUALCODER: Guiding Large Language Models in Code Execution with\n  Fine-grained Multimodal Chain-of-Thought Reasoning"
                },
                "summary": "Predicting program behavior and reasoning about code execution remain\nsignificant challenges in software engineering, particularly for large language\nmodels (LLMs) designed for code analysis. While these models excel at\nunderstanding static syntax, they often struggle with dynamic reasoning tasks.\nWe introduce Visual Coder, a simple yet effective approach that enhances code\nreasoning by integrating multimodal Chain-of-Thought (CoT) reasoning with a\nvisual Control Flow Graph (CFG). By aligning code snippets with their\ncorresponding CFGs, Visual Coder provides deeper insights into execution flow,\nenabling more accurate predictions of code behavior. Our experiments\ndemonstrate that augmenting LLMs with visual CFGs significantly outperforms\ntext-based CFG descriptions in code reasoning tasks. We address challenges in\nmultimodal CoT integration through a reference mechanism, ensuring consistency\nbetween code and its execution path, thereby improving performance in program\nbehavior prediction, error detection, and output generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting program behavior and reasoning about code execution remain\nsignificant challenges in software engineering, particularly for large language\nmodels (LLMs) designed for code analysis. While these models excel at\nunderstanding static syntax, they often struggle with dynamic reasoning tasks.\nWe introduce Visual Coder, a simple yet effective approach that enhances code\nreasoning by integrating multimodal Chain-of-Thought (CoT) reasoning with a\nvisual Control Flow Graph (CFG). By aligning code snippets with their\ncorresponding CFGs, Visual Coder provides deeper insights into execution flow,\nenabling more accurate predictions of code behavior. Our experiments\ndemonstrate that augmenting LLMs with visual CFGs significantly outperforms\ntext-based CFG descriptions in code reasoning tasks. We address challenges in\nmultimodal CoT integration through a reference mechanism, ensuring consistency\nbetween code and its execution path, thereby improving performance in program\nbehavior prediction, error detection, and output generation."
                },
                "authors": [
                    {
                        "name": "Cuong Chi Le"
                    },
                    {
                        "name": "Hoang-Chau Truong-Vinh"
                    },
                    {
                        "name": "Huy Nhat Phan"
                    },
                    {
                        "name": "Dung Duy Le"
                    },
                    {
                        "name": "Tien N. Nguyen"
                    },
                    {
                        "name": "Nghi D. Q. Bui"
                    }
                ],
                "author_detail": {
                    "name": "Nghi D. Q. Bui"
                },
                "author": "Nghi D. Q. Bui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23402v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23402v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08958v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08958v2",
                "updated": "2024-12-01T06:47:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    6,
                    47,
                    45,
                    6,
                    336,
                    0
                ],
                "published": "2024-09-13T16:23:17Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    16,
                    23,
                    17,
                    4,
                    257,
                    0
                ],
                "title": "PINNfluence: Influence Functions for Physics-Informed Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PINNfluence: Influence Functions for Physics-Informed Neural Networks"
                },
                "summary": "Recently, physics-informed neural networks (PINNs) have emerged as a flexible\nand promising application of deep learning to partial differential equations in\nthe physical sciences. While offering strong performance and competitive\ninference speeds on forward and inverse problems, their black-box nature limits\ninterpretability, particularly regarding alignment with expected physical\nbehavior. In the present work, we explore the application of influence\nfunctions (IFs) to validate and debug PINNs post-hoc. Specifically, we apply\nvariations of IF-based indicators to gauge the influence of different types of\ncollocation points on the prediction of PINNs applied to a 2D Navier-Stokes\nfluid flow problem. Our results demonstrate how IFs can be adapted to PINNs to\nreveal the potential for further studies. The code is publicly available at\nhttps://github.com/aleks-krasowski/PINNfluence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, physics-informed neural networks (PINNs) have emerged as a flexible\nand promising application of deep learning to partial differential equations in\nthe physical sciences. While offering strong performance and competitive\ninference speeds on forward and inverse problems, their black-box nature limits\ninterpretability, particularly regarding alignment with expected physical\nbehavior. In the present work, we explore the application of influence\nfunctions (IFs) to validate and debug PINNs post-hoc. Specifically, we apply\nvariations of IF-based indicators to gauge the influence of different types of\ncollocation points on the prediction of PINNs applied to a 2D Navier-Stokes\nfluid flow problem. Our results demonstrate how IFs can be adapted to PINNs to\nreveal the potential for further studies. The code is publicly available at\nhttps://github.com/aleks-krasowski/PINNfluence."
                },
                "authors": [
                    {
                        "name": "Jonas R. Naujoks"
                    },
                    {
                        "name": "Aleksander Krasowski"
                    },
                    {
                        "name": "Moritz Weckbecker"
                    },
                    {
                        "name": "Thomas Wiegand"
                    },
                    {
                        "name": "Sebastian Lapuschkin"
                    },
                    {
                        "name": "Wojciech Samek"
                    },
                    {
                        "name": "RenÃ© P. Klausen"
                    }
                ],
                "author_detail": {
                    "name": "RenÃ© P. Klausen"
                },
                "author": "RenÃ© P. Klausen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08958v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08958v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.02490v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.02490v4",
                "updated": "2024-12-01T05:46:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    5,
                    46,
                    3,
                    6,
                    336,
                    0
                ],
                "published": "2023-08-04T17:59:47Z",
                "published_parsed": [
                    2023,
                    8,
                    4,
                    17,
                    59,
                    47,
                    4,
                    216,
                    0
                ],
                "title": "MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities"
                },
                "summary": "We propose MM-Vet, an evaluation benchmark that examines large multimodal\nmodels (LMMs) on complicated multimodal tasks. Recent LMMs have shown various\nintriguing abilities, such as solving math problems written on the blackboard,\nreasoning about events and celebrities in news images, and explaining visual\njokes. Rapid model advancements pose challenges to evaluation benchmark\ndevelopment. Problems include: (1) How to systematically structure and evaluate\nthe complicated multimodal tasks; (2) How to design evaluation metrics that\nwork well across question and answer types; and (3) How to give model insights\nbeyond a simple performance ranking. To this end, we present MM-Vet, designed\nbased on the insight that the intriguing ability to solve complicated tasks is\noften achieved by a generalist model being able to integrate different core\nvision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and\nexamines the 16 integrations of interest derived from the capability\ncombination. For evaluation metrics, we propose an LLM-based evaluator for\nopen-ended outputs. The evaluator enables the evaluation across different\nquestion types and answer styles, resulting in a unified scoring metric. We\nevaluate representative LMMs on MM-Vet, providing insights into the\ncapabilities of different LMM system paradigms and models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose MM-Vet, an evaluation benchmark that examines large multimodal\nmodels (LMMs) on complicated multimodal tasks. Recent LMMs have shown various\nintriguing abilities, such as solving math problems written on the blackboard,\nreasoning about events and celebrities in news images, and explaining visual\njokes. Rapid model advancements pose challenges to evaluation benchmark\ndevelopment. Problems include: (1) How to systematically structure and evaluate\nthe complicated multimodal tasks; (2) How to design evaluation metrics that\nwork well across question and answer types; and (3) How to give model insights\nbeyond a simple performance ranking. To this end, we present MM-Vet, designed\nbased on the insight that the intriguing ability to solve complicated tasks is\noften achieved by a generalist model being able to integrate different core\nvision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and\nexamines the 16 integrations of interest derived from the capability\ncombination. For evaluation metrics, we propose an LLM-based evaluator for\nopen-ended outputs. The evaluator enables the evaluation across different\nquestion types and answer styles, resulting in a unified scoring metric. We\nevaluate representative LMMs on MM-Vet, providing insights into the\ncapabilities of different LMM system paradigms and models."
                },
                "authors": [
                    {
                        "name": "Weihao Yu"
                    },
                    {
                        "name": "Zhengyuan Yang"
                    },
                    {
                        "name": "Linjie Li"
                    },
                    {
                        "name": "Jianfeng Wang"
                    },
                    {
                        "name": "Kevin Lin"
                    },
                    {
                        "name": "Zicheng Liu"
                    },
                    {
                        "name": "Xinchao Wang"
                    },
                    {
                        "name": "Lijuan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Lijuan Wang"
                },
                "author": "Lijuan Wang",
                "arxiv_comment": "ICML 2024. Code, data and leaderboard:\n  https://github.com/yuweihao/MM-Vet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.02490v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.02490v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.12607v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.12607v2",
                "updated": "2024-12-01T04:00:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    4,
                    0,
                    16,
                    6,
                    336,
                    0
                ],
                "published": "2024-02-20T00:04:40Z",
                "published_parsed": [
                    2024,
                    2,
                    20,
                    0,
                    4,
                    40,
                    1,
                    51,
                    0
                ],
                "title": "Inference on LATEs with covariates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference on LATEs with covariates"
                },
                "summary": "In theory, two-stage least squares (TSLS) identifies a weighted average of\ncovariate-specific local average treatment effects (LATEs) from a saturated\nspecification, without making parametric assumptions on how available\ncovariates enter the model. In practice, TSLS is severely biased as saturation\nleads to a large number of control dummies and an equally large number of,\narguably weak, instruments. This paper derives asymptotically valid tests and\nconfidence intervals for the weighted average of LATEs that is targeted, yet\nmissed by saturated TSLS. The proposed inference procedure is robust to\nunobserved treatment effect heterogeneity, covariates with rich support, and\nweak identification. We find LATEs statistically significantly different from\nzero in applications in criminology, finance, health, and education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In theory, two-stage least squares (TSLS) identifies a weighted average of\ncovariate-specific local average treatment effects (LATEs) from a saturated\nspecification, without making parametric assumptions on how available\ncovariates enter the model. In practice, TSLS is severely biased as saturation\nleads to a large number of control dummies and an equally large number of,\narguably weak, instruments. This paper derives asymptotically valid tests and\nconfidence intervals for the weighted average of LATEs that is targeted, yet\nmissed by saturated TSLS. The proposed inference procedure is robust to\nunobserved treatment effect heterogeneity, covariates with rich support, and\nweak identification. We find LATEs statistically significantly different from\nzero in applications in criminology, finance, health, and education."
                },
                "authors": [
                    {
                        "name": "Tom Boot"
                    },
                    {
                        "name": "Didier Nibbering"
                    }
                ],
                "author_detail": {
                    "name": "Didier Nibbering"
                },
                "author": "Didier Nibbering",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.12607v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.12607v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08149v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08149v2",
                "updated": "2024-12-01T03:29:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    3,
                    29,
                    46,
                    6,
                    336,
                    0
                ],
                "published": "2024-08-15T13:35:59Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    13,
                    35,
                    59,
                    3,
                    228,
                    0
                ],
                "title": "Unsupervised Variational Translator for Bridging Image Restoration and\n  High-Level Vision Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised Variational Translator for Bridging Image Restoration and\n  High-Level Vision Tasks"
                },
                "summary": "Recent research tries to extend image restoration capabilities from human\nperception to machine perception, thereby enhancing the performance of\nhigh-level vision tasks in degraded environments. These methods, primarily\nbased on supervised learning, typically involve the retraining of restoration\nnetworks or high-level vision networks. However, collecting paired data in\nreal-world scenarios and retraining large-scale models are challenge. To this\nend, we propose an unsupervised learning method called \\textbf{Va}riational\n\\textbf{T}ranslator (VaT), which does not require retraining existing\nrestoration and high-level vision networks. Instead, it establishes a\nlightweight network that serves as an intermediate bridge between them. By\nvariational inference, VaT approximates the joint distribution of restoration\noutput and high-level vision input, dividing the optimization objective into\npreserving content and maximizing marginal likelihood associated with\nhigh-level vision tasks. By cleverly leveraging self-training paradigms, VaT\nachieves the above optimization objective without requiring labels. As a\nresult, the translated images maintain a close resemblance to their original\ncontent while also demonstrating exceptional performance on high-level vision\ntasks. Extensive experiments in dehazing and low-light enhancement for\ndetection and classification show the superiority of our method over other\nstate-of-the-art unsupervised counterparts, even significantly surpassing\nsupervised methods in some complex real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research tries to extend image restoration capabilities from human\nperception to machine perception, thereby enhancing the performance of\nhigh-level vision tasks in degraded environments. These methods, primarily\nbased on supervised learning, typically involve the retraining of restoration\nnetworks or high-level vision networks. However, collecting paired data in\nreal-world scenarios and retraining large-scale models are challenge. To this\nend, we propose an unsupervised learning method called \\textbf{Va}riational\n\\textbf{T}ranslator (VaT), which does not require retraining existing\nrestoration and high-level vision networks. Instead, it establishes a\nlightweight network that serves as an intermediate bridge between them. By\nvariational inference, VaT approximates the joint distribution of restoration\noutput and high-level vision input, dividing the optimization objective into\npreserving content and maximizing marginal likelihood associated with\nhigh-level vision tasks. By cleverly leveraging self-training paradigms, VaT\nachieves the above optimization objective without requiring labels. As a\nresult, the translated images maintain a close resemblance to their original\ncontent while also demonstrating exceptional performance on high-level vision\ntasks. Extensive experiments in dehazing and low-light enhancement for\ndetection and classification show the superiority of our method over other\nstate-of-the-art unsupervised counterparts, even significantly surpassing\nsupervised methods in some complex real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Jiawei Wu"
                    },
                    {
                        "name": "Zhi Jin"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Jin"
                },
                "author": "Zhi Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08149v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08149v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15876v2",
                "updated": "2024-12-01T02:38:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    2,
                    38,
                    17,
                    6,
                    336,
                    0
                ],
                "published": "2024-10-21T10:57:45Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    10,
                    57,
                    45,
                    0,
                    295,
                    0
                ],
                "title": "FlickerFusion: Intra-trajectory Domain Generalizing Multi-Agent RL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlickerFusion: Intra-trajectory Domain Generalizing Multi-Agent RL"
                },
                "summary": "Multi-agent reinforcement learning has demonstrated significant potential in\naddressing complex cooperative tasks across various real-world applications.\nHowever, existing MARL approaches often rely on the restrictive assumption that\nthe number of entities (e.g., agents, obstacles) remains constant between\ntraining and inference. This overlooks scenarios where entities are dynamically\nremoved or added during the inference trajectory -- a common occurrence in\nreal-world environments like search and rescue missions and dynamic combat\nsituations. In this paper, we tackle the challenge of intra-trajectory dynamic\nentity composition under zero-shot out-of-domain (OOD) generalization, where\nsuch dynamic changes cannot be anticipated beforehand. Our empirical studies\nreveal that existing MARL methods suffer significant performance degradation\nand increased uncertainty in these scenarios. In response, we propose\nFlickerFusion, a novel OOD generalization method that acts as a universally\napplicable augmentation technique for MARL backbone methods. FlickerFusion\nstochastically drops out parts of the observation space, emulating being\nin-domain when inferenced OOD. The results show that FlickerFusion not only\nachieves superior inference rewards but also uniquely reduces uncertainty\nvis-\\`a-vis the backbone, compared to existing methods. Benchmarks,\nimplementations, and model weights are organized and open-sourced at\nflickerfusion305.github.io, accompanied by ample demo video renderings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent reinforcement learning has demonstrated significant potential in\naddressing complex cooperative tasks across various real-world applications.\nHowever, existing MARL approaches often rely on the restrictive assumption that\nthe number of entities (e.g., agents, obstacles) remains constant between\ntraining and inference. This overlooks scenarios where entities are dynamically\nremoved or added during the inference trajectory -- a common occurrence in\nreal-world environments like search and rescue missions and dynamic combat\nsituations. In this paper, we tackle the challenge of intra-trajectory dynamic\nentity composition under zero-shot out-of-domain (OOD) generalization, where\nsuch dynamic changes cannot be anticipated beforehand. Our empirical studies\nreveal that existing MARL methods suffer significant performance degradation\nand increased uncertainty in these scenarios. In response, we propose\nFlickerFusion, a novel OOD generalization method that acts as a universally\napplicable augmentation technique for MARL backbone methods. FlickerFusion\nstochastically drops out parts of the observation space, emulating being\nin-domain when inferenced OOD. The results show that FlickerFusion not only\nachieves superior inference rewards but also uniquely reduces uncertainty\nvis-\\`a-vis the backbone, compared to existing methods. Benchmarks,\nimplementations, and model weights are organized and open-sourced at\nflickerfusion305.github.io, accompanied by ample demo video renderings."
                },
                "authors": [
                    {
                        "name": "Woosung Koh"
                    },
                    {
                        "name": "Wonbeen Oh"
                    },
                    {
                        "name": "Siyeol Kim"
                    },
                    {
                        "name": "Suhin Shin"
                    },
                    {
                        "name": "Hyeongjin Kim"
                    },
                    {
                        "name": "Jaein Jang"
                    },
                    {
                        "name": "Junghyun Lee"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "NeurIPS '24 Open-World Agents Workshop (v2: minor revision)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12279v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12279v3",
                "updated": "2024-12-01T02:12:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    2,
                    12,
                    8,
                    6,
                    336,
                    0
                ],
                "published": "2024-11-19T06:57:45Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    6,
                    57,
                    45,
                    1,
                    324,
                    0
                ],
                "title": "HouseLLM: LLM-Assisted Two-Phase Text-to-Floorplan Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HouseLLM: LLM-Assisted Two-Phase Text-to-Floorplan Generation"
                },
                "summary": "This paper proposes a two-phase text-to-floorplan generation method, which\nguides a Large Language Model (LLM) to generate an initial layout (Layout-LLM)\nand refines them into the final floorplans through conditional diffusion model.\nWe incorporate a Chain-of-Thought approach to prompt the LLM based on user text\nspecifications, enabling a more user-friendly and intuitive house layout\ndesign. This method allows users to describe their needs in natural language,\nenhancing accessibility and providing clearer geometric constraints. The final\nfloorplans generated by Layout-LLM through conditional diffusion refinement are\nmore accurate and better meet user requirements. Experimental results\ndemonstrate that our approach achieves state-of-the-art performance across all\nmetrics, validating its effectiveness in practical home design applications. We\nplan to release our code for public use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a two-phase text-to-floorplan generation method, which\nguides a Large Language Model (LLM) to generate an initial layout (Layout-LLM)\nand refines them into the final floorplans through conditional diffusion model.\nWe incorporate a Chain-of-Thought approach to prompt the LLM based on user text\nspecifications, enabling a more user-friendly and intuitive house layout\ndesign. This method allows users to describe their needs in natural language,\nenhancing accessibility and providing clearer geometric constraints. The final\nfloorplans generated by Layout-LLM through conditional diffusion refinement are\nmore accurate and better meet user requirements. Experimental results\ndemonstrate that our approach achieves state-of-the-art performance across all\nmetrics, validating its effectiveness in practical home design applications. We\nplan to release our code for public use."
                },
                "authors": [
                    {
                        "name": "Ziyang Zong"
                    },
                    {
                        "name": "Zhaohuan Zhan"
                    },
                    {
                        "name": "Guang Tan"
                    }
                ],
                "author_detail": {
                    "name": "Guang Tan"
                },
                "author": "Guang Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12279v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12279v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.12896v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.12896v2",
                "updated": "2024-12-01T02:00:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    2,
                    0,
                    18,
                    6,
                    336,
                    0
                ],
                "published": "2023-11-21T08:28:17Z",
                "published_parsed": [
                    2023,
                    11,
                    21,
                    8,
                    28,
                    17,
                    1,
                    325,
                    0
                ],
                "title": "Modified Kerr black holes surrounded by dark matter spike",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modified Kerr black holes surrounded by dark matter spike"
                },
                "summary": "We study supermassive black holes (SMBH), surrounded by a dark matter (DM)\nspike, that can be found at the centers of Milky Way and $\\text{M87}$ galaxies\nand are accompanied by a specific kind of topological defect. The investigation\nis developed within the framework of Bumblebee Gravity with a global monopole\n(BGGM). The dark matter spike is described by a power-law density profile. Our\nmain objective is to assess how the background arising from spontaneous Lorentz\nsymmetry breaking and the presence of a global monopole influence the\nproperties of the Kerr BH within the region affected by the spike. Using a\nspherically symmetric static BH with BGGM properties as the seed metric, we\nconstruct a non-rotating spacetime with a DM spike, resulting in a\nBGGM-motivated Schwarzschild-like BH by solving the modified\nTolman-Oppenheimer-Volkoff equations (TOV). Next, we extend this approach to\nthe case of a rotating spacetime resulting in the BGGM-motivated Kerr-like BH\n(BGMKLBH). This approach allows us to explore the spacetime structure, and the\nBGMKLBH shadows. Then, using available observational data for the DM spike\ndensity and considering the effects of BGGM on $\\text{Sgr A}^{*}$ and\n$\\text{M87}^{*}$ SMBHs, we analyse the shapes of their shadows and put\nconstraints on the BGGM parameter. Thus, we infer that the BGMKLBHs could be\nreliable candidates for the astrophysical BHs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study supermassive black holes (SMBH), surrounded by a dark matter (DM)\nspike, that can be found at the centers of Milky Way and $\\text{M87}$ galaxies\nand are accompanied by a specific kind of topological defect. The investigation\nis developed within the framework of Bumblebee Gravity with a global monopole\n(BGGM). The dark matter spike is described by a power-law density profile. Our\nmain objective is to assess how the background arising from spontaneous Lorentz\nsymmetry breaking and the presence of a global monopole influence the\nproperties of the Kerr BH within the region affected by the spike. Using a\nspherically symmetric static BH with BGGM properties as the seed metric, we\nconstruct a non-rotating spacetime with a DM spike, resulting in a\nBGGM-motivated Schwarzschild-like BH by solving the modified\nTolman-Oppenheimer-Volkoff equations (TOV). Next, we extend this approach to\nthe case of a rotating spacetime resulting in the BGGM-motivated Kerr-like BH\n(BGMKLBH). This approach allows us to explore the spacetime structure, and the\nBGMKLBH shadows. Then, using available observational data for the DM spike\ndensity and considering the effects of BGGM on $\\text{Sgr A}^{*}$ and\n$\\text{M87}^{*}$ SMBHs, we analyse the shapes of their shadows and put\nconstraints on the BGGM parameter. Thus, we infer that the BGMKLBHs could be\nreliable candidates for the astrophysical BHs."
                },
                "authors": [
                    {
                        "name": "Salvatore Capozziello"
                    },
                    {
                        "name": "Soroush Zare"
                    },
                    {
                        "name": "Luis M. Nieto"
                    },
                    {
                        "name": "Hassan Hassanabadi"
                    }
                ],
                "author_detail": {
                    "name": "Hassan Hassanabadi"
                },
                "author": "Hassan Hassanabadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.12896v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.12896v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.13148v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.13148v2",
                "updated": "2024-12-01T01:37:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    1,
                    37,
                    11,
                    6,
                    336,
                    0
                ],
                "published": "2023-12-20T16:12:37Z",
                "published_parsed": [
                    2023,
                    12,
                    20,
                    16,
                    12,
                    37,
                    2,
                    354,
                    0
                ],
                "title": "Partially factorized variational inference for high-dimensional mixed\n  models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Partially factorized variational inference for high-dimensional mixed\n  models"
                },
                "summary": "While generalized linear mixed models are a fundamental tool in applied\nstatistics, many specifications, such as those involving categorical factors\nwith many levels or interaction terms, can be computationally challenging to\nestimate due to the need to compute or approximate high-dimensional integrals.\nVariational inference is a popular way to perform such computations, especially\nin the Bayesian context. However, naive use of such methods can provide\nunreliable uncertainty quantification. We show that this is indeed the case for\nmixed models, proving that standard mean-field variational inference\ndramatically underestimates posterior uncertainty in high-dimensions. We then\nshow how appropriately relaxing the mean-field assumption leads to methods\nwhose uncertainty quantification does not deteriorate in high-dimensions, and\nwhose total computational cost scales linearly with the number of parameters\nand observations. Our theoretical and numerical results focus on mixed models\nwith Gaussian or binomial likelihoods, and rely on connections to random graph\ntheory to obtain sharp high-dimensional asymptotic analysis. We also provide\ngeneric results, which are of independent interest, relating the accuracy of\nvariational inference to the convergence rate of the corresponding coordinate\nascent algorithm that is used to find it. Our proposed methodology is\nimplemented in the R package, see https://github.com/mgoplerud/vglmer .\nNumerical results with simulated and real data examples illustrate the\nfavourable computation cost versus accuracy trade-off of our approach compared\nto various alternatives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While generalized linear mixed models are a fundamental tool in applied\nstatistics, many specifications, such as those involving categorical factors\nwith many levels or interaction terms, can be computationally challenging to\nestimate due to the need to compute or approximate high-dimensional integrals.\nVariational inference is a popular way to perform such computations, especially\nin the Bayesian context. However, naive use of such methods can provide\nunreliable uncertainty quantification. We show that this is indeed the case for\nmixed models, proving that standard mean-field variational inference\ndramatically underestimates posterior uncertainty in high-dimensions. We then\nshow how appropriately relaxing the mean-field assumption leads to methods\nwhose uncertainty quantification does not deteriorate in high-dimensions, and\nwhose total computational cost scales linearly with the number of parameters\nand observations. Our theoretical and numerical results focus on mixed models\nwith Gaussian or binomial likelihoods, and rely on connections to random graph\ntheory to obtain sharp high-dimensional asymptotic analysis. We also provide\ngeneric results, which are of independent interest, relating the accuracy of\nvariational inference to the convergence rate of the corresponding coordinate\nascent algorithm that is used to find it. Our proposed methodology is\nimplemented in the R package, see https://github.com/mgoplerud/vglmer .\nNumerical results with simulated and real data examples illustrate the\nfavourable computation cost versus accuracy trade-off of our approach compared\nto various alternatives."
                },
                "authors": [
                    {
                        "name": "Max Goplerud"
                    },
                    {
                        "name": "Omiros Papaspiliopoulos"
                    },
                    {
                        "name": "Giacomo Zanella"
                    }
                ],
                "author_detail": {
                    "name": "Giacomo Zanella"
                },
                "author": "Giacomo Zanella",
                "arxiv_doi": "10.1093/biomet/asae067",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/biomet/asae067",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.13148v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.13148v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted version available at DOI below; major revision to earlier\n  version",
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.17249v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.17249v3",
                "updated": "2024-12-01T01:36:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    1,
                    36,
                    50,
                    6,
                    336,
                    0
                ],
                "published": "2023-09-29T13:55:45Z",
                "published_parsed": [
                    2023,
                    9,
                    29,
                    13,
                    55,
                    45,
                    4,
                    272,
                    0
                ],
                "title": "Batch Calibration: Rethinking Calibration for In-Context Learning and\n  Prompt Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch Calibration: Rethinking Calibration for In-Context Learning and\n  Prompt Engineering"
                },
                "summary": "Prompting and in-context learning (ICL) have become efficient learning\nparadigms for large language models (LLMs). However, LLMs suffer from prompt\nbrittleness and various bias factors in the prompt, including but not limited\nto the formatting, the choice verbalizers, and the ICL examples. To address\nthis problem that results in unexpected performance degradation, calibration\nmethods have been developed to mitigate the effects of these biases while\nrecovering LLM performance. In this work, we first conduct a systematic\nanalysis of the existing calibration methods, where we both provide a unified\nview and reveal the failure cases. Inspired by these analyses, we propose Batch\nCalibration (BC), a simple yet intuitive method that controls the contextual\nbias from the batched input, unifies various prior approaches, and effectively\naddresses the aforementioned issues. BC is zero-shot, inference-only, and\nincurs negligible additional costs. In the few-shot setup, we further extend BC\nto allow it to learn the contextual bias from labeled data. We validate the\neffectiveness of BC with PaLM 2-(S, M, L) and CLIP models and demonstrate\nstate-of-the-art performance over previous calibration baselines across more\nthan 10 natural language understanding and image classification tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting and in-context learning (ICL) have become efficient learning\nparadigms for large language models (LLMs). However, LLMs suffer from prompt\nbrittleness and various bias factors in the prompt, including but not limited\nto the formatting, the choice verbalizers, and the ICL examples. To address\nthis problem that results in unexpected performance degradation, calibration\nmethods have been developed to mitigate the effects of these biases while\nrecovering LLM performance. In this work, we first conduct a systematic\nanalysis of the existing calibration methods, where we both provide a unified\nview and reveal the failure cases. Inspired by these analyses, we propose Batch\nCalibration (BC), a simple yet intuitive method that controls the contextual\nbias from the batched input, unifies various prior approaches, and effectively\naddresses the aforementioned issues. BC is zero-shot, inference-only, and\nincurs negligible additional costs. In the few-shot setup, we further extend BC\nto allow it to learn the contextual bias from labeled data. We validate the\neffectiveness of BC with PaLM 2-(S, M, L) and CLIP models and demonstrate\nstate-of-the-art performance over previous calibration baselines across more\nthan 10 natural language understanding and image classification tasks."
                },
                "authors": [
                    {
                        "name": "Han Zhou"
                    },
                    {
                        "name": "Xingchen Wan"
                    },
                    {
                        "name": "Lev Proleev"
                    },
                    {
                        "name": "Diana Mincu"
                    },
                    {
                        "name": "Jilin Chen"
                    },
                    {
                        "name": "Katherine Heller"
                    },
                    {
                        "name": "Subhrajit Roy"
                    }
                ],
                "author_detail": {
                    "name": "Subhrajit Roy"
                },
                "author": "Subhrajit Roy",
                "arxiv_comment": "ICLR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.17249v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.17249v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23904v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23904v2",
                "updated": "2024-12-01T01:35:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    1,
                    35,
                    19,
                    6,
                    336,
                    0
                ],
                "published": "2024-10-31T13:06:29Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    13,
                    6,
                    29,
                    3,
                    305,
                    0
                ],
                "title": "EZ-HOI: VLM Adaptation via Guided Prompt Learning for Zero-Shot HOI\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EZ-HOI: VLM Adaptation via Guided Prompt Learning for Zero-Shot HOI\n  Detection"
                },
                "summary": "Detecting Human-Object Interactions (HOI) in zero-shot settings, where models\nmust handle unseen classes, poses significant challenges. Existing methods that\nrely on aligning visual encoders with large Vision-Language Models (VLMs) to\ntap into the extensive knowledge of VLMs, require large, computationally\nexpensive models and encounter training difficulties. Adapting VLMs with prompt\nlearning offers an alternative to direct alignment. However, fine-tuning on\ntask-specific datasets often leads to overfitting to seen classes and\nsuboptimal performance on unseen classes, due to the absence of unseen class\nlabels. To address these challenges, we introduce a novel prompt learning-based\nframework for Efficient Zero-Shot HOI detection (EZ-HOI). First, we introduce\nLarge Language Model (LLM) and VLM guidance for learnable prompts, integrating\ndetailed HOI descriptions and visual semantics to adapt VLMs to HOI tasks.\nHowever, because training datasets contain seen-class labels alone, fine-tuning\nVLMs on such datasets tends to optimize learnable prompts for seen classes\ninstead of unseen ones. Therefore, we design prompt learning for unseen classes\nusing information from related seen classes, with LLMs utilized to highlight\nthe differences between unseen and related seen classes. Quantitative\nevaluations on benchmark datasets demonstrate that our EZ-HOI achieves\nstate-of-the-art performance across various zero-shot settings with only 10.35%\nto 33.95% of the trainable parameters compared to existing methods. Code is\navailable at https://github.com/ChelsieLei/EZ-HOI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Human-Object Interactions (HOI) in zero-shot settings, where models\nmust handle unseen classes, poses significant challenges. Existing methods that\nrely on aligning visual encoders with large Vision-Language Models (VLMs) to\ntap into the extensive knowledge of VLMs, require large, computationally\nexpensive models and encounter training difficulties. Adapting VLMs with prompt\nlearning offers an alternative to direct alignment. However, fine-tuning on\ntask-specific datasets often leads to overfitting to seen classes and\nsuboptimal performance on unseen classes, due to the absence of unseen class\nlabels. To address these challenges, we introduce a novel prompt learning-based\nframework for Efficient Zero-Shot HOI detection (EZ-HOI). First, we introduce\nLarge Language Model (LLM) and VLM guidance for learnable prompts, integrating\ndetailed HOI descriptions and visual semantics to adapt VLMs to HOI tasks.\nHowever, because training datasets contain seen-class labels alone, fine-tuning\nVLMs on such datasets tends to optimize learnable prompts for seen classes\ninstead of unseen ones. Therefore, we design prompt learning for unseen classes\nusing information from related seen classes, with LLMs utilized to highlight\nthe differences between unseen and related seen classes. Quantitative\nevaluations on benchmark datasets demonstrate that our EZ-HOI achieves\nstate-of-the-art performance across various zero-shot settings with only 10.35%\nto 33.95% of the trainable parameters compared to existing methods. Code is\navailable at https://github.com/ChelsieLei/EZ-HOI."
                },
                "authors": [
                    {
                        "name": "Qinqian Lei"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Robby T. Tan"
                    }
                ],
                "author_detail": {
                    "name": "Robby T. Tan"
                },
                "author": "Robby T. Tan",
                "arxiv_comment": "Accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23904v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23904v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2410.16208v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16208v3",
                "updated": "2024-12-02T18:59:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    18,
                    59,
                    28,
                    0,
                    337,
                    0
                ],
                "published": "2024-10-21T17:11:21Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    17,
                    11,
                    21,
                    0,
                    295,
                    0
                ],
                "title": "Compute-Constrained Data Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute-Constrained Data Selection"
                },
                "summary": "Data selection can reduce the amount of training data needed to finetune\nLLMs; however, the efficacy of data selection scales directly with its compute.\nMotivated by the practical challenge of compute-constrained finetuning, we\nconsider the setting in which both the cost of selecting data and training are\nbudgeted for. We first formalize the problem of data selection with a\ncost-aware utility function, and model the data selection problem as trading\noff initial-selection cost for training gain. We run a comprehensive sweep of\nexperiments across multiple tasks, varying compute budget by scaling finetuning\ntokens, model sizes, and data selection compute. Interestingly we find that\nmany powerful data selection methods are almost never compute-optimal, and that\ncheaper data selection alternatives dominate both from a theoretical and\nempirical perspective. For compute-optimal training, we find that perplexity\nand gradient data selection require training-to-selection model size ratios of\n5x and 10x, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data selection can reduce the amount of training data needed to finetune\nLLMs; however, the efficacy of data selection scales directly with its compute.\nMotivated by the practical challenge of compute-constrained finetuning, we\nconsider the setting in which both the cost of selecting data and training are\nbudgeted for. We first formalize the problem of data selection with a\ncost-aware utility function, and model the data selection problem as trading\noff initial-selection cost for training gain. We run a comprehensive sweep of\nexperiments across multiple tasks, varying compute budget by scaling finetuning\ntokens, model sizes, and data selection compute. Interestingly we find that\nmany powerful data selection methods are almost never compute-optimal, and that\ncheaper data selection alternatives dominate both from a theoretical and\nempirical perspective. For compute-optimal training, we find that perplexity\nand gradient data selection require training-to-selection model size ratios of\n5x and 10x, respectively."
                },
                "authors": [
                    {
                        "name": "Junjie Oscar Yin"
                    },
                    {
                        "name": "Alexander M. Rush"
                    }
                ],
                "author_detail": {
                    "name": "Alexander M. Rush"
                },
                "author": "Alexander M. Rush",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16208v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16208v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17501v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17501v2",
                "updated": "2024-12-02T18:54:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    18,
                    54,
                    28,
                    0,
                    337,
                    0
                ],
                "published": "2024-11-26T15:13:06Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    13,
                    6,
                    1,
                    331,
                    0
                ],
                "title": "Inference Scaling fLaws: The Limits of LLM Resampling with Imperfect\n  Verifiers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference Scaling fLaws: The Limits of LLM Resampling with Imperfect\n  Verifiers"
                },
                "summary": "Recent research has generated hope that inference scaling could allow weaker\nlanguage models to match or exceed the accuracy of stronger models, such as by\nrepeatedly sampling solutions to a coding problem until it passes unit tests.\nThe central thesis of this paper is that there is no free lunch for inference\nscaling: indefinite accuracy improvement through resampling can only be\nrealized if the \"verifier\" (in this case, a set of unit tests) is perfect. When\nthe verifier is imperfect, as it almost always is in domains such as reasoning\nor coding (for example, unit tests have imperfect coverage), there is a nonzero\nprobability of false positives: incorrect solutions that pass the verifier.\nResampling cannot decrease this probability, so it imposes an upper bound to\nthe accuracy of resampling-based inference scaling even with an infinite\ncompute budget. We find that there is a very strong correlation between the\nmodel's single-sample accuracy (i.e. accuracy without unit tests) and its false\npositive rate on coding benchmarks HumanEval and MBPP, whose unit tests have\nlimited coverage. Therefore, no amount of inference scaling of weaker models\ncan enable them to match the single-sample accuracy of a sufficiently strong\nmodel (Fig. 1a). When we consider that false positives have a negative utility\ncompared to abstaining from producing a solution, it bends the inference\nscaling curve further downward. Empirically, we find that the optimal number of\nsamples can be less than 10 under realistic assumptions (Fig. 1b). Finally, we\nshow that beyond accuracy, false positives may have other undesirable\nqualities, such as poor adherence to coding style conventions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has generated hope that inference scaling could allow weaker\nlanguage models to match or exceed the accuracy of stronger models, such as by\nrepeatedly sampling solutions to a coding problem until it passes unit tests.\nThe central thesis of this paper is that there is no free lunch for inference\nscaling: indefinite accuracy improvement through resampling can only be\nrealized if the \"verifier\" (in this case, a set of unit tests) is perfect. When\nthe verifier is imperfect, as it almost always is in domains such as reasoning\nor coding (for example, unit tests have imperfect coverage), there is a nonzero\nprobability of false positives: incorrect solutions that pass the verifier.\nResampling cannot decrease this probability, so it imposes an upper bound to\nthe accuracy of resampling-based inference scaling even with an infinite\ncompute budget. We find that there is a very strong correlation between the\nmodel's single-sample accuracy (i.e. accuracy without unit tests) and its false\npositive rate on coding benchmarks HumanEval and MBPP, whose unit tests have\nlimited coverage. Therefore, no amount of inference scaling of weaker models\ncan enable them to match the single-sample accuracy of a sufficiently strong\nmodel (Fig. 1a). When we consider that false positives have a negative utility\ncompared to abstaining from producing a solution, it bends the inference\nscaling curve further downward. Empirically, we find that the optimal number of\nsamples can be less than 10 under realistic assumptions (Fig. 1b). Finally, we\nshow that beyond accuracy, false positives may have other undesirable\nqualities, such as poor adherence to coding style conventions."
                },
                "authors": [
                    {
                        "name": "Benedikt Stroebl"
                    },
                    {
                        "name": "Sayash Kapoor"
                    },
                    {
                        "name": "Arvind Narayanan"
                    }
                ],
                "author_detail": {
                    "name": "Arvind Narayanan"
                },
                "author": "Arvind Narayanan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17501v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17501v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00170v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00170v2",
                "updated": "2024-12-02T18:37:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    18,
                    37,
                    1,
                    0,
                    337,
                    0
                ],
                "published": "2024-07-31T21:43:55Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    21,
                    43,
                    55,
                    2,
                    213,
                    0
                ],
                "title": "CREW: Facilitating Human-AI Teaming Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CREW: Facilitating Human-AI Teaming Research"
                },
                "summary": "With the increasing deployment of artificial intelligence (AI) technologies,\nthe potential of humans working with AI agents has been growing at a great\nspeed. Human-AI teaming is an important paradigm for studying various aspects\nwhen humans and AI agents work together. The unique aspect of Human-AI teaming\nresearch is the need to jointly study humans and AI agents, demanding\nmultidisciplinary research efforts from machine learning to human-computer\ninteraction, robotics, cognitive science, neuroscience, psychology, social\nscience, and complex systems. However, existing platforms for Human-AI teaming\nresearch are limited, often supporting oversimplified scenarios and a single\ntask, or specifically focusing on either human-teaming research or multi-agent\nAI algorithms. We introduce CREW, a platform to facilitate Human-AI teaming\nresearch in real-time decision-making scenarios and engage collaborations from\nmultiple scientific disciplines, with a strong emphasis on human involvement.\nIt includes pre-built tasks for cognitive studies and Human-AI teaming with\nexpandable potentials from our modular design. Following conventional cognitive\nneuroscience research, CREW also supports multimodal human physiological signal\nrecording for behavior analysis. Moreover, CREW benchmarks real-time\nhuman-guided reinforcement learning agents using state-of-the-art algorithms\nand well-tuned baselines. With CREW, we were able to conduct 50 human subject\nstudies within a week to verify the effectiveness of our benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing deployment of artificial intelligence (AI) technologies,\nthe potential of humans working with AI agents has been growing at a great\nspeed. Human-AI teaming is an important paradigm for studying various aspects\nwhen humans and AI agents work together. The unique aspect of Human-AI teaming\nresearch is the need to jointly study humans and AI agents, demanding\nmultidisciplinary research efforts from machine learning to human-computer\ninteraction, robotics, cognitive science, neuroscience, psychology, social\nscience, and complex systems. However, existing platforms for Human-AI teaming\nresearch are limited, often supporting oversimplified scenarios and a single\ntask, or specifically focusing on either human-teaming research or multi-agent\nAI algorithms. We introduce CREW, a platform to facilitate Human-AI teaming\nresearch in real-time decision-making scenarios and engage collaborations from\nmultiple scientific disciplines, with a strong emphasis on human involvement.\nIt includes pre-built tasks for cognitive studies and Human-AI teaming with\nexpandable potentials from our modular design. Following conventional cognitive\nneuroscience research, CREW also supports multimodal human physiological signal\nrecording for behavior analysis. Moreover, CREW benchmarks real-time\nhuman-guided reinforcement learning agents using state-of-the-art algorithms\nand well-tuned baselines. With CREW, we were able to conduct 50 human subject\nstudies within a week to verify the effectiveness of our benchmark."
                },
                "authors": [
                    {
                        "name": "Lingyu Zhang"
                    },
                    {
                        "name": "Zhengran Ji"
                    },
                    {
                        "name": "Boyuan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Boyuan Chen"
                },
                "author": "Boyuan Chen",
                "arxiv_comment": "Our project website is at: http://generalroboticslab.com/CREW",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00170v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00170v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19839v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19839v3",
                "updated": "2024-12-02T16:27:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    16,
                    27,
                    16,
                    0,
                    337,
                    0
                ],
                "published": "2024-09-30T00:41:51Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    0,
                    41,
                    51,
                    0,
                    274,
                    0
                ],
                "title": "ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities"
                },
                "summary": "Forecasts of future events are essential inputs into informed\ndecision-making. Machine learning (ML) systems have the potential to deliver\nforecasts at scale, but there is no framework for evaluating the accuracy of ML\nsystems on a standardized set of forecasting questions. To address this gap, we\nintroduce ForecastBench: a dynamic benchmark that evaluates the accuracy of ML\nsystems on an automatically generated and regularly updated set of 1,000\nforecasting questions. To avoid any possibility of data leakage, ForecastBench\nis comprised solely of questions about future events that have no known answer\nat the time of submission. We quantify the capabilities of current ML systems\nby collecting forecasts from expert (human) forecasters, the general public,\nand LLMs on a random subset of questions from the benchmark ($N=200$). While\nLLMs have achieved super-human performance on many benchmarks, they perform\nless well here: expert forecasters outperform the top-performing LLM (p-value\n$<0.01$). We display system and human scores in a public leaderboard at\nwww.forecastbench.org.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasts of future events are essential inputs into informed\ndecision-making. Machine learning (ML) systems have the potential to deliver\nforecasts at scale, but there is no framework for evaluating the accuracy of ML\nsystems on a standardized set of forecasting questions. To address this gap, we\nintroduce ForecastBench: a dynamic benchmark that evaluates the accuracy of ML\nsystems on an automatically generated and regularly updated set of 1,000\nforecasting questions. To avoid any possibility of data leakage, ForecastBench\nis comprised solely of questions about future events that have no known answer\nat the time of submission. We quantify the capabilities of current ML systems\nby collecting forecasts from expert (human) forecasters, the general public,\nand LLMs on a random subset of questions from the benchmark ($N=200$). While\nLLMs have achieved super-human performance on many benchmarks, they perform\nless well here: expert forecasters outperform the top-performing LLM (p-value\n$<0.01$). We display system and human scores in a public leaderboard at\nwww.forecastbench.org."
                },
                "authors": [
                    {
                        "name": "Ezra Karger"
                    },
                    {
                        "name": "Houtan Bastani"
                    },
                    {
                        "name": "Chen Yueh-Han"
                    },
                    {
                        "name": "Zachary Jacobs"
                    },
                    {
                        "name": "Danny Halawi"
                    },
                    {
                        "name": "Fred Zhang"
                    },
                    {
                        "name": "Philip E. Tetlock"
                    }
                ],
                "author_detail": {
                    "name": "Philip E. Tetlock"
                },
                "author": "Philip E. Tetlock",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19839v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19839v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17607v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17607v2",
                "updated": "2024-12-02T16:13:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    16,
                    13,
                    24,
                    0,
                    337,
                    0
                ],
                "published": "2024-11-26T17:19:09Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    19,
                    9,
                    1,
                    331,
                    0
                ],
                "title": "Scaling Speech-Text Pre-training with Synthetic Interleaved Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Speech-Text Pre-training with Synthetic Interleaved Data"
                },
                "summary": "Speech language models (SpeechLMs) accept speech input and produce speech\noutput, allowing for more natural human-computer interaction compared to\ntext-based large language models (LLMs). Traditional approaches for developing\nSpeechLMs are constrained by the limited availability of unsupervised speech\ndata and parallel speech-text data, which are significantly less abundant than\ntext pre-training data, thereby limiting their scalability as LLMs. We propose\na novel approach to scaling speech-text pre-training by leveraging large-scale\nsynthetic interleaved data derived from text corpora, eliminating the need for\nparallel speech-text datasets. Our method efficiently constructs speech-text\ninterleaved data by sampling text spans from existing text corpora and\nsynthesizing corresponding speech spans using a text-to-token model, bypassing\nthe need to generate actual speech. We also employ a supervised speech\ntokenizer derived from an automatic speech recognition (ASR) model by\nincorporating a vector-quantized bottleneck into the encoder. This supervised\ntraining approach results in discrete speech tokens with strong semantic\npreservation even at lower frame rates (e.g. 12.5Hz), while still maintaining\nspeech reconstruction quality. Starting from a pre-trained language model and\nscaling our pre-training to 1 trillion tokens (with 600B synthetic interleaved\nspeech-text data), we achieve state-of-the-art performance in speech language\nmodeling and spoken question answering, improving performance on spoken\nquestions tasks from the previous SOTA of 13% (Moshi) to 31%. We further\ndemonstrate that by fine-tuning the pre-trained model with speech dialogue\ndata, we can develop an end-to-end spoken chatbot that achieves competitive\nperformance comparable to existing baselines in both conversational abilities\nand speech quality, even operating exclusively in the speech domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech language models (SpeechLMs) accept speech input and produce speech\noutput, allowing for more natural human-computer interaction compared to\ntext-based large language models (LLMs). Traditional approaches for developing\nSpeechLMs are constrained by the limited availability of unsupervised speech\ndata and parallel speech-text data, which are significantly less abundant than\ntext pre-training data, thereby limiting their scalability as LLMs. We propose\na novel approach to scaling speech-text pre-training by leveraging large-scale\nsynthetic interleaved data derived from text corpora, eliminating the need for\nparallel speech-text datasets. Our method efficiently constructs speech-text\ninterleaved data by sampling text spans from existing text corpora and\nsynthesizing corresponding speech spans using a text-to-token model, bypassing\nthe need to generate actual speech. We also employ a supervised speech\ntokenizer derived from an automatic speech recognition (ASR) model by\nincorporating a vector-quantized bottleneck into the encoder. This supervised\ntraining approach results in discrete speech tokens with strong semantic\npreservation even at lower frame rates (e.g. 12.5Hz), while still maintaining\nspeech reconstruction quality. Starting from a pre-trained language model and\nscaling our pre-training to 1 trillion tokens (with 600B synthetic interleaved\nspeech-text data), we achieve state-of-the-art performance in speech language\nmodeling and spoken question answering, improving performance on spoken\nquestions tasks from the previous SOTA of 13% (Moshi) to 31%. We further\ndemonstrate that by fine-tuning the pre-trained model with speech dialogue\ndata, we can develop an end-to-end spoken chatbot that achieves competitive\nperformance comparable to existing baselines in both conversational abilities\nand speech quality, even operating exclusively in the speech domain."
                },
                "authors": [
                    {
                        "name": "Aohan Zeng"
                    },
                    {
                        "name": "Zhengxiao Du"
                    },
                    {
                        "name": "Mingdao Liu"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Shengmin Jiang"
                    },
                    {
                        "name": "Yuxiao Dong"
                    },
                    {
                        "name": "Jie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Tang"
                },
                "author": "Jie Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17607v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17607v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16867v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16867v2",
                "updated": "2024-12-02T15:42:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    15,
                    42,
                    53,
                    0,
                    337,
                    0
                ],
                "published": "2024-07-23T22:23:47Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    22,
                    23,
                    47,
                    1,
                    205,
                    0
                ],
                "title": "From Text to Insight: Large Language Models for Materials Science Data\n  Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Text to Insight: Large Language Models for Materials Science Data\n  Extraction"
                },
                "summary": "The vast majority of materials science knowledge exists in unstructured\nnatural language, yet structured data is crucial for innovative and systematic\nmaterials design. Traditionally, the field has relied on manual curation and\npartial automation for data extraction for specific use cases. The advent of\nlarge language models (LLMs) represents a significant shift, potentially\nenabling efficient extraction of structured, actionable data from unstructured\ntext by non-experts. While applying LLMs to materials science data extraction\npresents unique challenges, domain knowledge offers opportunities to guide and\nvalidate LLM outputs. This review provides a comprehensive overview of\nLLM-based structured data extraction in materials science, synthesizing current\nknowledge and outlining future directions. We address the lack of standardized\nguidelines and present frameworks for leveraging the synergy between LLMs and\nmaterials science expertise. This work serves as a foundational resource for\nresearchers aiming to harness LLMs for data-driven materials research. The\ninsights presented here could significantly enhance how researchers across\ndisciplines access and utilize scientific information, potentially accelerating\nthe development of novel materials for critical societal needs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The vast majority of materials science knowledge exists in unstructured\nnatural language, yet structured data is crucial for innovative and systematic\nmaterials design. Traditionally, the field has relied on manual curation and\npartial automation for data extraction for specific use cases. The advent of\nlarge language models (LLMs) represents a significant shift, potentially\nenabling efficient extraction of structured, actionable data from unstructured\ntext by non-experts. While applying LLMs to materials science data extraction\npresents unique challenges, domain knowledge offers opportunities to guide and\nvalidate LLM outputs. This review provides a comprehensive overview of\nLLM-based structured data extraction in materials science, synthesizing current\nknowledge and outlining future directions. We address the lack of standardized\nguidelines and present frameworks for leveraging the synergy between LLMs and\nmaterials science expertise. This work serves as a foundational resource for\nresearchers aiming to harness LLMs for data-driven materials research. The\ninsights presented here could significantly enhance how researchers across\ndisciplines access and utilize scientific information, potentially accelerating\nthe development of novel materials for critical societal needs."
                },
                "authors": [
                    {
                        "name": "Mara Schilling-Wilhelmi"
                    },
                    {
                        "name": "MartiÃ±o RÃ­os-GarcÃ­a"
                    },
                    {
                        "name": "Sherjeel Shabih"
                    },
                    {
                        "name": "MarÃ­a Victoria Gil"
                    },
                    {
                        "name": "Santiago Miret"
                    },
                    {
                        "name": "Christoph T. Koch"
                    },
                    {
                        "name": "JosÃ© A. MÃ¡rquez"
                    },
                    {
                        "name": "Kevin Maik Jablonka"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Maik Jablonka"
                },
                "author": "Kevin Maik Jablonka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16867v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16867v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.10886v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.10886v2",
                "updated": "2024-12-02T15:40:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    15,
                    40,
                    45,
                    0,
                    337,
                    0
                ],
                "published": "2024-02-16T18:43:10Z",
                "published_parsed": [
                    2024,
                    2,
                    16,
                    18,
                    43,
                    10,
                    4,
                    47,
                    0
                ],
                "title": "Reviewer2: Optimizing Review Generation Through Prompt Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reviewer2: Optimizing Review Generation Through Prompt Generation"
                },
                "summary": "Recent developments in LLMs offer new opportunities for assisting authors in\nimproving their work. In this paper, we envision a use case where authors can\nreceive LLM-generated reviews that uncover weak points in the current draft.\nWhile initial methods for automated review generation already exist, these\nmethods tend to produce reviews that lack detail, and they do not cover the\nrange of opinions that human reviewers produce. To address this shortcoming, we\npropose an efficient two-stage review generation framework called Reviewer2.\nUnlike prior work, this approach explicitly models the distribution of possible\naspects that the review may address. We show that this leads to more detailed\nreviews that better cover the range of aspects that human reviewers identify in\nthe draft. As part of the research, we generate a large-scale review dataset of\n27k papers and 99k reviews that we annotate with aspect prompts, which we make\navailable as a resource for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent developments in LLMs offer new opportunities for assisting authors in\nimproving their work. In this paper, we envision a use case where authors can\nreceive LLM-generated reviews that uncover weak points in the current draft.\nWhile initial methods for automated review generation already exist, these\nmethods tend to produce reviews that lack detail, and they do not cover the\nrange of opinions that human reviewers produce. To address this shortcoming, we\npropose an efficient two-stage review generation framework called Reviewer2.\nUnlike prior work, this approach explicitly models the distribution of possible\naspects that the review may address. We show that this leads to more detailed\nreviews that better cover the range of aspects that human reviewers identify in\nthe draft. As part of the research, we generate a large-scale review dataset of\n27k papers and 99k reviews that we annotate with aspect prompts, which we make\navailable as a resource for future research."
                },
                "authors": [
                    {
                        "name": "Zhaolin Gao"
                    },
                    {
                        "name": "KiantÃ© Brantley"
                    },
                    {
                        "name": "Thorsten Joachims"
                    }
                ],
                "author_detail": {
                    "name": "Thorsten Joachims"
                },
                "author": "Thorsten Joachims",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.10886v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.10886v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13729v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13729v2",
                "updated": "2024-12-02T14:59:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    14,
                    59,
                    8,
                    0,
                    337,
                    0
                ],
                "published": "2024-09-10T01:20:22Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    1,
                    20,
                    22,
                    1,
                    254,
                    0
                ],
                "title": "MathGLM-Vision: Solving Mathematical Problems with Multi-Modal Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MathGLM-Vision: Solving Mathematical Problems with Multi-Modal Large\n  Language Model"
                },
                "summary": "Large language models (LLMs) have demonstrated significant capabilities in\nmathematical reasoning, particularly with text-based mathematical problems.\nHowever, current multi-modal large language models (MLLMs), especially those\nspecialized in mathematics, tend to focus predominantly on solving geometric\nproblems but ignore the diversity of visual information available in other\nareas of mathematics. Moreover, the geometric information for these specialized\nmathematical MLLMs is derived from several public datasets, which are typically\nlimited in diversity and complexity. To address these limitations, we aim to\nconstruct a fine-tuning dataset named MathVL, and develop a series of\nspecialized mathematical MLLMs termed MathGLM-Vision by conducting Supervised\nFine-Tuning (SFT) on MathVL with various parameter-scale backbones. To\nextensively evaluate the effectiveness of MathGLM-Vision, we conduct\nexperiments on several public benchmarks and our curated MathVL-test consisting\nof 2,000 problems. Experimental results demonstrate that MathGLM-Vision\nachieves significant improvements compared with some existing models, including\nbackbone models and open-source mathematical MLLMs. These findings indicate the\nimportance of diversity dataset in enhancing the mathematical reasoning\nabilities of MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated significant capabilities in\nmathematical reasoning, particularly with text-based mathematical problems.\nHowever, current multi-modal large language models (MLLMs), especially those\nspecialized in mathematics, tend to focus predominantly on solving geometric\nproblems but ignore the diversity of visual information available in other\nareas of mathematics. Moreover, the geometric information for these specialized\nmathematical MLLMs is derived from several public datasets, which are typically\nlimited in diversity and complexity. To address these limitations, we aim to\nconstruct a fine-tuning dataset named MathVL, and develop a series of\nspecialized mathematical MLLMs termed MathGLM-Vision by conducting Supervised\nFine-Tuning (SFT) on MathVL with various parameter-scale backbones. To\nextensively evaluate the effectiveness of MathGLM-Vision, we conduct\nexperiments on several public benchmarks and our curated MathVL-test consisting\nof 2,000 problems. Experimental results demonstrate that MathGLM-Vision\nachieves significant improvements compared with some existing models, including\nbackbone models and open-source mathematical MLLMs. These findings indicate the\nimportance of diversity dataset in enhancing the mathematical reasoning\nabilities of MLLMs."
                },
                "authors": [
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Jinhao Chen"
                    },
                    {
                        "name": "Zhengxiao Du"
                    },
                    {
                        "name": "Wenmeng Yu"
                    },
                    {
                        "name": "Weihan Wang"
                    },
                    {
                        "name": "Wenyi Hong"
                    },
                    {
                        "name": "Zhihuan Jiang"
                    },
                    {
                        "name": "Bin Xu"
                    },
                    {
                        "name": "Jie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Tang"
                },
                "author": "Jie Tang",
                "arxiv_comment": "30 pages,19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13729v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13729v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17773v2",
                "updated": "2024-12-02T14:55:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    14,
                    55,
                    49,
                    0,
                    337,
                    0
                ],
                "published": "2024-11-26T09:36:02Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    9,
                    36,
                    2,
                    1,
                    331,
                    0
                ],
                "title": "Efficient Multi-modal Large Language Models via Visual Token Grouping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Multi-modal Large Language Models via Visual Token Grouping"
                },
                "summary": "The development of Multi-modal Large Language Models (MLLMs) enhances Large\nLanguage Models (LLMs) with the ability to perceive data formats beyond text,\nsignificantly advancing a range of downstream applications, such as visual\nquestion answering and image captioning. However, the substantial computational\ncosts associated with processing high-resolution images and videos pose a\nbarrier to their broader adoption. To address this challenge, compressing\nvision tokens in MLLMs has emerged as a promising approach to reduce inference\ncosts. While existing methods conduct token reduction in the feature alignment\nphase. In this paper, we introduce VisToG, a novel grouping mechanism that\nleverages the capabilities of pre-trained vision encoders to group similar\nimage segments without the need for segmentation masks. Specifically, we\nconcatenate semantic tokens to represent image semantic segments after the\nlinear projection layer before feeding into the vision encoder. Besides, with\nthe isolated attention we adopt, VisToG can identify and eliminate redundant\nvisual tokens utilizing the prior knowledge in the pre-trained vision encoder,\nwhich effectively reduces computational demands. Extensive experiments\ndemonstrate the effectiveness of VisToG, maintaining 98.1% of the original\nperformance while achieving a reduction of over 27\\% inference time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of Multi-modal Large Language Models (MLLMs) enhances Large\nLanguage Models (LLMs) with the ability to perceive data formats beyond text,\nsignificantly advancing a range of downstream applications, such as visual\nquestion answering and image captioning. However, the substantial computational\ncosts associated with processing high-resolution images and videos pose a\nbarrier to their broader adoption. To address this challenge, compressing\nvision tokens in MLLMs has emerged as a promising approach to reduce inference\ncosts. While existing methods conduct token reduction in the feature alignment\nphase. In this paper, we introduce VisToG, a novel grouping mechanism that\nleverages the capabilities of pre-trained vision encoders to group similar\nimage segments without the need for segmentation masks. Specifically, we\nconcatenate semantic tokens to represent image semantic segments after the\nlinear projection layer before feeding into the vision encoder. Besides, with\nthe isolated attention we adopt, VisToG can identify and eliminate redundant\nvisual tokens utilizing the prior knowledge in the pre-trained vision encoder,\nwhich effectively reduces computational demands. Extensive experiments\ndemonstrate the effectiveness of VisToG, maintaining 98.1% of the original\nperformance while achieving a reduction of over 27\\% inference time."
                },
                "authors": [
                    {
                        "name": "Minbin Huang"
                    },
                    {
                        "name": "Runhui Huang"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Chuanyang Zheng"
                    },
                    {
                        "name": "Xiangguo Sun"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Hong Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Hong Cheng"
                },
                "author": "Hong Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06154v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06154v2",
                "updated": "2024-12-02T14:49:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    14,
                    49,
                    55,
                    0,
                    337,
                    0
                ],
                "published": "2024-10-08T15:55:40Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    15,
                    55,
                    40,
                    1,
                    282,
                    0
                ],
                "title": "GLOV: Guided Large Language Models as Implicit Optimizers for Vision\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GLOV: Guided Large Language Models as Implicit Optimizers for Vision\n  Language Models"
                },
                "summary": "In this work, we propose a novel method (GLOV) enabling Large Language Models\n(LLMs) to act as implicit Optimizers for Vision-Langugage Models (VLMs) to\nenhance downstream vision tasks. Our GLOV meta-prompts an LLM with the\ndownstream task description, querying it for suitable VLM prompts (e.g., for\nzero-shot classification with CLIP). These prompts are ranked according to a\npurity measure obtained through a fitness function. In each respective\noptimization step, the ranked prompts are fed as in-context examples (with\ntheir accuracies) to equip the LLM with the knowledge of the type of text\nprompts preferred by the downstream VLM. Furthermore, we also explicitly steer\nthe LLM generation process in each optimization step by specifically adding an\noffset difference vector of the embeddings from the positive and negative\nsolutions found by the LLM, in previous optimization steps, to the intermediate\nlayer of the network for the next generation step. This offset vector steers\nthe LLM generation toward the type of language preferred by the downstream VLM,\nresulting in enhanced performance on the downstream vision tasks. We\ncomprehensively evaluate our GLOV on 16 diverse datasets using two families of\nVLMs, i.e., dual-encoder (e.g., CLIP) and encoder-decoder (e.g., LLaVa) models\n-- showing that the discovered solutions can enhance the recognition\nperformance by up to 15.0% and 57.5% (3.8% and 21.6% on average) for these\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we propose a novel method (GLOV) enabling Large Language Models\n(LLMs) to act as implicit Optimizers for Vision-Langugage Models (VLMs) to\nenhance downstream vision tasks. Our GLOV meta-prompts an LLM with the\ndownstream task description, querying it for suitable VLM prompts (e.g., for\nzero-shot classification with CLIP). These prompts are ranked according to a\npurity measure obtained through a fitness function. In each respective\noptimization step, the ranked prompts are fed as in-context examples (with\ntheir accuracies) to equip the LLM with the knowledge of the type of text\nprompts preferred by the downstream VLM. Furthermore, we also explicitly steer\nthe LLM generation process in each optimization step by specifically adding an\noffset difference vector of the embeddings from the positive and negative\nsolutions found by the LLM, in previous optimization steps, to the intermediate\nlayer of the network for the next generation step. This offset vector steers\nthe LLM generation toward the type of language preferred by the downstream VLM,\nresulting in enhanced performance on the downstream vision tasks. We\ncomprehensively evaluate our GLOV on 16 diverse datasets using two families of\nVLMs, i.e., dual-encoder (e.g., CLIP) and encoder-decoder (e.g., LLaVa) models\n-- showing that the discovered solutions can enhance the recognition\nperformance by up to 15.0% and 57.5% (3.8% and 21.6% on average) for these\nmodels."
                },
                "authors": [
                    {
                        "name": "M. Jehanzeb Mirza"
                    },
                    {
                        "name": "Mengjie Zhao"
                    },
                    {
                        "name": "Zhuoyuan Mao"
                    },
                    {
                        "name": "Sivan Doveh"
                    },
                    {
                        "name": "Wei Lin"
                    },
                    {
                        "name": "Paul Gavrikov"
                    },
                    {
                        "name": "Michael Dorkenwald"
                    },
                    {
                        "name": "Shiqi Yang"
                    },
                    {
                        "name": "Saurav Jha"
                    },
                    {
                        "name": "Hiromi Wakaki"
                    },
                    {
                        "name": "Yuki Mitsufuji"
                    },
                    {
                        "name": "Horst Possegger"
                    },
                    {
                        "name": "Rogerio Feris"
                    },
                    {
                        "name": "Leonid Karlinsky"
                    },
                    {
                        "name": "James Glass"
                    }
                ],
                "author_detail": {
                    "name": "James Glass"
                },
                "author": "James Glass",
                "arxiv_comment": "Code: https://github.com/jmiemirza/GLOV",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06154v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06154v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19655v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19655v2",
                "updated": "2024-12-02T14:28:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    14,
                    28,
                    7,
                    0,
                    337,
                    0
                ],
                "published": "2024-11-29T12:21:15Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    12,
                    21,
                    15,
                    4,
                    334,
                    0
                ],
                "title": "Truth or Mirage? Towards End-to-End Factuality Evaluation with LLM-Oasis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Truth or Mirage? Towards End-to-End Factuality Evaluation with LLM-Oasis"
                },
                "summary": "After the introduction of Large Language Models (LLMs), there have been\nsubstantial improvements in the performance of Natural Language Generation\n(NLG) tasks, including Text Summarization and Machine Translation. However,\nLLMs still produce outputs containing hallucinations, that is, content not\ngrounded in factual information. Therefore, developing methods to assess the\nfactuality of LLMs has become urgent.\n  Indeed, resources for factuality evaluation have recently emerged. Although\nchallenging, these resources face one or more of the following limitations: (i)\nthey are tailored to a specific task or domain; (ii) they are limited in size,\nthereby preventing the training of new factuality evaluators; (iii) they are\ndesigned for simpler verification tasks, such as claim verification.\n  To address these issues, we introduce LLM-Oasis, to the best of our knowledge\nthe largest resource for training end-to-end factuality evaluators. LLM-Oasis\nis constructed by extracting claims from Wikipedia, falsifying a subset of\nthese claims, and generating pairs of factual and unfactual texts. We then rely\non human annotators to both validate the quality of our dataset and to create a\ngold standard test set for benchmarking factuality evaluation systems.\n  Our experiments demonstrate that LLM-Oasis presents a significant challenge\nfor state-of-the-art LLMs, with GPT-4o achieving up to 60% accuracy in our\nproposed end-to-end factuality evaluation task, highlighting its potential to\ndrive future research in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "After the introduction of Large Language Models (LLMs), there have been\nsubstantial improvements in the performance of Natural Language Generation\n(NLG) tasks, including Text Summarization and Machine Translation. However,\nLLMs still produce outputs containing hallucinations, that is, content not\ngrounded in factual information. Therefore, developing methods to assess the\nfactuality of LLMs has become urgent.\n  Indeed, resources for factuality evaluation have recently emerged. Although\nchallenging, these resources face one or more of the following limitations: (i)\nthey are tailored to a specific task or domain; (ii) they are limited in size,\nthereby preventing the training of new factuality evaluators; (iii) they are\ndesigned for simpler verification tasks, such as claim verification.\n  To address these issues, we introduce LLM-Oasis, to the best of our knowledge\nthe largest resource for training end-to-end factuality evaluators. LLM-Oasis\nis constructed by extracting claims from Wikipedia, falsifying a subset of\nthese claims, and generating pairs of factual and unfactual texts. We then rely\non human annotators to both validate the quality of our dataset and to create a\ngold standard test set for benchmarking factuality evaluation systems.\n  Our experiments demonstrate that LLM-Oasis presents a significant challenge\nfor state-of-the-art LLMs, with GPT-4o achieving up to 60% accuracy in our\nproposed end-to-end factuality evaluation task, highlighting its potential to\ndrive future research in the field."
                },
                "authors": [
                    {
                        "name": "Alessandro ScirÃ¨"
                    },
                    {
                        "name": "Andrei Stefan Bejgu"
                    },
                    {
                        "name": "Simone Tedeschi"
                    },
                    {
                        "name": "Karim Ghonim"
                    },
                    {
                        "name": "Federico Martelli"
                    },
                    {
                        "name": "Roberto Navigli"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Navigli"
                },
                "author": "Roberto Navigli",
                "arxiv_comment": "15 pages. To be submitted to CL journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19655v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19655v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01639v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01639v2",
                "updated": "2024-12-02T14:25:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    14,
                    25,
                    30,
                    0,
                    337,
                    0
                ],
                "published": "2024-10-02T15:09:36Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    15,
                    9,
                    36,
                    2,
                    276,
                    0
                ],
                "title": "Moral Alignment for LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Moral Alignment for LLM Agents"
                },
                "summary": "Decision-making agents based on pre-trained Large Language Models (LLMs) are\nincreasingly being deployed across various domains of human activity. While\ntheir applications are currently rather specialized, several research efforts\nare under way to develop more generalist agents. As LLM-based systems become\nmore agentic, their influence on human activity will grow and the transparency\nof this will decrease. Consequently, developing effective methods for aligning\nthem to human values is vital.\n  The prevailing practice in alignment often relies on human preference data\n(e.g., in RLHF or DPO), in which values are implicit and are essentially\ndeduced from relative preferences over different model outputs. In this work,\ninstead of relying on human feedback, we introduce the design of reward\nfunctions that explicitly encode core human values for Reinforcement\nLearning-based fine-tuning of foundation agent models. Specifically, we use\nintrinsic rewards for the moral alignment of LLM agents.\n  We evaluate our approach using the traditional philosophical frameworks of\nDeontological Ethics and Utilitarianism, quantifying moral rewards for agents\nin terms of actions and consequences on the Iterated Prisoner's Dilemma (IPD)\nenvironment. We also show how moral fine-tuning can be deployed to enable an\nagent to unlearn a previously developed selfish strategy. Finally, we find that\ncertain moral strategies learned on the IPD game generalize to several other\nmatrix game environments. In summary, we demonstrate that fine-tuning with\nintrinsic rewards is a promising general solution for aligning LLM agents to\nhuman values, and it might represent a more transparent and cost-effective\nalternative to currently predominant alignment techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision-making agents based on pre-trained Large Language Models (LLMs) are\nincreasingly being deployed across various domains of human activity. While\ntheir applications are currently rather specialized, several research efforts\nare under way to develop more generalist agents. As LLM-based systems become\nmore agentic, their influence on human activity will grow and the transparency\nof this will decrease. Consequently, developing effective methods for aligning\nthem to human values is vital.\n  The prevailing practice in alignment often relies on human preference data\n(e.g., in RLHF or DPO), in which values are implicit and are essentially\ndeduced from relative preferences over different model outputs. In this work,\ninstead of relying on human feedback, we introduce the design of reward\nfunctions that explicitly encode core human values for Reinforcement\nLearning-based fine-tuning of foundation agent models. Specifically, we use\nintrinsic rewards for the moral alignment of LLM agents.\n  We evaluate our approach using the traditional philosophical frameworks of\nDeontological Ethics and Utilitarianism, quantifying moral rewards for agents\nin terms of actions and consequences on the Iterated Prisoner's Dilemma (IPD)\nenvironment. We also show how moral fine-tuning can be deployed to enable an\nagent to unlearn a previously developed selfish strategy. Finally, we find that\ncertain moral strategies learned on the IPD game generalize to several other\nmatrix game environments. In summary, we demonstrate that fine-tuning with\nintrinsic rewards is a promising general solution for aligning LLM agents to\nhuman values, and it might represent a more transparent and cost-effective\nalternative to currently predominant alignment techniques."
                },
                "authors": [
                    {
                        "name": "Elizaveta Tennant"
                    },
                    {
                        "name": "Stephen Hailes"
                    },
                    {
                        "name": "Mirco Musolesi"
                    }
                ],
                "author_detail": {
                    "name": "Mirco Musolesi"
                },
                "author": "Mirco Musolesi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01639v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01639v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03235v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03235v2",
                "updated": "2024-12-02T13:21:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    13,
                    21,
                    36,
                    0,
                    337,
                    0
                ],
                "published": "2024-10-04T09:00:06Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    9,
                    0,
                    6,
                    4,
                    278,
                    0
                ],
                "title": "Enriching Ontologies with Disjointness Axioms using Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enriching Ontologies with Disjointness Axioms using Large Language\n  Models"
                },
                "summary": "Ontologies often lack explicit disjointness declarations between classes,\ndespite their usefulness for sophisticated reasoning and consistency checking\nin Knowledge Graphs. In this study, we explore the potential of Large Language\nModels (LLMs) to enrich ontologies by identifying and asserting class\ndisjointness axioms. Our approach aims at leveraging the implicit knowledge\nembedded in LLMs, using prompt engineering to elicit this knowledge for\nclassifying ontological disjointness. We validate our methodology on the\nDBpedia ontology, focusing on open-source LLMs. Our findings suggest that LLMs,\nwhen guided by effective prompt strategies, can reliably identify disjoint\nclass relationships, thus streamlining the process of ontology completion\nwithout extensive manual input. For comprehensive disjointness enrichment, we\npropose a process that takes logical relationships between disjointness and\nsubclass statements into account in order to maintain satisfiability and reduce\nthe number of calls to the LLM. This work provides a foundation for future\napplications of LLMs in automated ontology enhancement and offers insights into\noptimizing LLM performance through strategic prompt design. Our code is\npublicly available on GitHub at https://github.com/n28div/llm-disjointness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontologies often lack explicit disjointness declarations between classes,\ndespite their usefulness for sophisticated reasoning and consistency checking\nin Knowledge Graphs. In this study, we explore the potential of Large Language\nModels (LLMs) to enrich ontologies by identifying and asserting class\ndisjointness axioms. Our approach aims at leveraging the implicit knowledge\nembedded in LLMs, using prompt engineering to elicit this knowledge for\nclassifying ontological disjointness. We validate our methodology on the\nDBpedia ontology, focusing on open-source LLMs. Our findings suggest that LLMs,\nwhen guided by effective prompt strategies, can reliably identify disjoint\nclass relationships, thus streamlining the process of ontology completion\nwithout extensive manual input. For comprehensive disjointness enrichment, we\npropose a process that takes logical relationships between disjointness and\nsubclass statements into account in order to maintain satisfiability and reduce\nthe number of calls to the LLM. This work provides a foundation for future\napplications of LLMs in automated ontology enhancement and offers insights into\noptimizing LLM performance through strategic prompt design. Our code is\npublicly available on GitHub at https://github.com/n28div/llm-disjointness."
                },
                "authors": [
                    {
                        "name": "Elias Crum"
                    },
                    {
                        "name": "Antonio De Santis"
                    },
                    {
                        "name": "Manon Ovide"
                    },
                    {
                        "name": "Jiaxin Pan"
                    },
                    {
                        "name": "Alessia Pisu"
                    },
                    {
                        "name": "Nicolas Lazzari"
                    },
                    {
                        "name": "Sebastian Rudolph"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Rudolph"
                },
                "author": "Sebastian Rudolph",
                "arxiv_comment": "Accepted at KBC-LM'24 workshop at ISWC 2024,\n  https://ceur-ws.org/Vol-3853/paper1.pdf",
                "arxiv_journal_ref": "Proc. of 2nd Workshop on Knowledge Base Construction from\n  Pre-Trained Language Models (KBC-LM 2024) co-located with ISWC 2024,\n  Baltimore, USA, November 12, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03235v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03235v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07123v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07123v2",
                "updated": "2024-12-02T13:04:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    13,
                    4,
                    18,
                    0,
                    337,
                    0
                ],
                "published": "2024-09-11T09:21:20Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    9,
                    21,
                    20,
                    2,
                    255,
                    0
                ],
                "title": "Cross-Refine: Improving Natural Language Explanation Generation by\n  Learning in Tandem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Refine: Improving Natural Language Explanation Generation by\n  Learning in Tandem"
                },
                "summary": "Natural language explanations (NLEs) are vital for elucidating the reasoning\nbehind large language model (LLM) decisions. Many techniques have been\ndeveloped to generate NLEs using LLMs. However, like humans, LLMs might not\nalways produce optimal NLEs on first attempt. Inspired by human learning\nprocesses, we introduce Cross-Refine, which employs role modeling by deploying\ntwo LLMs as generator and critic, respectively. The generator outputs a first\nNLE and then refines this initial explanation using feedback and suggestions\nprovided by the critic. Cross-Refine does not require any supervised training\ndata or additional training. We validate Cross-Refine across three NLP tasks\nusing three state-of-the-art open-source LLMs through automatic and human\nevaluation. We select Self-Refine (Madaan et al., 2023) as the baseline, which\nonly utilizes self-feedback to refine the explanations. Our findings from\nautomatic evaluation and a user study indicate that Cross-Refine outperforms\nSelf-Refine. Meanwhile, Cross-Refine can perform effectively with less powerful\nLLMs, whereas Self-Refine only yields strong results with ChatGPT.\nAdditionally, we conduct an ablation study to assess the importance of feedback\nand suggestions. Both of them play an important role in refining explanations.\nWe further evaluate Cross-Refine on a bilingual dataset in English and German.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural language explanations (NLEs) are vital for elucidating the reasoning\nbehind large language model (LLM) decisions. Many techniques have been\ndeveloped to generate NLEs using LLMs. However, like humans, LLMs might not\nalways produce optimal NLEs on first attempt. Inspired by human learning\nprocesses, we introduce Cross-Refine, which employs role modeling by deploying\ntwo LLMs as generator and critic, respectively. The generator outputs a first\nNLE and then refines this initial explanation using feedback and suggestions\nprovided by the critic. Cross-Refine does not require any supervised training\ndata or additional training. We validate Cross-Refine across three NLP tasks\nusing three state-of-the-art open-source LLMs through automatic and human\nevaluation. We select Self-Refine (Madaan et al., 2023) as the baseline, which\nonly utilizes self-feedback to refine the explanations. Our findings from\nautomatic evaluation and a user study indicate that Cross-Refine outperforms\nSelf-Refine. Meanwhile, Cross-Refine can perform effectively with less powerful\nLLMs, whereas Self-Refine only yields strong results with ChatGPT.\nAdditionally, we conduct an ablation study to assess the importance of feedback\nand suggestions. Both of them play an important role in refining explanations.\nWe further evaluate Cross-Refine on a bilingual dataset in English and German."
                },
                "authors": [
                    {
                        "name": "Qianli Wang"
                    },
                    {
                        "name": "Tatiana Anikina"
                    },
                    {
                        "name": "Nils Feldhus"
                    },
                    {
                        "name": "Simon Ostermann"
                    },
                    {
                        "name": "Sebastian MÃ¶ller"
                    },
                    {
                        "name": "Vera Schmitt"
                    }
                ],
                "author_detail": {
                    "name": "Vera Schmitt"
                },
                "author": "Vera Schmitt",
                "arxiv_comment": "Accepted at COLING 2025; long paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07123v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07123v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.15798v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.15798v4",
                "updated": "2024-12-02T12:58:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    12,
                    58,
                    23,
                    0,
                    337,
                    0
                ],
                "published": "2023-05-25T07:28:28Z",
                "published_parsed": [
                    2023,
                    5,
                    25,
                    7,
                    28,
                    28,
                    3,
                    145,
                    0
                ],
                "title": "BK-SDM: A Lightweight, Fast, and Cheap Version of Stable Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BK-SDM: A Lightweight, Fast, and Cheap Version of Stable Diffusion"
                },
                "summary": "Text-to-image (T2I) generation with Stable Diffusion models (SDMs) involves\nhigh computing demands due to billion-scale parameters. To enhance efficiency,\nrecent studies have reduced sampling steps and applied network quantization\nwhile retaining the original architectures. The lack of architectural reduction\nattempts may stem from worries over expensive retraining for such massive\nmodels. In this work, we uncover the surprising potential of block pruning and\nfeature distillation for low-cost general-purpose T2I. By removing several\nresidual and attention blocks from the U-Net of SDMs, we achieve 30%~50%\nreduction in model size, MACs, and latency. We show that distillation\nretraining is effective even under limited resources: using only 13 A100 days\nand a tiny dataset, our compact models can imitate the original SDMs (v1.4 and\nv2.1-base with over 6,000 A100 days). Benefiting from the transferred\nknowledge, our BK-SDMs deliver competitive results on zero-shot MS-COCO against\nlarger multi-billion parameter models. We further demonstrate the applicability\nof our lightweight backbones in personalized generation and image-to-image\ntranslation. Deployment of our models on edge devices attains 4-second\ninference. Code and models can be found at:\nhttps://github.com/Nota-NetsPresso/BK-SDM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image (T2I) generation with Stable Diffusion models (SDMs) involves\nhigh computing demands due to billion-scale parameters. To enhance efficiency,\nrecent studies have reduced sampling steps and applied network quantization\nwhile retaining the original architectures. The lack of architectural reduction\nattempts may stem from worries over expensive retraining for such massive\nmodels. In this work, we uncover the surprising potential of block pruning and\nfeature distillation for low-cost general-purpose T2I. By removing several\nresidual and attention blocks from the U-Net of SDMs, we achieve 30%~50%\nreduction in model size, MACs, and latency. We show that distillation\nretraining is effective even under limited resources: using only 13 A100 days\nand a tiny dataset, our compact models can imitate the original SDMs (v1.4 and\nv2.1-base with over 6,000 A100 days). Benefiting from the transferred\nknowledge, our BK-SDMs deliver competitive results on zero-shot MS-COCO against\nlarger multi-billion parameter models. We further demonstrate the applicability\nof our lightweight backbones in personalized generation and image-to-image\ntranslation. Deployment of our models on edge devices attains 4-second\ninference. Code and models can be found at:\nhttps://github.com/Nota-NetsPresso/BK-SDM"
                },
                "authors": [
                    {
                        "name": "Bo-Kyeong Kim"
                    },
                    {
                        "name": "Hyoung-Kyu Song"
                    },
                    {
                        "name": "Thibault Castells"
                    },
                    {
                        "name": "Shinkook Choi"
                    }
                ],
                "author_detail": {
                    "name": "Shinkook Choi"
                },
                "author": "Shinkook Choi",
                "arxiv_doi": "10.1007/978-3-031-72949-2",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-72949-2",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2305.15798v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.15798v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ECCV 2024 Camera-Ready Version",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07836v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07836v4",
                "updated": "2024-12-02T12:44:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    12,
                    44,
                    48,
                    0,
                    337,
                    0
                ],
                "published": "2024-10-10T11:52:07Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    52,
                    7,
                    3,
                    284,
                    0
                ],
                "title": "Masked Generative Priors Improve World Models Sequence Modelling\n  Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked Generative Priors Improve World Models Sequence Modelling\n  Capabilities"
                },
                "summary": "Deep Reinforcement Learning (RL) has become the leading approach for creating\nartificial agents in complex environments. Model-based approaches, which are RL\nmethods with world models that predict environment dynamics, are among the most\npromising directions for improving data efficiency, forming a critical step\ntoward bridging the gap between research and real-world deployment. In\nparticular, world models enhance sample efficiency by learning in imagination,\nwhich involves training a generative sequence model of the environment in a\nself-supervised manner. Recently, Masked Generative Modelling has emerged as a\nmore efficient and superior inductive bias for modelling and generating token\nsequences. Building on the Efficient Stochastic Transformer-based World Models\n(STORM) architecture, we replace the traditional MLP prior with a Masked\nGenerative Prior (e.g., MaskGIT Prior) and introduce GIT-STORM. We evaluate our\nmodel on two downstream tasks: reinforcement learning and video prediction.\nGIT-STORM demonstrates substantial performance gains in RL tasks on the Atari\n100k benchmark. Moreover, we apply Transformer-based World Models to continuous\naction environments for the first time, addressing a significant gap in prior\nresearch. To achieve this, we employ a state mixer function that integrates\nlatent state representations with actions, enabling our model to handle\ncontinuous control tasks. We validate this approach through qualitative and\nquantitative analyses on the DeepMind Control Suite, showcasing the\neffectiveness of Transformer-based World Models in this new domain. Our results\nhighlight the versatility and efficacy of the MaskGIT dynamics prior, paving\nthe way for more accurate world models and effective RL policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Reinforcement Learning (RL) has become the leading approach for creating\nartificial agents in complex environments. Model-based approaches, which are RL\nmethods with world models that predict environment dynamics, are among the most\npromising directions for improving data efficiency, forming a critical step\ntoward bridging the gap between research and real-world deployment. In\nparticular, world models enhance sample efficiency by learning in imagination,\nwhich involves training a generative sequence model of the environment in a\nself-supervised manner. Recently, Masked Generative Modelling has emerged as a\nmore efficient and superior inductive bias for modelling and generating token\nsequences. Building on the Efficient Stochastic Transformer-based World Models\n(STORM) architecture, we replace the traditional MLP prior with a Masked\nGenerative Prior (e.g., MaskGIT Prior) and introduce GIT-STORM. We evaluate our\nmodel on two downstream tasks: reinforcement learning and video prediction.\nGIT-STORM demonstrates substantial performance gains in RL tasks on the Atari\n100k benchmark. Moreover, we apply Transformer-based World Models to continuous\naction environments for the first time, addressing a significant gap in prior\nresearch. To achieve this, we employ a state mixer function that integrates\nlatent state representations with actions, enabling our model to handle\ncontinuous control tasks. We validate this approach through qualitative and\nquantitative analyses on the DeepMind Control Suite, showcasing the\neffectiveness of Transformer-based World Models in this new domain. Our results\nhighlight the versatility and efficacy of the MaskGIT dynamics prior, paving\nthe way for more accurate world models and effective RL policies."
                },
                "authors": [
                    {
                        "name": "Cristian Meo"
                    },
                    {
                        "name": "Mircea Lica"
                    },
                    {
                        "name": "Zarif Ikram"
                    },
                    {
                        "name": "Akihiro Nakano"
                    },
                    {
                        "name": "Vedant Shah"
                    },
                    {
                        "name": "Aniket Rajiv Didolkar"
                    },
                    {
                        "name": "Dianbo Liu"
                    },
                    {
                        "name": "Anirudh Goyal"
                    },
                    {
                        "name": "Justin Dauwels"
                    }
                ],
                "author_detail": {
                    "name": "Justin Dauwels"
                },
                "author": "Justin Dauwels",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07836v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07836v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16886v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16886v3",
                "updated": "2024-12-02T12:39:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    12,
                    39,
                    7,
                    0,
                    337,
                    0
                ],
                "published": "2024-08-29T20:19:10Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    20,
                    19,
                    10,
                    3,
                    242,
                    0
                ],
                "title": "LV-UNet: A Lightweight and Vanilla Model for Medical Image Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LV-UNet: A Lightweight and Vanilla Model for Medical Image Segmentation"
                },
                "summary": "While large models have achieved significant progress in computer vision,\nchallenges such as optimization complexity, the intricacy of transformer\narchitectures, computational constraints, and practical application demands\nhighlight the importance of simpler model designs in medical image\nsegmentation. This need is particularly pronounced in mobile medical devices,\nwhich require lightweight, deployable models with real-time performance.\nHowever, existing lightweight models often suffer from poor robustness across\ndatasets, limiting their widespread adoption. To address these challenges, this\npaper introduces LV-UNet, a lightweight and vanilla model that leverages\npre-trained MobileNetv3-Large backbones and incorporates fusible modules.\nLV-UNet employs an enhanced deep training strategy and switches to a deployment\nmode during inference by re-parametrization, significantly reducing parameter\ncount and computational overhead. Experimental results on ISIC 2016, BUSI,\nCVC-ClinicDB, CVC-ColonDB, and Kvair-SEG datasets demonstrate a better\ntrade-off between performance and the computational load. The code will be\nreleased at \\url{https://github.com/juntaoJianggavin/LV-UNet}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large models have achieved significant progress in computer vision,\nchallenges such as optimization complexity, the intricacy of transformer\narchitectures, computational constraints, and practical application demands\nhighlight the importance of simpler model designs in medical image\nsegmentation. This need is particularly pronounced in mobile medical devices,\nwhich require lightweight, deployable models with real-time performance.\nHowever, existing lightweight models often suffer from poor robustness across\ndatasets, limiting their widespread adoption. To address these challenges, this\npaper introduces LV-UNet, a lightweight and vanilla model that leverages\npre-trained MobileNetv3-Large backbones and incorporates fusible modules.\nLV-UNet employs an enhanced deep training strategy and switches to a deployment\nmode during inference by re-parametrization, significantly reducing parameter\ncount and computational overhead. Experimental results on ISIC 2016, BUSI,\nCVC-ClinicDB, CVC-ColonDB, and Kvair-SEG datasets demonstrate a better\ntrade-off between performance and the computational load. The code will be\nreleased at \\url{https://github.com/juntaoJianggavin/LV-UNet}."
                },
                "authors": [
                    {
                        "name": "Juntao Jiang"
                    },
                    {
                        "name": "Mengmeng Wang"
                    },
                    {
                        "name": "Huizhong Tian"
                    },
                    {
                        "name": "Lingbo Cheng"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "Accepted by IEEE BIBM2024 ML4BMI workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16886v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16886v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14655v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14655v2",
                "updated": "2024-12-02T12:37:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    12,
                    37,
                    46,
                    0,
                    337,
                    0
                ],
                "published": "2024-05-23T14:53:54Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    14,
                    53,
                    54,
                    3,
                    144,
                    0
                ],
                "title": "Multi-turn Reinforcement Learning from Preference Human Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn Reinforcement Learning from Preference Human Feedback"
                },
                "summary": "Reinforcement Learning from Human Feedback (RLHF) has become the standard\napproach for aligning Large Language Models (LLMs) with human preferences,\nallowing LLMs to demonstrate remarkable abilities in various tasks. Existing\nmethods work by emulating the preferences at the single decision (turn) level,\nlimiting their capabilities in settings that require planning or multi-turn\ninteractions to achieve a long-term goal. In this paper, we address this issue\nby developing novel methods for Reinforcement Learning (RL) from preference\nfeedback between two full multi-turn conversations. In the tabular setting, we\npresent a novel mirror-descent-based policy optimization algorithm for the\ngeneral multi-turn preference-based RL problem, and prove its convergence to\nNash equilibrium. To evaluate performance, we create a new environment,\nEducation Dialogue, where a teacher agent guides a student in learning a random\ntopic, and show that a deep RL variant of our algorithm outperforms RLHF\nbaselines. Finally, we show that in an environment with explicit rewards, our\nalgorithm recovers the same performance as a reward-based RL baseline, despite\nrelying solely on a weaker preference signal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Human Feedback (RLHF) has become the standard\napproach for aligning Large Language Models (LLMs) with human preferences,\nallowing LLMs to demonstrate remarkable abilities in various tasks. Existing\nmethods work by emulating the preferences at the single decision (turn) level,\nlimiting their capabilities in settings that require planning or multi-turn\ninteractions to achieve a long-term goal. In this paper, we address this issue\nby developing novel methods for Reinforcement Learning (RL) from preference\nfeedback between two full multi-turn conversations. In the tabular setting, we\npresent a novel mirror-descent-based policy optimization algorithm for the\ngeneral multi-turn preference-based RL problem, and prove its convergence to\nNash equilibrium. To evaluate performance, we create a new environment,\nEducation Dialogue, where a teacher agent guides a student in learning a random\ntopic, and show that a deep RL variant of our algorithm outperforms RLHF\nbaselines. Finally, we show that in an environment with explicit rewards, our\nalgorithm recovers the same performance as a reward-based RL baseline, despite\nrelying solely on a weaker preference signal."
                },
                "authors": [
                    {
                        "name": "Lior Shani"
                    },
                    {
                        "name": "Aviv Rosenberg"
                    },
                    {
                        "name": "Asaf Cassel"
                    },
                    {
                        "name": "Oran Lang"
                    },
                    {
                        "name": "Daniele Calandriello"
                    },
                    {
                        "name": "Avital Zipori"
                    },
                    {
                        "name": "Hila Noga"
                    },
                    {
                        "name": "Orgad Keller"
                    },
                    {
                        "name": "Bilal Piot"
                    },
                    {
                        "name": "Idan Szpektor"
                    },
                    {
                        "name": "Avinatan Hassidim"
                    },
                    {
                        "name": "Yossi Matias"
                    },
                    {
                        "name": "RÃ©mi Munos"
                    }
                ],
                "author_detail": {
                    "name": "RÃ©mi Munos"
                },
                "author": "RÃ©mi Munos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14655v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14655v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.07818v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.07818v5",
                "updated": "2024-12-02T12:29:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    12,
                    29,
                    47,
                    0,
                    337,
                    0
                ],
                "published": "2024-02-12T17:24:15Z",
                "published_parsed": [
                    2024,
                    2,
                    12,
                    17,
                    24,
                    15,
                    0,
                    43,
                    0
                ],
                "title": "Differentially Private Zeroth-Order Methods for Scalable Large Language\n  Model Finetuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentially Private Zeroth-Order Methods for Scalable Large Language\n  Model Finetuning"
                },
                "summary": "Fine-tuning on task-specific datasets is a widely-embraced paradigm of\nharnessing the powerful capability of pretrained LLMs for various downstream\ntasks. Due to the popularity of LLMs fine-tuning and its accompanying privacy\nconcerns, differentially private (DP) fine-tuning of pretrained LLMs has been\nwidely used to safeguarding the privacy of task-specific datasets. Lying at the\ndesign core of DP LLM fine-tuning methods is the satisfactory tradeoff among\nprivacy, utility, and scalability. Most existing methods build upon the seminal\nwork of DP-SGD. Despite pushing the scalability of DP-SGD to its limit,\nDP-SGD-based fine-tuning methods are unfortunately limited by the inherent\ninefficiency of SGD.\n  In this paper, we investigate the potential of DP zeroth-order methods for\nLLM pretraining, which avoids the scalability bottleneck of SGD by\napproximating the gradient with the more efficient zeroth-order gradient.\nRather than treating the zeroth-order method as a drop-in replacement for SGD,\nthis paper presents a comprehensive study both theoretically and empirically.\nFirst, we propose the stagewise DP zeroth-order method (DP-ZOSO) that\ndynamically schedules key hyperparameters. This design is grounded on the\nsynergy between DP random perturbation and the gradient approximation error of\nthe zeroth-order method, and its effect on fine-tuning trajectory.\n  We provide theoretical analysis for both proposed methods. We conduct\nextensive empirical analysis on both encoder-only masked language model and\ndecoder-only autoregressive language model, achieving impressive results in\nterms of scalability and utility regardless of the class of tasks (compared\nwith DPZero, DP-ZOPO improves $4.5\\%$ on SST-5, $5.5\\%$ on MNLI with\nRoBERTa-Large and 9.2\\% on CB, 3.9\\% on BoolQ with OPT-2.7b when $\\epsilon=4$,\ndemonstrates more significant enhancement in performance on more complicated\ntasks).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning on task-specific datasets is a widely-embraced paradigm of\nharnessing the powerful capability of pretrained LLMs for various downstream\ntasks. Due to the popularity of LLMs fine-tuning and its accompanying privacy\nconcerns, differentially private (DP) fine-tuning of pretrained LLMs has been\nwidely used to safeguarding the privacy of task-specific datasets. Lying at the\ndesign core of DP LLM fine-tuning methods is the satisfactory tradeoff among\nprivacy, utility, and scalability. Most existing methods build upon the seminal\nwork of DP-SGD. Despite pushing the scalability of DP-SGD to its limit,\nDP-SGD-based fine-tuning methods are unfortunately limited by the inherent\ninefficiency of SGD.\n  In this paper, we investigate the potential of DP zeroth-order methods for\nLLM pretraining, which avoids the scalability bottleneck of SGD by\napproximating the gradient with the more efficient zeroth-order gradient.\nRather than treating the zeroth-order method as a drop-in replacement for SGD,\nthis paper presents a comprehensive study both theoretically and empirically.\nFirst, we propose the stagewise DP zeroth-order method (DP-ZOSO) that\ndynamically schedules key hyperparameters. This design is grounded on the\nsynergy between DP random perturbation and the gradient approximation error of\nthe zeroth-order method, and its effect on fine-tuning trajectory.\n  We provide theoretical analysis for both proposed methods. We conduct\nextensive empirical analysis on both encoder-only masked language model and\ndecoder-only autoregressive language model, achieving impressive results in\nterms of scalability and utility regardless of the class of tasks (compared\nwith DPZero, DP-ZOPO improves $4.5\\%$ on SST-5, $5.5\\%$ on MNLI with\nRoBERTa-Large and 9.2\\% on CB, 3.9\\% on BoolQ with OPT-2.7b when $\\epsilon=4$,\ndemonstrates more significant enhancement in performance on more complicated\ntasks)."
                },
                "authors": [
                    {
                        "name": "Z Liu"
                    },
                    {
                        "name": "J Lou"
                    },
                    {
                        "name": "W Bao"
                    },
                    {
                        "name": "Y Hu"
                    },
                    {
                        "name": "B Li"
                    },
                    {
                        "name": "Z Qin"
                    },
                    {
                        "name": "K Ren"
                    }
                ],
                "author_detail": {
                    "name": "K Ren"
                },
                "author": "K Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.07818v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.07818v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14708v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14708v2",
                "updated": "2024-12-02T10:52:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    10,
                    52,
                    21,
                    0,
                    337,
                    0
                ],
                "published": "2024-11-22T03:33:51Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    3,
                    33,
                    51,
                    4,
                    327,
                    0
                ],
                "title": "Understanding LLM Embeddings for Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding LLM Embeddings for Regression"
                },
                "summary": "With the rise of large language models (LLMs) for flexibly processing\ninformation as strings, a natural application is regression, specifically by\npreprocessing string representations into LLM embeddings as downstream features\nfor metric prediction. In this paper, we provide one of the first comprehensive\ninvestigations into embedding-based regression and demonstrate that LLM\nembeddings as features can be better for high-dimensional regression tasks than\nusing traditional feature engineering. This regression performance can be\nexplained in part due to LLM embeddings over numeric data inherently preserving\nLipschitz continuity over the feature space. Furthermore, we quantify the\ncontribution of different model effects, most notably model size and language\nunderstanding, which we find surprisingly do not always improve regression\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of large language models (LLMs) for flexibly processing\ninformation as strings, a natural application is regression, specifically by\npreprocessing string representations into LLM embeddings as downstream features\nfor metric prediction. In this paper, we provide one of the first comprehensive\ninvestigations into embedding-based regression and demonstrate that LLM\nembeddings as features can be better for high-dimensional regression tasks than\nusing traditional feature engineering. This regression performance can be\nexplained in part due to LLM embeddings over numeric data inherently preserving\nLipschitz continuity over the feature space. Furthermore, we quantify the\ncontribution of different model effects, most notably model size and language\nunderstanding, which we find surprisingly do not always improve regression\nperformance."
                },
                "authors": [
                    {
                        "name": "Eric Tang"
                    },
                    {
                        "name": "Bangding Yang"
                    },
                    {
                        "name": "Xingyou Song"
                    }
                ],
                "author_detail": {
                    "name": "Xingyou Song"
                },
                "author": "Xingyou Song",
                "arxiv_comment": "16 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14708v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14708v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.19211v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.19211v2",
                "updated": "2024-12-02T10:44:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    10,
                    44,
                    8,
                    0,
                    337,
                    0
                ],
                "published": "2024-03-28T08:19:33Z",
                "published_parsed": [
                    2024,
                    3,
                    28,
                    8,
                    19,
                    33,
                    3,
                    88,
                    0
                ],
                "title": "Dual-Personalizing Adapter for Federated Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dual-Personalizing Adapter for Federated Foundation Models"
                },
                "summary": "Recently, foundation models, particularly large language models (LLMs), have\ndemonstrated an impressive ability to adapt to various tasks by fine-tuning\ndiverse instruction data. Notably, federated foundation models (FedFM) emerge\nas a privacy preservation method to fine-tune models collaboratively under\nfederated learning (FL) settings by leveraging many distributed datasets with\nnon-IID data. To alleviate communication and computation overhead,\nparameter-efficient methods are introduced for efficiency, and some research\nadapted personalization methods to FedFM for better user preferences alignment.\nHowever, a critical gap in existing research is the neglect of test-time\ndistribution shifts in real-world applications, and conventional methods for\ntest-time distribution shifts in personalized FL are less effective for FedFM\ndue to their failure to adapt to complex distribution shift scenarios and the\nrequirement to train all parameters. To bridge this gap, we refine the setting\nin FedFM, termed test-time personalization, which aims to learn personalized\nfederated foundation models on clients while effectively handling test-time\ndistribution shifts simultaneously. To address challenges in this setting, we\nexplore a simple yet effective solution, a Federated Dual-Personalizing Adapter\n(FedDPA) architecture. By co-working with a foundation model, a global adapter\nand a local adapter jointly tackle the test-time distribution shifts and\nclient-specific personalization. Additionally, we introduce an instance-wise\ndynamic weighting mechanism that dynamically integrates the global and local\nadapters for each test instance during inference, facilitating effective\ntest-time personalization. The effectiveness of the proposed method has been\nevaluated on benchmark datasets across different NLP tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, foundation models, particularly large language models (LLMs), have\ndemonstrated an impressive ability to adapt to various tasks by fine-tuning\ndiverse instruction data. Notably, federated foundation models (FedFM) emerge\nas a privacy preservation method to fine-tune models collaboratively under\nfederated learning (FL) settings by leveraging many distributed datasets with\nnon-IID data. To alleviate communication and computation overhead,\nparameter-efficient methods are introduced for efficiency, and some research\nadapted personalization methods to FedFM for better user preferences alignment.\nHowever, a critical gap in existing research is the neglect of test-time\ndistribution shifts in real-world applications, and conventional methods for\ntest-time distribution shifts in personalized FL are less effective for FedFM\ndue to their failure to adapt to complex distribution shift scenarios and the\nrequirement to train all parameters. To bridge this gap, we refine the setting\nin FedFM, termed test-time personalization, which aims to learn personalized\nfederated foundation models on clients while effectively handling test-time\ndistribution shifts simultaneously. To address challenges in this setting, we\nexplore a simple yet effective solution, a Federated Dual-Personalizing Adapter\n(FedDPA) architecture. By co-working with a foundation model, a global adapter\nand a local adapter jointly tackle the test-time distribution shifts and\nclient-specific personalization. Additionally, we introduce an instance-wise\ndynamic weighting mechanism that dynamically integrates the global and local\nadapters for each test instance during inference, facilitating effective\ntest-time personalization. The effectiveness of the proposed method has been\nevaluated on benchmark datasets across different NLP tasks."
                },
                "authors": [
                    {
                        "name": "Yiyuan Yang"
                    },
                    {
                        "name": "Guodong Long"
                    },
                    {
                        "name": "Tao Shen"
                    },
                    {
                        "name": "Jing Jiang"
                    },
                    {
                        "name": "Michael Blumenstein"
                    }
                ],
                "author_detail": {
                    "name": "Michael Blumenstein"
                },
                "author": "Michael Blumenstein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.19211v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.19211v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.18569v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.18569v2",
                "updated": "2024-12-02T10:15:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    10,
                    15,
                    56,
                    0,
                    337,
                    0
                ],
                "published": "2024-02-28T18:58:11Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    18,
                    58,
                    11,
                    2,
                    59,
                    0
                ],
                "title": "Energy-Aware Heterogeneous Federated Learning via Approximate DNN\n  Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy-Aware Heterogeneous Federated Learning via Approximate DNN\n  Accelerators"
                },
                "summary": "In Federated Learning (FL), devices that participate in the training usually\nhave heterogeneous resources, i.e., energy availability. In current deployments\nof FL, devices that do not fulfill certain hardware requirements are often\ndropped from the collaborative training. However, dropping devices in FL can\ndegrade training accuracy and introduce bias or unfairness. Several works have\ntackled this problem on an algorithm level, e.g., by letting constrained\ndevices train a subset of the server neural network (NN) model. However, it has\nbeen observed that these techniques are not effective w.r.t. accuracy.\nImportantly, they make simplistic assumptions about devices' resources via\nindirect metrics such as multiply accumulate (MAC) operations or peak memory\nrequirements. We observe that memory access costs (that are currently not\nconsidered in simplistic metrics) have a significant impact on the energy\nconsumption. In this work, for the first time, we consider on-device\naccelerator design for FL with heterogeneous devices. We utilize compressed\narithmetic formats and approximate computing, targeting to satisfy limited\nenergy budgets. Using a hardware-aware energy model, we observe that, contrary\nto the state of the art's moderate energy reduction, our technique allows for\nlowering the energy requirements (by 4x) while maintaining higher accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Federated Learning (FL), devices that participate in the training usually\nhave heterogeneous resources, i.e., energy availability. In current deployments\nof FL, devices that do not fulfill certain hardware requirements are often\ndropped from the collaborative training. However, dropping devices in FL can\ndegrade training accuracy and introduce bias or unfairness. Several works have\ntackled this problem on an algorithm level, e.g., by letting constrained\ndevices train a subset of the server neural network (NN) model. However, it has\nbeen observed that these techniques are not effective w.r.t. accuracy.\nImportantly, they make simplistic assumptions about devices' resources via\nindirect metrics such as multiply accumulate (MAC) operations or peak memory\nrequirements. We observe that memory access costs (that are currently not\nconsidered in simplistic metrics) have a significant impact on the energy\nconsumption. In this work, for the first time, we consider on-device\naccelerator design for FL with heterogeneous devices. We utilize compressed\narithmetic formats and approximate computing, targeting to satisfy limited\nenergy budgets. Using a hardware-aware energy model, we observe that, contrary\nto the state of the art's moderate energy reduction, our technique allows for\nlowering the energy requirements (by 4x) while maintaining higher accuracy."
                },
                "authors": [
                    {
                        "name": "Kilian Pfeiffer"
                    },
                    {
                        "name": "Konstantinos Balaskas"
                    },
                    {
                        "name": "Kostas Siozios"
                    },
                    {
                        "name": "JÃ¶rg Henkel"
                    }
                ],
                "author_detail": {
                    "name": "JÃ¶rg Henkel"
                },
                "author": "JÃ¶rg Henkel",
                "arxiv_doi": "10.1109/TCAD.2024.3509793",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TCAD.2024.3509793",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.18569v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.18569v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "accepted at IEEE TCAD",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.06125v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.06125v2",
                "updated": "2024-12-02T10:02:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    10,
                    2,
                    18,
                    0,
                    337,
                    0
                ],
                "published": "2024-07-08T17:00:51Z",
                "published_parsed": [
                    2024,
                    7,
                    8,
                    17,
                    0,
                    51,
                    0,
                    190,
                    0
                ],
                "title": "Depression Detection and Analysis using Large Language Models on Textual\n  and Audio-Visual Modalities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Depression Detection and Analysis using Large Language Models on Textual\n  and Audio-Visual Modalities"
                },
                "summary": "Depression has proven to be a significant public health issue, profoundly\naffecting the psychological well-being of individuals. If it remains\nundiagnosed, depression can lead to severe health issues, which can manifest\nphysically and even lead to suicide. Generally, Diagnosing depression or any\nother mental disorder involves conducting semi-structured interviews alongside\nsupplementary questionnaires, including variants of the Patient Health\nQuestionnaire (PHQ) by Clinicians and mental health professionals. This\napproach places significant reliance on the experience and judgment of trained\nphysicians, making the diagnosis susceptible to personal biases. Given that the\nunderlying mechanisms causing depression are still being actively researched,\nphysicians often face challenges in diagnosing and treating the condition,\nparticularly in its early stages of clinical presentation. Recently,\nsignificant strides have been made in Artificial neural computing to solve\nproblems involving text, image, and speech in various domains. Our analysis has\naimed to leverage these state-of-the-art (SOTA) models in our experiments to\nachieve optimal outcomes leveraging multiple modalities. The experiments were\nperformed on the Extended Distress Analysis Interview Corpus Wizard of Oz\ndataset (E-DAIC) corpus presented in the Audio/Visual Emotion Challenge (AVEC)\n2019 Challenge. The proposed solutions demonstrate better results achieved by\nProprietary and Open-source Large Language Models (LLMs), which achieved a Root\nMean Square Error (RMSE) score of 3.98 on Textual Modality, beating the AVEC\n2019 challenge baseline results and current SOTA regression analysis\narchitectures. Additionally, the proposed solution achieved an accuracy of\n71.43% in the classification task. The paper also includes a novel audio-visual\nmulti-modal network that predicts PHQ-8 scores with an RMSE of 6.51.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Depression has proven to be a significant public health issue, profoundly\naffecting the psychological well-being of individuals. If it remains\nundiagnosed, depression can lead to severe health issues, which can manifest\nphysically and even lead to suicide. Generally, Diagnosing depression or any\nother mental disorder involves conducting semi-structured interviews alongside\nsupplementary questionnaires, including variants of the Patient Health\nQuestionnaire (PHQ) by Clinicians and mental health professionals. This\napproach places significant reliance on the experience and judgment of trained\nphysicians, making the diagnosis susceptible to personal biases. Given that the\nunderlying mechanisms causing depression are still being actively researched,\nphysicians often face challenges in diagnosing and treating the condition,\nparticularly in its early stages of clinical presentation. Recently,\nsignificant strides have been made in Artificial neural computing to solve\nproblems involving text, image, and speech in various domains. Our analysis has\naimed to leverage these state-of-the-art (SOTA) models in our experiments to\nachieve optimal outcomes leveraging multiple modalities. The experiments were\nperformed on the Extended Distress Analysis Interview Corpus Wizard of Oz\ndataset (E-DAIC) corpus presented in the Audio/Visual Emotion Challenge (AVEC)\n2019 Challenge. The proposed solutions demonstrate better results achieved by\nProprietary and Open-source Large Language Models (LLMs), which achieved a Root\nMean Square Error (RMSE) score of 3.98 on Textual Modality, beating the AVEC\n2019 challenge baseline results and current SOTA regression analysis\narchitectures. Additionally, the proposed solution achieved an accuracy of\n71.43% in the classification task. The paper also includes a novel audio-visual\nmulti-modal network that predicts PHQ-8 scores with an RMSE of 6.51."
                },
                "authors": [
                    {
                        "name": "Chayan Tank"
                    },
                    {
                        "name": "Sarthak Pol"
                    },
                    {
                        "name": "Vinayak Katoch"
                    },
                    {
                        "name": "Shaina Mehta"
                    },
                    {
                        "name": "Avinash Anand"
                    },
                    {
                        "name": "Rajiv Ratn Shah"
                    }
                ],
                "author_detail": {
                    "name": "Rajiv Ratn Shah"
                },
                "author": "Rajiv Ratn Shah",
                "arxiv_comment": "12 pages, 9 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.06125v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.06125v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.10807v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.10807v3",
                "updated": "2024-12-02T09:59:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    9,
                    59,
                    54,
                    0,
                    337,
                    0
                ],
                "published": "2023-12-17T20:13:20Z",
                "published_parsed": [
                    2023,
                    12,
                    17,
                    20,
                    13,
                    20,
                    6,
                    351,
                    0
                ],
                "title": "Bridging Language and Action: A Survey of Language-Conditioned Robot\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Language and Action: A Survey of Language-Conditioned Robot\n  Manipulation"
                },
                "summary": "Language-conditioned robot manipulation is an emerging field aimed at\nenabling seamless communication and cooperation between humans and robotic\nagents by teaching robots to comprehend and execute instructions conveyed in\nnatural language. This interdisciplinary area integrates scene understanding,\nlanguage processing, and policy learning to bridge the gap between human\ninstructions and robotic actions. In this comprehensive survey, we\nsystematically explore recent advancements in language-conditioned robotic\nmanipulation. We categorize existing methods into language-conditioned reward\nshaping, language-conditioned policy learning, neuro-symbolic artificial\nintelligence, and the utilization of foundational models (FMs) such as large\nlanguage models (LLMs) and vision-language models (VLMs). Specifically, we\nanalyze state-of-the-art techniques concerning semantic information extraction,\nenvironment and evaluation, auxiliary tasks, and task representation\nstrategies. By conducting a comparative analysis, we highlight the strengths\nand limitations of current approaches in bridging language instructions with\nrobot actions. Finally, we discuss open challenges and future research\ndirections, focusing on potentially enhancing generalization capabilities and\naddressing safety issues in language-conditioned robot manipulators. The GitHub\nrepository of this paper can be found at\nhttps://github.com/hk-zh/language-conditioned-robot-manipulation-models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-conditioned robot manipulation is an emerging field aimed at\nenabling seamless communication and cooperation between humans and robotic\nagents by teaching robots to comprehend and execute instructions conveyed in\nnatural language. This interdisciplinary area integrates scene understanding,\nlanguage processing, and policy learning to bridge the gap between human\ninstructions and robotic actions. In this comprehensive survey, we\nsystematically explore recent advancements in language-conditioned robotic\nmanipulation. We categorize existing methods into language-conditioned reward\nshaping, language-conditioned policy learning, neuro-symbolic artificial\nintelligence, and the utilization of foundational models (FMs) such as large\nlanguage models (LLMs) and vision-language models (VLMs). Specifically, we\nanalyze state-of-the-art techniques concerning semantic information extraction,\nenvironment and evaluation, auxiliary tasks, and task representation\nstrategies. By conducting a comparative analysis, we highlight the strengths\nand limitations of current approaches in bridging language instructions with\nrobot actions. Finally, we discuss open challenges and future research\ndirections, focusing on potentially enhancing generalization capabilities and\naddressing safety issues in language-conditioned robot manipulators. The GitHub\nrepository of this paper can be found at\nhttps://github.com/hk-zh/language-conditioned-robot-manipulation-models."
                },
                "authors": [
                    {
                        "name": "Hongkuan Zhou"
                    },
                    {
                        "name": "Xiangtong Yao"
                    },
                    {
                        "name": "Oier Mees"
                    },
                    {
                        "name": "Yuan Meng"
                    },
                    {
                        "name": "Ted Xiao"
                    },
                    {
                        "name": "Yonatan Bisk"
                    },
                    {
                        "name": "Jean Oh"
                    },
                    {
                        "name": "Edward Johns"
                    },
                    {
                        "name": "Mohit Shridhar"
                    },
                    {
                        "name": "Dhruv Shah"
                    },
                    {
                        "name": "Jesse Thomason"
                    },
                    {
                        "name": "Kai Huang"
                    },
                    {
                        "name": "Joyce Chai"
                    },
                    {
                        "name": "Zhenshan Bing"
                    },
                    {
                        "name": "Alois Knoll"
                    }
                ],
                "author_detail": {
                    "name": "Alois Knoll"
                },
                "author": "Alois Knoll",
                "arxiv_comment": "37 pages, 15 figures, 4 tables, 353 citations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.10807v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.10807v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14377v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14377v2",
                "updated": "2024-12-02T09:48:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    9,
                    48,
                    21,
                    0,
                    337,
                    0
                ],
                "published": "2024-05-23T09:52:15Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    9,
                    52,
                    15,
                    3,
                    144,
                    0
                ],
                "title": "CoMERA: Computing- and Memory-Efficient Training via Rank-Adaptive\n  Tensor Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoMERA: Computing- and Memory-Efficient Training via Rank-Adaptive\n  Tensor Optimization"
                },
                "summary": "Training large AI models such as LLMs and DLRMs costs massive GPUs and\ncomputing time. The high training cost has become only affordable to big tech\ncompanies, meanwhile also causing increasing concerns about the environmental\nimpact. This paper presents CoMERA, a Computing- and Memory-Efficient training\nmethod via Rank-Adaptive tensor optimization. CoMERA achieves rank-adaptive\ntensor-compressed (pre)-training via a multi-objective optimization formulation\nand improves the training to provide both a high compression ratio and\nexcellent accuracy in the training process. Our optimized numerical computation\n(e.g., optimized tensorized embedding and tensor-network contractions) and GPU\nimplementation eliminate part of the run-time overhead in the tensorized\ntraining on GPU. This leads to, for the first time, $2-3\\times$ speedup per\ntraining epoch compared with standard training. CoMERA also outperforms the\nrecent GaLore in terms of both memory and computing efficiency. Specifically,\nCoMERA is $2\\times$ faster per training epoch and $9\\times$ more\nmemory-efficient than GaLore on a tested six-encoder transformer with\nsingle-batch training. Our method also shows $\\sim 2\\times$ speedup than\nstandard pre-training on a BERT-like code-generation LLM while achieving\n$4.23\\times$ compression ratio in pre-training. With further HPC optimization,\nCoMERA may reduce the pre-training cost of many other LLMs. An implementation\nof CoMERA is available at https://github.com/ziyangjoy/CoMERA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large AI models such as LLMs and DLRMs costs massive GPUs and\ncomputing time. The high training cost has become only affordable to big tech\ncompanies, meanwhile also causing increasing concerns about the environmental\nimpact. This paper presents CoMERA, a Computing- and Memory-Efficient training\nmethod via Rank-Adaptive tensor optimization. CoMERA achieves rank-adaptive\ntensor-compressed (pre)-training via a multi-objective optimization formulation\nand improves the training to provide both a high compression ratio and\nexcellent accuracy in the training process. Our optimized numerical computation\n(e.g., optimized tensorized embedding and tensor-network contractions) and GPU\nimplementation eliminate part of the run-time overhead in the tensorized\ntraining on GPU. This leads to, for the first time, $2-3\\times$ speedup per\ntraining epoch compared with standard training. CoMERA also outperforms the\nrecent GaLore in terms of both memory and computing efficiency. Specifically,\nCoMERA is $2\\times$ faster per training epoch and $9\\times$ more\nmemory-efficient than GaLore on a tested six-encoder transformer with\nsingle-batch training. Our method also shows $\\sim 2\\times$ speedup than\nstandard pre-training on a BERT-like code-generation LLM while achieving\n$4.23\\times$ compression ratio in pre-training. With further HPC optimization,\nCoMERA may reduce the pre-training cost of many other LLMs. An implementation\nof CoMERA is available at https://github.com/ziyangjoy/CoMERA."
                },
                "authors": [
                    {
                        "name": "Zi Yang"
                    },
                    {
                        "name": "Ziyue Liu"
                    },
                    {
                        "name": "Samridhi Choudhary"
                    },
                    {
                        "name": "Xinfeng Xie"
                    },
                    {
                        "name": "Cao Gao"
                    },
                    {
                        "name": "Siegfried Kunzmann"
                    },
                    {
                        "name": "Zheng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Zhang"
                },
                "author": "Zheng Zhang",
                "arxiv_comment": "Accepted by Neurips 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14377v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14377v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04125v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04125v2",
                "updated": "2024-12-02T09:42:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    9,
                    42,
                    24,
                    0,
                    337,
                    0
                ],
                "published": "2024-07-04T18:54:30Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    18,
                    54,
                    30,
                    3,
                    186,
                    0
                ],
                "title": "Query-Guided Self-Supervised Summarization of Nursing Notes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query-Guided Self-Supervised Summarization of Nursing Notes"
                },
                "summary": "Nursing notes, an important part of Electronic Health Records (EHRs), track a\npatient's health during a care episode. Summarizing key information in nursing\nnotes can help clinicians quickly understand patients' conditions. However,\nexisting summarization methods in the clinical setting, especially abstractive\nmethods, have overlooked nursing notes and require reference summaries for\ntraining. We introduce QGSumm, a novel query-guided self-supervised domain\nadaptation approach for abstractive nursing note summarization. The method uses\npatient-related clinical queries for guidance, and hence does not need\nreference summaries for training. Through automatic experiments and manual\nevaluation by an expert clinician, we study our approach and other\nstate-of-the-art Large Language Models (LLMs) for nursing note summarization.\nOur experiments show: 1) GPT-4 is competitive in maintaining information in the\noriginal nursing notes, 2) QGSumm can generate high-quality summaries with a\ngood balance between recall of the original content and hallucination rate\nlower than other top methods. Ultimately, our work offers a new perspective on\nconditional text summarization, tailored to clinical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nursing notes, an important part of Electronic Health Records (EHRs), track a\npatient's health during a care episode. Summarizing key information in nursing\nnotes can help clinicians quickly understand patients' conditions. However,\nexisting summarization methods in the clinical setting, especially abstractive\nmethods, have overlooked nursing notes and require reference summaries for\ntraining. We introduce QGSumm, a novel query-guided self-supervised domain\nadaptation approach for abstractive nursing note summarization. The method uses\npatient-related clinical queries for guidance, and hence does not need\nreference summaries for training. Through automatic experiments and manual\nevaluation by an expert clinician, we study our approach and other\nstate-of-the-art Large Language Models (LLMs) for nursing note summarization.\nOur experiments show: 1) GPT-4 is competitive in maintaining information in the\noriginal nursing notes, 2) QGSumm can generate high-quality summaries with a\ngood balance between recall of the original content and hallucination rate\nlower than other top methods. Ultimately, our work offers a new perspective on\nconditional text summarization, tailored to clinical applications."
                },
                "authors": [
                    {
                        "name": "Ya Gao"
                    },
                    {
                        "name": "Hans Moen"
                    },
                    {
                        "name": "Saila Koivusalo"
                    },
                    {
                        "name": "Miika Koskinen"
                    },
                    {
                        "name": "Pekka Marttinen"
                    }
                ],
                "author_detail": {
                    "name": "Pekka Marttinen"
                },
                "author": "Pekka Marttinen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04125v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04125v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14963v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14963v2",
                "updated": "2024-12-02T09:21:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    9,
                    21,
                    20,
                    0,
                    337,
                    0
                ],
                "published": "2024-08-27T11:10:37Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    11,
                    10,
                    37,
                    1,
                    240,
                    0
                ],
                "title": "The future of offshore wind power production: wake and climate impacts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The future of offshore wind power production: wake and climate impacts"
                },
                "summary": "Rapid deployment of offshore wind is expected within the coming decades to\nhelp meet climate goals. With offshore wind turbine lifetimes of 25-30 years,\nand new offshore leases spanning 60 years, it is vital to consider long-term\nchanges in potential wind power resource at the farm planning stage. Such\nchanges may arise from multiple sources, including climate change, and\nincreasing wake-induced power losses. In this work, we investigate and compare\nthese two sources of long-term change in wind power, for a case study\nconsisting of 21 wind farms within the German Bight. Consistent with previous\nstudies, we find a small but significant reduction in wind resource due to\nclimate change by the end of the 21st century under the high-emission RCP8.5\nscenario, compared with a historical period, with a mean power reduction (over\nan ensemble of seven climate models) of 2.1%. To assess the impact of\nwake-induced losses due to increasingly dense farm build-out, we model wakes\nwithin the German Bight region using an engineering wake model, under various\nstages of (planned) build-out corresponding to the years 2010-2027. By\nidentifying clusters of wind farms, we decompose wake effects into long-range\n(inter-cluster), medium-range (intra-cluster) and short-range (intra-farm)\neffects. Inter-cluster wake-induced losses increase from 0 for the 2010\nscenario to 2.5% for the 2027 scenario, with intra-cluster losses also\nincreasing from 0 to 4.3%. Intra-farm losses are relatively constant, at around\n13%. While the evolution of wake effects therefore outweighs the climate\neffect, and impacts over a shorter timescale, both factors are significant. We\nalso find evidence of an interaction between the climate and wake effects. Both\nclimate change and evolving wake effects must therefore be considered within\nresource assessment and wind farm planning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid deployment of offshore wind is expected within the coming decades to\nhelp meet climate goals. With offshore wind turbine lifetimes of 25-30 years,\nand new offshore leases spanning 60 years, it is vital to consider long-term\nchanges in potential wind power resource at the farm planning stage. Such\nchanges may arise from multiple sources, including climate change, and\nincreasing wake-induced power losses. In this work, we investigate and compare\nthese two sources of long-term change in wind power, for a case study\nconsisting of 21 wind farms within the German Bight. Consistent with previous\nstudies, we find a small but significant reduction in wind resource due to\nclimate change by the end of the 21st century under the high-emission RCP8.5\nscenario, compared with a historical period, with a mean power reduction (over\nan ensemble of seven climate models) of 2.1%. To assess the impact of\nwake-induced losses due to increasingly dense farm build-out, we model wakes\nwithin the German Bight region using an engineering wake model, under various\nstages of (planned) build-out corresponding to the years 2010-2027. By\nidentifying clusters of wind farms, we decompose wake effects into long-range\n(inter-cluster), medium-range (intra-cluster) and short-range (intra-farm)\neffects. Inter-cluster wake-induced losses increase from 0 for the 2010\nscenario to 2.5% for the 2027 scenario, with intra-cluster losses also\nincreasing from 0 to 4.3%. Intra-farm losses are relatively constant, at around\n13%. While the evolution of wake effects therefore outweighs the climate\neffect, and impacts over a shorter timescale, both factors are significant. We\nalso find evidence of an interaction between the climate and wake effects. Both\nclimate change and evolving wake effects must therefore be considered within\nresource assessment and wind farm planning."
                },
                "authors": [
                    {
                        "name": "Simon C Warder"
                    },
                    {
                        "name": "Matthew D Piggott"
                    }
                ],
                "author_detail": {
                    "name": "Matthew D Piggott"
                },
                "author": "Matthew D Piggott",
                "arxiv_doi": "10.1016/j.apenergy.2024.124956",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.apenergy.2024.124956",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.14963v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14963v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Applied Energy 380 (2025)",
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19876v2",
                "updated": "2024-12-02T08:58:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    58,
                    20,
                    0,
                    337,
                    0
                ],
                "published": "2024-11-29T17:38:56Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    17,
                    38,
                    56,
                    4,
                    334,
                    0
                ],
                "title": "LUMIA: Linear probing for Unimodal and MultiModal Membership Inference\n  Attacks leveraging internal LLM states",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LUMIA: Linear probing for Unimodal and MultiModal Membership Inference\n  Attacks leveraging internal LLM states"
                },
                "summary": "Large Language Models (LLMs) are increasingly used in a variety of\napplications, but concerns around membership inference have grown in parallel.\nPrevious efforts focus on black-to-grey-box models, thus neglecting the\npotential benefit from internal LLM information. To address this, we propose\nthe use of Linear Probes (LPs) as a method to detect Membership Inference\nAttacks (MIAs) by examining internal activations of LLMs. Our approach, dubbed\nLUMIA, applies LPs layer-by-layer to get fine-grained data on the model inner\nworkings. We test this method across several model architectures, sizes and\ndatasets, including unimodal and multimodal tasks. In unimodal MIA, LUMIA\nachieves an average gain of 15.71 % in Area Under the Curve (AUC) over previous\ntechniques. Remarkably, LUMIA reaches AUC>60% in 65.33% of cases -- an\nincrement of 46.80% against the state of the art. Furthermore, our approach\nreveals key insights, such as the model layers where MIAs are most detectable.\nIn multimodal models, LPs indicate that visual inputs can significantly\ncontribute to detect MIAs -- AUC>60% is reached in 85.90% of experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used in a variety of\napplications, but concerns around membership inference have grown in parallel.\nPrevious efforts focus on black-to-grey-box models, thus neglecting the\npotential benefit from internal LLM information. To address this, we propose\nthe use of Linear Probes (LPs) as a method to detect Membership Inference\nAttacks (MIAs) by examining internal activations of LLMs. Our approach, dubbed\nLUMIA, applies LPs layer-by-layer to get fine-grained data on the model inner\nworkings. We test this method across several model architectures, sizes and\ndatasets, including unimodal and multimodal tasks. In unimodal MIA, LUMIA\nachieves an average gain of 15.71 % in Area Under the Curve (AUC) over previous\ntechniques. Remarkably, LUMIA reaches AUC>60% in 65.33% of cases -- an\nincrement of 46.80% against the state of the art. Furthermore, our approach\nreveals key insights, such as the model layers where MIAs are most detectable.\nIn multimodal models, LPs indicate that visual inputs can significantly\ncontribute to detect MIAs -- AUC>60% is reached in 85.90% of experiments."
                },
                "authors": [
                    {
                        "name": "Luis Ibanez-Lissen"
                    },
                    {
                        "name": "Lorena Gonzalez-Manzano"
                    },
                    {
                        "name": "Jose Maria de Fuentes"
                    },
                    {
                        "name": "Nicolas Anciaux"
                    },
                    {
                        "name": "Joaquin Garcia-Alfaro"
                    }
                ],
                "author_detail": {
                    "name": "Joaquin Garcia-Alfaro"
                },
                "author": "Joaquin Garcia-Alfaro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19916v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19916v3",
                "updated": "2024-12-02T08:56:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    56,
                    26,
                    0,
                    337,
                    0
                ],
                "published": "2024-09-30T03:37:10Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    3,
                    37,
                    10,
                    0,
                    274,
                    0
                ],
                "title": "Deep Learning and Machine Learning, Advancing Big Data Analytics and\n  Management: Object-Oriented Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning and Machine Learning, Advancing Big Data Analytics and\n  Management: Object-Oriented Programming"
                },
                "summary": "Object-Oriented Programming (OOP) has become a crucial paradigm for managing\nthe growing complexity of modern software systems, particularly in fields like\nmachine learning, deep learning, large language models (LLM), and data\nanalytics. This work provides a comprehensive introduction to the integration\nof OOP techniques within these domains, with a focus on improving code\nmodularity, maintainability, and scalability. We begin by outlining the\nevolution of computing and the rise of OOP, followed by an in-depth discussion\nof key OOP principles such as encapsulation, inheritance, polymorphism, and\nabstraction. The practical application of these principles is demonstrated\nusing Python, a widely adopted language in AI and data science. Furthermore, we\nexamine how design patterns and modular programming can be employed to enhance\nthe structure and efficiency of machine learning systems. In subsequent\nsections, we apply these OOP concepts to real-world AI tasks, including the\nencapsulation of preprocessing workflows, machine learning model training, and\nevaluation. Detailed examples illustrate how OOP can be used to build reusable,\nscalable machine learning systems while maintaining code clarity and reducing\nredundancy.This work is intended to serve as a bridge for both beginners and\nexperienced developers, equipping them with the necessary knowledge to apply\nOOP methodologies in AI-driven projects, ultimately fostering the development\nof more robust and maintainable systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object-Oriented Programming (OOP) has become a crucial paradigm for managing\nthe growing complexity of modern software systems, particularly in fields like\nmachine learning, deep learning, large language models (LLM), and data\nanalytics. This work provides a comprehensive introduction to the integration\nof OOP techniques within these domains, with a focus on improving code\nmodularity, maintainability, and scalability. We begin by outlining the\nevolution of computing and the rise of OOP, followed by an in-depth discussion\nof key OOP principles such as encapsulation, inheritance, polymorphism, and\nabstraction. The practical application of these principles is demonstrated\nusing Python, a widely adopted language in AI and data science. Furthermore, we\nexamine how design patterns and modular programming can be employed to enhance\nthe structure and efficiency of machine learning systems. In subsequent\nsections, we apply these OOP concepts to real-world AI tasks, including the\nencapsulation of preprocessing workflows, machine learning model training, and\nevaluation. Detailed examples illustrate how OOP can be used to build reusable,\nscalable machine learning systems while maintaining code clarity and reducing\nredundancy.This work is intended to serve as a bridge for both beginners and\nexperienced developers, equipping them with the necessary knowledge to apply\nOOP methodologies in AI-driven projects, ultimately fostering the development\nof more robust and maintainable systems."
                },
                "authors": [
                    {
                        "name": "Tianyang Wang"
                    },
                    {
                        "name": "Ziqian Bi"
                    },
                    {
                        "name": "Keyu Chen"
                    },
                    {
                        "name": "Jiawei Xu"
                    },
                    {
                        "name": "Qian Niu"
                    },
                    {
                        "name": "Junyu Liu"
                    },
                    {
                        "name": "Benji Peng"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Sen Zhang"
                    },
                    {
                        "name": "Xuanhe Pan"
                    },
                    {
                        "name": "Jinlang Wang"
                    },
                    {
                        "name": "Pohsun Feng"
                    },
                    {
                        "name": "Caitlyn Heqi Yin"
                    },
                    {
                        "name": "Yizhu Wen"
                    },
                    {
                        "name": "Ming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ming Liu"
                },
                "author": "Ming Liu",
                "arxiv_comment": "49pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19916v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19916v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03274v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03274v3",
                "updated": "2024-12-02T08:53:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    53,
                    40,
                    0,
                    337,
                    0
                ],
                "published": "2024-09-05T06:31:37Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    6,
                    31,
                    37,
                    3,
                    249,
                    0
                ],
                "title": "Recent Advances in Attack and Defense Approaches of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Advances in Attack and Defense Approaches of Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) have revolutionized artificial intelligence and\nmachine learning through their advanced text processing and generating\ncapabilities. However, their widespread deployment has raised significant\nsafety and reliability concerns. Established vulnerabilities in deep neural\nnetworks, coupled with emerging threat models, may compromise security\nevaluations and create a false sense of security. Given the extensive research\nin the field of LLM security, we believe that summarizing the current state of\naffairs will help the research community better understand the present\nlandscape and inform future developments. This paper reviews current research\non LLM vulnerabilities and threats, and evaluates the effectiveness of\ncontemporary defense mechanisms. We analyze recent studies on attack vectors\nand model weaknesses, providing insights into attack mechanisms and the\nevolving threat landscape. We also examine current defense strategies,\nhighlighting their strengths and limitations. By contrasting advancements in\nattack and defense methodologies, we identify research gaps and propose future\ndirections to enhance LLM security. Our goal is to advance the understanding of\nLLM safety challenges and guide the development of more robust security\nmeasures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized artificial intelligence and\nmachine learning through their advanced text processing and generating\ncapabilities. However, their widespread deployment has raised significant\nsafety and reliability concerns. Established vulnerabilities in deep neural\nnetworks, coupled with emerging threat models, may compromise security\nevaluations and create a false sense of security. Given the extensive research\nin the field of LLM security, we believe that summarizing the current state of\naffairs will help the research community better understand the present\nlandscape and inform future developments. This paper reviews current research\non LLM vulnerabilities and threats, and evaluates the effectiveness of\ncontemporary defense mechanisms. We analyze recent studies on attack vectors\nand model weaknesses, providing insights into attack mechanisms and the\nevolving threat landscape. We also examine current defense strategies,\nhighlighting their strengths and limitations. By contrasting advancements in\nattack and defense methodologies, we identify research gaps and propose future\ndirections to enhance LLM security. Our goal is to advance the understanding of\nLLM safety challenges and guide the development of more robust security\nmeasures."
                },
                "authors": [
                    {
                        "name": "Jing Cui"
                    },
                    {
                        "name": "Yishi Xu"
                    },
                    {
                        "name": "Zhewei Huang"
                    },
                    {
                        "name": "Shuchang Zhou"
                    },
                    {
                        "name": "Jianbin Jiao"
                    },
                    {
                        "name": "Junge Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Junge Zhang"
                },
                "author": "Junge Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03274v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03274v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.02408v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.02408v2",
                "updated": "2024-12-02T07:47:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    7,
                    47,
                    0,
                    0,
                    337,
                    0
                ],
                "published": "2024-02-04T08:57:54Z",
                "published_parsed": [
                    2024,
                    2,
                    4,
                    8,
                    57,
                    54,
                    6,
                    35,
                    0
                ],
                "title": "GLaPE: Gold Label-agnostic Prompt Evaluation and Optimization for Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GLaPE: Gold Label-agnostic Prompt Evaluation and Optimization for Large\n  Language Model"
                },
                "summary": "Despite the rapid progress of large language models (LLMs), their task\nperformance remains sensitive to prompt design. Recent studies have explored\nleveraging the LLM itself as an optimizer to identify optimal prompts that\nmaximize task accuracy. However, when evaluating prompts, such approaches\nheavily rely on elusive manually annotated gold labels to calculate task\naccuracy for each candidate prompt, which hinders the widespread implementation\nand generality. To overcome the limitation, this work proposes a gold\nlabel-agnostic prompt evaluation (GLaPE) to alleviate dependence on gold\nlabels. Motivated by the observed correlation between self-consistency and the\naccuracy of the answer, we adopt self-consistency as the initial evaluation\nscore. Subsequently, we refine the scores of prompts producing identical\nanswers to be mutually consistent. Experimental results show that GLaPE\nprovides reliable evaluations uniform with accuracy, even in the absence of\ngold labels. Moreover, on six popular reasoning tasks, our GLaPE-based prompt\noptimization yields effective prompts comparable to accuracy-based ones. The\ncode is publicly available at https://github.com/thunderous77/GLaPE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the rapid progress of large language models (LLMs), their task\nperformance remains sensitive to prompt design. Recent studies have explored\nleveraging the LLM itself as an optimizer to identify optimal prompts that\nmaximize task accuracy. However, when evaluating prompts, such approaches\nheavily rely on elusive manually annotated gold labels to calculate task\naccuracy for each candidate prompt, which hinders the widespread implementation\nand generality. To overcome the limitation, this work proposes a gold\nlabel-agnostic prompt evaluation (GLaPE) to alleviate dependence on gold\nlabels. Motivated by the observed correlation between self-consistency and the\naccuracy of the answer, we adopt self-consistency as the initial evaluation\nscore. Subsequently, we refine the scores of prompts producing identical\nanswers to be mutually consistent. Experimental results show that GLaPE\nprovides reliable evaluations uniform with accuracy, even in the absence of\ngold labels. Moreover, on six popular reasoning tasks, our GLaPE-based prompt\noptimization yields effective prompts comparable to accuracy-based ones. The\ncode is publicly available at https://github.com/thunderous77/GLaPE."
                },
                "authors": [
                    {
                        "name": "Xuanchang Zhang"
                    },
                    {
                        "name": "Zhuosheng Zhang"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.02408v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.02408v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18363v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18363v2",
                "updated": "2024-12-02T07:04:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    7,
                    4,
                    40,
                    0,
                    337,
                    0
                ],
                "published": "2024-11-27T14:11:10Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    14,
                    11,
                    10,
                    2,
                    332,
                    0
                ],
                "title": "ChatRex: Taming Multimodal LLM for Joint Perception and Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatRex: Taming Multimodal LLM for Joint Perception and Understanding"
                },
                "summary": "Perception and understanding are two pillars of computer vision. While\nmultimodal large language models (MLLM) have demonstrated remarkable visual\nunderstanding capabilities, they arguably lack accurate perception abilities,\ne.g. the stage-of-the-art model Qwen2-VL only achieves a 43.9 recall rate on\nthe COCO dataset, limiting many tasks requiring the combination of perception\nand understanding. In this work, we aim to bridge this perception gap from both\nmodel designing and data development perspectives. We first introduce ChatRex,\nan MLLM with a decoupled perception design. Instead of having the LLM directly\npredict box coordinates, we feed the output boxes from a universal proposal\nnetwork into the LLM, allowing it to output the corresponding box indices to\nrepresent its detection results, turning the regression task into a\nretrieval-based task that LLM handles more proficiently. From the data\nperspective, we build a fully automated data engine and construct the\nRexverse-2M dataset which possesses multiple granularities to support the joint\ntraining of perception and understanding. After standard two-stage training,\nChatRex demonstrates strong perception capabilities while preserving multimodal\nunderstanding performance. The combination of these two capabilities\nsimultaneously unlocks many attractive applications, demonstrating the\ncomplementary roles of both perception and understanding in MLLM. Code is\navailable at \\url{https://github.com/IDEA-Research/ChatRex}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perception and understanding are two pillars of computer vision. While\nmultimodal large language models (MLLM) have demonstrated remarkable visual\nunderstanding capabilities, they arguably lack accurate perception abilities,\ne.g. the stage-of-the-art model Qwen2-VL only achieves a 43.9 recall rate on\nthe COCO dataset, limiting many tasks requiring the combination of perception\nand understanding. In this work, we aim to bridge this perception gap from both\nmodel designing and data development perspectives. We first introduce ChatRex,\nan MLLM with a decoupled perception design. Instead of having the LLM directly\npredict box coordinates, we feed the output boxes from a universal proposal\nnetwork into the LLM, allowing it to output the corresponding box indices to\nrepresent its detection results, turning the regression task into a\nretrieval-based task that LLM handles more proficiently. From the data\nperspective, we build a fully automated data engine and construct the\nRexverse-2M dataset which possesses multiple granularities to support the joint\ntraining of perception and understanding. After standard two-stage training,\nChatRex demonstrates strong perception capabilities while preserving multimodal\nunderstanding performance. The combination of these two capabilities\nsimultaneously unlocks many attractive applications, demonstrating the\ncomplementary roles of both perception and understanding in MLLM. Code is\navailable at \\url{https://github.com/IDEA-Research/ChatRex}."
                },
                "authors": [
                    {
                        "name": "Qing Jiang"
                    },
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Yuqin Yang"
                    },
                    {
                        "name": "Yuda Xiong"
                    },
                    {
                        "name": "Yihao Chen"
                    },
                    {
                        "name": "Zhaoyang Zeng"
                    },
                    {
                        "name": "Tianhe Ren"
                    },
                    {
                        "name": "Lei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zhang"
                },
                "author": "Lei Zhang",
                "arxiv_comment": "35 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18363v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18363v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10825v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10825v3",
                "updated": "2024-12-02T07:00:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    7,
                    0,
                    57,
                    0,
                    337,
                    0
                ],
                "published": "2024-09-17T01:37:57Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    1,
                    37,
                    57,
                    1,
                    261,
                    0
                ],
                "title": "Unveiling and Mitigating Bias in Large Language Model Recommendations: A\n  Path to Fairness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling and Mitigating Bias in Large Language Model Recommendations: A\n  Path to Fairness"
                },
                "summary": "excel in delivering comprehensive suggestions by deeply analyzing content and\nuser behavior. However, they often inherit biases from skewed training data,\nfavoring mainstream content while underrepresenting diverse or non-traditional\noptions. This study explores the interplay between bias and LLM-based\nrecommendation systems, focusing on music, song, and book recommendations\nacross diverse demographic and cultural groups. This paper analyzes bias in\nLLM-based recommendation systems across multiple models (GPT, LLaMA, and\nGemini), revealing its deep and pervasive impact on outcomes. Intersecting\nidentities and contextual factors, like socioeconomic status, further amplify\nbiases, complicating fair recommendations across diverse groups. Our findings\nreveal that bias in these systems is deeply ingrained, yet even simple\ninterventions like prompt engineering can significantly reduce it. We further\npropose a retrieval-augmented generation strategy to mitigate bias more\neffectively. Numerical experiments validate these strategies, demonstrating\nboth the pervasive nature of bias and the impact of the proposed solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "excel in delivering comprehensive suggestions by deeply analyzing content and\nuser behavior. However, they often inherit biases from skewed training data,\nfavoring mainstream content while underrepresenting diverse or non-traditional\noptions. This study explores the interplay between bias and LLM-based\nrecommendation systems, focusing on music, song, and book recommendations\nacross diverse demographic and cultural groups. This paper analyzes bias in\nLLM-based recommendation systems across multiple models (GPT, LLaMA, and\nGemini), revealing its deep and pervasive impact on outcomes. Intersecting\nidentities and contextual factors, like socioeconomic status, further amplify\nbiases, complicating fair recommendations across diverse groups. Our findings\nreveal that bias in these systems is deeply ingrained, yet even simple\ninterventions like prompt engineering can significantly reduce it. We further\npropose a retrieval-augmented generation strategy to mitigate bias more\neffectively. Numerical experiments validate these strategies, demonstrating\nboth the pervasive nature of bias and the impact of the proposed solutions."
                },
                "authors": [
                    {
                        "name": "Anindya Bijoy Das"
                    },
                    {
                        "name": "Shahnewaz Karim Sakib"
                    }
                ],
                "author_detail": {
                    "name": "Shahnewaz Karim Sakib"
                },
                "author": "Shahnewaz Karim Sakib",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10825v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10825v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19951v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19951v2",
                "updated": "2024-12-02T06:54:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    6,
                    54,
                    47,
                    0,
                    337,
                    0
                ],
                "published": "2024-11-29T18:59:54Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    18,
                    59,
                    54,
                    4,
                    334,
                    0
                ],
                "title": "T2Vid: Translating Long Text into Multi-Image is the Catalyst for\n  Video-LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T2Vid: Translating Long Text into Multi-Image is the Catalyst for\n  Video-LLMs"
                },
                "summary": "The success of Multimodal Large Language Models (MLLMs) in the image domain\nhas garnered wide attention from the research community. Drawing on previous\nsuccessful experiences, researchers have recently explored extending the\nsuccess to the video understanding realms. Apart from training from scratch, an\nefficient way is to utilize the pre-trained image-LLMs, leading to two\nmainstream approaches, i.e. zero-shot inference and further fine-tuning with\nvideo data. In this work, our study of these approaches harvests an effective\ndata augmentation method. We first make a deeper inspection of the zero-shot\ninference way and identify two limitations, i.e. limited generalization and\nlack of temporal understanding capabilities. Thus, we further investigate the\nfine-tuning approach and find a low learning efficiency when simply using all\nthe video data samples, which can be attributed to a lack of instruction\ndiversity. Aiming at this issue, we develop a method called T2Vid to synthesize\nvideo-like samples to enrich the instruction diversity in the training corpus.\nIntegrating these data enables a simple and efficient training scheme, which\nachieves performance comparable to or even superior to using full video\ndatasets by training with just 15% the sample size. Meanwhile, we find that the\nproposed scheme can boost the performance of long video understanding without\ntraining with long video samples. We hope our study will spark more thinking\nabout using MLLMs for video understanding and curation of high-quality data.\nThe code is released at https://github.com/xjtupanda/T2Vid.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of Multimodal Large Language Models (MLLMs) in the image domain\nhas garnered wide attention from the research community. Drawing on previous\nsuccessful experiences, researchers have recently explored extending the\nsuccess to the video understanding realms. Apart from training from scratch, an\nefficient way is to utilize the pre-trained image-LLMs, leading to two\nmainstream approaches, i.e. zero-shot inference and further fine-tuning with\nvideo data. In this work, our study of these approaches harvests an effective\ndata augmentation method. We first make a deeper inspection of the zero-shot\ninference way and identify two limitations, i.e. limited generalization and\nlack of temporal understanding capabilities. Thus, we further investigate the\nfine-tuning approach and find a low learning efficiency when simply using all\nthe video data samples, which can be attributed to a lack of instruction\ndiversity. Aiming at this issue, we develop a method called T2Vid to synthesize\nvideo-like samples to enrich the instruction diversity in the training corpus.\nIntegrating these data enables a simple and efficient training scheme, which\nachieves performance comparable to or even superior to using full video\ndatasets by training with just 15% the sample size. Meanwhile, we find that the\nproposed scheme can boost the performance of long video understanding without\ntraining with long video samples. We hope our study will spark more thinking\nabout using MLLMs for video understanding and curation of high-quality data.\nThe code is released at https://github.com/xjtupanda/T2Vid."
                },
                "authors": [
                    {
                        "name": "Shukang Yin"
                    },
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Sirui Zhao"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Chunjiang Ge"
                    },
                    {
                        "name": "Yan Yang"
                    },
                    {
                        "name": "Zuwei Long"
                    },
                    {
                        "name": "Yuhan Dai"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Ran He"
                    },
                    {
                        "name": "Caifeng Shan"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "arxiv_comment": "Project page: https://github.com/xjtupanda/T2Vid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19951v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19951v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01221v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01221v2",
                "updated": "2024-12-02T06:41:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    6,
                    41,
                    12,
                    0,
                    337,
                    0
                ],
                "published": "2024-05-02T12:04:35Z",
                "published_parsed": [
                    2024,
                    5,
                    2,
                    12,
                    4,
                    35,
                    3,
                    123,
                    0
                ],
                "title": "A Survey on Semantic Communication Networks: Architecture, Security, and\n  Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Semantic Communication Networks: Architecture, Security, and\n  Privacy"
                },
                "summary": "With the rapid advancement and deployment of intelligent agents and\nartificial general intelligence (AGI), a fundamental challenge for future\nnetworks is enabling efficient communications among agents. Unlike traditional\nhuman-centric, data-driven communication networks, the primary goal of\nagent-based communication is to facilitate coordination among agents.\nTherefore, task comprehension and collaboration become the key objectives of\ncommunications, rather than data synchronization. Semantic communication\n(SemCom) aims to align information and knowledge among agents to expedite task\ncomprehension. While significant research has been conducted on SemCom for\ntwo-agent systems, the development of semantic communication networks\n(SemComNet) for multi-agent systems remains largely unexplored. In this paper,\nwe provide a comprehensive and up-to-date survey of SemComNet, focusing on\ntheir fundamentals, security, and privacy aspects. We introduce a novel\nthree-layer architecture for multi-agent interaction, comprising the control\nlayer, semantic transmission layer, and cognitive sensing layer. We explore\nworking modes and enabling technologies, and present a taxonomy of security and\nprivacy threats, along with state-of-the-art defense mechanisms. Finally, we\noutline future research directions, paving the way toward intelligent, robust,\nand energy-efficient SemComNet. This survey represents the first comprehensive\nanalysis of SemComNet, offering detailed insights into its core principles as\nwell as associated security and privacy challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement and deployment of intelligent agents and\nartificial general intelligence (AGI), a fundamental challenge for future\nnetworks is enabling efficient communications among agents. Unlike traditional\nhuman-centric, data-driven communication networks, the primary goal of\nagent-based communication is to facilitate coordination among agents.\nTherefore, task comprehension and collaboration become the key objectives of\ncommunications, rather than data synchronization. Semantic communication\n(SemCom) aims to align information and knowledge among agents to expedite task\ncomprehension. While significant research has been conducted on SemCom for\ntwo-agent systems, the development of semantic communication networks\n(SemComNet) for multi-agent systems remains largely unexplored. In this paper,\nwe provide a comprehensive and up-to-date survey of SemComNet, focusing on\ntheir fundamentals, security, and privacy aspects. We introduce a novel\nthree-layer architecture for multi-agent interaction, comprising the control\nlayer, semantic transmission layer, and cognitive sensing layer. We explore\nworking modes and enabling technologies, and present a taxonomy of security and\nprivacy threats, along with state-of-the-art defense mechanisms. Finally, we\noutline future research directions, paving the way toward intelligent, robust,\nand energy-efficient SemComNet. This survey represents the first comprehensive\nanalysis of SemComNet, offering detailed insights into its core principles as\nwell as associated security and privacy challenges."
                },
                "authors": [
                    {
                        "name": "Shaolong Guo"
                    },
                    {
                        "name": "Yuntao Wang"
                    },
                    {
                        "name": "Ning Zhang"
                    },
                    {
                        "name": "Zhou Su"
                    },
                    {
                        "name": "Tom H. Luan"
                    },
                    {
                        "name": "Zhiyi Tian"
                    },
                    {
                        "name": "Xuemin"
                    },
                    {
                        "name": "Shen"
                    }
                ],
                "author_detail": {
                    "name": "Shen"
                },
                "arxiv_affiliation": "Sherman",
                "author": "Shen",
                "arxiv_comment": "To appear in IEEE Communications Surveys & Tutorials",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.01221v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01221v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13025v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13025v2",
                "updated": "2024-12-02T06:40:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    6,
                    40,
                    50,
                    0,
                    337,
                    0
                ],
                "published": "2024-10-16T20:33:06Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    20,
                    33,
                    6,
                    2,
                    290,
                    0
                ],
                "title": "LoRA Soups: Merging LoRAs for Practical Skill Composition Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRA Soups: Merging LoRAs for Practical Skill Composition Tasks"
                },
                "summary": "Low-Rank Adaptation (LoRA) is a popular technique for parameter-efficient\nfine-tuning of Large Language Models (LLMs). We study how different LoRA\nmodules can be merged to achieve skill composition -- testing the performance\nof the merged model on a target task that involves combining multiple skills,\neach skill coming from a single LoRA. This setup is favorable when it is\ndifficult to obtain training data for the target task and when it can be\ndecomposed into multiple skills. First, we identify practically occurring\nuse-cases that can be studied under the realm of skill composition, e.g.\nsolving hard math-word problems with code, creating a bot to answer questions\non proprietary manuals or about domain-specialized corpora. Our main\ncontribution is to show that concatenation of LoRAs (CAT), which optimally\nweights LoRAs that were individually trained on different skills, outperforms\nexisting model- and data- merging techniques; for instance on math-word\nproblems, CAT beats these methods by an average of 43% and 12% respectively.\nThus, this paper advocates model merging as an efficient way to solve\ncompositional tasks and underscores CAT as a simple, compute-friendly and\neffective procedure. To our knowledge, this is the first work demonstrating the\nsuperiority of model merging over data mixing for binary skill composition\ntasks. Code and data are available at https://github.com/aksh555/LoRA-Soups",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) is a popular technique for parameter-efficient\nfine-tuning of Large Language Models (LLMs). We study how different LoRA\nmodules can be merged to achieve skill composition -- testing the performance\nof the merged model on a target task that involves combining multiple skills,\neach skill coming from a single LoRA. This setup is favorable when it is\ndifficult to obtain training data for the target task and when it can be\ndecomposed into multiple skills. First, we identify practically occurring\nuse-cases that can be studied under the realm of skill composition, e.g.\nsolving hard math-word problems with code, creating a bot to answer questions\non proprietary manuals or about domain-specialized corpora. Our main\ncontribution is to show that concatenation of LoRAs (CAT), which optimally\nweights LoRAs that were individually trained on different skills, outperforms\nexisting model- and data- merging techniques; for instance on math-word\nproblems, CAT beats these methods by an average of 43% and 12% respectively.\nThus, this paper advocates model merging as an efficient way to solve\ncompositional tasks and underscores CAT as a simple, compute-friendly and\neffective procedure. To our knowledge, this is the first work demonstrating the\nsuperiority of model merging over data mixing for binary skill composition\ntasks. Code and data are available at https://github.com/aksh555/LoRA-Soups"
                },
                "authors": [
                    {
                        "name": "Akshara Prabhakar"
                    },
                    {
                        "name": "Yuanzhi Li"
                    },
                    {
                        "name": "Karthik Narasimhan"
                    },
                    {
                        "name": "Sham Kakade"
                    },
                    {
                        "name": "Eran Malach"
                    },
                    {
                        "name": "Samy Jelassi"
                    }
                ],
                "author_detail": {
                    "name": "Samy Jelassi"
                },
                "author": "Samy Jelassi",
                "arxiv_comment": "COLING 2025 Industry track; 9 pages plus references and appendices",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13025v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13025v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19943v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19943v2",
                "updated": "2024-12-02T06:26:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    6,
                    26,
                    38,
                    0,
                    337,
                    0
                ],
                "published": "2024-11-29T18:58:22Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    18,
                    58,
                    22,
                    4,
                    334,
                    0
                ],
                "title": "Critical Tokens Matter: Token-Level Contrastive Estimation Enhances\n  LLM's Reasoning Capability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critical Tokens Matter: Token-Level Contrastive Estimation Enhances\n  LLM's Reasoning Capability"
                },
                "summary": "Large Language Models (LLMs) have exhibited remarkable performance on\nreasoning tasks. They utilize autoregressive token generation to construct\nreasoning trajectories, enabling the development of a coherent chain of\nthought. In this work, we explore the impact of individual tokens on the final\noutcomes of reasoning tasks. We identify the existence of ``critical tokens''\nthat lead to incorrect reasoning trajectories in LLMs. Specifically, we find\nthat LLMs tend to produce positive outcomes when forced to decode other tokens\ninstead of critical tokens. Motivated by this observation, we propose a novel\napproach - cDPO - designed to automatically recognize and conduct token-level\nrewards for the critical tokens during the alignment process. Specifically, we\ndevelop a contrastive estimation approach to automatically identify critical\ntokens. It is achieved by comparing the generation likelihood of positive and\nnegative models. To achieve this, we separately fine-tune the positive and\nnegative models on various reasoning trajectories, consequently, they are\ncapable of identifying identify critical tokens within incorrect trajectories\nthat contribute to erroneous outcomes. Moreover, to further align the model\nwith the critical token information during the alignment process, we extend the\nconventional DPO algorithms to token-level DPO and utilize the differential\nlikelihood from the aforementioned positive and negative model as important\nweight for token-level DPO learning.Experimental results on GSM8K and MATH500\nbenchmarks with two-widely used models Llama-3 (8B and 70B) and deepseek-math\n(7B) demonstrate the effectiveness of the propsoed approach cDPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited remarkable performance on\nreasoning tasks. They utilize autoregressive token generation to construct\nreasoning trajectories, enabling the development of a coherent chain of\nthought. In this work, we explore the impact of individual tokens on the final\noutcomes of reasoning tasks. We identify the existence of ``critical tokens''\nthat lead to incorrect reasoning trajectories in LLMs. Specifically, we find\nthat LLMs tend to produce positive outcomes when forced to decode other tokens\ninstead of critical tokens. Motivated by this observation, we propose a novel\napproach - cDPO - designed to automatically recognize and conduct token-level\nrewards for the critical tokens during the alignment process. Specifically, we\ndevelop a contrastive estimation approach to automatically identify critical\ntokens. It is achieved by comparing the generation likelihood of positive and\nnegative models. To achieve this, we separately fine-tune the positive and\nnegative models on various reasoning trajectories, consequently, they are\ncapable of identifying identify critical tokens within incorrect trajectories\nthat contribute to erroneous outcomes. Moreover, to further align the model\nwith the critical token information during the alignment process, we extend the\nconventional DPO algorithms to token-level DPO and utilize the differential\nlikelihood from the aforementioned positive and negative model as important\nweight for token-level DPO learning.Experimental results on GSM8K and MATH500\nbenchmarks with two-widely used models Llama-3 (8B and 70B) and deepseek-math\n(7B) demonstrate the effectiveness of the propsoed approach cDPO."
                },
                "authors": [
                    {
                        "name": "Zicheng Lin"
                    },
                    {
                        "name": "Tian Liang"
                    },
                    {
                        "name": "Jiahao Xu"
                    },
                    {
                        "name": "Xing Wang"
                    },
                    {
                        "name": "Ruilin Luo"
                    },
                    {
                        "name": "Chufan Shi"
                    },
                    {
                        "name": "Siheng Li"
                    },
                    {
                        "name": "Yujiu Yang"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhaopeng Tu"
                },
                "author": "Zhaopeng Tu",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19943v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19943v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11285v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11285v2",
                "updated": "2024-12-02T05:22:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    5,
                    22,
                    1,
                    0,
                    337,
                    0
                ],
                "published": "2024-06-17T07:46:45Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    7,
                    46,
                    45,
                    0,
                    169,
                    0
                ],
                "title": "Self and Cross-Model Distillation for LLMs: Effective Methods for\n  Refusal Pattern Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self and Cross-Model Distillation for LLMs: Effective Methods for\n  Refusal Pattern Alignment"
                },
                "summary": "Large Language Models (LLMs) like OpenAI's GPT series, Anthropic's Claude,\nand Meta's LLaMa have shown remarkable capabilities in text generation.\nHowever, their susceptibility to toxic prompts presents significant security\nchallenges. This paper investigates alignment techniques, including Supervised\nFine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), to\nmitigate these risks. We conduct an empirical study on refusal patterns across\nnine LLMs, revealing that models with uniform refusal patterns, such as\nClaude3, exhibit higher security. Based on these findings, we propose\nself-distilling and cross-model distilling methods to enhance LLM security. Our\nresults show that these methods significantly improve refusal rates and reduce\nunsafe content, with cross-model distilling achieving refusal rates close to\nClaude3's 94.51%. These findings underscore the potential of distillation-based\nalignment in securing LLMs against toxic prompts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) like OpenAI's GPT series, Anthropic's Claude,\nand Meta's LLaMa have shown remarkable capabilities in text generation.\nHowever, their susceptibility to toxic prompts presents significant security\nchallenges. This paper investigates alignment techniques, including Supervised\nFine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), to\nmitigate these risks. We conduct an empirical study on refusal patterns across\nnine LLMs, revealing that models with uniform refusal patterns, such as\nClaude3, exhibit higher security. Based on these findings, we propose\nself-distilling and cross-model distilling methods to enhance LLM security. Our\nresults show that these methods significantly improve refusal rates and reduce\nunsafe content, with cross-model distilling achieving refusal rates close to\nClaude3's 94.51%. These findings underscore the potential of distillation-based\nalignment in securing LLMs against toxic prompts."
                },
                "authors": [
                    {
                        "name": "Jie Li"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Chongyang Liu"
                    },
                    {
                        "name": "Xiaoning Ren"
                    },
                    {
                        "name": "Ling Shi"
                    },
                    {
                        "name": "Weisong Sun"
                    },
                    {
                        "name": "Yinxing Xue"
                    }
                ],
                "author_detail": {
                    "name": "Yinxing Xue"
                },
                "author": "Yinxing Xue",
                "arxiv_comment": "The method used in the paper has obvious problems and ambiguities.\n  The security enhancement method we used cannot be considered distillation,\n  but it is described as distillation in the paper, and the experiment lacks\n  comparison and baseline, which has been criticized by many peers. In order to\n  avoid further dissemination, we have decided to withdraw the paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11285v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11285v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07656v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07656v2",
                "updated": "2024-12-02T04:36:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    4,
                    36,
                    45,
                    0,
                    337,
                    0
                ],
                "published": "2024-11-12T09:14:16Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    9,
                    14,
                    16,
                    1,
                    317,
                    0
                ],
                "title": "Mitigating Bias in Queer Representation within Large Language Models: A\n  Collaborative Agent Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Bias in Queer Representation within Large Language Models: A\n  Collaborative Agent Approach"
                },
                "summary": "Large Language Models (LLMs) often perpetuate biases in pronoun usage,\nleading to misrepresentation or exclusion of queer individuals. This paper\naddresses the specific problem of biased pronoun usage in LLM outputs,\nparticularly the inappropriate use of traditionally gendered pronouns (\"he,\"\n\"she\") when inclusive language is needed to accurately represent all\nidentities. We introduce a collaborative agent pipeline designed to mitigate\nthese biases by analyzing and optimizing pronoun usage for inclusivity. Our\nmulti-agent framework includes specialized agents for both bias detection and\ncorrection. Experimental evaluations using the Tango dataset-a benchmark\nfocused on gender pronoun usage-demonstrate that our approach significantly\nimproves inclusive pronoun classification, achieving a 32.6 percentage point\nincrease over GPT-4o in correctly disagreeing with inappropriate traditionally\ngendered pronouns $(\\chi^2 = 38.57, p < 0.0001)$. These results accentuate the\npotential of agent-driven frameworks in enhancing fairness and inclusivity in\nAI-generated content, demonstrating their efficacy in reducing biases and\npromoting socially responsible AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often perpetuate biases in pronoun usage,\nleading to misrepresentation or exclusion of queer individuals. This paper\naddresses the specific problem of biased pronoun usage in LLM outputs,\nparticularly the inappropriate use of traditionally gendered pronouns (\"he,\"\n\"she\") when inclusive language is needed to accurately represent all\nidentities. We introduce a collaborative agent pipeline designed to mitigate\nthese biases by analyzing and optimizing pronoun usage for inclusivity. Our\nmulti-agent framework includes specialized agents for both bias detection and\ncorrection. Experimental evaluations using the Tango dataset-a benchmark\nfocused on gender pronoun usage-demonstrate that our approach significantly\nimproves inclusive pronoun classification, achieving a 32.6 percentage point\nincrease over GPT-4o in correctly disagreeing with inappropriate traditionally\ngendered pronouns $(\\chi^2 = 38.57, p < 0.0001)$. These results accentuate the\npotential of agent-driven frameworks in enhancing fairness and inclusivity in\nAI-generated content, demonstrating their efficacy in reducing biases and\npromoting socially responsible AI."
                },
                "authors": [
                    {
                        "name": "Tianyi Huang"
                    },
                    {
                        "name": "Arya Somasundaram"
                    }
                ],
                "author_detail": {
                    "name": "Arya Somasundaram"
                },
                "author": "Arya Somasundaram",
                "arxiv_comment": "NeurIPS 2024 Queer in AI Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07656v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07656v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05600v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05600v3",
                "updated": "2024-12-02T03:51:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    3,
                    51,
                    34,
                    0,
                    337,
                    0
                ],
                "published": "2024-06-09T00:23:20Z",
                "published_parsed": [
                    2024,
                    6,
                    9,
                    0,
                    23,
                    20,
                    6,
                    161,
                    0
                ],
                "title": "61A Bot Report: AI Assistants in CS1 Save Students Homework Time and\n  Reduce Demands on Staff. (Now What?)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "61A Bot Report: AI Assistants in CS1 Save Students Homework Time and\n  Reduce Demands on Staff. (Now What?)"
                },
                "summary": "LLM-based chatbots enable students to get immediate, interactive help on\nhomework assignments, but even a thoughtfully-designed bot may not serve all\npedagogical goals. We report here on the development and deployment of a\nGPT-4-based interactive homework assistant (\"61A Bot\") for students in a large\nCS1 course; over 2000 students made over 100,000 requests of our Bot across two\nsemesters. Our assistant offers one-shot, contextual feedback within the\ncommand-line \"autograder\" students use to test their code. Our Bot wraps\nstudent code in a custom prompt that supports our pedagogical goals and avoids\nproviding solutions directly. Analyzing student feedback, questions, and\nautograder data, we find reductions in homework-related question rates in our\ncourse forum, as well as reductions in homework completion time when our Bot is\navailable. For students in the 50th-80th percentile, reductions can exceed 30\nminutes per assignment, up to 50% less time than students at the same\npercentile rank in prior semesters. Finally, we discuss these observations,\npotential impacts on student learning, and other potential costs and benefits\nof AI assistance in CS1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based chatbots enable students to get immediate, interactive help on\nhomework assignments, but even a thoughtfully-designed bot may not serve all\npedagogical goals. We report here on the development and deployment of a\nGPT-4-based interactive homework assistant (\"61A Bot\") for students in a large\nCS1 course; over 2000 students made over 100,000 requests of our Bot across two\nsemesters. Our assistant offers one-shot, contextual feedback within the\ncommand-line \"autograder\" students use to test their code. Our Bot wraps\nstudent code in a custom prompt that supports our pedagogical goals and avoids\nproviding solutions directly. Analyzing student feedback, questions, and\nautograder data, we find reductions in homework-related question rates in our\ncourse forum, as well as reductions in homework completion time when our Bot is\navailable. For students in the 50th-80th percentile, reductions can exceed 30\nminutes per assignment, up to 50% less time than students at the same\npercentile rank in prior semesters. Finally, we discuss these observations,\npotential impacts on student learning, and other potential costs and benefits\nof AI assistance in CS1."
                },
                "authors": [
                    {
                        "name": "J. D. Zamfirescu-Pereira"
                    },
                    {
                        "name": "Laryn Qi"
                    },
                    {
                        "name": "BjÃ¶rn Hartmann"
                    },
                    {
                        "name": "John DeNero"
                    },
                    {
                        "name": "Narges Norouzi"
                    }
                ],
                "author_detail": {
                    "name": "Narges Norouzi"
                },
                "author": "Narges Norouzi",
                "arxiv_doi": "10.1145/3641554.3701864",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3641554.3701864",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.05600v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05600v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "6 pages, 3 figures, 1 table, 1 page of references",
                "arxiv_journal_ref": "SIGCSE TS 2025, February 26-March 1, 2025, Pittsburgh, PA, USA",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15950v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15950v2",
                "updated": "2024-12-02T03:48:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    3,
                    48,
                    43,
                    0,
                    337,
                    0
                ],
                "published": "2024-08-28T17:08:56Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    8,
                    56,
                    2,
                    241,
                    0
                ],
                "title": "Atari-GPT: Benchmarking Multimodal Large Language Models as Low-Level\n  Policies in Atari Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Atari-GPT: Benchmarking Multimodal Large Language Models as Low-Level\n  Policies in Atari Games"
                },
                "summary": "Recent advancements in large language models (LLMs) have expanded their\ncapabilities beyond traditional text-based tasks to multimodal domains,\nintegrating visual, auditory, and textual data. While multimodal LLMs have been\nextensively explored for high-level planning in domains like robotics and\ngames, their potential as low-level controllers remains largely untapped. In\nthis paper, we introduce a novel benchmark aimed at testing the emergent\ncapabilities of multimodal LLMs as low-level policies in Atari games. Unlike\ntraditional reinforcement learning (RL) methods that require training for each\nnew environment and reward function specification, these LLMs utilize\npre-existing multimodal knowledge to directly engage with game environments.\nOur study assesses the performances of multiple multimodal LLMs against\ntraditional RL agents, human players, and random agents, focusing on their\nability to understand and interact with complex visual scenes and formulate\nstrategic responses. Our results show that these multimodal LLMs are not yet\ncapable of being zero-shot low-level policies. Furthermore, we see that this\nis, in part, due to their visual and spatial reasoning. Additional results and\nvideos are available on our project webpage:\nhttps://dev1nw.github.io/atari-gpt/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have expanded their\ncapabilities beyond traditional text-based tasks to multimodal domains,\nintegrating visual, auditory, and textual data. While multimodal LLMs have been\nextensively explored for high-level planning in domains like robotics and\ngames, their potential as low-level controllers remains largely untapped. In\nthis paper, we introduce a novel benchmark aimed at testing the emergent\ncapabilities of multimodal LLMs as low-level policies in Atari games. Unlike\ntraditional reinforcement learning (RL) methods that require training for each\nnew environment and reward function specification, these LLMs utilize\npre-existing multimodal knowledge to directly engage with game environments.\nOur study assesses the performances of multiple multimodal LLMs against\ntraditional RL agents, human players, and random agents, focusing on their\nability to understand and interact with complex visual scenes and formulate\nstrategic responses. Our results show that these multimodal LLMs are not yet\ncapable of being zero-shot low-level policies. Furthermore, we see that this\nis, in part, due to their visual and spatial reasoning. Additional results and\nvideos are available on our project webpage:\nhttps://dev1nw.github.io/atari-gpt/."
                },
                "authors": [
                    {
                        "name": "Nicholas R. Waytowich"
                    },
                    {
                        "name": "Devin White"
                    },
                    {
                        "name": "MD Sunbeam"
                    },
                    {
                        "name": "Vinicius G. Goecks"
                    }
                ],
                "author_detail": {
                    "name": "Vinicius G. Goecks"
                },
                "author": "Vinicius G. Goecks",
                "arxiv_comment": "Currently under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15950v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15950v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01245v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01245v3",
                "updated": "2024-12-02T03:27:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    3,
                    27,
                    10,
                    0,
                    337,
                    0
                ],
                "published": "2024-04-01T17:03:41Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    17,
                    3,
                    41,
                    0,
                    92,
                    0
                ],
                "title": "A Statistical Framework of Watermarks for Large Language Models: Pivot,\n  Detection Efficiency and Optimal Rules",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Statistical Framework of Watermarks for Large Language Models: Pivot,\n  Detection Efficiency and Optimal Rules"
                },
                "summary": "Since ChatGPT was introduced in November 2022, embedding (nearly)\nunnoticeable statistical signals into text generated by large language models\n(LLMs), also known as watermarking, has been used as a principled approach to\nprovable detection of LLM-generated text from its human-written counterpart. In\nthis paper, we introduce a general and flexible framework for reasoning about\nthe statistical efficiency of watermarks and designing powerful detection\nrules. Inspired by the hypothesis testing formulation of watermark detection,\nour framework starts by selecting a pivotal statistic of the text and a secret\nkey -- provided by the LLM to the verifier -- to enable controlling the false\npositive rate (the error of mistakenly detecting human-written text as\nLLM-generated). Next, this framework allows one to evaluate the power of\nwatermark detection rules by obtaining a closed-form expression of the\nasymptotic false negative rate (the error of incorrectly classifying\nLLM-generated text as human-written). Our framework further reduces the problem\nof determining the optimal detection rule to solving a minimax optimization\nprogram. We apply this framework to two representative watermarks -- one of\nwhich has been internally implemented at OpenAI -- and obtain several findings\nthat can be instrumental in guiding the practice of implementing watermarks. In\nparticular, we derive optimal detection rules for these watermarks under our\nframework. These theoretically derived detection rules are demonstrated to be\ncompetitive and sometimes enjoy a higher power than existing detection\napproaches through numerical experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since ChatGPT was introduced in November 2022, embedding (nearly)\nunnoticeable statistical signals into text generated by large language models\n(LLMs), also known as watermarking, has been used as a principled approach to\nprovable detection of LLM-generated text from its human-written counterpart. In\nthis paper, we introduce a general and flexible framework for reasoning about\nthe statistical efficiency of watermarks and designing powerful detection\nrules. Inspired by the hypothesis testing formulation of watermark detection,\nour framework starts by selecting a pivotal statistic of the text and a secret\nkey -- provided by the LLM to the verifier -- to enable controlling the false\npositive rate (the error of mistakenly detecting human-written text as\nLLM-generated). Next, this framework allows one to evaluate the power of\nwatermark detection rules by obtaining a closed-form expression of the\nasymptotic false negative rate (the error of incorrectly classifying\nLLM-generated text as human-written). Our framework further reduces the problem\nof determining the optimal detection rule to solving a minimax optimization\nprogram. We apply this framework to two representative watermarks -- one of\nwhich has been internally implemented at OpenAI -- and obtain several findings\nthat can be instrumental in guiding the practice of implementing watermarks. In\nparticular, we derive optimal detection rules for these watermarks under our\nframework. These theoretically derived detection rules are demonstrated to be\ncompetitive and sometimes enjoy a higher power than existing detection\napproaches through numerical experiments."
                },
                "authors": [
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Feng Ruan"
                    },
                    {
                        "name": "Huiyuan Wang"
                    },
                    {
                        "name": "Qi Long"
                    },
                    {
                        "name": "Weijie J. Su"
                    }
                ],
                "author_detail": {
                    "name": "Weijie J. Su"
                },
                "author": "Weijie J. Su",
                "arxiv_comment": "To appear in the Annals of Statistics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01245v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01245v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17912v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17912v2",
                "updated": "2024-12-02T02:42:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    2,
                    42,
                    5,
                    0,
                    337,
                    0
                ],
                "published": "2024-11-26T22:06:39Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    22,
                    6,
                    39,
                    1,
                    331,
                    0
                ],
                "title": "Can LLMs plan paths in the real world?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs plan paths in the real world?"
                },
                "summary": "As large language models (LLMs) increasingly integrate into vehicle\nnavigation systems, understanding their path-planning capability is crucial. We\ntested three LLMs through six real-world path-planning scenarios in various\nsettings and with various difficulties. Our experiments showed that all LLMs\nmade numerous errors in all scenarios, revealing that they are unreliable path\nplanners. We suggest that future work focus on implementing mechanisms for\nreality checks, enhancing model transparency, and developing smaller models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) increasingly integrate into vehicle\nnavigation systems, understanding their path-planning capability is crucial. We\ntested three LLMs through six real-world path-planning scenarios in various\nsettings and with various difficulties. Our experiments showed that all LLMs\nmade numerous errors in all scenarios, revealing that they are unreliable path\nplanners. We suggest that future work focus on implementing mechanisms for\nreality checks, enhancing model transparency, and developing smaller models."
                },
                "authors": [
                    {
                        "name": "Wanyi Chen"
                    },
                    {
                        "name": "Meng-Wen Su"
                    },
                    {
                        "name": "Nafisa Mehjabin"
                    },
                    {
                        "name": "Mary L. Cummings"
                    }
                ],
                "author_detail": {
                    "name": "Mary L. Cummings"
                },
                "author": "Mary L. Cummings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17912v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17912v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12735v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12735v4",
                "updated": "2024-12-02T02:34:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    2,
                    34,
                    47,
                    0,
                    337,
                    0
                ],
                "published": "2024-07-17T16:55:42Z",
                "published_parsed": [
                    2024,
                    7,
                    17,
                    16,
                    55,
                    42,
                    2,
                    199,
                    0
                ],
                "title": "EchoSight: Advancing Visual-Language Models with Wiki Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EchoSight: Advancing Visual-Language Models with Wiki Knowledge"
                },
                "summary": "Knowledge-based Visual Question Answering (KVQA) tasks require answering\nquestions about images using extensive background knowledge. Despite\nsignificant advancements, generative models often struggle with these tasks due\nto the limited integration of external knowledge. In this paper, we introduce\nEchoSight, a novel multimodal Retrieval-Augmented Generation (RAG) framework\nthat enables large language models (LLMs) to answer visual questions requiring\nfine-grained encyclopedic knowledge. To strive for high-performing retrieval,\nEchoSight first searches wiki articles by using visual-only information,\nsubsequently, these candidate articles are further reranked according to their\nrelevance to the combined text-image query. This approach significantly\nimproves the integration of multimodal knowledge, leading to enhanced retrieval\noutcomes and more accurate VQA responses. Our experimental results on the\nEncyclopedic VQA and InfoSeek datasets demonstrate that EchoSight establishes\nnew state-of-the-art results in knowledge-based VQA, achieving an accuracy of\n41.8% on Encyclopedic VQA and 31.3% on InfoSeek.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-based Visual Question Answering (KVQA) tasks require answering\nquestions about images using extensive background knowledge. Despite\nsignificant advancements, generative models often struggle with these tasks due\nto the limited integration of external knowledge. In this paper, we introduce\nEchoSight, a novel multimodal Retrieval-Augmented Generation (RAG) framework\nthat enables large language models (LLMs) to answer visual questions requiring\nfine-grained encyclopedic knowledge. To strive for high-performing retrieval,\nEchoSight first searches wiki articles by using visual-only information,\nsubsequently, these candidate articles are further reranked according to their\nrelevance to the combined text-image query. This approach significantly\nimproves the integration of multimodal knowledge, leading to enhanced retrieval\noutcomes and more accurate VQA responses. Our experimental results on the\nEncyclopedic VQA and InfoSeek datasets demonstrate that EchoSight establishes\nnew state-of-the-art results in knowledge-based VQA, achieving an accuracy of\n41.8% on Encyclopedic VQA and 31.3% on InfoSeek."
                },
                "authors": [
                    {
                        "name": "Yibin Yan"
                    },
                    {
                        "name": "Weidi Xie"
                    }
                ],
                "author_detail": {
                    "name": "Weidi Xie"
                },
                "author": "Weidi Xie",
                "arxiv_doi": "10.18653/v1/2024.findings-emnlp.83",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2024.findings-emnlp.83",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.12735v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12735v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by EMNLP 2024 findings; Project Page:\n  https://go2heart.github.io/echosight",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11266v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11266v3",
                "updated": "2024-12-02T02:27:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    2,
                    27,
                    17,
                    0,
                    337,
                    0
                ],
                "published": "2024-11-18T03:45:34Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    3,
                    45,
                    34,
                    0,
                    323,
                    0
                ],
                "title": "VersaTune: An Efficient Data Composition Framework for Training\n  Multi-Capability LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VersaTune: An Efficient Data Composition Framework for Training\n  Multi-Capability LLMs"
                },
                "summary": "Large-scale pretrained models, particularly Large Language Models (LLMs),\nhave exhibited remarkable capabilities in handling multiple tasks across\ndomains due to their emergent properties. These capabilities are further\naugmented during the Supervised Fine-Tuning (SFT) phase. Despite their\npotential, existing work mainly focuses on domain-specific enhancements during\nfine-tuning, the challenge of which lies in catastrophic forgetting of\nknowledge across other domains. In this study, we introduce VersaTune, a novel\ndata composition framework designed for enhancing LLMs' overall multi-ability\nperformances during training. We categorize knowledge into distinct domains\nincluding law, medicine, finance, science, code, etc. We begin with detecting\nthe distribution of domain-specific knowledge within the base model, followed\nby the training data composition that aligns with the model's existing\nknowledge distribution. During the training process, domain weights are\ndynamically adjusted based on their learnable potential and forgetting degree.\nExperimental results demonstrate that VersaTune achieves significant\nimprovements in multi-domain performance, with an 35.21% enhancement in\ncomprehensive multi-domain tasks. Additionally, in scenarios where specific\ndomain optimization is required, VersaTune reduces the degradation of\nperformance in other domains by 38.77%, without compromising the target\ndomain's training efficacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale pretrained models, particularly Large Language Models (LLMs),\nhave exhibited remarkable capabilities in handling multiple tasks across\ndomains due to their emergent properties. These capabilities are further\naugmented during the Supervised Fine-Tuning (SFT) phase. Despite their\npotential, existing work mainly focuses on domain-specific enhancements during\nfine-tuning, the challenge of which lies in catastrophic forgetting of\nknowledge across other domains. In this study, we introduce VersaTune, a novel\ndata composition framework designed for enhancing LLMs' overall multi-ability\nperformances during training. We categorize knowledge into distinct domains\nincluding law, medicine, finance, science, code, etc. We begin with detecting\nthe distribution of domain-specific knowledge within the base model, followed\nby the training data composition that aligns with the model's existing\nknowledge distribution. During the training process, domain weights are\ndynamically adjusted based on their learnable potential and forgetting degree.\nExperimental results demonstrate that VersaTune achieves significant\nimprovements in multi-domain performance, with an 35.21% enhancement in\ncomprehensive multi-domain tasks. Additionally, in scenarios where specific\ndomain optimization is required, VersaTune reduces the degradation of\nperformance in other domains by 38.77%, without compromising the target\ndomain's training efficacy."
                },
                "authors": [
                    {
                        "name": "Keer Lu"
                    },
                    {
                        "name": "Keshi Zhao"
                    },
                    {
                        "name": "Zheng Liang"
                    },
                    {
                        "name": "Da Pan"
                    },
                    {
                        "name": "Shusen Zhang"
                    },
                    {
                        "name": "Xin Wu"
                    },
                    {
                        "name": "Weipeng Chen"
                    },
                    {
                        "name": "Zenan Zhou"
                    },
                    {
                        "name": "Guosheng Dong"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11266v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11266v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.02326v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.02326v2",
                "updated": "2024-12-02T01:59:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    1,
                    59,
                    30,
                    0,
                    337,
                    0
                ],
                "published": "2024-04-23T18:55:49Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    18,
                    55,
                    49,
                    1,
                    114,
                    0
                ],
                "title": "Evaluating LLMs for Hardware Design and Test",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLMs for Hardware Design and Test"
                },
                "summary": "Large Language Models (LLMs) have demonstrated capabilities for producing\ncode in Hardware Description Languages (HDLs). However, most of the focus\nremains on their abilities to write functional code, not test code. The\nhardware design process consists of both design and test, and so eschewing\nvalidation and verification leaves considerable potential benefit unexplored,\ngiven that a design and test framework may allow for progress towards full\nautomation of the digital design pipeline. In this work, we perform one of the\nfirst studies exploring how a LLM can both design and test hardware modules\nfrom provided specifications. Using a suite of 8 representative benchmarks, we\nexamined the capabilities and limitations of the state-of-the-art\nconversational LLMs when producing Verilog for functional and verification\npurposes. We taped out the benchmarks on a Skywater 130nm shuttle and received\nthe functional chip.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated capabilities for producing\ncode in Hardware Description Languages (HDLs). However, most of the focus\nremains on their abilities to write functional code, not test code. The\nhardware design process consists of both design and test, and so eschewing\nvalidation and verification leaves considerable potential benefit unexplored,\ngiven that a design and test framework may allow for progress towards full\nautomation of the digital design pipeline. In this work, we perform one of the\nfirst studies exploring how a LLM can both design and test hardware modules\nfrom provided specifications. Using a suite of 8 representative benchmarks, we\nexamined the capabilities and limitations of the state-of-the-art\nconversational LLMs when producing Verilog for functional and verification\npurposes. We taped out the benchmarks on a Skywater 130nm shuttle and received\nthe functional chip."
                },
                "authors": [
                    {
                        "name": "Jason Blocklove"
                    },
                    {
                        "name": "Siddharth Garg"
                    },
                    {
                        "name": "Ramesh Karri"
                    },
                    {
                        "name": "Hammond Pearce"
                    }
                ],
                "author_detail": {
                    "name": "Hammond Pearce"
                },
                "author": "Hammond Pearce",
                "arxiv_doi": "10.1109/LAD62341.2024.10691811",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/LAD62341.2024.10691811",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.02326v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.02326v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17367v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17367v2",
                "updated": "2024-12-01T22:48:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    22,
                    48,
                    24,
                    6,
                    336,
                    0
                ],
                "published": "2024-11-26T12:20:18Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    12,
                    20,
                    18,
                    1,
                    331,
                    0
                ],
                "title": "Efficient Deployment of Transformer Models in Analog In-Memory Computing\n  Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Deployment of Transformer Models in Analog In-Memory Computing\n  Hardware"
                },
                "summary": "Analog in-memory computing (AIMC) has emerged as a promising solution to\novercome the von Neumann bottleneck, accelerating neural network computations\nand improving computational efficiency. While AIMC has demonstrated success\nwith architectures such as CNNs, MLPs, and RNNs, deploying transformer-based\nmodels using AIMC presents unique challenges. Transformers are expected to\nhandle diverse downstream tasks and adapt to new user data or instructions\nafter deployment, which requires more flexible approaches to suit AIMC\nconstraints.\n  In this paper, we propose a novel method for deploying pre-trained\ntransformer models onto AIMC hardware. Unlike traditional approaches requiring\nhardware-aware training, our technique allows direct deployment without the\nneed for retraining the original model. Instead, we utilize lightweight,\nlow-rank adapters -- compact modules stored in digital cores -- to adapt the\nmodel to hardware constraints. We validate our approach on MobileBERT,\ndemonstrating accuracy on par with, or even exceeding, a traditional\nhardware-aware training approach. Our method is particularly appealing in\nmulti-task scenarios, as it enables a single analog model to be reused across\nmultiple tasks. Moreover, it supports on-chip adaptation to new hardware\nconstraints and tasks without updating analog weights, providing a flexible and\nversatile solution for real-world AI applications. Code is available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analog in-memory computing (AIMC) has emerged as a promising solution to\novercome the von Neumann bottleneck, accelerating neural network computations\nand improving computational efficiency. While AIMC has demonstrated success\nwith architectures such as CNNs, MLPs, and RNNs, deploying transformer-based\nmodels using AIMC presents unique challenges. Transformers are expected to\nhandle diverse downstream tasks and adapt to new user data or instructions\nafter deployment, which requires more flexible approaches to suit AIMC\nconstraints.\n  In this paper, we propose a novel method for deploying pre-trained\ntransformer models onto AIMC hardware. Unlike traditional approaches requiring\nhardware-aware training, our technique allows direct deployment without the\nneed for retraining the original model. Instead, we utilize lightweight,\nlow-rank adapters -- compact modules stored in digital cores -- to adapt the\nmodel to hardware constraints. We validate our approach on MobileBERT,\ndemonstrating accuracy on par with, or even exceeding, a traditional\nhardware-aware training approach. Our method is particularly appealing in\nmulti-task scenarios, as it enables a single analog model to be reused across\nmultiple tasks. Moreover, it supports on-chip adaptation to new hardware\nconstraints and tasks without updating analog weights, providing a flexible and\nversatile solution for real-world AI applications. Code is available."
                },
                "authors": [
                    {
                        "name": "Chen Li"
                    },
                    {
                        "name": "Corey Lammie"
                    },
                    {
                        "name": "Manuel Le Gallo"
                    },
                    {
                        "name": "Bipin Rajendran"
                    }
                ],
                "author_detail": {
                    "name": "Bipin Rajendran"
                },
                "author": "Bipin Rajendran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17367v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17367v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13166v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13166v2",
                "updated": "2024-12-01T22:34:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    22,
                    34,
                    5,
                    6,
                    336,
                    0
                ],
                "published": "2024-06-19T02:45:32Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    2,
                    45,
                    32,
                    2,
                    171,
                    0
                ],
                "title": "Enhancing supply chain security with automated machine learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing supply chain security with automated machine learning"
                },
                "summary": "The increasing scale and complexity of global supply chains have led to new\nchallenges spanning various fields, such as supply chain disruptions due to\nlong waiting lines at the ports, material shortages, and inflation. Coupled\nwith the size of supply chains and the availability of vast amounts of data,\nefforts towards tackling such challenges have led to an increasing interest in\napplying machine learning methods in many aspects of supply chains. Unlike\nother solutions, ML techniques, including Random Forest, XGBoost, LightGBM, and\nNeural Networks, make predictions and approximate optimal solutions faster.\nThis paper presents an automated ML framework to enhance supply chain security\nby detecting fraudulent activities, predicting maintenance needs, and\nforecasting material backorders. Using datasets of varying sizes, results show\nthat fraud detection achieves an 88% accuracy rate using sampling methods,\nmachine failure prediction reaches 93.4% accuracy, and material backorder\nprediction achieves 89.3% accuracy. Hyperparameter tuning significantly\nimproved the performance of these models, with certain supervised techniques\nlike XGBoost and LightGBM reaching up to 100% precision. This research\ncontributes to supply chain security by streamlining data preprocessing,\nfeature selection, model optimization, and inference deployment, addressing\ncritical challenges and boosting operational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing scale and complexity of global supply chains have led to new\nchallenges spanning various fields, such as supply chain disruptions due to\nlong waiting lines at the ports, material shortages, and inflation. Coupled\nwith the size of supply chains and the availability of vast amounts of data,\nefforts towards tackling such challenges have led to an increasing interest in\napplying machine learning methods in many aspects of supply chains. Unlike\nother solutions, ML techniques, including Random Forest, XGBoost, LightGBM, and\nNeural Networks, make predictions and approximate optimal solutions faster.\nThis paper presents an automated ML framework to enhance supply chain security\nby detecting fraudulent activities, predicting maintenance needs, and\nforecasting material backorders. Using datasets of varying sizes, results show\nthat fraud detection achieves an 88% accuracy rate using sampling methods,\nmachine failure prediction reaches 93.4% accuracy, and material backorder\nprediction achieves 89.3% accuracy. Hyperparameter tuning significantly\nimproved the performance of these models, with certain supervised techniques\nlike XGBoost and LightGBM reaching up to 100% precision. This research\ncontributes to supply chain security by streamlining data preprocessing,\nfeature selection, model optimization, and inference deployment, addressing\ncritical challenges and boosting operational efficiency."
                },
                "authors": [
                    {
                        "name": "Haibo Wang"
                    },
                    {
                        "name": "Lutfu S. Sua"
                    },
                    {
                        "name": "Bahram Alidaee"
                    }
                ],
                "author_detail": {
                    "name": "Bahram Alidaee"
                },
                "author": "Bahram Alidaee",
                "arxiv_comment": "26 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13166v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13166v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.10792v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.10792v8",
                "updated": "2024-12-01T22:01:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    22,
                    1,
                    51,
                    6,
                    336,
                    0
                ],
                "published": "2023-08-21T15:35:16Z",
                "published_parsed": [
                    2023,
                    8,
                    21,
                    15,
                    35,
                    16,
                    0,
                    233,
                    0
                ],
                "title": "Instruction Tuning for Large Language Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction Tuning for Large Language Models: A Survey"
                },
                "summary": "This paper surveys research works in the quickly advancing field of\ninstruction tuning (IT), which can also be referred to as supervised\nfine-tuning (SFT)\\footnote{In this paper, unless specified otherwise,\nsupervised fine-tuning (SFT) and instruction tuning (IT) are used\ninterchangeably.}, a crucial technique to enhance the capabilities and\ncontrollability of large language models (LLMs). Instruction tuning refers to\nthe process of further training LLMs on a dataset consisting of\n\\textsc{(instruction, output)} pairs in a supervised fashion, which bridges the\ngap between the next-word prediction objective of LLMs and the users' objective\nof having LLMs adhere to human instructions. In this work, we make a systematic\nreview of the literature, including the general methodology of SFT, the\nconstruction of SFT datasets, the training of SFT models, and applications to\ndifferent modalities, domains and application, along with analysis on aspects\nthat influence the outcome of SFT (e.g., generation of instruction outputs,\nsize of the instruction dataset, etc). We also review the potential pitfalls of\nSFT along with criticism against it, along with efforts pointing out current\ndeficiencies of existing strategies and suggest some avenues for fruitful\nresearch. Project Page: github.com/xiaoya-li/Instruction-Tuning-Survey",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper surveys research works in the quickly advancing field of\ninstruction tuning (IT), which can also be referred to as supervised\nfine-tuning (SFT)\\footnote{In this paper, unless specified otherwise,\nsupervised fine-tuning (SFT) and instruction tuning (IT) are used\ninterchangeably.}, a crucial technique to enhance the capabilities and\ncontrollability of large language models (LLMs). Instruction tuning refers to\nthe process of further training LLMs on a dataset consisting of\n\\textsc{(instruction, output)} pairs in a supervised fashion, which bridges the\ngap between the next-word prediction objective of LLMs and the users' objective\nof having LLMs adhere to human instructions. In this work, we make a systematic\nreview of the literature, including the general methodology of SFT, the\nconstruction of SFT datasets, the training of SFT models, and applications to\ndifferent modalities, domains and application, along with analysis on aspects\nthat influence the outcome of SFT (e.g., generation of instruction outputs,\nsize of the instruction dataset, etc). We also review the potential pitfalls of\nSFT along with criticism against it, along with efforts pointing out current\ndeficiencies of existing strategies and suggest some avenues for fruitful\nresearch. Project Page: github.com/xiaoya-li/Instruction-Tuning-Survey"
                },
                "authors": [
                    {
                        "name": "Shengyu Zhang"
                    },
                    {
                        "name": "Linfeng Dong"
                    },
                    {
                        "name": "Xiaoya Li"
                    },
                    {
                        "name": "Sen Zhang"
                    },
                    {
                        "name": "Xiaofei Sun"
                    },
                    {
                        "name": "Shuhe Wang"
                    },
                    {
                        "name": "Jiwei Li"
                    },
                    {
                        "name": "Runyi Hu"
                    },
                    {
                        "name": "Tianwei Zhang"
                    },
                    {
                        "name": "Fei Wu"
                    },
                    {
                        "name": "Guoyin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guoyin Wang"
                },
                "author": "Guoyin Wang",
                "arxiv_comment": "V5; Last update: Dec. 1, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.10792v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.10792v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.09905v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.09905v2",
                "updated": "2024-12-01T21:42:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    21,
                    42,
                    37,
                    6,
                    336,
                    0
                ],
                "published": "2024-03-14T22:33:22Z",
                "published_parsed": [
                    2024,
                    3,
                    14,
                    22,
                    33,
                    22,
                    3,
                    74,
                    0
                ],
                "title": "Right Place, Right Time! Generalizing ObjectNav to Dynamic Environments\n  with Portable Targets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Right Place, Right Time! Generalizing ObjectNav to Dynamic Environments\n  with Portable Targets"
                },
                "summary": "ObjectNav is a popular task in Embodied AI, where an agent navigates to a\ntarget object in an unseen environment. Prior literature makes the assumption\nof a static environment with stationary objects, which lacks realism. To\naddress this, we present a novel formulation to generalize ObjectNav to dynamic\nenvironments with non-stationary objects, and refer to it as Portable ObjectNav\nor P-ObjectNav. In our formulation, we first address several challenging issues\nwith dynamizing existing topological scene graphs by developing a novel method\nthat introduces multiple transition behaviors to portable objects in the scene.\nWe use this technique to dynamize Matterport3D, a popular simulator for\nevaluating embodied tasks. We then present a benchmark for P-ObjectNav using a\ncombination of heuristic, reinforcement learning, and Large Language Model\n(LLM)-based navigation approaches on the dynamized environment, while\nintroducing novel evaluation metrics tailored for our task. Our work\nfundamentally challenges the \"static-environment\" notion of prior ObjectNav\nwork; the code and dataset for P-ObjectNav will be made publicly available to\nfoster research on embodied navigation in dynamic scenes. We provide an\nanonymized repository for our code and dataset:\nhttps://anonymous.4open.science/r/PObjectNav-1C6D.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ObjectNav is a popular task in Embodied AI, where an agent navigates to a\ntarget object in an unseen environment. Prior literature makes the assumption\nof a static environment with stationary objects, which lacks realism. To\naddress this, we present a novel formulation to generalize ObjectNav to dynamic\nenvironments with non-stationary objects, and refer to it as Portable ObjectNav\nor P-ObjectNav. In our formulation, we first address several challenging issues\nwith dynamizing existing topological scene graphs by developing a novel method\nthat introduces multiple transition behaviors to portable objects in the scene.\nWe use this technique to dynamize Matterport3D, a popular simulator for\nevaluating embodied tasks. We then present a benchmark for P-ObjectNav using a\ncombination of heuristic, reinforcement learning, and Large Language Model\n(LLM)-based navigation approaches on the dynamized environment, while\nintroducing novel evaluation metrics tailored for our task. Our work\nfundamentally challenges the \"static-environment\" notion of prior ObjectNav\nwork; the code and dataset for P-ObjectNav will be made publicly available to\nfoster research on embodied navigation in dynamic scenes. We provide an\nanonymized repository for our code and dataset:\nhttps://anonymous.4open.science/r/PObjectNav-1C6D."
                },
                "authors": [
                    {
                        "name": "Vishnu Sashank Dorbala"
                    },
                    {
                        "name": "Bhrij Patel"
                    },
                    {
                        "name": "Amrit Singh Bedi"
                    },
                    {
                        "name": "Dinesh Manocha"
                    }
                ],
                "author_detail": {
                    "name": "Dinesh Manocha"
                },
                "author": "Dinesh Manocha",
                "arxiv_comment": "19",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.09905v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.09905v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17855v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17855v2",
                "updated": "2024-12-01T19:02:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    19,
                    2,
                    28,
                    6,
                    336,
                    0
                ],
                "published": "2024-11-26T20:11:46Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    20,
                    11,
                    46,
                    1,
                    331,
                    0
                ],
                "title": "\"Give me the code\" -- Log Analysis of First-Year CS Students'\n  Interactions With GPT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Give me the code\" -- Log Analysis of First-Year CS Students'\n  Interactions With GPT"
                },
                "summary": "The impact of Large Language Models (LLMs) like GPT-3, GPT-4, and Bard in\ncomputer science (CS) education is expected to be profound. Students now have\nthe power to generate code solutions for a wide array of programming\nassignments. For first-year students, this may be particularly problematic\nsince the foundational skills are still in development and an over-reliance on\ngenerative AI tools can hinder their ability to grasp essential programming\nconcepts. This paper analyzes the prompts used by 69 freshmen undergraduate\nstudents to solve a certain programming problem within a project assignment,\nwithout giving them prior prompt training. We also present the rules of the\nexercise that motivated the prompts, designed to foster critical thinking\nskills during the interaction. Despite using unsophisticated prompting\ntechniques, our findings suggest that the majority of students successfully\nleveraged GPT, incorporating the suggested solutions into their projects.\nAdditionally, half of the students demonstrated the ability to exercise\njudgment in selecting from multiple GPT-generated solutions, showcasing the\ndevelopment of their critical thinking skills in evaluating AI-generated code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The impact of Large Language Models (LLMs) like GPT-3, GPT-4, and Bard in\ncomputer science (CS) education is expected to be profound. Students now have\nthe power to generate code solutions for a wide array of programming\nassignments. For first-year students, this may be particularly problematic\nsince the foundational skills are still in development and an over-reliance on\ngenerative AI tools can hinder their ability to grasp essential programming\nconcepts. This paper analyzes the prompts used by 69 freshmen undergraduate\nstudents to solve a certain programming problem within a project assignment,\nwithout giving them prior prompt training. We also present the rules of the\nexercise that motivated the prompts, designed to foster critical thinking\nskills during the interaction. Despite using unsophisticated prompting\ntechniques, our findings suggest that the majority of students successfully\nleveraged GPT, incorporating the suggested solutions into their projects.\nAdditionally, half of the students demonstrated the ability to exercise\njudgment in selecting from multiple GPT-generated solutions, showcasing the\ndevelopment of their critical thinking skills in evaluating AI-generated code."
                },
                "authors": [
                    {
                        "name": "Pedro Alves"
                    },
                    {
                        "name": "Bruno Pereira Cipriano"
                    }
                ],
                "author_detail": {
                    "name": "Bruno Pereira Cipriano"
                },
                "author": "Bruno Pereira Cipriano",
                "arxiv_comment": "This is the author's version of the work. It is posted here for your\n  personal use. Not for redistribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17855v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17855v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12644v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12644v3",
                "updated": "2024-12-01T17:45:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    17,
                    45,
                    28,
                    6,
                    336,
                    0
                ],
                "published": "2024-06-18T14:12:27Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    14,
                    12,
                    27,
                    1,
                    170,
                    0
                ],
                "title": "Hierarchical Prompting Taxonomy: A Universal Evaluation Framework for\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Prompting Taxonomy: A Universal Evaluation Framework for\n  Large Language Models"
                },
                "summary": "Assessing the effectiveness of large language models (LLMs) in performing\ndifferent tasks is crucial for understanding their strengths and weaknesses.\nThis paper presents the Hierarchical Prompting Taxonomy (HPT), grounded on\nhuman cognitive principles and designed to assess LLMs by examining the\ncognitive demands of various tasks. The HPT uses the Hierarchical Prompting\nFramework (HPF), a prompt selection framework that organizes five distinct\nprompting strategies by their cognitive load on LLMs. This study introduces the\nHierarchical Prompting Index (HPI) to measure task complexity, which\ndemonstrates LLMs' abilities across different datasets and serves as a\nuniversal metric for task complexity. The HPT offers a reliable method for\nevaluating LLMs' problem-solving skills in diverse scenarios, leading to\nclearer conclusions. Extensive experiments with multiple datasets and LLMs show\nthat the HPF enhances LLM performance by 2\\% to 63\\% compared to standard\nbenchmark datasets, confirming the effectiveness of the HPT. To support future\nresearch in this domain, the implementations of HPT and HPF are publicly\navailable",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the effectiveness of large language models (LLMs) in performing\ndifferent tasks is crucial for understanding their strengths and weaknesses.\nThis paper presents the Hierarchical Prompting Taxonomy (HPT), grounded on\nhuman cognitive principles and designed to assess LLMs by examining the\ncognitive demands of various tasks. The HPT uses the Hierarchical Prompting\nFramework (HPF), a prompt selection framework that organizes five distinct\nprompting strategies by their cognitive load on LLMs. This study introduces the\nHierarchical Prompting Index (HPI) to measure task complexity, which\ndemonstrates LLMs' abilities across different datasets and serves as a\nuniversal metric for task complexity. The HPT offers a reliable method for\nevaluating LLMs' problem-solving skills in diverse scenarios, leading to\nclearer conclusions. Extensive experiments with multiple datasets and LLMs show\nthat the HPF enhances LLM performance by 2\\% to 63\\% compared to standard\nbenchmark datasets, confirming the effectiveness of the HPT. To support future\nresearch in this domain, the implementations of HPT and HPF are publicly\navailable"
                },
                "authors": [
                    {
                        "name": "Devichand Budagam"
                    },
                    {
                        "name": "Ashutosh Kumar"
                    },
                    {
                        "name": "Mahsa Khoshnoodi"
                    },
                    {
                        "name": "Sankalp KJ"
                    },
                    {
                        "name": "Vinija Jain"
                    },
                    {
                        "name": "Aman Chadha"
                    }
                ],
                "author_detail": {
                    "name": "Aman Chadha"
                },
                "author": "Aman Chadha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12644v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12644v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01444v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01444v2",
                "updated": "2024-12-01T16:11:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    16,
                    11,
                    18,
                    6,
                    336,
                    0
                ],
                "published": "2024-07-21T16:11:00Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    16,
                    11,
                    0,
                    6,
                    203,
                    0
                ],
                "title": "No Size Fits All: The Perils and Pitfalls of Leveraging LLMs Vary with\n  Company Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No Size Fits All: The Perils and Pitfalls of Leveraging LLMs Vary with\n  Company Size"
                },
                "summary": "Large language models (LLMs) are playing a pivotal role in deploying\nstrategic use cases across a range of organizations, from large pan-continental\ncompanies to emerging startups. The issues and challenges involved in the\nsuccessful utilization of LLMs can vary significantly depending on the size of\nthe organization. It is important to study and discuss these pertinent issues\nof LLM adaptation with a focus on the scale of the industrial concerns and\nbrainstorm possible solutions and prospective directions. Such a study has not\nbeen prominently featured in the current research literature. In this study, we\nadopt a threefold strategy: first, we conduct a case study with industry\npractitioners to formulate the key research questions; second, we examine\nexisting industrial publications to address these questions; and finally, we\nprovide a practical guide for industries to utilize LLMs more efficiently. We\nrelease the\nGitHub\\footnote{\\url{https://github.com/vinayakcse/IndustrialLLMsPapers}}\nrepository with the most recent papers in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are playing a pivotal role in deploying\nstrategic use cases across a range of organizations, from large pan-continental\ncompanies to emerging startups. The issues and challenges involved in the\nsuccessful utilization of LLMs can vary significantly depending on the size of\nthe organization. It is important to study and discuss these pertinent issues\nof LLM adaptation with a focus on the scale of the industrial concerns and\nbrainstorm possible solutions and prospective directions. Such a study has not\nbeen prominently featured in the current research literature. In this study, we\nadopt a threefold strategy: first, we conduct a case study with industry\npractitioners to formulate the key research questions; second, we examine\nexisting industrial publications to address these questions; and finally, we\nprovide a practical guide for industries to utilize LLMs more efficiently. We\nrelease the\nGitHub\\footnote{\\url{https://github.com/vinayakcse/IndustrialLLMsPapers}}\nrepository with the most recent papers in the field."
                },
                "authors": [
                    {
                        "name": "Ashok Urlana"
                    },
                    {
                        "name": "Charaka Vinayak Kumar"
                    },
                    {
                        "name": "Bala Mallikarjunarao Garlapati"
                    },
                    {
                        "name": "Ajeet Kumar Singh"
                    },
                    {
                        "name": "Rahul Mishra"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Mishra"
                },
                "author": "Rahul Mishra",
                "arxiv_comment": "COLING2025 Industry track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01444v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01444v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19128v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19128v3",
                "updated": "2024-12-01T15:07:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    7,
                    29,
                    6,
                    336,
                    0
                ],
                "published": "2024-10-24T19:56:28Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    19,
                    56,
                    28,
                    3,
                    298,
                    0
                ],
                "title": "Retrieving Implicit and Explicit Emotional Events Using Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieving Implicit and Explicit Emotional Events Using Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have garnered significant attention in recent\nyears due to their impressive performance. While considerable research has\nevaluated these models from various perspectives, the extent to which LLMs can\nperform implicit and explicit emotion retrieval remains largely unexplored. To\naddress this gap, this study investigates LLMs' emotion retrieval capabilities\nin commonsense. Through extensive experiments involving multiple models, we\nsystematically evaluate the ability of LLMs on emotion retrieval. Specifically,\nwe propose a supervised contrastive probing method to verify LLMs' performance\nfor implicit and explicit emotion retrieval, as well as the diversity of the\nemotional events they retrieve. The results offer valuable insights into the\nstrengths and limitations of LLMs in handling emotion retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have garnered significant attention in recent\nyears due to their impressive performance. While considerable research has\nevaluated these models from various perspectives, the extent to which LLMs can\nperform implicit and explicit emotion retrieval remains largely unexplored. To\naddress this gap, this study investigates LLMs' emotion retrieval capabilities\nin commonsense. Through extensive experiments involving multiple models, we\nsystematically evaluate the ability of LLMs on emotion retrieval. Specifically,\nwe propose a supervised contrastive probing method to verify LLMs' performance\nfor implicit and explicit emotion retrieval, as well as the diversity of the\nemotional events they retrieve. The results offer valuable insights into the\nstrengths and limitations of LLMs in handling emotion retrieval."
                },
                "authors": [
                    {
                        "name": "Guimin Hu"
                    },
                    {
                        "name": "Hasti Seifi"
                    }
                ],
                "author_detail": {
                    "name": "Hasti Seifi"
                },
                "author": "Hasti Seifi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19128v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19128v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17017v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17017v2",
                "updated": "2024-12-01T14:37:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    14,
                    37,
                    22,
                    6,
                    336,
                    0
                ],
                "published": "2024-11-26T01:00:09Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    1,
                    0,
                    9,
                    1,
                    331,
                    0
                ],
                "title": "TED-VITON: Transformer-Empowered Diffusion Models for Virtual Try-On",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TED-VITON: Transformer-Empowered Diffusion Models for Virtual Try-On"
                },
                "summary": "Recent advancements in Virtual Try-On (VTO) have demonstrated exceptional\nefficacy in generating realistic images and preserving garment details, largely\nattributed to the robust generative capabilities of text-to-image (T2I)\ndiffusion backbones. However, the T2I models that underpin these methods have\nbecome outdated, thereby limiting the potential for further improvement in VTO.\nAdditionally, current methods face notable challenges in accurately rendering\ntext on garments without distortion and preserving fine-grained details, such\nas textures and material fidelity. The emergence of Diffusion Transformer (DiT)\nbased T2I models has showcased impressive performance and offers a promising\nopportunity for advancing VTO. Directly applying existing VTO techniques to\ntransformer-based T2I models is ineffective due to substantial architectural\ndifferences, which hinder their ability to fully leverage the models' advanced\ncapabilities for improved text generation. To address these challenges and\nunlock the full potential of DiT-based T2I models for VTO, we propose\nTED-VITON, a novel framework that integrates a Garment Semantic (GS) Adapter\nfor enhancing garment-specific features, a Text Preservation Loss to ensure\naccurate and distortion-free text rendering, and a constraint mechanism to\ngenerate prompts by optimizing Large Language Model (LLM). These innovations\nenable state-of-the-art (SOTA) performance in visual quality and text fidelity,\nestablishing a new benchmark for VTO task. Project page:\n\\url{https://zhenchenwan.github.io/TED-VITON/}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Virtual Try-On (VTO) have demonstrated exceptional\nefficacy in generating realistic images and preserving garment details, largely\nattributed to the robust generative capabilities of text-to-image (T2I)\ndiffusion backbones. However, the T2I models that underpin these methods have\nbecome outdated, thereby limiting the potential for further improvement in VTO.\nAdditionally, current methods face notable challenges in accurately rendering\ntext on garments without distortion and preserving fine-grained details, such\nas textures and material fidelity. The emergence of Diffusion Transformer (DiT)\nbased T2I models has showcased impressive performance and offers a promising\nopportunity for advancing VTO. Directly applying existing VTO techniques to\ntransformer-based T2I models is ineffective due to substantial architectural\ndifferences, which hinder their ability to fully leverage the models' advanced\ncapabilities for improved text generation. To address these challenges and\nunlock the full potential of DiT-based T2I models for VTO, we propose\nTED-VITON, a novel framework that integrates a Garment Semantic (GS) Adapter\nfor enhancing garment-specific features, a Text Preservation Loss to ensure\naccurate and distortion-free text rendering, and a constraint mechanism to\ngenerate prompts by optimizing Large Language Model (LLM). These innovations\nenable state-of-the-art (SOTA) performance in visual quality and text fidelity,\nestablishing a new benchmark for VTO task. Project page:\n\\url{https://zhenchenwan.github.io/TED-VITON/}"
                },
                "authors": [
                    {
                        "name": "Zhenchen Wan"
                    },
                    {
                        "name": "Yanwu Xu"
                    },
                    {
                        "name": "Zhaoqing Wang"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Tongliang Liu"
                    },
                    {
                        "name": "Mingming Gong"
                    }
                ],
                "author_detail": {
                    "name": "Mingming Gong"
                },
                "author": "Mingming Gong",
                "arxiv_comment": "Project page: \\href{https://github.com/ZhenchenWan/TED-VITON}{this\n  URL}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17017v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17017v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.14225v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.14225v3",
                "updated": "2024-12-01T13:55:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    13,
                    55,
                    56,
                    6,
                    336,
                    0
                ],
                "published": "2023-05-23T16:40:07Z",
                "published_parsed": [
                    2023,
                    5,
                    23,
                    16,
                    40,
                    7,
                    1,
                    143,
                    0
                ],
                "title": "ManiTweet: A New Benchmark for Identifying Manipulation of News on\n  Social Media",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ManiTweet: A New Benchmark for Identifying Manipulation of News on\n  Social Media"
                },
                "summary": "Considerable advancements have been made to tackle the misrepresentation of\ninformation derived from reference articles in the domains of fact-checking and\nfaithful summarization. However, an unaddressed aspect remains - the\nidentification of social media posts that manipulate information within\nassociated news articles. This task presents a significant challenge, primarily\ndue to the prevalence of personal opinions in such posts. We present a novel\ntask, identifying manipulation of news on social media, which aims to detect\nmanipulation in social media posts and identify manipulated or inserted\ninformation. To study this task, we have proposed a data collection schema and\ncurated a dataset called ManiTweet, consisting of 3.6K pairs of tweets and\ncorresponding articles. Our analysis demonstrates that this task is highly\nchallenging, with large language models (LLMs) yielding unsatisfactory\nperformance. Additionally, we have developed a simple yet effective basic model\nthat outperforms LLMs significantly on the ManiTweet dataset. Finally, we have\nconducted an exploratory analysis of human-written tweets, unveiling intriguing\nconnections between manipulation and the domain and factuality of news\narticles, as well as revealing that manipulated sentences are more likely to\nencapsulate the main story or consequences of a news outlet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Considerable advancements have been made to tackle the misrepresentation of\ninformation derived from reference articles in the domains of fact-checking and\nfaithful summarization. However, an unaddressed aspect remains - the\nidentification of social media posts that manipulate information within\nassociated news articles. This task presents a significant challenge, primarily\ndue to the prevalence of personal opinions in such posts. We present a novel\ntask, identifying manipulation of news on social media, which aims to detect\nmanipulation in social media posts and identify manipulated or inserted\ninformation. To study this task, we have proposed a data collection schema and\ncurated a dataset called ManiTweet, consisting of 3.6K pairs of tweets and\ncorresponding articles. Our analysis demonstrates that this task is highly\nchallenging, with large language models (LLMs) yielding unsatisfactory\nperformance. Additionally, we have developed a simple yet effective basic model\nthat outperforms LLMs significantly on the ManiTweet dataset. Finally, we have\nconducted an exploratory analysis of human-written tweets, unveiling intriguing\nconnections between manipulation and the domain and factuality of news\narticles, as well as revealing that manipulated sentences are more likely to\nencapsulate the main story or consequences of a news outlet."
                },
                "authors": [
                    {
                        "name": "Kung-Hsiang Huang"
                    },
                    {
                        "name": "Hou Pong Chan"
                    },
                    {
                        "name": "Kathleen McKeown"
                    },
                    {
                        "name": "Heng Ji"
                    }
                ],
                "author_detail": {
                    "name": "Heng Ji"
                },
                "author": "Heng Ji",
                "arxiv_comment": "COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.14225v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.14225v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06237v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06237v2",
                "updated": "2024-12-01T13:31:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    13,
                    31,
                    14,
                    6,
                    336,
                    0
                ],
                "published": "2024-11-09T17:38:01Z",
                "published_parsed": [
                    2024,
                    11,
                    9,
                    17,
                    38,
                    1,
                    5,
                    314,
                    0
                ],
                "title": "Leveraging Retrieval-Augmented Generation for Persian University\n  Knowledge Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Retrieval-Augmented Generation for Persian University\n  Knowledge Retrieval"
                },
                "summary": "This paper introduces an innovative approach using Retrieval-Augmented\nGeneration (RAG) pipelines with Large Language Models (LLMs) to enhance\ninformation retrieval and query response systems for university-related\nquestion answering. By systematically extracting data from the university\nofficial webpage and employing advanced prompt engineering techniques, we\ngenerate accurate, contextually relevant responses to user queries.\n  We developed a comprehensive university benchmark, UniversityQuestionBench\n(UQB), to rigorously evaluate our system performance, based on common key\nmetrics in the filed of RAG pipelines, assessing accuracy and reliability\nthrough various metrics and real-world scenarios. Our experimental results\ndemonstrate significant improvements in the precision and relevance of\ngenerated responses, enhancing user experience and reducing the time required\nto obtain relevant answers. In summary, this paper presents a novel application\nof RAG pipelines and LLMs, supported by a meticulously prepared university\nbenchmark, offering valuable insights into advanced AI techniques for academic\ndata retrieval and setting the stage for future research in this domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an innovative approach using Retrieval-Augmented\nGeneration (RAG) pipelines with Large Language Models (LLMs) to enhance\ninformation retrieval and query response systems for university-related\nquestion answering. By systematically extracting data from the university\nofficial webpage and employing advanced prompt engineering techniques, we\ngenerate accurate, contextually relevant responses to user queries.\n  We developed a comprehensive university benchmark, UniversityQuestionBench\n(UQB), to rigorously evaluate our system performance, based on common key\nmetrics in the filed of RAG pipelines, assessing accuracy and reliability\nthrough various metrics and real-world scenarios. Our experimental results\ndemonstrate significant improvements in the precision and relevance of\ngenerated responses, enhancing user experience and reducing the time required\nto obtain relevant answers. In summary, this paper presents a novel application\nof RAG pipelines and LLMs, supported by a meticulously prepared university\nbenchmark, offering valuable insights into advanced AI techniques for academic\ndata retrieval and setting the stage for future research in this domain."
                },
                "authors": [
                    {
                        "name": "Arshia Hemmat"
                    },
                    {
                        "name": "Kianoosh Vadaei"
                    },
                    {
                        "name": "Mohammad Hassan Heydari"
                    },
                    {
                        "name": "Afsaneh Fatemi"
                    }
                ],
                "author_detail": {
                    "name": "Afsaneh Fatemi"
                },
                "author": "Afsaneh Fatemi",
                "arxiv_comment": "6 pages, 2 figures, 1 table, Submitted to 15th IKT conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06237v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06237v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19456v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19456v2",
                "updated": "2024-12-01T11:27:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    11,
                    27,
                    9,
                    6,
                    336,
                    0
                ],
                "published": "2024-10-25T10:30:21Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    10,
                    30,
                    21,
                    4,
                    299,
                    0
                ],
                "title": "Computational Bottlenecks of Training Small-scale Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational Bottlenecks of Training Small-scale Large Language Models"
                },
                "summary": "While large language models (LLMs) dominate the AI landscape, Small-scale\nlarge Language Models (SLMs) are gaining attention due to cost and efficiency\ndemands from consumers. However, there is limited research on the training\nbehavior and computational requirements of SLMs. In this study, we explore the\ncomputational bottlenecks of training SLMs (up to 2B parameters) by examining\nthe effects of various hyperparameters and configurations, including GPU type,\nbatch size, model size, communication protocol, attention type, and the number\nof GPUs. We assess these factors on popular cloud services using metrics such\nas loss per dollar and tokens per second. Our findings aim to support the\nbroader adoption and optimization of language model training for low-resource\nAI research institutes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) dominate the AI landscape, Small-scale\nlarge Language Models (SLMs) are gaining attention due to cost and efficiency\ndemands from consumers. However, there is limited research on the training\nbehavior and computational requirements of SLMs. In this study, we explore the\ncomputational bottlenecks of training SLMs (up to 2B parameters) by examining\nthe effects of various hyperparameters and configurations, including GPU type,\nbatch size, model size, communication protocol, attention type, and the number\nof GPUs. We assess these factors on popular cloud services using metrics such\nas loss per dollar and tokens per second. Our findings aim to support the\nbroader adoption and optimization of language model training for low-resource\nAI research institutes."
                },
                "authors": [
                    {
                        "name": "Saleh Ashkboos"
                    },
                    {
                        "name": "Iman Mirzadeh"
                    },
                    {
                        "name": "Keivan Alizadeh"
                    },
                    {
                        "name": "Mohammad Hossein Sekhavat"
                    },
                    {
                        "name": "Moin Nabi"
                    },
                    {
                        "name": "Mehrdad Farajtabar"
                    },
                    {
                        "name": "Fartash Faghri"
                    }
                ],
                "author_detail": {
                    "name": "Fartash Faghri"
                },
                "author": "Fartash Faghri",
                "arxiv_comment": "8 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19456v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19456v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17305v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17305v2",
                "updated": "2024-12-01T09:02:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    9,
                    2,
                    35,
                    6,
                    336,
                    0
                ],
                "published": "2024-06-25T06:24:50Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    6,
                    24,
                    50,
                    1,
                    177,
                    0
                ],
                "title": "Retrieval Augmented Instruction Tuning for Open NER with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Instruction Tuning for Open NER with Large Language\n  Models"
                },
                "summary": "The strong capability of large language models (LLMs) has been applied to\ninformation extraction (IE) through either retrieval augmented prompting or\ninstruction tuning (IT). However, the best way to incorporate information with\nLLMs for IE remains an open question. In this paper, we explore Retrieval\nAugmented Instruction Tuning (RA-IT) for IE, focusing on the task of open named\nentity recognition (NER). Specifically, for each training sample, we retrieve\nsemantically similar examples from the training dataset as the context and\nprepend them to the input of the original instruction. To evaluate our RA-IT\napproach more thoroughly, we construct a Chinese IT dataset for open NER and\nevaluate RA-IT in both English and Chinese scenarios. Experimental results\nverify the effectiveness of RA-IT across various data sizes and in both English\nand Chinese scenarios. We also conduct thorough studies to explore the impacts\nof various retrieval strategies in the proposed RA-IT framework. Code and data\nare available at: https://github.com/Emma1066/Retrieval-Augmented-IT-OpenNER",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The strong capability of large language models (LLMs) has been applied to\ninformation extraction (IE) through either retrieval augmented prompting or\ninstruction tuning (IT). However, the best way to incorporate information with\nLLMs for IE remains an open question. In this paper, we explore Retrieval\nAugmented Instruction Tuning (RA-IT) for IE, focusing on the task of open named\nentity recognition (NER). Specifically, for each training sample, we retrieve\nsemantically similar examples from the training dataset as the context and\nprepend them to the input of the original instruction. To evaluate our RA-IT\napproach more thoroughly, we construct a Chinese IT dataset for open NER and\nevaluate RA-IT in both English and Chinese scenarios. Experimental results\nverify the effectiveness of RA-IT across various data sizes and in both English\nand Chinese scenarios. We also conduct thorough studies to explore the impacts\nof various retrieval strategies in the proposed RA-IT framework. Code and data\nare available at: https://github.com/Emma1066/Retrieval-Augmented-IT-OpenNER"
                },
                "authors": [
                    {
                        "name": "Tingyu Xie"
                    },
                    {
                        "name": "Jian Zhang"
                    },
                    {
                        "name": "Yan Zhang"
                    },
                    {
                        "name": "Yuanyuan Liang"
                    },
                    {
                        "name": "Qi Li"
                    },
                    {
                        "name": "Hongwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hongwei Wang"
                },
                "author": "Hongwei Wang",
                "arxiv_comment": "To be appeared at COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17305v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17305v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14491v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14491v3",
                "updated": "2024-12-01T08:37:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    8,
                    37,
                    51,
                    6,
                    336,
                    0
                ],
                "published": "2024-11-20T12:34:44Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    12,
                    34,
                    44,
                    2,
                    325,
                    0
                ],
                "title": "A Survey on Human-Centric LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Human-Centric LLMs"
                },
                "summary": "The rapid evolution of large language models (LLMs) and their capacity to\nsimulate human cognition and behavior has given rise to LLM-based frameworks\nand tools that are evaluated and applied based on their ability to perform\ntasks traditionally performed by humans, namely those involving cognition,\ndecision-making, and social interaction. This survey provides a comprehensive\nexamination of such human-centric LLM capabilities, focusing on their\nperformance in both individual tasks (where an LLM acts as a stand-in for a\nsingle human) and collective tasks (where multiple LLMs coordinate to mimic\ngroup dynamics). We first evaluate LLM competencies across key areas including\nreasoning, perception, and social cognition, comparing their abilities to\nhuman-like skills. Then, we explore real-world applications of LLMs in\nhuman-centric domains such as behavioral science, political science, and\nsociology, assessing their effectiveness in replicating human behaviors and\ninteractions. Finally, we identify challenges and future research directions,\nsuch as improving LLM adaptability, emotional intelligence, and cultural\nsensitivity, while addressing inherent biases and enhancing frameworks for\nhuman-AI collaboration. This survey aims to provide a foundational\nunderstanding of LLMs from a human-centric perspective, offering insights into\ntheir current capabilities and potential for future development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of large language models (LLMs) and their capacity to\nsimulate human cognition and behavior has given rise to LLM-based frameworks\nand tools that are evaluated and applied based on their ability to perform\ntasks traditionally performed by humans, namely those involving cognition,\ndecision-making, and social interaction. This survey provides a comprehensive\nexamination of such human-centric LLM capabilities, focusing on their\nperformance in both individual tasks (where an LLM acts as a stand-in for a\nsingle human) and collective tasks (where multiple LLMs coordinate to mimic\ngroup dynamics). We first evaluate LLM competencies across key areas including\nreasoning, perception, and social cognition, comparing their abilities to\nhuman-like skills. Then, we explore real-world applications of LLMs in\nhuman-centric domains such as behavioral science, political science, and\nsociology, assessing their effectiveness in replicating human behaviors and\ninteractions. Finally, we identify challenges and future research directions,\nsuch as improving LLM adaptability, emotional intelligence, and cultural\nsensitivity, while addressing inherent biases and enhancing frameworks for\nhuman-AI collaboration. This survey aims to provide a foundational\nunderstanding of LLMs from a human-centric perspective, offering insights into\ntheir current capabilities and potential for future development."
                },
                "authors": [
                    {
                        "name": "Jing Yi Wang"
                    },
                    {
                        "name": "Nicholas Sukiennik"
                    },
                    {
                        "name": "Tong Li"
                    },
                    {
                        "name": "Weikang Su"
                    },
                    {
                        "name": "Qianyue Hao"
                    },
                    {
                        "name": "Jingbo Xu"
                    },
                    {
                        "name": "Zihan Huang"
                    },
                    {
                        "name": "Fengli Xu"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14491v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14491v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23402v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23402v2",
                "updated": "2024-12-01T07:55:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    7,
                    55,
                    12,
                    6,
                    336,
                    0
                ],
                "published": "2024-10-30T19:07:01Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    19,
                    7,
                    1,
                    2,
                    304,
                    0
                ],
                "title": "VISUALCODER: Guiding Large Language Models in Code Execution with\n  Fine-grained Multimodal Chain-of-Thought Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VISUALCODER: Guiding Large Language Models in Code Execution with\n  Fine-grained Multimodal Chain-of-Thought Reasoning"
                },
                "summary": "Predicting program behavior and reasoning about code execution remain\nsignificant challenges in software engineering, particularly for large language\nmodels (LLMs) designed for code analysis. While these models excel at\nunderstanding static syntax, they often struggle with dynamic reasoning tasks.\nWe introduce Visual Coder, a simple yet effective approach that enhances code\nreasoning by integrating multimodal Chain-of-Thought (CoT) reasoning with a\nvisual Control Flow Graph (CFG). By aligning code snippets with their\ncorresponding CFGs, Visual Coder provides deeper insights into execution flow,\nenabling more accurate predictions of code behavior. Our experiments\ndemonstrate that augmenting LLMs with visual CFGs significantly outperforms\ntext-based CFG descriptions in code reasoning tasks. We address challenges in\nmultimodal CoT integration through a reference mechanism, ensuring consistency\nbetween code and its execution path, thereby improving performance in program\nbehavior prediction, error detection, and output generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting program behavior and reasoning about code execution remain\nsignificant challenges in software engineering, particularly for large language\nmodels (LLMs) designed for code analysis. While these models excel at\nunderstanding static syntax, they often struggle with dynamic reasoning tasks.\nWe introduce Visual Coder, a simple yet effective approach that enhances code\nreasoning by integrating multimodal Chain-of-Thought (CoT) reasoning with a\nvisual Control Flow Graph (CFG). By aligning code snippets with their\ncorresponding CFGs, Visual Coder provides deeper insights into execution flow,\nenabling more accurate predictions of code behavior. Our experiments\ndemonstrate that augmenting LLMs with visual CFGs significantly outperforms\ntext-based CFG descriptions in code reasoning tasks. We address challenges in\nmultimodal CoT integration through a reference mechanism, ensuring consistency\nbetween code and its execution path, thereby improving performance in program\nbehavior prediction, error detection, and output generation."
                },
                "authors": [
                    {
                        "name": "Cuong Chi Le"
                    },
                    {
                        "name": "Hoang-Chau Truong-Vinh"
                    },
                    {
                        "name": "Huy Nhat Phan"
                    },
                    {
                        "name": "Dung Duy Le"
                    },
                    {
                        "name": "Tien N. Nguyen"
                    },
                    {
                        "name": "Nghi D. Q. Bui"
                    }
                ],
                "author_detail": {
                    "name": "Nghi D. Q. Bui"
                },
                "author": "Nghi D. Q. Bui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23402v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23402v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.02490v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.02490v4",
                "updated": "2024-12-01T05:46:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    5,
                    46,
                    3,
                    6,
                    336,
                    0
                ],
                "published": "2023-08-04T17:59:47Z",
                "published_parsed": [
                    2023,
                    8,
                    4,
                    17,
                    59,
                    47,
                    4,
                    216,
                    0
                ],
                "title": "MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities"
                },
                "summary": "We propose MM-Vet, an evaluation benchmark that examines large multimodal\nmodels (LMMs) on complicated multimodal tasks. Recent LMMs have shown various\nintriguing abilities, such as solving math problems written on the blackboard,\nreasoning about events and celebrities in news images, and explaining visual\njokes. Rapid model advancements pose challenges to evaluation benchmark\ndevelopment. Problems include: (1) How to systematically structure and evaluate\nthe complicated multimodal tasks; (2) How to design evaluation metrics that\nwork well across question and answer types; and (3) How to give model insights\nbeyond a simple performance ranking. To this end, we present MM-Vet, designed\nbased on the insight that the intriguing ability to solve complicated tasks is\noften achieved by a generalist model being able to integrate different core\nvision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and\nexamines the 16 integrations of interest derived from the capability\ncombination. For evaluation metrics, we propose an LLM-based evaluator for\nopen-ended outputs. The evaluator enables the evaluation across different\nquestion types and answer styles, resulting in a unified scoring metric. We\nevaluate representative LMMs on MM-Vet, providing insights into the\ncapabilities of different LMM system paradigms and models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose MM-Vet, an evaluation benchmark that examines large multimodal\nmodels (LMMs) on complicated multimodal tasks. Recent LMMs have shown various\nintriguing abilities, such as solving math problems written on the blackboard,\nreasoning about events and celebrities in news images, and explaining visual\njokes. Rapid model advancements pose challenges to evaluation benchmark\ndevelopment. Problems include: (1) How to systematically structure and evaluate\nthe complicated multimodal tasks; (2) How to design evaluation metrics that\nwork well across question and answer types; and (3) How to give model insights\nbeyond a simple performance ranking. To this end, we present MM-Vet, designed\nbased on the insight that the intriguing ability to solve complicated tasks is\noften achieved by a generalist model being able to integrate different core\nvision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and\nexamines the 16 integrations of interest derived from the capability\ncombination. For evaluation metrics, we propose an LLM-based evaluator for\nopen-ended outputs. The evaluator enables the evaluation across different\nquestion types and answer styles, resulting in a unified scoring metric. We\nevaluate representative LMMs on MM-Vet, providing insights into the\ncapabilities of different LMM system paradigms and models."
                },
                "authors": [
                    {
                        "name": "Weihao Yu"
                    },
                    {
                        "name": "Zhengyuan Yang"
                    },
                    {
                        "name": "Linjie Li"
                    },
                    {
                        "name": "Jianfeng Wang"
                    },
                    {
                        "name": "Kevin Lin"
                    },
                    {
                        "name": "Zicheng Liu"
                    },
                    {
                        "name": "Xinchao Wang"
                    },
                    {
                        "name": "Lijuan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Lijuan Wang"
                },
                "author": "Lijuan Wang",
                "arxiv_comment": "ICML 2024. Code, data and leaderboard:\n  https://github.com/yuweihao/MM-Vet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.02490v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.02490v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12279v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12279v3",
                "updated": "2024-12-01T02:12:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    2,
                    12,
                    8,
                    6,
                    336,
                    0
                ],
                "published": "2024-11-19T06:57:45Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    6,
                    57,
                    45,
                    1,
                    324,
                    0
                ],
                "title": "HouseLLM: LLM-Assisted Two-Phase Text-to-Floorplan Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HouseLLM: LLM-Assisted Two-Phase Text-to-Floorplan Generation"
                },
                "summary": "This paper proposes a two-phase text-to-floorplan generation method, which\nguides a Large Language Model (LLM) to generate an initial layout (Layout-LLM)\nand refines them into the final floorplans through conditional diffusion model.\nWe incorporate a Chain-of-Thought approach to prompt the LLM based on user text\nspecifications, enabling a more user-friendly and intuitive house layout\ndesign. This method allows users to describe their needs in natural language,\nenhancing accessibility and providing clearer geometric constraints. The final\nfloorplans generated by Layout-LLM through conditional diffusion refinement are\nmore accurate and better meet user requirements. Experimental results\ndemonstrate that our approach achieves state-of-the-art performance across all\nmetrics, validating its effectiveness in practical home design applications. We\nplan to release our code for public use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a two-phase text-to-floorplan generation method, which\nguides a Large Language Model (LLM) to generate an initial layout (Layout-LLM)\nand refines them into the final floorplans through conditional diffusion model.\nWe incorporate a Chain-of-Thought approach to prompt the LLM based on user text\nspecifications, enabling a more user-friendly and intuitive house layout\ndesign. This method allows users to describe their needs in natural language,\nenhancing accessibility and providing clearer geometric constraints. The final\nfloorplans generated by Layout-LLM through conditional diffusion refinement are\nmore accurate and better meet user requirements. Experimental results\ndemonstrate that our approach achieves state-of-the-art performance across all\nmetrics, validating its effectiveness in practical home design applications. We\nplan to release our code for public use."
                },
                "authors": [
                    {
                        "name": "Ziyang Zong"
                    },
                    {
                        "name": "Zhaohuan Zhan"
                    },
                    {
                        "name": "Guang Tan"
                    }
                ],
                "author_detail": {
                    "name": "Guang Tan"
                },
                "author": "Guang Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12279v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12279v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.17249v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.17249v3",
                "updated": "2024-12-01T01:36:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    1,
                    36,
                    50,
                    6,
                    336,
                    0
                ],
                "published": "2023-09-29T13:55:45Z",
                "published_parsed": [
                    2023,
                    9,
                    29,
                    13,
                    55,
                    45,
                    4,
                    272,
                    0
                ],
                "title": "Batch Calibration: Rethinking Calibration for In-Context Learning and\n  Prompt Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch Calibration: Rethinking Calibration for In-Context Learning and\n  Prompt Engineering"
                },
                "summary": "Prompting and in-context learning (ICL) have become efficient learning\nparadigms for large language models (LLMs). However, LLMs suffer from prompt\nbrittleness and various bias factors in the prompt, including but not limited\nto the formatting, the choice verbalizers, and the ICL examples. To address\nthis problem that results in unexpected performance degradation, calibration\nmethods have been developed to mitigate the effects of these biases while\nrecovering LLM performance. In this work, we first conduct a systematic\nanalysis of the existing calibration methods, where we both provide a unified\nview and reveal the failure cases. Inspired by these analyses, we propose Batch\nCalibration (BC), a simple yet intuitive method that controls the contextual\nbias from the batched input, unifies various prior approaches, and effectively\naddresses the aforementioned issues. BC is zero-shot, inference-only, and\nincurs negligible additional costs. In the few-shot setup, we further extend BC\nto allow it to learn the contextual bias from labeled data. We validate the\neffectiveness of BC with PaLM 2-(S, M, L) and CLIP models and demonstrate\nstate-of-the-art performance over previous calibration baselines across more\nthan 10 natural language understanding and image classification tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting and in-context learning (ICL) have become efficient learning\nparadigms for large language models (LLMs). However, LLMs suffer from prompt\nbrittleness and various bias factors in the prompt, including but not limited\nto the formatting, the choice verbalizers, and the ICL examples. To address\nthis problem that results in unexpected performance degradation, calibration\nmethods have been developed to mitigate the effects of these biases while\nrecovering LLM performance. In this work, we first conduct a systematic\nanalysis of the existing calibration methods, where we both provide a unified\nview and reveal the failure cases. Inspired by these analyses, we propose Batch\nCalibration (BC), a simple yet intuitive method that controls the contextual\nbias from the batched input, unifies various prior approaches, and effectively\naddresses the aforementioned issues. BC is zero-shot, inference-only, and\nincurs negligible additional costs. In the few-shot setup, we further extend BC\nto allow it to learn the contextual bias from labeled data. We validate the\neffectiveness of BC with PaLM 2-(S, M, L) and CLIP models and demonstrate\nstate-of-the-art performance over previous calibration baselines across more\nthan 10 natural language understanding and image classification tasks."
                },
                "authors": [
                    {
                        "name": "Han Zhou"
                    },
                    {
                        "name": "Xingchen Wan"
                    },
                    {
                        "name": "Lev Proleev"
                    },
                    {
                        "name": "Diana Mincu"
                    },
                    {
                        "name": "Jilin Chen"
                    },
                    {
                        "name": "Katherine Heller"
                    },
                    {
                        "name": "Subhrajit Roy"
                    }
                ],
                "author_detail": {
                    "name": "Subhrajit Roy"
                },
                "author": "Subhrajit Roy",
                "arxiv_comment": "ICLR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.17249v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.17249v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23904v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23904v2",
                "updated": "2024-12-01T01:35:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    1,
                    35,
                    19,
                    6,
                    336,
                    0
                ],
                "published": "2024-10-31T13:06:29Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    13,
                    6,
                    29,
                    3,
                    305,
                    0
                ],
                "title": "EZ-HOI: VLM Adaptation via Guided Prompt Learning for Zero-Shot HOI\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EZ-HOI: VLM Adaptation via Guided Prompt Learning for Zero-Shot HOI\n  Detection"
                },
                "summary": "Detecting Human-Object Interactions (HOI) in zero-shot settings, where models\nmust handle unseen classes, poses significant challenges. Existing methods that\nrely on aligning visual encoders with large Vision-Language Models (VLMs) to\ntap into the extensive knowledge of VLMs, require large, computationally\nexpensive models and encounter training difficulties. Adapting VLMs with prompt\nlearning offers an alternative to direct alignment. However, fine-tuning on\ntask-specific datasets often leads to overfitting to seen classes and\nsuboptimal performance on unseen classes, due to the absence of unseen class\nlabels. To address these challenges, we introduce a novel prompt learning-based\nframework for Efficient Zero-Shot HOI detection (EZ-HOI). First, we introduce\nLarge Language Model (LLM) and VLM guidance for learnable prompts, integrating\ndetailed HOI descriptions and visual semantics to adapt VLMs to HOI tasks.\nHowever, because training datasets contain seen-class labels alone, fine-tuning\nVLMs on such datasets tends to optimize learnable prompts for seen classes\ninstead of unseen ones. Therefore, we design prompt learning for unseen classes\nusing information from related seen classes, with LLMs utilized to highlight\nthe differences between unseen and related seen classes. Quantitative\nevaluations on benchmark datasets demonstrate that our EZ-HOI achieves\nstate-of-the-art performance across various zero-shot settings with only 10.35%\nto 33.95% of the trainable parameters compared to existing methods. Code is\navailable at https://github.com/ChelsieLei/EZ-HOI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Human-Object Interactions (HOI) in zero-shot settings, where models\nmust handle unseen classes, poses significant challenges. Existing methods that\nrely on aligning visual encoders with large Vision-Language Models (VLMs) to\ntap into the extensive knowledge of VLMs, require large, computationally\nexpensive models and encounter training difficulties. Adapting VLMs with prompt\nlearning offers an alternative to direct alignment. However, fine-tuning on\ntask-specific datasets often leads to overfitting to seen classes and\nsuboptimal performance on unseen classes, due to the absence of unseen class\nlabels. To address these challenges, we introduce a novel prompt learning-based\nframework for Efficient Zero-Shot HOI detection (EZ-HOI). First, we introduce\nLarge Language Model (LLM) and VLM guidance for learnable prompts, integrating\ndetailed HOI descriptions and visual semantics to adapt VLMs to HOI tasks.\nHowever, because training datasets contain seen-class labels alone, fine-tuning\nVLMs on such datasets tends to optimize learnable prompts for seen classes\ninstead of unseen ones. Therefore, we design prompt learning for unseen classes\nusing information from related seen classes, with LLMs utilized to highlight\nthe differences between unseen and related seen classes. Quantitative\nevaluations on benchmark datasets demonstrate that our EZ-HOI achieves\nstate-of-the-art performance across various zero-shot settings with only 10.35%\nto 33.95% of the trainable parameters compared to existing methods. Code is\navailable at https://github.com/ChelsieLei/EZ-HOI."
                },
                "authors": [
                    {
                        "name": "Qinqian Lei"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Robby T. Tan"
                    }
                ],
                "author_detail": {
                    "name": "Robby T. Tan"
                },
                "author": "Robby T. Tan",
                "arxiv_comment": "Accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23904v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23904v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14592v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14592v2",
                "updated": "2024-12-01T01:21:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    1,
                    21,
                    24,
                    6,
                    336,
                    0
                ],
                "published": "2024-11-21T21:22:58Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    21,
                    22,
                    58,
                    3,
                    326,
                    0
                ],
                "title": "G-RAG: Knowledge Expansion in Material Science",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "G-RAG: Knowledge Expansion in Material Science"
                },
                "summary": "In the field of Material Science, effective information retrieval systems are\nessential for facilitating research. Traditional Retrieval-Augmented Generation\n(RAG) approaches in Large Language Models (LLMs) often encounter challenges\nsuch as outdated information, hallucinations, limited interpretability due to\ncontext constraints, and inaccurate retrieval. To address these issues, Graph\nRAG integrates graph databases to enhance the retrieval process. Our proposed\nmethod processes Material Science documents by extracting key entities\n(referred to as MatIDs) from sentences, which are then utilized to query\nexternal Wikipedia knowledge bases (KBs) for additional relevant information.\nWe implement an agent-based parsing technique to achieve a more detailed\nrepresentation of the documents. Our improved version of Graph RAG called G-RAG\nfurther leverages a graph database to capture relationships between these\nentities, improving both retrieval accuracy and contextual understanding. This\nenhanced approach demonstrates significant improvements in performance for\ndomains that require precise information retrieval, such as Material Science.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of Material Science, effective information retrieval systems are\nessential for facilitating research. Traditional Retrieval-Augmented Generation\n(RAG) approaches in Large Language Models (LLMs) often encounter challenges\nsuch as outdated information, hallucinations, limited interpretability due to\ncontext constraints, and inaccurate retrieval. To address these issues, Graph\nRAG integrates graph databases to enhance the retrieval process. Our proposed\nmethod processes Material Science documents by extracting key entities\n(referred to as MatIDs) from sentences, which are then utilized to query\nexternal Wikipedia knowledge bases (KBs) for additional relevant information.\nWe implement an agent-based parsing technique to achieve a more detailed\nrepresentation of the documents. Our improved version of Graph RAG called G-RAG\nfurther leverages a graph database to capture relationships between these\nentities, improving both retrieval accuracy and contextual understanding. This\nenhanced approach demonstrates significant improvements in performance for\ndomains that require precise information retrieval, such as Material Science."
                },
                "authors": [
                    {
                        "name": "Radeen Mostafa"
                    },
                    {
                        "name": "Mirza Nihal Baig"
                    },
                    {
                        "name": "Mashaekh Tausif Ehsan"
                    },
                    {
                        "name": "Jakir Hasan"
                    }
                ],
                "author_detail": {
                    "name": "Jakir Hasan"
                },
                "author": "Jakir Hasan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14592v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14592v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13323v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13323v2",
                "updated": "2024-11-30T23:44:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    30,
                    23,
                    44,
                    43,
                    5,
                    335,
                    0
                ],
                "published": "2024-11-20T13:46:04Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    13,
                    46,
                    4,
                    2,
                    325,
                    0
                ],
                "title": "Are Large Language Models Memorizing Bug Benchmarks?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Large Language Models Memorizing Bug Benchmarks?"
                },
                "summary": "Large Language Models (LLMs) have become integral to various software\nengineering tasks, including code generation, bug detection, and repair. To\nevaluate model performance in these domains, numerous bug benchmarks containing\nreal-world bugs from software projects have been developed. However, a growing\nconcern within the software engineering community is that these benchmarks may\nnot reliably reflect true LLM performance due to the risk of data leakage.\nDespite this concern, limited research has been conducted to quantify the\nimpact of potential leakage. In this paper, we systematically evaluate popular\nLLMs to assess their susceptibility to data leakage from widely used bug\nbenchmarks. To identify potential leakage, we use multiple metrics, including a\nstudy of benchmark membership within commonly used training datasets, as well\nas analyses of negative log-likelihood and n-gram accuracy. Our findings show\nthat certain models, in particular codegen-multi, exhibit significant evidence\nof memorization in widely used benchmarks like Defects4J, while newer models\ntrained on larger datasets like LLaMa 3.1 exhibit limited signs of leakage.\nThese results highlight the need for careful benchmark selection and the\nadoption of robust metrics to adequately assess models capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become integral to various software\nengineering tasks, including code generation, bug detection, and repair. To\nevaluate model performance in these domains, numerous bug benchmarks containing\nreal-world bugs from software projects have been developed. However, a growing\nconcern within the software engineering community is that these benchmarks may\nnot reliably reflect true LLM performance due to the risk of data leakage.\nDespite this concern, limited research has been conducted to quantify the\nimpact of potential leakage. In this paper, we systematically evaluate popular\nLLMs to assess their susceptibility to data leakage from widely used bug\nbenchmarks. To identify potential leakage, we use multiple metrics, including a\nstudy of benchmark membership within commonly used training datasets, as well\nas analyses of negative log-likelihood and n-gram accuracy. Our findings show\nthat certain models, in particular codegen-multi, exhibit significant evidence\nof memorization in widely used benchmarks like Defects4J, while newer models\ntrained on larger datasets like LLaMa 3.1 exhibit limited signs of leakage.\nThese results highlight the need for careful benchmark selection and the\nadoption of robust metrics to adequately assess models capabilities."
                },
                "authors": [
                    {
                        "name": "Daniel Ramos"
                    },
                    {
                        "name": "Claudia Mamede"
                    },
                    {
                        "name": "Kush Jain"
                    },
                    {
                        "name": "Paulo Canelas"
                    },
                    {
                        "name": "Catarina Gamboa"
                    },
                    {
                        "name": "Claire Le Goues"
                    }
                ],
                "author_detail": {
                    "name": "Claire Le Goues"
                },
                "author": "Claire Le Goues",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13323v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13323v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05804v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05804v6",
                "updated": "2024-11-30T22:38:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    30,
                    22,
                    38,
                    57,
                    5,
                    335,
                    0
                ],
                "published": "2024-06-09T14:42:55Z",
                "published_parsed": [
                    2024,
                    6,
                    9,
                    14,
                    42,
                    55,
                    6,
                    161,
                    0
                ],
                "title": "A Review of Prominent Paradigms for LLM-Based Agents: Tool Use\n  (Including RAG), Planning, and Feedback Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Review of Prominent Paradigms for LLM-Based Agents: Tool Use\n  (Including RAG), Planning, and Feedback Learning"
                },
                "summary": "Tool use, planning, and feedback learning are currently three prominent\nparadigms for developing Large Language Model (LLM)-based agents across various\ntasks. Although numerous frameworks have been devised for each paradigm, their\nintricate workflows and inconsistent taxonomy create challenges in\nunderstanding and reviewing the frameworks across different paradigms. This\nsurvey introduces a unified taxonomy to systematically review and discuss these\nframeworks. Specifically, 1) the taxonomy defines environments/tasks, common\nLLM-profiled roles or LMPRs (policy models, evaluators, and dynamic models),\nand universally applicable workflows found in prior work, and 2) it enables a\ncomparison of key perspectives on the implementations of LMPRs and workflow\ndesigns across different agent paradigms and frameworks. 3) Finally, we\nidentify three limitations in existing workflow designs and systematically\ndiscuss the future work. Resources have been made publicly available at in our\nGitHub repository https://github.com/xinzhel/LLM-Agent-Survey.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool use, planning, and feedback learning are currently three prominent\nparadigms for developing Large Language Model (LLM)-based agents across various\ntasks. Although numerous frameworks have been devised for each paradigm, their\nintricate workflows and inconsistent taxonomy create challenges in\nunderstanding and reviewing the frameworks across different paradigms. This\nsurvey introduces a unified taxonomy to systematically review and discuss these\nframeworks. Specifically, 1) the taxonomy defines environments/tasks, common\nLLM-profiled roles or LMPRs (policy models, evaluators, and dynamic models),\nand universally applicable workflows found in prior work, and 2) it enables a\ncomparison of key perspectives on the implementations of LMPRs and workflow\ndesigns across different agent paradigms and frameworks. 3) Finally, we\nidentify three limitations in existing workflow designs and systematically\ndiscuss the future work. Resources have been made publicly available at in our\nGitHub repository https://github.com/xinzhel/LLM-Agent-Survey."
                },
                "authors": [
                    {
                        "name": "Xinzhe Li"
                    }
                ],
                "author_detail": {
                    "name": "Xinzhe Li"
                },
                "author": "Xinzhe Li",
                "arxiv_comment": "CoLing 2025 Camera Ready (extended to 9 pages)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05804v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05804v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14165v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14165v3",
                "updated": "2024-11-30T22:21:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    30,
                    22,
                    21,
                    30,
                    5,
                    335,
                    0
                ],
                "published": "2024-09-21T15:07:37Z",
                "published_parsed": [
                    2024,
                    9,
                    21,
                    15,
                    7,
                    37,
                    5,
                    265,
                    0
                ],
                "title": "A Survey on Large Language Model-empowered Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Large Language Model-empowered Autonomous Driving"
                },
                "summary": "Artificial intelligence (AI) plays a crucial role in autonomous driving (AD)\nresearch, propelling its development towards intelligence and efficiency.\nCurrently, the development of AD technology follows two main technical paths:\nmodularization and end-to-end. Modularization decompose the driving task into\nmodules such as perception, prediction, planning, and control, and train them\nseparately. Due to the inconsistency of training objectives between modules,\nthe integrated effect suffers from bias. End-to-end attempts to address this\nissue by utilizing a single model that directly maps from sensor data to\ncontrol signals. This path has limited learning capabilities in a comprehensive\nset of features and struggles to handle unpredictable long-tail events and\ncomplex urban traffic scenarios. In the face of challenges encountered in both\npaths, many researchers believe that large language models (LLMs) with powerful\nreasoning capabilities and extensive knowledge understanding may be the\nsolution, expecting LLMs to provide AD systems with deeper levels of\nunderstanding and decision-making capabilities. In light of the challenges\nfaced by both paths, many researchers believe that LLMs, with their powerful\nreasoning abilities and extensive knowledge, could offer a solution. To\nunderstand if LLMs could enhance AD, this paper conducts a thorough analysis of\nthe potential applications of LLMs in AD systems, including exploring their\noptimization strategies in both modular and end-to-end approaches, with a\nparticular focus on how LLMs can tackle the problems and challenges present in\ncurrent solutions. Furthermore, we discuss an important question: Can LLM-based\nartificial general intelligence (AGI) be a key to achieve high-level AD? We\nfurther analyze the potential limitations and challenges that LLMs may\nencounter in promoting the development of AD technology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence (AI) plays a crucial role in autonomous driving (AD)\nresearch, propelling its development towards intelligence and efficiency.\nCurrently, the development of AD technology follows two main technical paths:\nmodularization and end-to-end. Modularization decompose the driving task into\nmodules such as perception, prediction, planning, and control, and train them\nseparately. Due to the inconsistency of training objectives between modules,\nthe integrated effect suffers from bias. End-to-end attempts to address this\nissue by utilizing a single model that directly maps from sensor data to\ncontrol signals. This path has limited learning capabilities in a comprehensive\nset of features and struggles to handle unpredictable long-tail events and\ncomplex urban traffic scenarios. In the face of challenges encountered in both\npaths, many researchers believe that large language models (LLMs) with powerful\nreasoning capabilities and extensive knowledge understanding may be the\nsolution, expecting LLMs to provide AD systems with deeper levels of\nunderstanding and decision-making capabilities. In light of the challenges\nfaced by both paths, many researchers believe that LLMs, with their powerful\nreasoning abilities and extensive knowledge, could offer a solution. To\nunderstand if LLMs could enhance AD, this paper conducts a thorough analysis of\nthe potential applications of LLMs in AD systems, including exploring their\noptimization strategies in both modular and end-to-end approaches, with a\nparticular focus on how LLMs can tackle the problems and challenges present in\ncurrent solutions. Furthermore, we discuss an important question: Can LLM-based\nartificial general intelligence (AGI) be a key to achieve high-level AD? We\nfurther analyze the potential limitations and challenges that LLMs may\nencounter in promoting the development of AD technology."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhu"
                    },
                    {
                        "name": "Shiyi Wang"
                    },
                    {
                        "name": "Wenqing Zhong"
                    },
                    {
                        "name": "Nianchen Shen"
                    },
                    {
                        "name": "Yunqi Li"
                    },
                    {
                        "name": "Siqi Wang"
                    },
                    {
                        "name": "Zhiheng Li"
                    },
                    {
                        "name": "Cathy Wu"
                    },
                    {
                        "name": "Zhengbing He"
                    },
                    {
                        "name": "Li Li"
                    }
                ],
                "author_detail": {
                    "name": "Li Li"
                },
                "author": "Li Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14165v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14165v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11796v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11796v3",
                "updated": "2024-11-30T22:01:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    30,
                    22,
                    1,
                    7,
                    5,
                    335,
                    0
                ],
                "published": "2024-08-21T17:38:48Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    38,
                    48,
                    2,
                    234,
                    0
                ],
                "title": "LLM Pruning and Distillation in Practice: The Minitron Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Pruning and Distillation in Practice: The Minitron Approach"
                },
                "summary": "We present a comprehensive report on compressing the Llama 3.1 8B and Mistral\nNeMo 12B models to 4B and 8B parameters, respectively, using pruning and\ndistillation. We explore two distinct pruning strategies: (1) depth pruning and\n(2) joint hidden/attention/MLP (width) pruning, and evaluate the results on\ncommon benchmarks from the LM Evaluation Harness. The models are then aligned\nwith NeMo Aligner and tested in instruct-tuned versions. This approach produces\na compelling 4B model from Llama 3.1 8B and a state-of-the-art\nMistral-NeMo-Minitron-8B (MN-Minitron-8B for brevity) model from Mistral NeMo\n12B. We found that with no access to the original data, it is beneficial to\nslightly fine-tune teacher models on the distillation dataset. We open-source\nour base model weights on Hugging Face with a permissive license.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a comprehensive report on compressing the Llama 3.1 8B and Mistral\nNeMo 12B models to 4B and 8B parameters, respectively, using pruning and\ndistillation. We explore two distinct pruning strategies: (1) depth pruning and\n(2) joint hidden/attention/MLP (width) pruning, and evaluate the results on\ncommon benchmarks from the LM Evaluation Harness. The models are then aligned\nwith NeMo Aligner and tested in instruct-tuned versions. This approach produces\na compelling 4B model from Llama 3.1 8B and a state-of-the-art\nMistral-NeMo-Minitron-8B (MN-Minitron-8B for brevity) model from Mistral NeMo\n12B. We found that with no access to the original data, it is beneficial to\nslightly fine-tune teacher models on the distillation dataset. We open-source\nour base model weights on Hugging Face with a permissive license."
                },
                "authors": [
                    {
                        "name": "Sharath Turuvekere Sreenivas"
                    },
                    {
                        "name": "Saurav Muralidharan"
                    },
                    {
                        "name": "Raviraj Joshi"
                    },
                    {
                        "name": "Marcin Chochowski"
                    },
                    {
                        "name": "Mostofa Patwary"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Mohammad Shoeybi"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Ameya Sunil Mahabaleshwarkar"
                    },
                    {
                        "name": "Gerald Shen"
                    },
                    {
                        "name": "Jiaqi Zeng"
                    },
                    {
                        "name": "Oleksii Kuchaiev"
                    },
                    {
                        "name": "Zijia Chen"
                    },
                    {
                        "name": "Yoshi Suhara"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Chenhan Yu"
                    },
                    {
                        "name": "Wei-Chun Chen"
                    },
                    {
                        "name": "Hayley Ross"
                    },
                    {
                        "name": "Daniel Korzekwa"
                    },
                    {
                        "name": "Oluwatobi Olabiyi"
                    },
                    {
                        "name": "Ashwath Aithal"
                    },
                    {
                        "name": "Bryan Catanzaro"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Catanzaro"
                },
                "author": "Bryan Catanzaro",
                "arxiv_comment": "v3: Update author list, other changes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11796v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11796v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02532v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02532v3",
                "updated": "2024-11-30T21:33:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    30,
                    21,
                    33,
                    59,
                    5,
                    335,
                    0
                ],
                "published": "2024-06-04T17:53:36Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    53,
                    36,
                    1,
                    156,
                    0
                ],
                "title": "SpecExec: Massively Parallel Speculative Decoding for Interactive LLM\n  Inference on Consumer Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecExec: Massively Parallel Speculative Decoding for Interactive LLM\n  Inference on Consumer Devices"
                },
                "summary": "As large language models gain widespread adoption, running them efficiently\nbecomes crucial. Recent works on LLM inference use speculative decoding to\nachieve extreme speedups. However, most of these works implicitly design their\nalgorithms for high-end datacenter hardware. In this work, we ask the opposite\nquestion: how fast can we run LLMs on consumer machines? Consumer GPUs can no\nlonger fit the largest available models (50B+ parameters) and must offload them\nto RAM or SSD. When running with offloaded parameters, the inference engine can\nprocess batches of hundreds or thousands of tokens at the same time as just one\ntoken, making it a natural fit for speculative decoding. We propose SpecExec\n(Speculative Execution), a simple parallel decoding method that can generate up\nto 20 tokens per target model iteration for popular LLM families. It utilizes\nthe high spikiness of the token probabilities distribution in modern LLMs and a\nhigh degree of alignment between model output probabilities. SpecExec takes the\nmost probable tokens continuation from the draft model to build a \"cache\" tree\nfor the target model, which then gets validated in a single pass. Using\nSpecExec, we demonstrate inference of 50B+ parameter LLMs on consumer GPUs with\nRAM offloading at 4-6 tokens per second with 4-bit quantization or 2-3 tokens\nper second with 16-bit weights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models gain widespread adoption, running them efficiently\nbecomes crucial. Recent works on LLM inference use speculative decoding to\nachieve extreme speedups. However, most of these works implicitly design their\nalgorithms for high-end datacenter hardware. In this work, we ask the opposite\nquestion: how fast can we run LLMs on consumer machines? Consumer GPUs can no\nlonger fit the largest available models (50B+ parameters) and must offload them\nto RAM or SSD. When running with offloaded parameters, the inference engine can\nprocess batches of hundreds or thousands of tokens at the same time as just one\ntoken, making it a natural fit for speculative decoding. We propose SpecExec\n(Speculative Execution), a simple parallel decoding method that can generate up\nto 20 tokens per target model iteration for popular LLM families. It utilizes\nthe high spikiness of the token probabilities distribution in modern LLMs and a\nhigh degree of alignment between model output probabilities. SpecExec takes the\nmost probable tokens continuation from the draft model to build a \"cache\" tree\nfor the target model, which then gets validated in a single pass. Using\nSpecExec, we demonstrate inference of 50B+ parameter LLMs on consumer GPUs with\nRAM offloading at 4-6 tokens per second with 4-bit quantization or 2-3 tokens\nper second with 16-bit weights."
                },
                "authors": [
                    {
                        "name": "Ruslan Svirschevski"
                    },
                    {
                        "name": "Avner May"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Max Ryabinin"
                    }
                ],
                "author_detail": {
                    "name": "Max Ryabinin"
                },
                "author": "Max Ryabinin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02532v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02532v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10492v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10492v2",
                "updated": "2024-11-30T21:27:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    30,
                    21,
                    27,
                    19,
                    5,
                    335,
                    0
                ],
                "published": "2024-06-15T04:09:31Z",
                "published_parsed": [
                    2024,
                    6,
                    15,
                    4,
                    9,
                    31,
                    5,
                    167,
                    0
                ],
                "title": "Large Language Models as Interpolated and Extrapolated Event Predictors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models as Interpolated and Extrapolated Event Predictors"
                },
                "summary": "Salient facts of sociopolitical events are distilled into quadruples\nfollowing a format of subject, relation, object, and timestamp. Machine\nlearning methods, such as graph neural networks (GNNs) and recurrent neural\nnetworks (RNNs), have been built to make predictions and infer relations on the\nquadruple-based knowledge graphs (KGs). In many applications, quadruples are\nextended to quintuples with auxiliary attributes such as text summaries that\ndescribe the quadruple events. In this paper, we comprehensively investigate\nhow large language models (LLMs) streamline the design of event prediction\nframeworks using quadruple-based or quintuple-based data while maintaining\ncompetitive accuracy. We propose LEAP, a unified framework that leverages large\nlanguage models as event predictors. Specifically, we develop multiple prompt\ntemplates to frame the object prediction (OP) task as a standard\nquestion-answering (QA) task, suitable for instruction fine-tuning with an\nencoder-decoder LLM. For multi-event forecasting (MEF) task, we design a simple\nyet effective prompt template for each event quintuple. This novel approach\nremoves the need for GNNs and RNNs, instead utilizing an encoder-only LLM to\ngenerate fixed intermediate embeddings, which are processed by a customized\ndownstream head with a self-attention mechanism to predict potential relation\noccurrences in the future. Extensive experiments on multiple real-world\ndatasets using various evaluation metrics validate the effectiveness of our\napproach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Salient facts of sociopolitical events are distilled into quadruples\nfollowing a format of subject, relation, object, and timestamp. Machine\nlearning methods, such as graph neural networks (GNNs) and recurrent neural\nnetworks (RNNs), have been built to make predictions and infer relations on the\nquadruple-based knowledge graphs (KGs). In many applications, quadruples are\nextended to quintuples with auxiliary attributes such as text summaries that\ndescribe the quadruple events. In this paper, we comprehensively investigate\nhow large language models (LLMs) streamline the design of event prediction\nframeworks using quadruple-based or quintuple-based data while maintaining\ncompetitive accuracy. We propose LEAP, a unified framework that leverages large\nlanguage models as event predictors. Specifically, we develop multiple prompt\ntemplates to frame the object prediction (OP) task as a standard\nquestion-answering (QA) task, suitable for instruction fine-tuning with an\nencoder-decoder LLM. For multi-event forecasting (MEF) task, we design a simple\nyet effective prompt template for each event quintuple. This novel approach\nremoves the need for GNNs and RNNs, instead utilizing an encoder-only LLM to\ngenerate fixed intermediate embeddings, which are processed by a customized\ndownstream head with a self-attention mechanism to predict potential relation\noccurrences in the future. Extensive experiments on multiple real-world\ndatasets using various evaluation metrics validate the effectiveness of our\napproach."
                },
                "authors": [
                    {
                        "name": "Libo Zhang"
                    },
                    {
                        "name": "Yue Ning"
                    }
                ],
                "author_detail": {
                    "name": "Yue Ning"
                },
                "author": "Yue Ning",
                "arxiv_comment": "11 pages, 3 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10492v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10492v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.00378v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.00378v5",
                "updated": "2024-11-30T19:08:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    30,
                    19,
                    8,
                    48,
                    5,
                    335,
                    0
                ],
                "published": "2023-09-01T10:27:04Z",
                "published_parsed": [
                    2023,
                    9,
                    1,
                    10,
                    27,
                    4,
                    4,
                    244,
                    0
                ],
                "title": "Long-Term Ad Memorability: Understanding & Generating Memorable Ads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Term Ad Memorability: Understanding & Generating Memorable Ads"
                },
                "summary": "Despite the importance of long-term memory in marketing and brand building,\nuntil now, there has been no large-scale study on the memorability of ads. All\nprevious memorability studies have been conducted on short-term recall on\nspecific content types like action videos. On the other hand, long-term\nmemorability is crucial for the advertising industry, and ads are almost always\nhighly multimodal. Therefore, we release the first memorability dataset,\nLAMBDA, consisting of 1749 participants and 2205 ads covering 276 brands.\nRunning statistical tests over different participant subpopulations and ad\ntypes, we find many interesting insights into what makes an ad memorable, e.g.,\nfast-moving ads are more memorable than those with slower scenes; people who\nuse ad-blockers remember a lower number of ads than those who don't. Next, we\npresent a model, Henry, to predict the memorability of a content. Henry\nachieves state-of-the-art performance across all prominent literature\nmemorability datasets. It shows strong generalization performance with better\nresults in 0-shot on unseen datasets. Finally, with the intent of memorable ad\ngeneration, we present a scalable method to build a high-quality memorable ad\ngeneration model by leveraging automatically annotated data. Our approach, SEED\n(Self rEwarding mEmorability Modeling), starts with a language model trained on\nLAMBDA as seed data and progressively trains an LLM to generate more memorable\nads. We show that the generated advertisements have 44% higher memorability\nscores than the original ads. We release this large-scale ad dataset,\nUltraLAMBDA, consisting of 5 million ads. Our code and the datasets, LAMBDA and\nUltraLAMBDA, are open-sourced at\nhttps://behavior-in-the-wild.github.io/memorability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the importance of long-term memory in marketing and brand building,\nuntil now, there has been no large-scale study on the memorability of ads. All\nprevious memorability studies have been conducted on short-term recall on\nspecific content types like action videos. On the other hand, long-term\nmemorability is crucial for the advertising industry, and ads are almost always\nhighly multimodal. Therefore, we release the first memorability dataset,\nLAMBDA, consisting of 1749 participants and 2205 ads covering 276 brands.\nRunning statistical tests over different participant subpopulations and ad\ntypes, we find many interesting insights into what makes an ad memorable, e.g.,\nfast-moving ads are more memorable than those with slower scenes; people who\nuse ad-blockers remember a lower number of ads than those who don't. Next, we\npresent a model, Henry, to predict the memorability of a content. Henry\nachieves state-of-the-art performance across all prominent literature\nmemorability datasets. It shows strong generalization performance with better\nresults in 0-shot on unseen datasets. Finally, with the intent of memorable ad\ngeneration, we present a scalable method to build a high-quality memorable ad\ngeneration model by leveraging automatically annotated data. Our approach, SEED\n(Self rEwarding mEmorability Modeling), starts with a language model trained on\nLAMBDA as seed data and progressively trains an LLM to generate more memorable\nads. We show that the generated advertisements have 44% higher memorability\nscores than the original ads. We release this large-scale ad dataset,\nUltraLAMBDA, consisting of 5 million ads. Our code and the datasets, LAMBDA and\nUltraLAMBDA, are open-sourced at\nhttps://behavior-in-the-wild.github.io/memorability."
                },
                "authors": [
                    {
                        "name": "Harini SI"
                    },
                    {
                        "name": "Somesh Singh"
                    },
                    {
                        "name": "Yaman K Singla"
                    },
                    {
                        "name": "Aanisha Bhattacharyya"
                    },
                    {
                        "name": "Veeky Baths"
                    },
                    {
                        "name": "Changyou Chen"
                    },
                    {
                        "name": "Rajiv Ratn Shah"
                    },
                    {
                        "name": "Balaji Krishnamurthy"
                    }
                ],
                "author_detail": {
                    "name": "Balaji Krishnamurthy"
                },
                "author": "Balaji Krishnamurthy",
                "arxiv_comment": "Published in WACV-2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.00378v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.00378v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05168v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05168v3",
                "updated": "2024-11-30T16:10:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    30,
                    16,
                    10,
                    21,
                    5,
                    335,
                    0
                ],
                "published": "2024-10-07T16:25:39Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    16,
                    25,
                    39,
                    0,
                    281,
                    0
                ],
                "title": "ReasoningRank: Teaching Student Models to Rank through Reasoning-Based\n  Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReasoningRank: Teaching Student Models to Rank through Reasoning-Based\n  Knowledge Distillation"
                },
                "summary": "Reranking documents based on their relevance to a given query is a critical\ntask in information retrieval. Traditional reranking methods often lack\ntransparency and rely on proprietary models, hindering reproducibility and\ninterpretability. We propose Reason-to-Rank (R2R), a novel open-source\nreranking approach that enhances transparency by generating two types of\nreasoning: direct relevance reasoning, which explains how a document addresses\nthe query, and comparison reasoning, which justifies the relevance of one\ndocument over another. We leverage large language models (LLMs) as teacher\nmodels to generate these explanations and distill this knowledge into smaller,\nopenly available student models. Our student models are trained to generate\nmeaningful reasoning and rerank documents, achieving competitive performance\nacross multiple datasets, including MSMARCO and BRIGHT. Experiments demonstrate\nthat R2R not only improves reranking accuracy but also provides valuable\ninsights into the decision-making process. By offering a structured and\ninterpretable solution with openly accessible resources, R2R aims to bridge the\ngap between effectiveness and transparency in information retrieval, fostering\nreproducibility and further research in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reranking documents based on their relevance to a given query is a critical\ntask in information retrieval. Traditional reranking methods often lack\ntransparency and rely on proprietary models, hindering reproducibility and\ninterpretability. We propose Reason-to-Rank (R2R), a novel open-source\nreranking approach that enhances transparency by generating two types of\nreasoning: direct relevance reasoning, which explains how a document addresses\nthe query, and comparison reasoning, which justifies the relevance of one\ndocument over another. We leverage large language models (LLMs) as teacher\nmodels to generate these explanations and distill this knowledge into smaller,\nopenly available student models. Our student models are trained to generate\nmeaningful reasoning and rerank documents, achieving competitive performance\nacross multiple datasets, including MSMARCO and BRIGHT. Experiments demonstrate\nthat R2R not only improves reranking accuracy but also provides valuable\ninsights into the decision-making process. By offering a structured and\ninterpretable solution with openly accessible resources, R2R aims to bridge the\ngap between effectiveness and transparency in information retrieval, fostering\nreproducibility and further research in the field."
                },
                "authors": [
                    {
                        "name": "Yuelyu Ji"
                    },
                    {
                        "name": "Zhuochun Li"
                    },
                    {
                        "name": "Rui Meng"
                    },
                    {
                        "name": "Daqing He"
                    }
                ],
                "author_detail": {
                    "name": "Daqing He"
                },
                "author": "Daqing He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05168v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05168v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02224v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02224v3",
                "updated": "2024-11-30T14:27:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    30,
                    14,
                    27,
                    59,
                    5,
                    335,
                    0
                ],
                "published": "2024-06-04T11:36:09Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    11,
                    36,
                    9,
                    1,
                    156,
                    0
                ],
                "title": "FedMKT: Federated Mutual Knowledge Transfer for Large and Small Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedMKT: Federated Mutual Knowledge Transfer for Large and Small Language\n  Models"
                },
                "summary": "Recent research in federated large language models (LLMs) has primarily\nfocused on enabling clients to fine-tune their locally deployed homogeneous\nLLMs collaboratively or on transferring knowledge from server-based LLMs to\nsmall language models (SLMs) at downstream clients. However, a significant gap\nremains in the simultaneous mutual enhancement of both the server's LLM and\nclients' SLMs. To bridge this gap, we propose FedMKT, a parameter-efficient\nfederated mutual knowledge transfer framework for large and small language\nmodels. This framework is designed to adaptively transfer knowledge from the\nserver's LLM to clients' SLMs while concurrently enriching the LLM with\nclients' unique domain insights. We facilitate token alignment using minimum\nedit distance (MinED) and then selective mutual knowledge transfer between\nclient-side SLMs and a server-side LLM, aiming to collectively enhance their\nperformance. Through extensive experiments across three distinct scenarios, we\nevaluate the effectiveness of FedMKT using various public LLMs and SLMs on a\nrange of NLP text generation tasks. Empirical results demonstrate that FedMKT\nsimultaneously boosts the performance of both LLMs and SLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research in federated large language models (LLMs) has primarily\nfocused on enabling clients to fine-tune their locally deployed homogeneous\nLLMs collaboratively or on transferring knowledge from server-based LLMs to\nsmall language models (SLMs) at downstream clients. However, a significant gap\nremains in the simultaneous mutual enhancement of both the server's LLM and\nclients' SLMs. To bridge this gap, we propose FedMKT, a parameter-efficient\nfederated mutual knowledge transfer framework for large and small language\nmodels. This framework is designed to adaptively transfer knowledge from the\nserver's LLM to clients' SLMs while concurrently enriching the LLM with\nclients' unique domain insights. We facilitate token alignment using minimum\nedit distance (MinED) and then selective mutual knowledge transfer between\nclient-side SLMs and a server-side LLM, aiming to collectively enhance their\nperformance. Through extensive experiments across three distinct scenarios, we\nevaluate the effectiveness of FedMKT using various public LLMs and SLMs on a\nrange of NLP text generation tasks. Empirical results demonstrate that FedMKT\nsimultaneously boosts the performance of both LLMs and SLMs."
                },
                "authors": [
                    {
                        "name": "Tao Fan"
                    },
                    {
                        "name": "Guoqiang Ma"
                    },
                    {
                        "name": "Yan Kang"
                    },
                    {
                        "name": "Hanlin Gu"
                    },
                    {
                        "name": "Yuanfeng Song"
                    },
                    {
                        "name": "Lixin Fan"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Qiang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Yang"
                },
                "author": "Qiang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02224v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02224v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00177v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00177v3",
                "updated": "2024-11-30T14:01:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    30,
                    14,
                    1,
                    56,
                    5,
                    335,
                    0
                ],
                "published": "2024-10-31T19:48:12Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    19,
                    48,
                    12,
                    3,
                    305,
                    0
                ],
                "title": "LLM4Mat-Bench: Benchmarking Large Language Models for Materials Property\n  Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4Mat-Bench: Benchmarking Large Language Models for Materials Property\n  Prediction"
                },
                "summary": "Large language models (LLMs) are increasingly being used in materials\nscience. However, little attention has been given to benchmarking and\nstandardized evaluation for LLM-based materials property prediction, which\nhinders progress. We present LLM4Mat-Bench, the largest benchmark to date for\nevaluating the performance of LLMs in predicting the properties of crystalline\nmaterials. LLM4Mat-Bench contains about 1.9M crystal structures in total,\ncollected from 10 publicly available materials data sources, and 45 distinct\nproperties. LLM4Mat-Bench features different input modalities: crystal\ncomposition, CIF, and crystal text description, with 4.7M, 615.5M, and 3.1B\ntokens in total for each modality, respectively. We use LLM4Mat-Bench to\nfine-tune models with different sizes, including LLM-Prop and MatBERT, and\nprovide zero-shot and few-shot prompts to evaluate the property prediction\ncapabilities of LLM-chat-like models, including Llama, Gemma, and Mistral. The\nresults highlight the challenges of general-purpose LLMs in materials science\nand the need for task-specific predictive models and task-specific\ninstruction-tuned LLMs in materials property prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being used in materials\nscience. However, little attention has been given to benchmarking and\nstandardized evaluation for LLM-based materials property prediction, which\nhinders progress. We present LLM4Mat-Bench, the largest benchmark to date for\nevaluating the performance of LLMs in predicting the properties of crystalline\nmaterials. LLM4Mat-Bench contains about 1.9M crystal structures in total,\ncollected from 10 publicly available materials data sources, and 45 distinct\nproperties. LLM4Mat-Bench features different input modalities: crystal\ncomposition, CIF, and crystal text description, with 4.7M, 615.5M, and 3.1B\ntokens in total for each modality, respectively. We use LLM4Mat-Bench to\nfine-tune models with different sizes, including LLM-Prop and MatBERT, and\nprovide zero-shot and few-shot prompts to evaluate the property prediction\ncapabilities of LLM-chat-like models, including Llama, Gemma, and Mistral. The\nresults highlight the challenges of general-purpose LLMs in materials science\nand the need for task-specific predictive models and task-specific\ninstruction-tuned LLMs in materials property prediction."
                },
                "authors": [
                    {
                        "name": "Andre Niyongabo Rubungo"
                    },
                    {
                        "name": "Kangming Li"
                    },
                    {
                        "name": "Jason Hattrick-Simpers"
                    },
                    {
                        "name": "Adji Bousso Dieng"
                    }
                ],
                "author_detail": {
                    "name": "Adji Bousso Dieng"
                },
                "author": "Adji Bousso Dieng",
                "arxiv_comment": "Accepted at NeurIPS 2024-AI4Mat Workshop. The Benchmark and code can\n  be found at https://github.com/vertaix/LLM4Mat-Bench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00177v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00177v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02953v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02953v3",
                "updated": "2024-11-30T12:16:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    30,
                    12,
                    16,
                    51,
                    5,
                    335,
                    0
                ],
                "published": "2024-10-03T19:53:47Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    19,
                    53,
                    47,
                    3,
                    277,
                    0
                ],
                "title": "Unlocking Structured Thinking in Language Models with Cognitive\n  Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking Structured Thinking in Language Models with Cognitive\n  Prompting"
                },
                "summary": "We propose cognitive prompting as a novel approach to guide problem-solving\nin large language models (LLMs) through structured, human-like cognitive\noperations, such as goal clarification, decomposition, filtering, abstraction,\nand pattern recognition. By employing systematic, step-by-step reasoning,\ncognitive prompting enables LLMs to tackle complex, multi-step tasks more\nefficiently. We introduce three variants: a deterministic sequence of cognitive\noperations, a self-adaptive variant in which the LLM dynamically selects the\nsequence of cognitive operations, and a hybrid variant that uses generated\ncorrect solutions as few-shot chain-of-thought prompts. Experiments with LLaMA,\nGemma~2, and Qwen models in each two sizes on the arithmetic reasoning\nbenchmark GSM8K demonstrate that cognitive prompting significantly improves\nperformance compared to standard question answering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose cognitive prompting as a novel approach to guide problem-solving\nin large language models (LLMs) through structured, human-like cognitive\noperations, such as goal clarification, decomposition, filtering, abstraction,\nand pattern recognition. By employing systematic, step-by-step reasoning,\ncognitive prompting enables LLMs to tackle complex, multi-step tasks more\nefficiently. We introduce three variants: a deterministic sequence of cognitive\noperations, a self-adaptive variant in which the LLM dynamically selects the\nsequence of cognitive operations, and a hybrid variant that uses generated\ncorrect solutions as few-shot chain-of-thought prompts. Experiments with LLaMA,\nGemma~2, and Qwen models in each two sizes on the arithmetic reasoning\nbenchmark GSM8K demonstrate that cognitive prompting significantly improves\nperformance compared to standard question answering."
                },
                "authors": [
                    {
                        "name": "Oliver Kramer"
                    },
                    {
                        "name": "Jill Baumann"
                    }
                ],
                "author_detail": {
                    "name": "Jill Baumann"
                },
                "author": "Jill Baumann",
                "arxiv_comment": "6 pages, submitted to ESANN 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02953v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02953v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17258v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17258v2",
                "updated": "2024-11-30T12:13:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    30,
                    12,
                    13,
                    1,
                    5,
                    335,
                    0
                ],
                "published": "2024-08-30T12:56:17Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    12,
                    56,
                    17,
                    4,
                    243,
                    0
                ],
                "title": "Joint Estimation and Prediction of City-wide Delivery Demand: A Large\n  Language Model Empowered Graph-based Learning Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Estimation and Prediction of City-wide Delivery Demand: A Large\n  Language Model Empowered Graph-based Learning Approach"
                },
                "summary": "The proliferation of e-commerce and urbanization has significantly\nintensified delivery operations in urban areas, boosting the volume and\ncomplexity of delivery demand. Data-driven predictive methods, especially those\nutilizing machine learning techniques, have emerged to handle these\ncomplexities in urban delivery demand management problems. One particularly\npressing issue that has yet to be sufficiently addressed is the joint\nestimation and prediction of city-wide delivery demand, as well as the\ngeneralization of the model to new cities. To this end, we formulate this\nproblem as a transferable graph-based spatiotemporal learning task. First, an\nindividual-collective message-passing neural network model is formalized to\ncapture the interaction between demand patterns of associated regions. Second,\nby exploiting recent advances in large language models (LLMs), we extract\ngeneral geospatial knowledge encodings from the unstructured locational data\nusing the embedding generated by LLMs. Last, to encourage the cross-city\ngeneralization of the model, we integrate the encoding into the demand\npredictor in a transferable way. Comprehensive empirical evaluation results on\ntwo real-world delivery datasets, including eight cities in China and the US,\ndemonstrate that our model significantly outperforms state-of-the-art baselines\nin accuracy, efficiency, and transferability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of e-commerce and urbanization has significantly\nintensified delivery operations in urban areas, boosting the volume and\ncomplexity of delivery demand. Data-driven predictive methods, especially those\nutilizing machine learning techniques, have emerged to handle these\ncomplexities in urban delivery demand management problems. One particularly\npressing issue that has yet to be sufficiently addressed is the joint\nestimation and prediction of city-wide delivery demand, as well as the\ngeneralization of the model to new cities. To this end, we formulate this\nproblem as a transferable graph-based spatiotemporal learning task. First, an\nindividual-collective message-passing neural network model is formalized to\ncapture the interaction between demand patterns of associated regions. Second,\nby exploiting recent advances in large language models (LLMs), we extract\ngeneral geospatial knowledge encodings from the unstructured locational data\nusing the embedding generated by LLMs. Last, to encourage the cross-city\ngeneralization of the model, we integrate the encoding into the demand\npredictor in a transferable way. Comprehensive empirical evaluation results on\ntwo real-world delivery datasets, including eight cities in China and the US,\ndemonstrate that our model significantly outperforms state-of-the-art baselines\nin accuracy, efficiency, and transferability."
                },
                "authors": [
                    {
                        "name": "Tong Nie"
                    },
                    {
                        "name": "Junlin He"
                    },
                    {
                        "name": "Yuewen Mei"
                    },
                    {
                        "name": "Guoyang Qin"
                    },
                    {
                        "name": "Guilong Li"
                    },
                    {
                        "name": "Jian Sun"
                    },
                    {
                        "name": "Wei Ma"
                    }
                ],
                "author_detail": {
                    "name": "Wei Ma"
                },
                "author": "Wei Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17258v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17258v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17106v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17106v2",
                "updated": "2024-11-30T11:59:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    30,
                    11,
                    59,
                    54,
                    5,
                    335,
                    0
                ],
                "published": "2024-11-26T04:49:42Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    4,
                    49,
                    42,
                    1,
                    331,
                    0
                ],
                "title": "PassionSR: Post-Training Quantization with Adaptive Scale in One-Step\n  Diffusion based Image Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PassionSR: Post-Training Quantization with Adaptive Scale in One-Step\n  Diffusion based Image Super-Resolution"
                },
                "summary": "Diffusion-based image super-resolution (SR) models have shown superior\nperformance at the cost of multiple denoising steps. However, even though the\ndenoising step has been reduced to one, they require high computational costs\nand storage requirements, making it difficult for deployment on hardware\ndevices. To address these issues, we propose a novel post-training quantization\napproach with adaptive scale in one-step diffusion (OSD) image SR, PassionSR.\nFirst, we simplify OSD model to two core components, UNet and Variational\nAutoencoder (VAE) by removing the CLIPEncoder. Secondly, we propose Learnable\nBoundary Quantizer (LBQ) and Learnable Equivalent Transformation (LET) to\noptimize the quantization process and manipulate activation distributions for\nbetter quantization. Finally, we design a Distributed Quantization Calibration\n(DQC) strategy that stabilizes the training of quantized parameters for rapid\nconvergence. Comprehensive experiments demonstrate that PassionSR with 8-bit\nand 6-bit obtains comparable visual results with full-precision model.\nMoreover, our PassionSR achieves significant advantages over recent leading\nlow-bit quantization methods for image SR. Our code will be at\nhttps://github.com/libozhu03/PassionSR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based image super-resolution (SR) models have shown superior\nperformance at the cost of multiple denoising steps. However, even though the\ndenoising step has been reduced to one, they require high computational costs\nand storage requirements, making it difficult for deployment on hardware\ndevices. To address these issues, we propose a novel post-training quantization\napproach with adaptive scale in one-step diffusion (OSD) image SR, PassionSR.\nFirst, we simplify OSD model to two core components, UNet and Variational\nAutoencoder (VAE) by removing the CLIPEncoder. Secondly, we propose Learnable\nBoundary Quantizer (LBQ) and Learnable Equivalent Transformation (LET) to\noptimize the quantization process and manipulate activation distributions for\nbetter quantization. Finally, we design a Distributed Quantization Calibration\n(DQC) strategy that stabilizes the training of quantized parameters for rapid\nconvergence. Comprehensive experiments demonstrate that PassionSR with 8-bit\nand 6-bit obtains comparable visual results with full-precision model.\nMoreover, our PassionSR achieves significant advantages over recent leading\nlow-bit quantization methods for image SR. Our code will be at\nhttps://github.com/libozhu03/PassionSR."
                },
                "authors": [
                    {
                        "name": "Libo Zhu"
                    },
                    {
                        "name": "Jianze Li"
                    },
                    {
                        "name": "Haotong Qin"
                    },
                    {
                        "name": "Wenbo Li"
                    },
                    {
                        "name": "Yulun Zhang"
                    },
                    {
                        "name": "Yong Guo"
                    },
                    {
                        "name": "Xiaokang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaokang Yang"
                },
                "author": "Xiaokang Yang",
                "arxiv_comment": "https://github.com/libozhu03/PassionSR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17106v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17106v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03845v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03845v2",
                "updated": "2024-11-30T11:19:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    30,
                    11,
                    19,
                    39,
                    5,
                    335,
                    0
                ],
                "published": "2024-10-04T18:22:58Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    18,
                    22,
                    58,
                    4,
                    278,
                    0
                ],
                "title": "ORAssistant: A Custom RAG-based Conversational Assistant for OpenROAD",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ORAssistant: A Custom RAG-based Conversational Assistant for OpenROAD"
                },
                "summary": "Open-source Electronic Design Automation (EDA) tools are rapidly transforming\nchip design by addressing key barriers of commercial EDA tools such as\ncomplexity, costs, and access. Recent advancements in Large Language Models\n(LLMs) have further enhanced efficiency in chip design by providing user\nassistance across a range of tasks like setup, decision-making, and flow\nautomation. This paper introduces ORAssistant, a conversational assistant for\nOpenROAD, based on Retrieval-Augmented Generation (RAG). ORAssistant aims to\nimprove the user experience for the OpenROAD flow, from RTL-GDSII by providing\ncontext-specific responses to common user queries, including installation,\ncommand usage, flow setup, and execution, in prose format. Currently,\nORAssistant integrates OpenROAD, OpenROAD-flow-scripts, Yosys, OpenSTA, and\nKLayout. The data model is built from publicly available documentation and\nGitHub resources. The proposed architecture is scalable, supporting extensions\nto other open-source tools, operating modes, and LLM models. We use Google\nGemini as the base LLM model to build and test ORAssistant. Early evaluation\nresults of the RAG-based model show notable improvements in performance and\naccuracy compared to non-fine-tuned LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-source Electronic Design Automation (EDA) tools are rapidly transforming\nchip design by addressing key barriers of commercial EDA tools such as\ncomplexity, costs, and access. Recent advancements in Large Language Models\n(LLMs) have further enhanced efficiency in chip design by providing user\nassistance across a range of tasks like setup, decision-making, and flow\nautomation. This paper introduces ORAssistant, a conversational assistant for\nOpenROAD, based on Retrieval-Augmented Generation (RAG). ORAssistant aims to\nimprove the user experience for the OpenROAD flow, from RTL-GDSII by providing\ncontext-specific responses to common user queries, including installation,\ncommand usage, flow setup, and execution, in prose format. Currently,\nORAssistant integrates OpenROAD, OpenROAD-flow-scripts, Yosys, OpenSTA, and\nKLayout. The data model is built from publicly available documentation and\nGitHub resources. The proposed architecture is scalable, supporting extensions\nto other open-source tools, operating modes, and LLM models. We use Google\nGemini as the base LLM model to build and test ORAssistant. Early evaluation\nresults of the RAG-based model show notable improvements in performance and\naccuracy compared to non-fine-tuned LLMs."
                },
                "authors": [
                    {
                        "name": "Aviral Kaintura"
                    },
                    {
                        "name": "Palaniappan R"
                    },
                    {
                        "name": "Shui Song Luar"
                    },
                    {
                        "name": "Indira Iyer Almeida"
                    }
                ],
                "author_detail": {
                    "name": "Indira Iyer Almeida"
                },
                "author": "Indira Iyer Almeida",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03845v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03845v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.08074v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.08074v3",
                "updated": "2024-11-30T10:48:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    30,
                    10,
                    48,
                    21,
                    5,
                    335,
                    0
                ],
                "published": "2024-06-12T10:48:53Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    10,
                    48,
                    53,
                    2,
                    164,
                    0
                ],
                "title": "A Concept-Based Explainability Framework for Large Multimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Concept-Based Explainability Framework for Large Multimodal Models"
                },
                "summary": "Large multimodal models (LMMs) combine unimodal encoders and large language\nmodels (LLMs) to perform multimodal tasks. Despite recent advancements towards\nthe interpretability of these models, understanding internal representations of\nLMMs remains largely a mystery. In this paper, we present a novel framework for\nthe interpretation of LMMs. We propose a dictionary learning based approach,\napplied to the representation of tokens. The elements of the learned dictionary\ncorrespond to our proposed concepts. We show that these concepts are well\nsemantically grounded in both vision and text. Thus we refer to these as\n``multi-modal concepts''. We qualitatively and quantitatively evaluate the\nresults of the learnt concepts. We show that the extracted multimodal concepts\nare useful to interpret representations of test samples. Finally, we evaluate\nthe disentanglement between different concepts and the quality of grounding\nconcepts visually and textually. Our code is publicly available at\nhttps://github.com/mshukor/xl-vlms",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large multimodal models (LMMs) combine unimodal encoders and large language\nmodels (LLMs) to perform multimodal tasks. Despite recent advancements towards\nthe interpretability of these models, understanding internal representations of\nLMMs remains largely a mystery. In this paper, we present a novel framework for\nthe interpretation of LMMs. We propose a dictionary learning based approach,\napplied to the representation of tokens. The elements of the learned dictionary\ncorrespond to our proposed concepts. We show that these concepts are well\nsemantically grounded in both vision and text. Thus we refer to these as\n``multi-modal concepts''. We qualitatively and quantitatively evaluate the\nresults of the learnt concepts. We show that the extracted multimodal concepts\nare useful to interpret representations of test samples. Finally, we evaluate\nthe disentanglement between different concepts and the quality of grounding\nconcepts visually and textually. Our code is publicly available at\nhttps://github.com/mshukor/xl-vlms"
                },
                "authors": [
                    {
                        "name": "Jayneel Parekh"
                    },
                    {
                        "name": "Pegah Khayatan"
                    },
                    {
                        "name": "Mustafa Shukor"
                    },
                    {
                        "name": "Alasdair Newson"
                    },
                    {
                        "name": "Matthieu Cord"
                    }
                ],
                "author_detail": {
                    "name": "Matthieu Cord"
                },
                "author": "Matthieu Cord",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.08074v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.08074v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15380v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15380v2",
                "updated": "2024-11-30T09:57:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    30,
                    9,
                    57,
                    9,
                    5,
                    335,
                    0
                ],
                "published": "2024-09-20T15:01:21Z",
                "published_parsed": [
                    2024,
                    9,
                    20,
                    15,
                    1,
                    21,
                    4,
                    264,
                    0
                ],
                "title": "Kalahi: A handcrafted, grassroots cultural LLM evaluation suite for\n  Filipino",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kalahi: A handcrafted, grassroots cultural LLM evaluation suite for\n  Filipino"
                },
                "summary": "Multilingual large language models (LLMs) today may not necessarily provide\nculturally appropriate and relevant responses to its Filipino users. We\nintroduce Kalahi, a cultural LLM evaluation suite collaboratively created by\nnative Filipino speakers. It is composed of 150 high-quality, handcrafted and\nnuanced prompts that test LLMs for generations that are relevant to shared\nFilipino cultural knowledge and values. Strong LLM performance in Kalahi\nindicates a model's ability to generate responses similar to what an average\nFilipino would say or do in a given situation. We conducted experiments on LLMs\nwith multilingual and Filipino language support. Results show that Kalahi,\nwhile trivial for Filipinos, is challenging for LLMs, with the best model\nanswering only 46.0% of the questions correctly compared to native Filipino\nperformance of 89.10%. Thus, Kalahi can be used to accurately and reliably\nevaluate Filipino cultural representation in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual large language models (LLMs) today may not necessarily provide\nculturally appropriate and relevant responses to its Filipino users. We\nintroduce Kalahi, a cultural LLM evaluation suite collaboratively created by\nnative Filipino speakers. It is composed of 150 high-quality, handcrafted and\nnuanced prompts that test LLMs for generations that are relevant to shared\nFilipino cultural knowledge and values. Strong LLM performance in Kalahi\nindicates a model's ability to generate responses similar to what an average\nFilipino would say or do in a given situation. We conducted experiments on LLMs\nwith multilingual and Filipino language support. Results show that Kalahi,\nwhile trivial for Filipinos, is challenging for LLMs, with the best model\nanswering only 46.0% of the questions correctly compared to native Filipino\nperformance of 89.10%. Thus, Kalahi can be used to accurately and reliably\nevaluate Filipino cultural representation in LLMs."
                },
                "authors": [
                    {
                        "name": "Jann Railey Montalan"
                    },
                    {
                        "name": "Jian Gang Ngui"
                    },
                    {
                        "name": "Wei Qi Leong"
                    },
                    {
                        "name": "Yosephine Susanto"
                    },
                    {
                        "name": "Hamsawardhini Rengarajan"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    },
                    {
                        "name": "William Chandra Tjhi"
                    }
                ],
                "author_detail": {
                    "name": "William Chandra Tjhi"
                },
                "author": "William Chandra Tjhi",
                "arxiv_comment": "Accepted for presentation at Paclic 38, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15380v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15380v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12038v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12038v5",
                "updated": "2024-11-30T08:52:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    30,
                    8,
                    52,
                    29,
                    5,
                    335,
                    0
                ],
                "published": "2024-04-18T09:46:25Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    9,
                    46,
                    25,
                    3,
                    109,
                    0
                ],
                "title": "Uncovering Safety Risks of Large Language Models through Concept\n  Activation Vector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering Safety Risks of Large Language Models through Concept\n  Activation Vector"
                },
                "summary": "Despite careful safety alignment, current large language models (LLMs) remain\nvulnerable to various attacks. To further unveil the safety risks of LLMs, we\nintroduce a Safety Concept Activation Vector (SCAV) framework, which\neffectively guides the attacks by accurately interpreting LLMs' safety\nmechanisms. We then develop an SCAV-guided attack method that can generate both\nattack prompts and embedding-level attacks with automatically selected\nperturbation hyperparameters. Both automatic and human evaluations demonstrate\nthat our attack method significantly improves the attack success rate and\nresponse quality while requiring less training data. Additionally, we find that\nour generated attack prompts may be transferable to GPT-4, and the\nembedding-level attacks may also be transferred to other white-box LLMs whose\nparameters are known. Our experiments further uncover the safety risks present\nin current LLMs. For example, in our evaluation of seven open-source LLMs, we\nobserve an average attack success rate of 99.14%, based on the classic\nkeyword-matching criterion. Finally, we provide insights into the safety\nmechanism of LLMs. The code is available at\nhttps://github.com/SproutNan/AI-Safety_SCAV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite careful safety alignment, current large language models (LLMs) remain\nvulnerable to various attacks. To further unveil the safety risks of LLMs, we\nintroduce a Safety Concept Activation Vector (SCAV) framework, which\neffectively guides the attacks by accurately interpreting LLMs' safety\nmechanisms. We then develop an SCAV-guided attack method that can generate both\nattack prompts and embedding-level attacks with automatically selected\nperturbation hyperparameters. Both automatic and human evaluations demonstrate\nthat our attack method significantly improves the attack success rate and\nresponse quality while requiring less training data. Additionally, we find that\nour generated attack prompts may be transferable to GPT-4, and the\nembedding-level attacks may also be transferred to other white-box LLMs whose\nparameters are known. Our experiments further uncover the safety risks present\nin current LLMs. For example, in our evaluation of seven open-source LLMs, we\nobserve an average attack success rate of 99.14%, based on the classic\nkeyword-matching criterion. Finally, we provide insights into the safety\nmechanism of LLMs. The code is available at\nhttps://github.com/SproutNan/AI-Safety_SCAV."
                },
                "authors": [
                    {
                        "name": "Zhihao Xu"
                    },
                    {
                        "name": "Ruixuan Huang"
                    },
                    {
                        "name": "Changyu Chen"
                    },
                    {
                        "name": "Xiting Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiting Wang"
                },
                "author": "Xiting Wang",
                "arxiv_comment": "10 pages, accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12038v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12038v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08188v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08188v2",
                "updated": "2024-11-30T06:09:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    30,
                    6,
                    9,
                    33,
                    5,
                    335,
                    0
                ],
                "published": "2024-08-15T14:46:13Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    14,
                    46,
                    13,
                    3,
                    228,
                    0
                ],
                "title": "Nl2Hltl2Plan: Scaling Up Natural Language Understanding for Multi-Robots\n  Through Hierarchical Temporal Logic Task Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nl2Hltl2Plan: Scaling Up Natural Language Understanding for Multi-Robots\n  Through Hierarchical Temporal Logic Task Representation"
                },
                "summary": "To enable non-experts to specify long-horizon, multi-robot collaborative\ntasks, language models are increasingly used to translate natural language\ncommands into formal specifications. However, because translation can occur in\nmultiple ways, such translations may lack accuracy or lead to inefficient\nmulti-robot planning. Our key insight is that concise hierarchical\nspecifications can simplify planning while remaining straightforward to derive\nfrom human instructions. We propose~\\acronym{}, a framework that translates\nnatural language commands into hierarchical Linear Temporal Logic (LTL) and\nsolves the corresponding planning problem. The translation involves two steps\nleveraging Large Language Models (LLMs). First, an LLM transforms instructions\ninto a Hierarchical Task Tree, capturing logical and temporal relations. Next,\na fine-tuned LLM converts sub-tasks into flat LTL formulas, which are\naggregated into hierarchical specifications, with the lowest level\ncorresponding to ordered robot actions. These specifications are then used with\noff-the-shelf planners. Our~\\acronym{} demonstrates the potential of LLMs in\nhierarchical reasoning for multi-robot task planning. Evaluations in simulation\nand real-world experiments with human participants show that~\\acronym{}\noutperforms existing methods, handling more complex instructions while\nachieving higher success rates and lower costs in task allocation and planning.\nAdditional details are available at https://nl2hltl2plan.github.io .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To enable non-experts to specify long-horizon, multi-robot collaborative\ntasks, language models are increasingly used to translate natural language\ncommands into formal specifications. However, because translation can occur in\nmultiple ways, such translations may lack accuracy or lead to inefficient\nmulti-robot planning. Our key insight is that concise hierarchical\nspecifications can simplify planning while remaining straightforward to derive\nfrom human instructions. We propose~\\acronym{}, a framework that translates\nnatural language commands into hierarchical Linear Temporal Logic (LTL) and\nsolves the corresponding planning problem. The translation involves two steps\nleveraging Large Language Models (LLMs). First, an LLM transforms instructions\ninto a Hierarchical Task Tree, capturing logical and temporal relations. Next,\na fine-tuned LLM converts sub-tasks into flat LTL formulas, which are\naggregated into hierarchical specifications, with the lowest level\ncorresponding to ordered robot actions. These specifications are then used with\noff-the-shelf planners. Our~\\acronym{} demonstrates the potential of LLMs in\nhierarchical reasoning for multi-robot task planning. Evaluations in simulation\nand real-world experiments with human participants show that~\\acronym{}\noutperforms existing methods, handling more complex instructions while\nachieving higher success rates and lower costs in task allocation and planning.\nAdditional details are available at https://nl2hltl2plan.github.io ."
                },
                "authors": [
                    {
                        "name": "Shaojun Xu"
                    },
                    {
                        "name": "Xusheng Luo"
                    },
                    {
                        "name": "Yutong Huang"
                    },
                    {
                        "name": "Letian Leng"
                    },
                    {
                        "name": "Ruixuan Liu"
                    },
                    {
                        "name": "Changliu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Changliu Liu"
                },
                "author": "Changliu Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08188v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08188v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14332v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14332v2",
                "updated": "2024-11-30T05:56:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    30,
                    5,
                    56,
                    52,
                    5,
                    335,
                    0
                ],
                "published": "2024-10-18T09:44:25Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    9,
                    44,
                    25,
                    4,
                    292,
                    0
                ],
                "title": "Croc: Pretraining Large Multimodal Models with Cross-Modal Comprehension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Croc: Pretraining Large Multimodal Models with Cross-Modal Comprehension"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have catalyzed the\ndevelopment of Large Multimodal Models (LMMs). However, existing research\nprimarily focuses on tuning language and image instructions, ignoring the\ncritical pretraining phase where models learn to process textual and visual\nmodalities jointly. In this paper, we propose a new pretraining paradigm for\nLMMs to enhance the visual comprehension capabilities of LLMs by introducing a\nnovel cross-modal comprehension stage. Specifically, we design a dynamically\nlearnable prompt token pool and employ the Hungarian algorithm to replace part\nof the original visual tokens with the most relevant prompt tokens. Then, we\nconceptualize visual tokens as analogous to a \"foreign language\" for the LLMs\nand propose a mixed attention mechanism with bidirectional visual attention and\nunidirectional textual attention to comprehensively enhance the understanding\nof visual tokens. Meanwhile, we integrate a detailed caption generation task,\nleveraging rich descriptions to further facilitate LLMs in understanding visual\nsemantic information. After pretraining on 1.5 million publicly accessible\ndata, we present a new foundation model called Croc. Experimental results\ndemonstrate that Croc achieves new state-of-the-art performance on massive\nvision-language benchmarks. To support reproducibility and facilitate further\nresearch, we release the training code and pre-trained model weights at\nhttps://github.com/deepglint/Croc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have catalyzed the\ndevelopment of Large Multimodal Models (LMMs). However, existing research\nprimarily focuses on tuning language and image instructions, ignoring the\ncritical pretraining phase where models learn to process textual and visual\nmodalities jointly. In this paper, we propose a new pretraining paradigm for\nLMMs to enhance the visual comprehension capabilities of LLMs by introducing a\nnovel cross-modal comprehension stage. Specifically, we design a dynamically\nlearnable prompt token pool and employ the Hungarian algorithm to replace part\nof the original visual tokens with the most relevant prompt tokens. Then, we\nconceptualize visual tokens as analogous to a \"foreign language\" for the LLMs\nand propose a mixed attention mechanism with bidirectional visual attention and\nunidirectional textual attention to comprehensively enhance the understanding\nof visual tokens. Meanwhile, we integrate a detailed caption generation task,\nleveraging rich descriptions to further facilitate LLMs in understanding visual\nsemantic information. After pretraining on 1.5 million publicly accessible\ndata, we present a new foundation model called Croc. Experimental results\ndemonstrate that Croc achieves new state-of-the-art performance on massive\nvision-language benchmarks. To support reproducibility and facilitate further\nresearch, we release the training code and pre-trained model weights at\nhttps://github.com/deepglint/Croc."
                },
                "authors": [
                    {
                        "name": "Yin Xie"
                    },
                    {
                        "name": "Kaicheng Yang"
                    },
                    {
                        "name": "Ninghua Yang"
                    },
                    {
                        "name": "Weimo Deng"
                    },
                    {
                        "name": "Xiangzi Dai"
                    },
                    {
                        "name": "Tiancheng Gu"
                    },
                    {
                        "name": "Yumeng Wang"
                    },
                    {
                        "name": "Xiang An"
                    },
                    {
                        "name": "Yongle Zhao"
                    },
                    {
                        "name": "Ziyong Feng"
                    },
                    {
                        "name": "Jiankang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Jiankang Deng"
                },
                "author": "Jiankang Deng",
                "arxiv_comment": "18 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14332v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14332v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06169v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06169v3",
                "updated": "2024-11-30T05:32:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    30,
                    5,
                    32,
                    51,
                    5,
                    335,
                    0
                ],
                "published": "2024-10-08T16:13:24Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    16,
                    13,
                    24,
                    1,
                    282,
                    0
                ],
                "title": "Treat Visual Tokens as Text? But Your MLLM Only Needs Fewer Efforts to\n  See",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Treat Visual Tokens as Text? But Your MLLM Only Needs Fewer Efforts to\n  See"
                },
                "summary": "By treating visual tokens from visual encoders as text tokens, Multimodal\nLarge Language Models (MLLMs) have achieved remarkable progress across diverse\nvisual understanding tasks, leveraging the robust architectures of Large\nLanguage Models (LLMs). However, as token counts grow, the quadratic scaling of\ncomputation in LLMs introduces a significant efficiency bottleneck, impeding\nfurther scalability. Although recent approaches have explored pruning visual\ntokens or employing lighter LLM architectures, the computational overhead from\nan increasing number of visual tokens remains a substantial challenge.\n  In this study, we investigate the redundancy in visual computation at both\nthe parameter and computational pattern levels within LLaVA, a representative\nMLLM, and introduce a suite of streamlined strategies to enhance efficiency.\nThese include neighbor-aware visual token attention, pruning of inactive visual\nattention heads, and selective layer dropping for visual computations. By\nimplementing these strategies in LLaVA, we achieve a reduction in computational\ndemands of 88% while maintaining model performance across key benchmarks.\nAdditionally, we validate the existence of visual computational redundancy in\nother MLLMs, such as Qwen2-VL-7B and InternVL-2.0-4B/8B/26B. These results\npresent a novel pathway for MLLMs to handle dense visual tokens with minimal\ncomputational costs. Code and model checkpoints will be released to support\nfurther research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "By treating visual tokens from visual encoders as text tokens, Multimodal\nLarge Language Models (MLLMs) have achieved remarkable progress across diverse\nvisual understanding tasks, leveraging the robust architectures of Large\nLanguage Models (LLMs). However, as token counts grow, the quadratic scaling of\ncomputation in LLMs introduces a significant efficiency bottleneck, impeding\nfurther scalability. Although recent approaches have explored pruning visual\ntokens or employing lighter LLM architectures, the computational overhead from\nan increasing number of visual tokens remains a substantial challenge.\n  In this study, we investigate the redundancy in visual computation at both\nthe parameter and computational pattern levels within LLaVA, a representative\nMLLM, and introduce a suite of streamlined strategies to enhance efficiency.\nThese include neighbor-aware visual token attention, pruning of inactive visual\nattention heads, and selective layer dropping for visual computations. By\nimplementing these strategies in LLaVA, we achieve a reduction in computational\ndemands of 88% while maintaining model performance across key benchmarks.\nAdditionally, we validate the existence of visual computational redundancy in\nother MLLMs, such as Qwen2-VL-7B and InternVL-2.0-4B/8B/26B. These results\npresent a novel pathway for MLLMs to handle dense visual tokens with minimal\ncomputational costs. Code and model checkpoints will be released to support\nfurther research."
                },
                "authors": [
                    {
                        "name": "Zeliang Zhang"
                    },
                    {
                        "name": "Phu Pham"
                    },
                    {
                        "name": "Wentian Zhao"
                    },
                    {
                        "name": "Kun Wan"
                    },
                    {
                        "name": "Yu-Jhe Li"
                    },
                    {
                        "name": "Jianing Zhou"
                    },
                    {
                        "name": "Daniel Miranda"
                    },
                    {
                        "name": "Ajinkya Kale"
                    },
                    {
                        "name": "Chenliang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chenliang Xu"
                },
                "author": "Chenliang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06169v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06169v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.18964v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.18964v3",
                "updated": "2024-11-30T02:56:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    30,
                    2,
                    56,
                    48,
                    5,
                    335,
                    0
                ],
                "published": "2023-10-29T10:07:32Z",
                "published_parsed": [
                    2023,
                    10,
                    29,
                    10,
                    7,
                    32,
                    6,
                    302,
                    0
                ],
                "title": "LLMs and Finetuning: Benchmarking cross-domain performance for hate\n  speech detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs and Finetuning: Benchmarking cross-domain performance for hate\n  speech detection"
                },
                "summary": "In the evolving landscape of online communication, hate speech detection\nremains a formidable challenge, further compounded by the diversity of digital\nplatforms. This study investigates the effectiveness and adaptability of\npre-trained and fine-tuned Large Language Models (LLMs) in identifying hate\nspeech, to address two central questions: (1) To what extent does the model\nperformance depend on the fine-tuning and training parameters?, (2) To what\nextent do models generalize to cross-domain hate speech detection? and (3) What\nare the specific features of the datasets or models that influence the\ngeneralization potential? The experiment shows that LLMs offer a huge advantage\nover the state-of-the-art even without pretraining. Ordinary least squares\nanalyses suggest that the advantage of training with fine-grained hate speech\nlabels is washed away with the increase in dataset size. We conclude with a\nvision for the future of hate speech detection, emphasizing cross-domain\ngeneralizability and appropriate benchmarking practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the evolving landscape of online communication, hate speech detection\nremains a formidable challenge, further compounded by the diversity of digital\nplatforms. This study investigates the effectiveness and adaptability of\npre-trained and fine-tuned Large Language Models (LLMs) in identifying hate\nspeech, to address two central questions: (1) To what extent does the model\nperformance depend on the fine-tuning and training parameters?, (2) To what\nextent do models generalize to cross-domain hate speech detection? and (3) What\nare the specific features of the datasets or models that influence the\ngeneralization potential? The experiment shows that LLMs offer a huge advantage\nover the state-of-the-art even without pretraining. Ordinary least squares\nanalyses suggest that the advantage of training with fine-grained hate speech\nlabels is washed away with the increase in dataset size. We conclude with a\nvision for the future of hate speech detection, emphasizing cross-domain\ngeneralizability and appropriate benchmarking practices."
                },
                "authors": [
                    {
                        "name": "Ahmad Nasir"
                    },
                    {
                        "name": "Aadish Sharma"
                    },
                    {
                        "name": "Kokil Jaidka"
                    }
                ],
                "author_detail": {
                    "name": "Kokil Jaidka"
                },
                "author": "Kokil Jaidka",
                "arxiv_comment": "10 pages, 3 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.18964v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.18964v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05285v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05285v2",
                "updated": "2024-11-30T02:55:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    30,
                    2,
                    55,
                    48,
                    5,
                    335,
                    0
                ],
                "published": "2024-11-08T02:31:03Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    31,
                    3,
                    4,
                    313,
                    0
                ],
                "title": "AgentOps: Enabling Observability of LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentOps: Enabling Observability of LLM Agents"
                },
                "summary": "Large language model (LLM) agents have demonstrated remarkable capabilities\nacross various domains, gaining extensive attention from academia and industry.\nHowever, these agents raise significant concerns on AI safety due to their\nautonomous and non-deterministic behavior, as well as continuous evolving\nnature . From a DevOps perspective, enabling observability in agents is\nnecessary to ensuring AI safety, as stakeholders can gain insights into the\nagents' inner workings, allowing them to proactively understand the agents,\ndetect anomalies, and prevent potential failures. Therefore, in this paper, we\npresent a comprehensive taxonomy of AgentOps, identifying the artifacts and\nassociated data that should be traced throughout the entire lifecycle of agents\nto achieve effective observability. The taxonomy is developed based on a\nsystematic mapping study of existing AgentOps tools. Our taxonomy serves as a\nreference template for developers to design and implement AgentOps\ninfrastructure that supports monitoring, logging, and analytics. thereby\nensuring AI safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) agents have demonstrated remarkable capabilities\nacross various domains, gaining extensive attention from academia and industry.\nHowever, these agents raise significant concerns on AI safety due to their\nautonomous and non-deterministic behavior, as well as continuous evolving\nnature . From a DevOps perspective, enabling observability in agents is\nnecessary to ensuring AI safety, as stakeholders can gain insights into the\nagents' inner workings, allowing them to proactively understand the agents,\ndetect anomalies, and prevent potential failures. Therefore, in this paper, we\npresent a comprehensive taxonomy of AgentOps, identifying the artifacts and\nassociated data that should be traced throughout the entire lifecycle of agents\nto achieve effective observability. The taxonomy is developed based on a\nsystematic mapping study of existing AgentOps tools. Our taxonomy serves as a\nreference template for developers to design and implement AgentOps\ninfrastructure that supports monitoring, logging, and analytics. thereby\nensuring AI safety."
                },
                "authors": [
                    {
                        "name": "Liming Dong"
                    },
                    {
                        "name": "Qinghua Lu"
                    },
                    {
                        "name": "Liming Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Liming Zhu"
                },
                "author": "Liming Zhu",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05285v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05285v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.7; D.2.9; D.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11579v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11579v3",
                "updated": "2024-11-30T02:29:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    30,
                    2,
                    29,
                    59,
                    5,
                    335,
                    0
                ],
                "published": "2024-09-17T22:06:46Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    22,
                    6,
                    46,
                    1,
                    261,
                    0
                ],
                "title": "HEARTS: A Holistic Framework for Explainable, Sustainable and Robust\n  Text Stereotype Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HEARTS: A Holistic Framework for Explainable, Sustainable and Robust\n  Text Stereotype Detection"
                },
                "summary": "Stereotypes are generalised assumptions about societal groups, and even\nstate-of-the-art LLMs using in-context learning struggle to identify them\naccurately. Due to the subjective nature of stereotypes, where what constitutes\na stereotype can vary widely depending on cultural, social, and individual\nperspectives, robust explainability is crucial. Explainable models ensure that\nthese nuanced judgments can be understood and validated by human users,\npromoting trust and accountability. We address these challenges by introducing\nHEARTS (Holistic Framework for Explainable, Sustainable, and Robust Text\nStereotype Detection), a framework that enhances model performance, minimises\ncarbon footprint, and provides transparent, interpretable explanations. We\nestablish the Expanded Multi-Grain Stereotype Dataset (EMGSD), comprising\n57,201 labelled texts across six groups, including under-represented\ndemographics like LGBTQ+ and regional stereotypes. Ablation studies confirm\nthat BERT models fine-tuned on EMGSD outperform those trained on individual\ncomponents. We then analyse a fine-tuned, carbon-efficient ALBERT-V2 model\nusing SHAP to generate token-level importance values, ensuring alignment with\nhuman understanding, and calculate explainability confidence scores by\ncomparing SHAP and LIME outputs...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stereotypes are generalised assumptions about societal groups, and even\nstate-of-the-art LLMs using in-context learning struggle to identify them\naccurately. Due to the subjective nature of stereotypes, where what constitutes\na stereotype can vary widely depending on cultural, social, and individual\nperspectives, robust explainability is crucial. Explainable models ensure that\nthese nuanced judgments can be understood and validated by human users,\npromoting trust and accountability. We address these challenges by introducing\nHEARTS (Holistic Framework for Explainable, Sustainable, and Robust Text\nStereotype Detection), a framework that enhances model performance, minimises\ncarbon footprint, and provides transparent, interpretable explanations. We\nestablish the Expanded Multi-Grain Stereotype Dataset (EMGSD), comprising\n57,201 labelled texts across six groups, including under-represented\ndemographics like LGBTQ+ and regional stereotypes. Ablation studies confirm\nthat BERT models fine-tuned on EMGSD outperform those trained on individual\ncomponents. We then analyse a fine-tuned, carbon-efficient ALBERT-V2 model\nusing SHAP to generate token-level importance values, ensuring alignment with\nhuman understanding, and calculate explainability confidence scores by\ncomparing SHAP and LIME outputs..."
                },
                "authors": [
                    {
                        "name": "Theo King"
                    },
                    {
                        "name": "Zekun Wu"
                    },
                    {
                        "name": "Adriano Koshiyama"
                    },
                    {
                        "name": "Emre Kazim"
                    },
                    {
                        "name": "Philip Treleaven"
                    }
                ],
                "author_detail": {
                    "name": "Philip Treleaven"
                },
                "author": "Philip Treleaven",
                "arxiv_comment": "NeurIPS 2024 SoLaR Workshop and NeurIPS 2024 Safety Gen AI Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11579v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11579v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11353v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11353v3",
                "updated": "2024-11-30T02:27:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    30,
                    2,
                    27,
                    9,
                    5,
                    335,
                    0
                ],
                "published": "2024-09-17T16:55:25Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    16,
                    55,
                    25,
                    1,
                    261,
                    0
                ],
                "title": "THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation\n  in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation\n  in Large Language Models"
                },
                "summary": "Hallucination, the generation of factually incorrect content, is a growing\nchallenge in Large Language Models (LLMs). Existing detection and mitigation\nmethods are often isolated and insufficient for domain-specific needs, lacking\na standardized pipeline. This paper introduces THaMES (Tool for Hallucination\nMitigations and EvaluationS), an integrated framework and library addressing\nthis gap. THaMES offers an end-to-end solution for evaluating and mitigating\nhallucinations in LLMs, featuring automated test set generation, multifaceted\nbenchmarking, and adaptable mitigation strategies. It automates test set\ncreation from any corpus, ensuring high data quality, diversity, and\ncost-efficiency through techniques like batch processing, weighted sampling,\nand counterfactual validation. THaMES assesses a model's ability to detect and\nreduce hallucinations across various tasks, including text generation and\nbinary classification, applying optimal mitigation strategies like In-Context\nLearning (ICL), Retrieval Augmented Generation (RAG), and Parameter-Efficient\nFine-tuning (PEFT). Evaluations of state-of-the-art LLMs using a knowledge base\nof academic papers, political news, and Wikipedia reveal that commercial models\nlike GPT-4o benefit more from RAG than ICL, while open-weight models like\nLlama-3.1-8B-Instruct and Mistral-Nemo gain more from ICL. Additionally, PEFT\nsignificantly enhances the performance of Llama-3.1-8B-Instruct in both\nevaluation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination, the generation of factually incorrect content, is a growing\nchallenge in Large Language Models (LLMs). Existing detection and mitigation\nmethods are often isolated and insufficient for domain-specific needs, lacking\na standardized pipeline. This paper introduces THaMES (Tool for Hallucination\nMitigations and EvaluationS), an integrated framework and library addressing\nthis gap. THaMES offers an end-to-end solution for evaluating and mitigating\nhallucinations in LLMs, featuring automated test set generation, multifaceted\nbenchmarking, and adaptable mitigation strategies. It automates test set\ncreation from any corpus, ensuring high data quality, diversity, and\ncost-efficiency through techniques like batch processing, weighted sampling,\nand counterfactual validation. THaMES assesses a model's ability to detect and\nreduce hallucinations across various tasks, including text generation and\nbinary classification, applying optimal mitigation strategies like In-Context\nLearning (ICL), Retrieval Augmented Generation (RAG), and Parameter-Efficient\nFine-tuning (PEFT). Evaluations of state-of-the-art LLMs using a knowledge base\nof academic papers, political news, and Wikipedia reveal that commercial models\nlike GPT-4o benefit more from RAG than ICL, while open-weight models like\nLlama-3.1-8B-Instruct and Mistral-Nemo gain more from ICL. Additionally, PEFT\nsignificantly enhances the performance of Llama-3.1-8B-Instruct in both\nevaluation tasks."
                },
                "authors": [
                    {
                        "name": "Mengfei Liang"
                    },
                    {
                        "name": "Archish Arun"
                    },
                    {
                        "name": "Zekun Wu"
                    },
                    {
                        "name": "Cristian Munoz"
                    },
                    {
                        "name": "Jonathan Lutch"
                    },
                    {
                        "name": "Emre Kazim"
                    },
                    {
                        "name": "Adriano Koshiyama"
                    },
                    {
                        "name": "Philip Treleaven"
                    }
                ],
                "author_detail": {
                    "name": "Philip Treleaven"
                },
                "author": "Philip Treleaven",
                "arxiv_comment": "NeurIPS 2024 SoLaR (Socially Responsible Language Modelling Research\n  ) Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11353v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11353v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00024v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00024v3",
                "updated": "2024-11-30T00:17:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    30,
                    0,
                    17,
                    1,
                    5,
                    335,
                    0
                ],
                "published": "2024-10-28T22:30:06Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    22,
                    30,
                    6,
                    0,
                    302,
                    0
                ],
                "title": "A Perspective for Adapting Generalist AI to Specialized Medical AI\n  Applications and Their Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Perspective for Adapting Generalist AI to Specialized Medical AI\n  Applications and Their Challenges"
                },
                "summary": "The integration of Large Language Models (LLMs) into medical applications has\nsparked widespread interest across the healthcare industry, from drug discovery\nand development to clinical decision support, assisting telemedicine, medical\ndevices, and healthcare insurance applications. This perspective paper aims to\ndiscuss the inner workings of building LLM-powered medical AI applications and\nintroduces a comprehensive framework for their development. We review existing\nliterature and outline the unique challenges of applying LLMs in specialized\nmedical contexts. Additionally, we introduce a three-step framework to organize\nmedical LLM research activities: 1) Modeling: breaking down complex medical\nworkflows into manageable steps for developing medical-specific models; 2)\nOptimization: optimizing the model performance with crafted prompts and\nintegrating external knowledge and tools, and 3) System engineering:\ndecomposing complex tasks into subtasks and leveraging human expertise for\nbuilding medical AI applications. Furthermore, we offer a detailed use case\nplaybook that describes various LLM-powered medical AI applications, such as\noptimizing clinical trial design, enhancing clinical decision support, and\nadvancing medical imaging analysis. Finally, we discuss various challenges and\nconsiderations for building medical AI applications with LLMs, such as handling\nhallucination issues, data ownership and compliance, privacy, intellectual\nproperty considerations, compute cost, sustainability issues, and responsible\nAI requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into medical applications has\nsparked widespread interest across the healthcare industry, from drug discovery\nand development to clinical decision support, assisting telemedicine, medical\ndevices, and healthcare insurance applications. This perspective paper aims to\ndiscuss the inner workings of building LLM-powered medical AI applications and\nintroduces a comprehensive framework for their development. We review existing\nliterature and outline the unique challenges of applying LLMs in specialized\nmedical contexts. Additionally, we introduce a three-step framework to organize\nmedical LLM research activities: 1) Modeling: breaking down complex medical\nworkflows into manageable steps for developing medical-specific models; 2)\nOptimization: optimizing the model performance with crafted prompts and\nintegrating external knowledge and tools, and 3) System engineering:\ndecomposing complex tasks into subtasks and leveraging human expertise for\nbuilding medical AI applications. Furthermore, we offer a detailed use case\nplaybook that describes various LLM-powered medical AI applications, such as\noptimizing clinical trial design, enhancing clinical decision support, and\nadvancing medical imaging analysis. Finally, we discuss various challenges and\nconsiderations for building medical AI applications with LLMs, such as handling\nhallucination issues, data ownership and compliance, privacy, intellectual\nproperty considerations, compute cost, sustainability issues, and responsible\nAI requirements."
                },
                "authors": [
                    {
                        "name": "Zifeng Wang"
                    },
                    {
                        "name": "Hanyin Wang"
                    },
                    {
                        "name": "Benjamin Danek"
                    },
                    {
                        "name": "Ying Li"
                    },
                    {
                        "name": "Christina Mack"
                    },
                    {
                        "name": "Hoifung Poon"
                    },
                    {
                        "name": "Yajuan Wang"
                    },
                    {
                        "name": "Pranav Rajpurkar"
                    },
                    {
                        "name": "Jimeng Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jimeng Sun"
                },
                "author": "Jimeng Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00024v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00024v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06874v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06874v3",
                "updated": "2024-11-29T23:41:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    23,
                    41,
                    57,
                    4,
                    334,
                    0
                ],
                "published": "2024-06-11T01:20:53Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    1,
                    20,
                    53,
                    1,
                    163,
                    0
                ],
                "title": "Learning Reward and Policy Jointly from Demonstration and Preference\n  Improves Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Reward and Policy Jointly from Demonstration and Preference\n  Improves Alignment"
                },
                "summary": "Aligning human preference and value is an important requirement for building\ncontemporary foundation models and embodied AI. However, popular approaches\nsuch as reinforcement learning with human feedback (RLHF) break down the task\ninto successive stages, such as supervised fine-tuning (SFT), reward modeling\n(RM), and reinforcement learning (RL), each performing one specific learning\ntask. Such a sequential approach results in serious issues such as significant\nunder-utilization of data and distribution mismatch between the learned reward\nmodel and generated policy, which eventually lead to poor alignment\nperformance. We develop a single stage approach named Alignment with Integrated\nHuman Feedback (AIHF), capable of integrating both human preference and\ndemonstration to train reward models and the policy. The proposed approach\nadmits a suite of efficient algorithms, which can easily reduce to, and\nleverage, popular alignment algorithms such as RLHF and Directly Policy\nOptimization (DPO), and only requires minor changes to the existing alignment\npipelines. We demonstrate the efficiency of the proposed solutions with\nextensive experiments involving alignment problems in LLMs and robotic control\nproblems in MuJoCo. We observe that the proposed solutions outperform the\nexisting alignment algorithms such as RLHF and DPO by large margins, especially\nwhen the amount of high-quality preference data is relatively limited.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning human preference and value is an important requirement for building\ncontemporary foundation models and embodied AI. However, popular approaches\nsuch as reinforcement learning with human feedback (RLHF) break down the task\ninto successive stages, such as supervised fine-tuning (SFT), reward modeling\n(RM), and reinforcement learning (RL), each performing one specific learning\ntask. Such a sequential approach results in serious issues such as significant\nunder-utilization of data and distribution mismatch between the learned reward\nmodel and generated policy, which eventually lead to poor alignment\nperformance. We develop a single stage approach named Alignment with Integrated\nHuman Feedback (AIHF), capable of integrating both human preference and\ndemonstration to train reward models and the policy. The proposed approach\nadmits a suite of efficient algorithms, which can easily reduce to, and\nleverage, popular alignment algorithms such as RLHF and Directly Policy\nOptimization (DPO), and only requires minor changes to the existing alignment\npipelines. We demonstrate the efficiency of the proposed solutions with\nextensive experiments involving alignment problems in LLMs and robotic control\nproblems in MuJoCo. We observe that the proposed solutions outperform the\nexisting alignment algorithms such as RLHF and DPO by large margins, especially\nwhen the amount of high-quality preference data is relatively limited."
                },
                "authors": [
                    {
                        "name": "Chenliang Li"
                    },
                    {
                        "name": "Siliang Zeng"
                    },
                    {
                        "name": "Zeyi Liao"
                    },
                    {
                        "name": "Jiaxiang Li"
                    },
                    {
                        "name": "Dongyeop Kang"
                    },
                    {
                        "name": "Alfredo Garcia"
                    },
                    {
                        "name": "Mingyi Hong"
                    }
                ],
                "author_detail": {
                    "name": "Mingyi Hong"
                },
                "author": "Mingyi Hong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06874v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06874v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17255v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17255v2",
                "updated": "2024-11-29T23:23:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    23,
                    23,
                    36,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-26T09:31:28Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    9,
                    31,
                    28,
                    1,
                    331,
                    0
                ],
                "title": "APT: Architectural Planning and Text-to-Blueprint Construction Using\n  Large Language Models for Open-World Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APT: Architectural Planning and Text-to-Blueprint Construction Using\n  Large Language Models for Open-World Agents"
                },
                "summary": "We present APT, an advanced Large Language Model (LLM)-driven framework that\nenables autonomous agents to construct complex and creative structures within\nthe Minecraft environment. Unlike previous approaches that primarily\nconcentrate on skill-based open-world tasks or rely on image-based diffusion\nmodels for generating voxel-based structures, our method leverages the\nintrinsic spatial reasoning capabilities of LLMs. By employing chain-of-thought\ndecomposition along with multimodal inputs, the framework generates detailed\narchitectural layouts and blueprints that the agent can execute under zero-shot\nor few-shot learning scenarios. Our agent incorporates both memory and\nreflection modules to facilitate lifelong learning, adaptive refinement, and\nerror correction throughout the building process. To rigorously evaluate the\nagent's performance in this emerging research area, we introduce a\ncomprehensive benchmark consisting of diverse construction tasks designed to\ntest creativity, spatial reasoning, adherence to in-game rules, and the\neffective integration of multimodal instructions. Experimental results using\nvarious GPT-based LLM backends and agent configurations demonstrate the agent's\ncapacity to accurately interpret extensive instructions involving numerous\nitems, their positions, and orientations. The agent successfully produces\ncomplex structures complete with internal functionalities such as\nRedstone-powered systems. A/B testing indicates that the inclusion of a memory\nmodule leads to a significant increase in performance, emphasizing its role in\nenabling continuous learning and the reuse of accumulated experience.\nAdditionally, the agent's unexpected emergence of scaffolding behavior\nhighlights the potential of future LLM-driven agents to utilize subroutine\nplanning and leverage the emergence ability of LLMs to autonomously develop\nhuman-like problem-solving techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present APT, an advanced Large Language Model (LLM)-driven framework that\nenables autonomous agents to construct complex and creative structures within\nthe Minecraft environment. Unlike previous approaches that primarily\nconcentrate on skill-based open-world tasks or rely on image-based diffusion\nmodels for generating voxel-based structures, our method leverages the\nintrinsic spatial reasoning capabilities of LLMs. By employing chain-of-thought\ndecomposition along with multimodal inputs, the framework generates detailed\narchitectural layouts and blueprints that the agent can execute under zero-shot\nor few-shot learning scenarios. Our agent incorporates both memory and\nreflection modules to facilitate lifelong learning, adaptive refinement, and\nerror correction throughout the building process. To rigorously evaluate the\nagent's performance in this emerging research area, we introduce a\ncomprehensive benchmark consisting of diverse construction tasks designed to\ntest creativity, spatial reasoning, adherence to in-game rules, and the\neffective integration of multimodal instructions. Experimental results using\nvarious GPT-based LLM backends and agent configurations demonstrate the agent's\ncapacity to accurately interpret extensive instructions involving numerous\nitems, their positions, and orientations. The agent successfully produces\ncomplex structures complete with internal functionalities such as\nRedstone-powered systems. A/B testing indicates that the inclusion of a memory\nmodule leads to a significant increase in performance, emphasizing its role in\nenabling continuous learning and the reuse of accumulated experience.\nAdditionally, the agent's unexpected emergence of scaffolding behavior\nhighlights the potential of future LLM-driven agents to utilize subroutine\nplanning and leverage the emergence ability of LLMs to autonomously develop\nhuman-like problem-solving techniques."
                },
                "authors": [
                    {
                        "name": "Jun Yu Chen"
                    },
                    {
                        "name": "Tao Gao"
                    }
                ],
                "author_detail": {
                    "name": "Tao Gao"
                },
                "author": "Tao Gao",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17255v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17255v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.07066v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.07066v5",
                "updated": "2024-11-29T21:50:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    21,
                    50,
                    16,
                    4,
                    334,
                    0
                ],
                "published": "2024-04-10T14:56:40Z",
                "published_parsed": [
                    2024,
                    4,
                    10,
                    14,
                    56,
                    40,
                    2,
                    101,
                    0
                ],
                "title": "Exploring Concept Depth: How Large Language Models Acquire Knowledge at\n  Different Layers?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Concept Depth: How Large Language Models Acquire Knowledge at\n  Different Layers?"
                },
                "summary": "Large language models (LLMs) have shown remarkable performances across a wide\nrange of tasks. However, the mechanisms by which these models encode tasks of\nvarying complexities remain poorly understood. In this paper, we explore the\nhypothesis that LLMs process concepts of varying complexities in different\nlayers, introducing the idea of \"Concept Depth\" to suggest that more complex\nconcepts are typically acquired in deeper layers. Specifically, we categorize\nconcepts based on their level of abstraction, defining them in the order of\nincreasing complexity within factual, emotional, and inferential tasks. We\nconduct extensive probing experiments using layer-wise representations across\nvarious LLM families (Gemma, LLaMA, Qwen) on various datasets spanning the\nthree domains of tasks. Our findings reveal that models could efficiently\nconduct probing for simpler tasks in shallow layers, and more complex tasks\ntypically necessitate deeper layers for accurate understanding. Additionally,\nwe examine how external factors, such as adding noise to the input and\nquantizing the model weights, might affect layer-wise representations. Our\nfindings suggest that these factors can impede the development of a conceptual\nunderstanding of LLMs until deeper layers are explored. We hope that our\nproposed concept and experimental insights will enhance the understanding of\nthe mechanisms underlying LLMs. Our codes are available at\n\\url{https://github.com/Luckfort/CD}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable performances across a wide\nrange of tasks. However, the mechanisms by which these models encode tasks of\nvarying complexities remain poorly understood. In this paper, we explore the\nhypothesis that LLMs process concepts of varying complexities in different\nlayers, introducing the idea of \"Concept Depth\" to suggest that more complex\nconcepts are typically acquired in deeper layers. Specifically, we categorize\nconcepts based on their level of abstraction, defining them in the order of\nincreasing complexity within factual, emotional, and inferential tasks. We\nconduct extensive probing experiments using layer-wise representations across\nvarious LLM families (Gemma, LLaMA, Qwen) on various datasets spanning the\nthree domains of tasks. Our findings reveal that models could efficiently\nconduct probing for simpler tasks in shallow layers, and more complex tasks\ntypically necessitate deeper layers for accurate understanding. Additionally,\nwe examine how external factors, such as adding noise to the input and\nquantizing the model weights, might affect layer-wise representations. Our\nfindings suggest that these factors can impede the development of a conceptual\nunderstanding of LLMs until deeper layers are explored. We hope that our\nproposed concept and experimental insights will enhance the understanding of\nthe mechanisms underlying LLMs. Our codes are available at\n\\url{https://github.com/Luckfort/CD}."
                },
                "authors": [
                    {
                        "name": "Mingyu Jin"
                    },
                    {
                        "name": "Qinkai Yu"
                    },
                    {
                        "name": "Jingyuan Huang"
                    },
                    {
                        "name": "Qingcheng Zeng"
                    },
                    {
                        "name": "Zhenting Wang"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Haiyan Zhao"
                    },
                    {
                        "name": "Kai Mei"
                    },
                    {
                        "name": "Yanda Meng"
                    },
                    {
                        "name": "Kaize Ding"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Mengnan Du"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "arxiv_comment": "COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.07066v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.07066v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05914v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05914v2",
                "updated": "2024-11-29T19:25:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    19,
                    25,
                    16,
                    4,
                    334,
                    0
                ],
                "published": "2024-06-09T20:56:38Z",
                "published_parsed": [
                    2024,
                    6,
                    9,
                    20,
                    56,
                    38,
                    6,
                    161,
                    0
                ],
                "title": "Soundscape Captioning using Sound Affective Quality Network and Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soundscape Captioning using Sound Affective Quality Network and Large\n  Language Model"
                },
                "summary": "We live in a rich and varied acoustic world, which is experienced by\nindividuals or communities as a soundscape. Computational auditory scene\nanalysis, disentangling acoustic scenes by detecting and classifying events,\nfocuses on objective attributes of sounds, such as their category and temporal\ncharacteristics, ignoring their effects on people, such as the emotions they\nevoke within a context. To fill this gap, we propose the soundscape captioning\ntask, which enables automated soundscape analysis, thus avoiding\nlabour-intensive subjective ratings and surveys in conventional methods. With\nsoundscape captioning, context-aware descriptions are generated for soundscape\nby capturing the acoustic scene, event information, and the corresponding human\naffective qualities (AQs). To this end, we propose an automatic soundscape\ncaptioner (SoundSCaper) system composed of an acoustic model, i.e. SoundAQnet,\nand a large language model (LLM). SoundAQnet simultaneously models multi-scale\ninformation about acoustic scenes, events, and perceived AQs, while the LLM\ndescribes the soundscape with captions by parsing the information captured with\nSoundAQnet. The soundscape caption's quality is assessed by a jury of 16\naudio/soundscape experts. The average score (out of 5) of SoundSCaper-generated\ncaptions is lower than the score of captions generated by two soundscape\nexperts by 0.21 and 0.25, respectively, on the evaluation set and the\nmodel-unknown mixed external dataset with varying lengths and acoustic\nproperties, but the differences are not statistically significant. Overall, the\nproposed SoundSCaper shows promising performance, with captions generated being\ncomparable to those annotated by soundscape experts. The code of models, LLM\nscripts, human assessment data and instructions, and expert evaluation\nstatistics are all publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We live in a rich and varied acoustic world, which is experienced by\nindividuals or communities as a soundscape. Computational auditory scene\nanalysis, disentangling acoustic scenes by detecting and classifying events,\nfocuses on objective attributes of sounds, such as their category and temporal\ncharacteristics, ignoring their effects on people, such as the emotions they\nevoke within a context. To fill this gap, we propose the soundscape captioning\ntask, which enables automated soundscape analysis, thus avoiding\nlabour-intensive subjective ratings and surveys in conventional methods. With\nsoundscape captioning, context-aware descriptions are generated for soundscape\nby capturing the acoustic scene, event information, and the corresponding human\naffective qualities (AQs). To this end, we propose an automatic soundscape\ncaptioner (SoundSCaper) system composed of an acoustic model, i.e. SoundAQnet,\nand a large language model (LLM). SoundAQnet simultaneously models multi-scale\ninformation about acoustic scenes, events, and perceived AQs, while the LLM\ndescribes the soundscape with captions by parsing the information captured with\nSoundAQnet. The soundscape caption's quality is assessed by a jury of 16\naudio/soundscape experts. The average score (out of 5) of SoundSCaper-generated\ncaptions is lower than the score of captions generated by two soundscape\nexperts by 0.21 and 0.25, respectively, on the evaluation set and the\nmodel-unknown mixed external dataset with varying lengths and acoustic\nproperties, but the differences are not statistically significant. Overall, the\nproposed SoundSCaper shows promising performance, with captions generated being\ncomparable to those annotated by soundscape experts. The code of models, LLM\nscripts, human assessment data and instructions, and expert evaluation\nstatistics are all publicly available."
                },
                "authors": [
                    {
                        "name": "Yuanbo Hou"
                    },
                    {
                        "name": "Qiaoqiao Ren"
                    },
                    {
                        "name": "Andrew Mitchell"
                    },
                    {
                        "name": "Wenwu Wang"
                    },
                    {
                        "name": "Jian Kang"
                    },
                    {
                        "name": "Tony Belpaeme"
                    },
                    {
                        "name": "Dick Botteldooren"
                    }
                ],
                "author_detail": {
                    "name": "Dick Botteldooren"
                },
                "author": "Dick Botteldooren",
                "arxiv_comment": "Code: https://github.com/Yuanbo2020/SoundSCaper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05914v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05914v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12049v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12049v2",
                "updated": "2024-11-29T19:00:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    19,
                    0,
                    58,
                    4,
                    334,
                    0
                ],
                "published": "2024-10-15T20:37:34Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    20,
                    37,
                    34,
                    1,
                    289,
                    0
                ],
                "title": "SabiÃ¡-3 Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SabiÃ¡-3 Technical Report"
                },
                "summary": "This report presents Sabi\\'a-3, our new flagship language model, and\nSabiazinho-3, a more cost-effective sibling. The models were trained on a large\nbrazilian-centric corpus. Evaluations across diverse professional and academic\nbenchmarks show a strong performance on Portuguese and Brazil-related tasks.\nSabi\\'a-3 shows large improvements in comparison to our previous best of model,\nSabia-2 Medium, especially in reasoning-intensive tasks. Notably, Sabi\\'a-3's\naverage performance matches frontier LLMs, while it is offered at a three to\nfour times lower cost per token, reinforcing the benefits of domain\nspecialization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report presents Sabi\\'a-3, our new flagship language model, and\nSabiazinho-3, a more cost-effective sibling. The models were trained on a large\nbrazilian-centric corpus. Evaluations across diverse professional and academic\nbenchmarks show a strong performance on Portuguese and Brazil-related tasks.\nSabi\\'a-3 shows large improvements in comparison to our previous best of model,\nSabia-2 Medium, especially in reasoning-intensive tasks. Notably, Sabi\\'a-3's\naverage performance matches frontier LLMs, while it is offered at a three to\nfour times lower cost per token, reinforcing the benefits of domain\nspecialization."
                },
                "authors": [
                    {
                        "name": "Hugo Abonizio"
                    },
                    {
                        "name": "Thales Sales Almeida"
                    },
                    {
                        "name": "Thiago Laitz"
                    },
                    {
                        "name": "Roseval Malaquias Junior"
                    },
                    {
                        "name": "Giovana Kerche BonÃ¡s"
                    },
                    {
                        "name": "Rodrigo Nogueira"
                    },
                    {
                        "name": "Ramon Pires"
                    }
                ],
                "author_detail": {
                    "name": "Ramon Pires"
                },
                "author": "Ramon Pires",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12049v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12049v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19921v1",
                "updated": "2024-11-29T18:36:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    18,
                    36,
                    15,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T18:36:15Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    18,
                    36,
                    15,
                    4,
                    334,
                    0
                ],
                "title": "SIMS: Simulating Human-Scene Interactions with Real World Script\n  Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SIMS: Simulating Human-Scene Interactions with Real World Script\n  Planning"
                },
                "summary": "Simulating long-term human-scene interaction is a challenging yet fascinating\ntask. Previous works have not effectively addressed the generation of long-term\nhuman scene interactions with detailed narratives for physics-based animation.\nThis paper introduces a novel framework for the planning and controlling of\nlong-horizon physical plausible human-scene interaction. On the one hand, films\nand shows with stylish human locomotions or interactions with scenes are\nabundantly available on the internet, providing a rich source of data for\nscript planning. On the other hand, Large Language Models (LLMs) can understand\nand generate logical storylines.\n  This motivates us to marry the two by using an LLM-based pipeline to extract\nscripts from videos, and then employ LLMs to imitate and create new scripts,\ncapturing complex, time-series human behaviors and interactions with\nenvironments. By leveraging this, we utilize a dual-aware policy that achieves\nboth language comprehension and scene understanding to guide character motions\nwithin contextual and spatial constraints. To facilitate training and\nevaluation, we contribute a comprehensive planning dataset containing diverse\nmotion sequences extracted from real-world videos and expand them with large\nlanguage models. We also collect and re-annotate motion clips from existing\nkinematic datasets to enable our policy learn diverse skills. Extensive\nexperiments demonstrate the effectiveness of our framework in versatile task\nexecution and its generalization ability to various scenarios, showing\nremarkably enhanced performance compared with existing methods. Our code and\ndata will be publicly available soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating long-term human-scene interaction is a challenging yet fascinating\ntask. Previous works have not effectively addressed the generation of long-term\nhuman scene interactions with detailed narratives for physics-based animation.\nThis paper introduces a novel framework for the planning and controlling of\nlong-horizon physical plausible human-scene interaction. On the one hand, films\nand shows with stylish human locomotions or interactions with scenes are\nabundantly available on the internet, providing a rich source of data for\nscript planning. On the other hand, Large Language Models (LLMs) can understand\nand generate logical storylines.\n  This motivates us to marry the two by using an LLM-based pipeline to extract\nscripts from videos, and then employ LLMs to imitate and create new scripts,\ncapturing complex, time-series human behaviors and interactions with\nenvironments. By leveraging this, we utilize a dual-aware policy that achieves\nboth language comprehension and scene understanding to guide character motions\nwithin contextual and spatial constraints. To facilitate training and\nevaluation, we contribute a comprehensive planning dataset containing diverse\nmotion sequences extracted from real-world videos and expand them with large\nlanguage models. We also collect and re-annotate motion clips from existing\nkinematic datasets to enable our policy learn diverse skills. Extensive\nexperiments demonstrate the effectiveness of our framework in versatile task\nexecution and its generalization ability to various scenarios, showing\nremarkably enhanced performance compared with existing methods. Our code and\ndata will be publicly available soon."
                },
                "authors": [
                    {
                        "name": "Wenjia Wang"
                    },
                    {
                        "name": "Liang Pan"
                    },
                    {
                        "name": "Zhiyang Dou"
                    },
                    {
                        "name": "Zhouyingcheng Liao"
                    },
                    {
                        "name": "Yuke Lou"
                    },
                    {
                        "name": "Lei Yang"
                    },
                    {
                        "name": "Jingbo Wang"
                    },
                    {
                        "name": "Taku Komura"
                    }
                ],
                "author_detail": {
                    "name": "Taku Komura"
                },
                "author": "Taku Komura",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19886v1",
                "updated": "2024-11-29T17:52:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    17,
                    52,
                    39,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T17:52:39Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    17,
                    52,
                    39,
                    4,
                    334,
                    0
                ],
                "title": "PDDLFuse: A Tool for Generating Diverse Planning Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PDDLFuse: A Tool for Generating Diverse Planning Domains"
                },
                "summary": "Various real-world challenges require planning algorithms that can adapt to a\nbroad range of domains. Traditionally, the creation of planning domains has\nrelied heavily on human implementation, which limits the scale and diversity of\navailable domains. While recent advancements have leveraged generative AI\ntechnologies such as large language models (LLMs) for domain creation, these\nefforts have predominantly focused on translating existing domains from natural\nlanguage descriptions rather than generating novel ones. In contrast, the\nconcept of domain randomization, which has been highly effective in\nreinforcement learning, enhances performance and generalizability by training\non a diverse array of randomized new domains. Inspired by this success, our\ntool, PDDLFuse, aims to bridge this gap in Planning Domain Definition Language\n(PDDL). PDDLFuse is designed to generate new, diverse planning domains that can\nbe used to validate new planners or test foundational planning models. We have\ndeveloped methods to adjust the domain generators parameters to modulate the\ndifficulty of the domains it generates. This adaptability is crucial as\nexisting domain-independent planners often struggle with more complex problems.\nInitial tests indicate that PDDLFuse efficiently creates intricate and varied\ndomains, representing a significant advancement over traditional domain\ngeneration methods and making a contribution towards planning research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Various real-world challenges require planning algorithms that can adapt to a\nbroad range of domains. Traditionally, the creation of planning domains has\nrelied heavily on human implementation, which limits the scale and diversity of\navailable domains. While recent advancements have leveraged generative AI\ntechnologies such as large language models (LLMs) for domain creation, these\nefforts have predominantly focused on translating existing domains from natural\nlanguage descriptions rather than generating novel ones. In contrast, the\nconcept of domain randomization, which has been highly effective in\nreinforcement learning, enhances performance and generalizability by training\non a diverse array of randomized new domains. Inspired by this success, our\ntool, PDDLFuse, aims to bridge this gap in Planning Domain Definition Language\n(PDDL). PDDLFuse is designed to generate new, diverse planning domains that can\nbe used to validate new planners or test foundational planning models. We have\ndeveloped methods to adjust the domain generators parameters to modulate the\ndifficulty of the domains it generates. This adaptability is crucial as\nexisting domain-independent planners often struggle with more complex problems.\nInitial tests indicate that PDDLFuse efficiently creates intricate and varied\ndomains, representing a significant advancement over traditional domain\ngeneration methods and making a contribution towards planning research."
                },
                "authors": [
                    {
                        "name": "Vedant Khandelwal"
                    },
                    {
                        "name": "Amit Sheth"
                    },
                    {
                        "name": "Forest Agostinelli"
                    }
                ],
                "author_detail": {
                    "name": "Forest Agostinelli"
                },
                "author": "Forest Agostinelli",
                "arxiv_comment": "218 Tables, 3 Figures, 4 Algorithms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19869v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19869v1",
                "updated": "2024-11-29T17:31:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    17,
                    31,
                    42,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T17:31:42Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    17,
                    31,
                    42,
                    4,
                    334,
                    0
                ],
                "title": "AIDetx: a compression-based method for identification of\n  machine-learning generated text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIDetx: a compression-based method for identification of\n  machine-learning generated text"
                },
                "summary": "This paper introduces AIDetx, a novel method for detecting machine-generated\ntext using data compression techniques. Traditional approaches, such as deep\nlearning classifiers, often suffer from high computational costs and limited\ninterpretability. To address these limitations, we propose a compression-based\nclassification framework that leverages finite-context models (FCMs). AIDetx\nconstructs distinct compression models for human-written and AI-generated text,\nclassifying new inputs based on which model achieves a higher compression\nratio. We evaluated AIDetx on two benchmark datasets, achieving F1 scores\nexceeding 97% and 99%, respectively, highlighting its high accuracy. Compared\nto current methods, such as large language models (LLMs), AIDetx offers a more\ninterpretable and computationally efficient solution, significantly reducing\nboth training time and hardware requirements (e.g., no GPUs needed). The full\nimplementation is publicly available at https://github.com/AIDetx/AIDetx.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces AIDetx, a novel method for detecting machine-generated\ntext using data compression techniques. Traditional approaches, such as deep\nlearning classifiers, often suffer from high computational costs and limited\ninterpretability. To address these limitations, we propose a compression-based\nclassification framework that leverages finite-context models (FCMs). AIDetx\nconstructs distinct compression models for human-written and AI-generated text,\nclassifying new inputs based on which model achieves a higher compression\nratio. We evaluated AIDetx on two benchmark datasets, achieving F1 scores\nexceeding 97% and 99%, respectively, highlighting its high accuracy. Compared\nto current methods, such as large language models (LLMs), AIDetx offers a more\ninterpretable and computationally efficient solution, significantly reducing\nboth training time and hardware requirements (e.g., no GPUs needed). The full\nimplementation is publicly available at https://github.com/AIDetx/AIDetx."
                },
                "authors": [
                    {
                        "name": "Leonardo Almeida"
                    },
                    {
                        "name": "Pedro Rodrigues"
                    },
                    {
                        "name": "Diogo MagalhÃ£es"
                    },
                    {
                        "name": "Armando J. Pinho"
                    },
                    {
                        "name": "Diogo Pratas"
                    }
                ],
                "author_detail": {
                    "name": "Diogo Pratas"
                },
                "author": "Diogo Pratas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19869v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19869v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19865v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19865v1",
                "updated": "2024-11-29T17:27:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    17,
                    27,
                    5,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T17:27:05Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    17,
                    27,
                    5,
                    4,
                    334,
                    0
                ],
                "title": "Reverse Thinking Makes LLMs Stronger Reasoners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reverse Thinking Makes LLMs Stronger Reasoners"
                },
                "summary": "Reverse thinking plays a crucial role in human reasoning. Humans can reason\nnot only from a problem to a solution but also in reverse, i.e., start from the\nsolution and reason towards the problem. This often enhances overall reasoning\nperformance as it enables consistency checks between their forward and backward\nthinking. To enable Large Language Models (LLMs) to perform reverse thinking,\nwe introduce Reverse-Enhanced Thinking (RevThink), a framework composed of data\naugmentation and learning objectives. In RevThink, we augment the dataset by\ncollecting structured forward-backward reasoning from a teacher model,\nconsisting of: (1) the original question, (2) forward reasoning, (3) backward\nquestion, and (4) backward reasoning. We then employ three objectives to train\na smaller student model in a multi-task learning fashion: (a) generate forward\nreasoning from a question, (b) generate a backward question from a question,\nand (c) generate backward reasoning from the backward question. Experiments\nacross 12 datasets covering commonsense, math, and logical reasoning show an\naverage 13.53% improvement over the student model's zero-shot performance and a\n6.84% improvement over the strongest knowledge distillation baselines.\nMoreover, our method demonstrates sample efficiency -- using only 10% of the\ncorrect forward reasoning from the training data, it outperforms a standard\nfine-tuning method trained on 10x more forward reasoning. RevThink also\nexhibits strong generalization to out-of-distribution held-out datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reverse thinking plays a crucial role in human reasoning. Humans can reason\nnot only from a problem to a solution but also in reverse, i.e., start from the\nsolution and reason towards the problem. This often enhances overall reasoning\nperformance as it enables consistency checks between their forward and backward\nthinking. To enable Large Language Models (LLMs) to perform reverse thinking,\nwe introduce Reverse-Enhanced Thinking (RevThink), a framework composed of data\naugmentation and learning objectives. In RevThink, we augment the dataset by\ncollecting structured forward-backward reasoning from a teacher model,\nconsisting of: (1) the original question, (2) forward reasoning, (3) backward\nquestion, and (4) backward reasoning. We then employ three objectives to train\na smaller student model in a multi-task learning fashion: (a) generate forward\nreasoning from a question, (b) generate a backward question from a question,\nand (c) generate backward reasoning from the backward question. Experiments\nacross 12 datasets covering commonsense, math, and logical reasoning show an\naverage 13.53% improvement over the student model's zero-shot performance and a\n6.84% improvement over the strongest knowledge distillation baselines.\nMoreover, our method demonstrates sample efficiency -- using only 10% of the\ncorrect forward reasoning from the training data, it outperforms a standard\nfine-tuning method trained on 10x more forward reasoning. RevThink also\nexhibits strong generalization to out-of-distribution held-out datasets."
                },
                "authors": [
                    {
                        "name": "Justin Chih-Yao Chen"
                    },
                    {
                        "name": "Zifeng Wang"
                    },
                    {
                        "name": "Hamid Palangi"
                    },
                    {
                        "name": "Rujun Han"
                    },
                    {
                        "name": "Sayna Ebrahimi"
                    },
                    {
                        "name": "Long Le"
                    },
                    {
                        "name": "Vincent Perot"
                    },
                    {
                        "name": "Swaroop Mishra"
                    },
                    {
                        "name": "Mohit Bansal"
                    },
                    {
                        "name": "Chen-Yu Lee"
                    },
                    {
                        "name": "Tomas Pfister"
                    }
                ],
                "author_detail": {
                    "name": "Tomas Pfister"
                },
                "author": "Tomas Pfister",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19865v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]